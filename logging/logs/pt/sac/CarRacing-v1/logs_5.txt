Model: <class 'src.models.pytorch.agents.sac.SACAgent'>, Env: CarRacing-v1, Date: 16/05/2020 17:51:16
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-51-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: 2242ee7a5e827e07369e628113071cf5389fe2ba
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 10
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.995
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   env_name = CarRacing-v1
   rank = 0
   size = 17
   split = 17
   model = sac
   framework = pt
   train_prop = 1.0
   tcp_ports = [9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 500000
   render = False
   trial = False
   icm = False
   rs = False,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f6d9814dc50> 
	env = <GymEnv<CarRacing<CarRacing-v1>>> 
		env = <CarRacing<CarRacing-v1>> 
			channel = <mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel object at 0x7f6d9814d810>
			scale_sim = <function CarRacing.__init__.<locals>.<lambda> at 0x7f6d9810ea70>
			env = <UnityToGymWrapper instance> 
				visual_obs = None
				game_over = False
				name = CarBehavior?team=0
				group_spec = BehaviorSpec(observation_shapes=[(30,)], action_type=<ActionType.CONTINUOUS: 1>, action_shape=3)
				use_visual = False
				uint8_visual = False
			cost_model = <src.envs.CarRacing.objective.cost.CostModel object at 0x7f6d9811b850> 
				track = <src.envs.CarRacing.objective.track.Track object at 0x7f6d9811b290> 
					track = <list len=500>
					X = (1.540585208684206, 1.5814536064863205, 1.6016383588314056, 1.6350171357393264, 1.6559478223323822, 1.6717498254776002, 1.709812204837799, 1.7354034245014192, 1.7725858569145203, 1.8077154874801635, 1.958074402809143, 2.0178433418273927, 2.1851138830184937, 2.258661150932312, 2.3439700841903686, 2.452700424194336, 2.586679172515869, 2.782884216308594, 3.047244071960449, 3.4783129692077637, 3.9734771251678467, 4.596014499664307, 5.29957389831543, 6.05716609954834, 6.824328422546387, 7.646727561950684, 8.59219741821289, 9.675070762634277, 10.77119255065918, 11.868535041809082, 12.83842658996582, 13.727555274963379, 14.569844245910645, 15.391722679138184, 16.204023361206055, 17.02372169494629, 17.626384735107422, 18.072078704833984, 18.462026596069336, 18.803436279296875, 19.08125877380371, 19.200590133666992, 19.074377059936523, 18.833162307739258, 18.582487106323242, 18.339160919189453, 17.97744369506836, 17.59515380859375, 17.09140968322754, 16.50218391418457, 15.817791938781738, 14.983868598937988, 13.986822128295898, 12.817933082580566, 11.528505325317383, 10.241579055786133, 8.946599960327148, 7.588953971862793, 6.2032341957092285, 4.799948692321777, 3.3720505237579346, 1.9454675912857056, 0.4815756678581238, -0.9242660999298096, -2.3082480430603027, -3.7190709114074707, -5.090760231018066, -6.490819931030273, -7.933252811431885, -9.48039722442627, -11.141877174377441, -12.927711486816406, -14.796602249145508, -16.603300094604492, -18.390233993530273, -20.1385498046875, -21.805997848510742, -23.41408920288086, -25.02754783630371, -26.801597595214844, -28.776451110839844, -30.972705841064453, -33.385520935058594, -35.90762710571289, -38.527618408203125, -41.362369537353516, -44.435585021972656, -47.831398010253906, -51.587188720703125, -55.642662048339844, -59.980804443359375, -64.55036163330078, -69.1060562133789, -73.4732666015625, -77.65788269042969, -81.6474380493164, -85.45370483398438, -89.12055206298828, -92.67816925048828, -96.15220642089844, -99.54827117919922, -102.86875915527344, -106.01786804199219, -109.03597259521484, -111.96282958984375, -114.75870513916016, -117.48453521728516, -120.2335205078125, -123.01750946044922, -125.81232452392578, -128.56246948242188, -131.20936584472656, -133.767333984375, -136.21359252929688, -138.6573486328125, -141.0603485107422, -143.3613739013672, -145.4899444580078, -147.5723114013672, -149.41514587402344, -150.9908905029297, -152.32089233398438, -153.6006622314453, -154.83030700683594, -156.0063018798828, -157.14691162109375, -158.23680114746094, -159.30880737304688, -160.30152893066406, -161.2411651611328, -162.03582763671875, -162.72186279296875, -163.28753662109375, -163.81460571289062, -164.31549072265625, -164.78814697265625, -165.1201171875, -165.26596069335938, -165.24961853027344, -165.20376586914062, -165.07931518554688, -165.0469512939453, -165.03262329101562, -164.86660766601562, -164.62220764160156, -164.3842315673828, -164.145263671875, -163.90011596679688, -163.64981079101562, -163.3218231201172, -162.726318359375, -161.83493041992188, -160.71856689453125, -159.4139862060547, -157.9736328125, -156.54212951660156, -155.10464477539062, -153.63636779785156, -152.13641357421875, -150.6412811279297, -149.1659698486328, -147.64437866210938, -146.01336669921875, -144.21286010742188, -142.3518829345703, -140.49502563476562, -138.6591796875, -136.8135986328125, -134.9413604736328, -132.9547882080078, -130.7132110595703, -128.1597137451172, -125.3279037475586, -122.26266479492188, -118.97386932373047, -115.49871826171875, -111.90750122070312, -108.16539764404297, -104.34297180175781, -100.58757781982422, -96.96247863769531, -93.51396942138672, -90.1981201171875, -86.93607330322266, -83.70171356201172, -80.58210754394531, -77.49177551269531, -74.4620132446289, -71.53809356689453, -68.60317993164062, -65.52932739257812, -62.46957778930664, -59.48895263671875, -56.56187057495117, -53.813289642333984, -51.1711311340332, -48.648197174072266, -46.242332458496094, -43.94118118286133, -41.766075134277344, -39.70472717285156, -37.813140869140625, -36.01365280151367, -34.269657135009766, -32.50520706176758, -30.680166244506836, -28.837051391601562, -27.001256942749023, -25.25333023071289, -23.701873779296875, -22.668081283569336, -22.199195861816406, -22.169893264770508, -22.46630859375, -23.134033203125, -24.32797622680664, -26.001781463623047, -27.869766235351562, -29.80392074584961, -31.775949478149414, -33.793365478515625, -35.771907806396484, -37.70563888549805, -39.61886215209961, -41.516029357910156, -43.41127014160156, -45.27768325805664, -47.11109924316406, -48.94091796875, -50.77583694458008, -52.619163513183594, -54.48332977294922, -56.314815521240234, -58.103755950927734, -59.823333740234375, -61.56585693359375, -63.30061340332031, -64.97642517089844, -66.51130676269531, -67.94270324707031, -69.3357925415039, -70.66708374023438, -71.93402099609375, -73.18978118896484, -74.31753540039062, -75.23255920410156, -75.95966339111328, -76.61920166015625, -77.26768493652344, -77.9359130859375, -78.5946273803711, -79.26289367675781, -79.79534912109375, -80.2015380859375, -80.60335540771484, -81.02714538574219, -81.53772735595703, -82.04193878173828, -82.53047180175781, -83.04158020019531, -83.56088256835938, -84.14714813232422, -84.81393432617188, -85.55133056640625, -86.36656188964844, -87.24837493896484, -88.13751983642578, -88.99240112304688, -89.81124877929688, -90.60415649414062, -91.33631896972656, -92.02133178710938, -92.65229034423828, -93.23121643066406, -93.7853012084961, -94.3372573852539, -94.88070678710938, -95.41710662841797, -95.84803771972656, -96.24778747558594, -96.6568374633789, -97.0496826171875, -97.41992950439453, -97.77052307128906, -97.91485595703125, -97.96147155761719, -97.87026977539062, -97.53227233886719, -96.85386657714844, -95.81302642822266, -94.54135131835938, -93.15739440917969, -91.603271484375, -89.95466613769531, -88.35015106201172, -86.80291748046875, -85.39144134521484, -84.07344055175781, -82.86149597167969, -81.5972671508789, -80.11182403564453, -78.36345672607422, -76.40621948242188, -74.32894134521484, -72.0761489868164, -69.69659423828125, -67.17849731445312, -64.48152160644531, -61.61235046386719, -58.499427795410156, -55.10073471069336, -51.55522918701172, -47.74736785888672, -43.832923889160156, -39.801971435546875, -35.743858337402344, -31.80649757385254, -28.028738021850586, -24.38759994506836, -20.836519241333008, -17.374597549438477, -14.002902030944824, -10.617079734802246, -7.34421443939209, -4.187110424041748, -1.115414023399353, 2.037353277206421, 5.401520252227783, 8.870983123779297, 12.423381805419922, 16.180818557739258, 20.157392501831055, 24.33769989013672, 28.77823829650879, 33.3828010559082, 38.12346267700195, 42.767642974853516, 47.21396255493164, 51.497074127197266, 55.640106201171875, 59.61445999145508, 63.45794677734375, 67.16992950439453, 70.71627044677734, 74.12809753417969, 77.53622436523438, 80.97876739501953, 84.45626068115234, 87.9986572265625, 91.61026000976562, 95.1865234375, 98.68260192871094, 102.08172607421875, 105.37554168701172, 108.5978012084961, 111.72406005859375, 114.72969818115234, 117.6103515625, 120.28418731689453, 122.77039337158203, 125.10813903808594, 127.35991668701172, 129.5707550048828, 131.73577880859375, 133.8451385498047, 135.88076782226562, 137.81361389160156, 139.69195556640625, 141.56494140625, 143.51321411132812, 145.43582153320312, 147.37954711914062, 149.30592346191406, 151.1349334716797, 152.76832580566406, 154.18382263183594, 155.40008544921875, 156.48155212402344, 157.39840698242188, 158.19866943359375, 158.91281127929688, 159.4974822998047, 160.02337646484375, 160.31883239746094, 160.23129272460938, 159.7694854736328, 159.0675506591797, 158.11312866210938, 157.08311462402344, 155.8784942626953, 154.47816467285156, 152.8489990234375, 151.00660705566406, 149.11109924316406, 147.24368286132812, 145.35427856445312, 143.4554443359375, 141.39073181152344, 139.07090759277344, 136.57705688476562, 134.08177185058594, 131.63348388671875, 129.23263549804688, 126.91446685791016, 124.63007354736328, 122.27965545654297, 119.90943145751953, 117.51732635498047, 115.1493148803711, 112.83964538574219, 110.53994750976562, 108.22462463378906, 105.85285949707031, 103.4562759399414, 101.13794708251953, 98.82323455810547, 96.44384765625, 93.94629669189453, 91.3570556640625, 88.73168182373047, 86.05917358398438, 83.26211547851562, 80.25263214111328, 77.10718536376953, 73.97905731201172, 70.96484375, 68.1133804321289, 65.44701385498047, 62.890159606933594, 60.41355514526367, 57.95263671875, 55.59248352050781, 53.20044708251953, 50.7462272644043, 48.28958511352539, 45.88505935668945, 43.5562744140625, 41.31084442138672, 39.171634674072266, 37.183380126953125, 35.43268966674805, 33.800804138183594, 32.20466613769531, 30.66669273376465, 29.13826560974121, 27.552635192871094, 25.97852325439453, 24.294662475585938, 22.565439224243164, 20.874217987060547, 19.30082893371582, 17.831933975219727, 16.408084869384766, 15.044317245483398, 13.766607284545898, 12.577005386352539, 11.475253105163574, 10.496495246887207, 9.622332572937012, 8.769275665283203, 7.927954196929932, 7.112521648406982, 6.322704315185547, 5.563619136810303, 4.829586982727051, 4.113427639007568, 3.3697121143341064, 2.5567243099212646, 1.7977246046066284, 1.0246542692184448, 0.2572939395904541, -0.4480553865432739, -1.1242897510528564, -1.6556841135025024, -2.0525705814361572, -2.214649200439453, -2.169621467590332, -2.035892963409424, -1.9102517366409302, -1.7909443378448486, -1.7162281274795532, -1.651557445526123, -1.5775796175003052, -1.5097243785858154, -1.4451829195022583, -1.3808107376098633, -1.3076838254928589, -1.1195673942565918, -0.8252816200256348, -0.5349398255348206, -0.2580118477344513, 0.009828831069171429, 0.2716897428035736, 0.5349469780921936, 0.7902784943580627, 1.052398443222046, 1.31592857837677, 1.570581078529358, 1.6137370109558105, 1.6365979194641114)
					Z = (-0.8819639682769775, -0.8812801241874695, -0.8804802298545837, -0.8791921734809875, -0.8777425289154053, -0.8758563995361328, -0.873963475227356, -0.8539403676986694, -0.7802032232284546, -0.761174201965332, -0.7716957926750183, -0.8395041823387146, -0.8772552609443665, -0.8344407081604004, -0.788372814655304, -0.80742347240448, -0.8527643084526062, -0.8346409797668457, -0.824370265007019, -0.8134136199951172, -0.7967275381088257, -0.7752544283866882, -0.7417746782302856, -0.6927484273910522, -0.633834719657898, -0.5747796297073364, -0.5113369226455688, -0.4433113932609558, -0.3737497925758362, -0.3008161187171936, -0.2312106341123581, -0.16523221135139465, -0.09990986436605453, -0.033577218651771545, 0.03842548280954361, 0.11881522089242935, 0.1981208622455597, 0.28177762031555176, 0.38250869512557983, 0.5017393231391907, 0.625041127204895, 0.7394312620162964, 0.8367793560028076, 0.9279725551605225, 1.0242633819580078, 1.1258037090301514, 1.2272775173187256, 1.3421326875686646, 1.4506069421768188, 1.561546802520752, 1.6706804037094116, 1.7743912935256958, 1.8515067100524902, 1.9097793102264404, 1.948763370513916, 1.9814872741699219, 2.0233898162841797, 2.07637095451355, 2.132861375808716, 2.17509126663208, 2.2180161476135254, 2.274773597717285, 2.3546767234802246, 2.4420950412750244, 2.5328733921051025, 2.6344215869903564, 2.7358694076538086, 2.8366494178771973, 2.9418249130249023, 3.0620920658111572, 3.1827614307403564, 3.30625581741333, 3.427833080291748, 3.5489587783813477, 3.675954818725586, 3.79117488861084, 3.901960849761963, 4.005653381347656, 4.107993125915527, 4.2158284187316895, 4.328779220581055, 4.445080280303955, 4.569532871246338, 4.690032005310059, 4.799752712249756, 4.872299671173096, 4.92843770980835, 4.985036849975586, 5.057000637054443, 5.13352108001709, 5.213327884674072, 5.295718193054199, 5.3766703605651855, 5.451817512512207, 5.519579887390137, 5.582165718078613, 5.639312267303467, 5.692175388336182, 5.7414727210998535, 5.787367820739746, 5.830183506011963, 5.869744300842285, 5.905086994171143, 5.936120986938477, 5.963281154632568, 5.987318992614746, 6.008669376373291, 6.027542591094971, 6.044310569763184, 6.057828903198242, 6.067286968231201, 6.074985504150391, 6.081448554992676, 6.086737155914307, 6.091536998748779, 6.096595764160156, 6.1012773513793945, 6.104137420654297, 6.10720682144165, 6.105283260345459, 6.09289026260376, 6.069871425628662, 6.042582988739014, 6.011574745178223, 5.977062702178955, 5.945542812347412, 5.9195661544799805, 5.900696277618408, 5.875031471252441, 5.850343227386475, 5.822032451629639, 5.787215232849121, 5.749323844909668, 5.708043575286865, 5.672667503356934, 5.640613079071045, 5.58774995803833, 5.510519504547119, 5.4132280349731445, 5.318352222442627, 5.21757173538208, 5.129578113555908, 5.049224376678467, 4.955892086029053, 4.855170726776123, 4.759181022644043, 4.6699957847595215, 4.590251922607422, 4.507761478424072, 4.420248508453369, 4.298507213592529, 4.1367998123168945, 3.954977035522461, 3.7536673545837402, 3.5393548011779785, 3.336235761642456, 3.13871431350708, 2.941469192504883, 2.743802785873413, 2.5500059127807617, 2.362222671508789, 2.172161817550659, 1.9712504148483276, 1.7527763843536377, 1.5335578918457031, 1.3216581344604492, 1.11974036693573, 0.924856424331665, 0.7362942099571228, 0.548167884349823, 0.3510936498641968, 0.14911779761314392, -0.04503828287124634, -0.22794248163700104, -0.3905165493488312, -0.5209499597549438, -0.6174218654632568, -0.6916936039924622, -0.7458155751228333, -0.7768694162368774, -0.7899942994117737, -0.7893635630607605, -0.7789414525032043, -0.7635725736618042, -0.7461717128753662, -0.7283236980438232, -0.704211413860321, -0.6622856855392456, -0.5993924140930176, -0.5216199159622192, -0.426088809967041, -0.3150973916053772, -0.1974087506532669, -0.07835512608289719, 0.03133012354373932, 0.13556505739688873, 0.24022513628005981, 0.3493971824645996, 0.45991453528404236, 0.5715771317481995, 0.6827750205993652, 0.7940959930419922, 0.907843291759491, 1.025125503540039, 1.148614764213562, 1.2811535596847534, 1.417541265487671, 1.5532535314559937, 1.6824359893798828, 1.7986339330673218, 1.8819316625595093, 1.9304401874542236, 1.9543043375015259, 1.9636659622192383, 1.9588732719421387, 1.916387915611267, 1.8345577716827393, 1.7349056005477905, 1.6296110153198242, 1.5208213329315186, 1.405418872833252, 1.2866981029510498, 1.16438889503479, 1.0394600629806519, 0.9107307195663452, 0.7798608541488647, 0.6512886881828308, 0.5262399315834045, 0.4030036926269531, 0.2815271019935608, 0.16398224234580994, 0.05072043836116791, -0.05590145289897919, -0.15327762067317963, -0.24135041236877441, -0.3243723213672638, -0.3988741636276245, -0.4620799124240875, -0.542617678642273, -0.646656334400177, -0.7287228107452393, -0.7844877243041992, -0.806078314781189, -0.8148013949394226, -0.8116025924682617, -0.8039451837539673, -0.7978506088256836, -0.8006065487861633, -0.8066939115524292, -0.8129818439483643, -0.8215823173522949, -0.8290983438491821, -0.8362972736358643, -0.8428731560707092, -0.8489797711372375, -0.8558133840560913, -0.8626493811607361, -0.8682581186294556, -0.8741699457168579, -0.879978597164154, -0.8859436511993408, -0.8909560441970825, -0.8937748670578003, -0.8939367532730103, -0.8897822499275208, -0.8787690997123718, -0.8593403697013855, -0.8307321667671204, -0.8021003603935242, -0.7821503281593323, -0.7700151801109314, -0.7592963576316833, -0.7492351531982422, -0.7390634417533875, -0.7314242720603943, -0.7212424278259277, -0.7080341577529907, -0.6888165473937988, -0.66937655210495, -0.6463529467582703, -0.6128187775611877, -0.5654257535934448, -0.5037499666213989, -0.42715343832969666, -0.34471648931503296, -0.25006303191185, -0.14578062295913696, -0.03818090260028839, 0.0759134441614151, 0.21288788318634033, 0.35622480511665344, 0.515775203704834, 0.6532223224639893, 0.7738814949989319, 0.8932506442070007, 1.0421302318572998, 1.2146294116973877, 1.385721206665039, 1.5515326261520386, 1.7406084537506104, 1.9566478729248047, 2.214561700820923, 2.5135207176208496, 2.8274102210998535, 3.160696268081665, 3.501220941543579, 3.8431997299194336, 4.200472354888916, 4.574350357055664, 4.894090175628662, 5.0936360359191895, 5.216364860534668, 5.390469074249268, 5.586197853088379, 5.784314155578613, 5.985593795776367, 6.1828765869140625, 6.373883247375488, 6.556783199310303, 6.733740329742432, 6.906088829040527, 7.071183204650879, 7.233142852783203, 7.3868231773376465, 7.530625343322754, 7.665377616882324, 7.797634124755859, 7.930730819702148, 8.059279441833496, 8.180848121643066, 8.296680450439453, 8.406368255615234, 8.505520820617676, 8.589674949645996, 8.655287742614746, 8.70052719116211, 8.722027778625488, 8.70865249633789, 8.652679443359375, 8.560135841369629, 8.443024635314941, 8.307100296020508, 8.149582862854004, 7.971302032470703, 7.780361175537109, 7.575259685516357, 7.355491638183594, 7.124767303466797, 6.885737419128418, 6.638427257537842, 6.395895481109619, 6.166090488433838, 5.953654766082764, 5.738729953765869, 5.529703140258789, 5.342148303985596, 5.179572105407715, 5.024766445159912, 4.851255416870117, 4.646117210388184, 4.430662155151367, 4.217848777770996, 4.0131144523620605, 3.7878849506378174, 3.559556245803833, 3.3353841304779053, 3.1190574169158936, 2.9180359840393066, 2.7267343997955322, 2.5381720066070557, 2.3227102756500244, 2.0959630012512207, 1.8809078931808472, 1.6847819089889526, 1.495663046836853, 1.3055880069732666, 1.1171165704727173, 0.9520562887191772, 0.8042331337928772, 0.681337833404541, 0.5795820951461792, 0.5025584101676941, 0.46133852005004883, 0.4328932762145996, 0.3858243227005005, 0.3234015107154846, 0.2624247372150421, 0.19709435105323792, 0.15313704311847687, 0.11826862394809723, 0.08544927090406418, 0.04712279140949249, 0.0015682056546211243, -0.026410788297653198, -0.03486667573451996, -0.027389593422412872, -0.0065015703439712524, 0.0059362053871154785, 0.002570606768131256, -0.006264716386795044, -0.013282939791679382, -0.018584154546260834, -0.022372961044311523, -0.0232115238904953, -0.02133723348379135, -0.030498042702674866, -0.057736508548259735, -0.09805164486169815, -0.13833804428577423, -0.17615404725074768, -0.21290594339370728, -0.24737012386322021, -0.26589956879615784, -0.2773838937282562, -0.2822290062904358, -0.2861996591091156, -0.2940981388092041, -0.2990141808986664, -0.3035801351070404, -0.3050832152366638, -0.3049992024898529, -0.30373987555503845, -0.3003387153148651, -0.29614898562431335, -0.2985635995864868, -0.31389492750167847, -0.34401920437812805, -0.3844596743583679, -0.4300534129142761, -0.4741150140762329, -0.5105020999908447, -0.5354415774345398, -0.552415132522583, -0.5600359439849854, -0.5654557943344116, -0.5681073665618896, -0.5666967630386353, -0.5622239112854004, -0.5597591996192932, -0.5650179386138916, -0.579081654548645, -0.5969113707542419, -0.6101321578025818, -0.622231125831604, -0.6340838074684143, -0.6458472609519958, -0.657522976398468, -0.6685013771057129, -0.6801296472549438, -0.6912583708763123, -0.7032382488250732, -0.7155491709709167, -0.7265709042549133, -0.7348979115486145, -0.7445682287216187, -0.7536845207214355, -0.761847198009491, -0.7706142067909241, -0.7806366682052612, -0.7898868322372437, -0.7978246212005615, -0.8051745295524597, -0.8114349842071533, -0.8171375393867493, -0.821597158908844, -0.8264663219451904, -0.8312869071960449, -0.8363567590713501, -0.8399266004562378, -0.8434712290763855, -0.8482410907745361, -0.8517320156097412, -0.8557907342910767, -0.8605977296829224, -0.864855170249939, -0.8680832982063293, -0.869952917098999, -0.8720065951347351, -0.8741781711578369, -0.8759156465530396, -0.8775535821914673, -0.8793764710426331, -0.8817098140716553, -0.8832718729972839, -0.8847836852073669, -0.8870889544487, -0.8891378045082092, -0.8896875977516174, -0.8895387649536133, -0.8889559507369995, -0.8881706595420837, -0.8874912261962891, -0.8865614533424377, -0.8851791024208069, -0.8832001686096191, -0.8809881806373596, -0.8781297206878662, -0.8746054172515869, -0.8718098402023315, -0.8688086271286011)
					Y = (0.24426956474781036, 0.4990326166152954, 0.819128692150116, 1.153626799583435, 1.5026447772979736, 1.8859440088272095, 2.373248815536499, 2.968236207962036, 3.61586332321167, 4.355114459991455, 5.173743724822998, 6.038478374481201, 6.951005458831787, 7.899267673492432, 8.918261528015137, 10.051026344299316, 11.312947273254395, 12.90755558013916, 14.871548652648926, 17.198680877685547, 19.908754348754883, 22.898487091064453, 26.10063934326172, 29.397844314575195, 32.636375427246094, 35.74137878417969, 38.707183837890625, 41.484439849853516, 44.07951736450195, 46.60736846923828, 49.15201187133789, 51.65317916870117, 54.06341552734375, 56.4561882019043, 58.852813720703125, 61.29132080078125, 63.84211730957031, 66.49172973632812, 69.07376861572266, 71.62057495117188, 74.08918762207031, 76.49169158935547, 78.78299713134766, 80.95753479003906, 83.06936645507812, 85.1029281616211, 87.12429809570312, 89.12969970703125, 91.03314971923828, 92.87902069091797, 94.55635070800781, 96.09061431884766, 97.33863830566406, 98.26770782470703, 98.91900634765625, 99.34143829345703, 99.79500579833984, 100.22048950195312, 100.46652221679688, 100.50714111328125, 100.43055725097656, 100.3218765258789, 100.27439880371094, 100.24840545654297, 100.22171020507812, 100.19712829589844, 100.16851043701172, 100.09687042236328, 100.02641296386719, 99.95970153808594, 99.8285140991211, 99.58265686035156, 99.25724792480469, 98.94861602783203, 98.7610855102539, 98.6032943725586, 98.43841552734375, 98.27819061279297, 98.11662292480469, 97.93367004394531, 97.72758483886719, 97.4378662109375, 97.10028839111328, 96.74153900146484, 96.36189270019531, 95.95005798339844, 95.50723266601562, 95.01679229736328, 94.47090911865234, 93.8803482055664, 93.24833679199219, 92.5796127319336, 91.90768432617188, 91.14244079589844, 90.31917572021484, 89.48597717285156, 88.64861297607422, 87.82418823242188, 87.01628875732422, 86.22871398925781, 85.56230163574219, 84.96900177001953, 84.57625579833984, 84.36016082763672, 84.20700073242188, 84.08193969726562, 83.97764587402344, 83.87611389160156, 83.92423248291016, 84.14193725585938, 84.41809844970703, 84.70330810546875, 85.00025939941406, 85.29436492919922, 85.68895721435547, 86.27693176269531, 87.06804656982422, 88.0323715209961, 89.15747833251953, 90.61774444580078, 92.43035125732422, 94.46464538574219, 96.57106018066406, 98.82080078125, 101.0973129272461, 103.33666229248047, 105.50848388671875, 107.6570053100586, 109.891357421875, 112.15137481689453, 114.42011260986328, 116.68489074707031, 118.90473175048828, 121.11170959472656, 123.25049591064453, 125.32403564453125, 127.53121185302734, 129.89825439453125, 132.2855987548828, 134.6158905029297, 136.92697143554688, 139.15802001953125, 141.3134002685547, 143.4351806640625, 145.5569305419922, 147.65158081054688, 149.7096405029297, 151.71261596679688, 153.65261840820312, 155.51608276367188, 157.31924438476562, 159.11117553710938, 160.7533416748047, 162.2732696533203, 163.74002075195312, 165.19287109375, 166.6624298095703, 168.05679321289062, 169.36721801757812, 170.6645965576172, 171.94862365722656, 173.23680114746094, 174.46946716308594, 175.60227966308594, 176.68606567382812, 177.7667236328125, 178.8304901123047, 179.89537048339844, 180.9698944091797, 182.1023712158203, 183.38099670410156, 184.83396911621094, 186.4405059814453, 188.17733764648438, 190.03277587890625, 191.99041748046875, 193.9769287109375, 195.76626586914062, 197.2998809814453, 198.64427185058594, 199.84442138671875, 201.0236358642578, 202.19769287109375, 203.31591796875, 204.40118408203125, 205.4407196044922, 206.46392822265625, 207.45944213867188, 208.4150848388672, 209.36993408203125, 210.36520385742188, 211.35165405273438, 212.19497680664062, 212.80360412597656, 212.99081420898438, 212.8595428466797, 212.59893798828125, 212.30372619628906, 211.88113403320312, 211.2249298095703, 210.27505493164062, 209.16802978515625, 207.95042419433594, 206.6737060546875, 205.3536376953125, 203.98805236816406, 202.4827117919922, 200.79603576660156, 198.84075927734375, 196.52613830566406, 193.94662475585938, 191.1892852783203, 188.33187866210938, 185.4967803955078, 182.7758331298828, 180.3319091796875, 178.08534240722656, 175.87472534179688, 173.57350158691406, 171.1052703857422, 168.51658630371094, 165.9554443359375, 163.4188995361328, 160.97314453125, 158.5869903564453, 156.26071166992188, 154.0010223388672, 151.86273193359375, 149.84214782714844, 147.8561553955078, 145.87100219726562, 143.8812255859375, 141.9394073486328, 140.04071044921875, 138.22088623046875, 136.38259887695312, 134.54953002929688, 132.78271484375, 130.9574737548828, 129.08750915527344, 127.25975799560547, 125.4315185546875, 123.64933013916016, 121.882080078125, 120.05531311035156, 118.18463134765625, 116.25498962402344, 114.34269714355469, 112.4908447265625, 110.6985092163086, 108.94164276123047, 107.16153717041016, 105.32911682128906, 103.44462585449219, 101.6138916015625, 99.76459503173828, 97.91300964355469, 96.16510772705078, 94.41311645507812, 92.58258056640625, 90.4946517944336, 88.02781677246094, 85.19628143310547, 82.00907135009766, 78.48986053466797, 74.69635772705078, 70.86166381835938, 67.15168762207031, 63.572113037109375, 60.10674285888672, 56.803375244140625, 53.6189079284668, 50.549373626708984, 47.61164474487305, 44.77302932739258, 41.92876434326172, 39.06986999511719, 36.2219352722168, 33.32758331298828, 30.242610931396484, 26.973918914794922, 23.662368774414062, 20.41046714782715, 17.231449127197266, 14.126823425292969, 11.168815612792969, 8.347853660583496, 5.706920623779297, 3.3018741607666016, 1.2335699796676636, -0.5328974723815918, -2.043576717376709, -3.110535144805908, -3.740983486175537, -4.098943710327148, -4.4906511306762695, -4.8972249031066895, -5.2530198097229, -5.577995777130127, -5.934023857116699, -6.255759239196777, -6.630918025970459, -7.013139724731445, -7.412384033203125, -7.725191116333008, -8.017799377441406, -8.335323333740234, -8.662646293640137, -9.008383750915527, -9.383427619934082, -9.718378067016602, -10.013775825500488, -10.301630973815918, -10.562592506408691, -10.815587997436523, -11.065951347351074, -11.301687240600586, -11.448249816894531, -11.537090301513672, -11.524465560913086, -11.443005561828613, -11.383244514465332, -11.339241981506348, -11.295818328857422, -11.257658004760742, -11.223909378051758, -11.219079971313477, -11.304905891418457, -11.446738243103027, -11.616390228271484, -11.812542915344238, -12.02774429321289, -12.266841888427734, -12.534515380859375, -12.815123558044434, -13.006359100341797, -13.117430686950684, -13.182148933410645, -13.210461616516113, -13.223767280578613, -13.236565589904785, -13.257308006286621, -13.364906311035156, -13.60283374786377, -13.906349182128906, -14.247852325439453, -14.630463600158691, -15.034890174865723, -15.458684921264648, -15.909191131591797, -16.372478485107422, -16.83634376525879, -17.298728942871094, -17.954330444335938, -18.74985694885254, -19.579227447509766, -20.42566680908203, -21.43193817138672, -22.800357818603516, -24.44293212890625, -26.13048553466797, -27.82823944091797, -29.55722427368164, -31.477741241455078, -33.487709045410156, -35.511478424072266, -37.493263244628906, -39.456016540527344, -41.433685302734375, -43.504295349121094, -45.86669158935547, -48.45779037475586, -51.14822006225586, -53.83092498779297, -56.52829360961914, -59.291015625, -62.107452392578125, -64.86852264404297, -67.60960388183594, -70.36067199707031, -73.03939819335938, -75.66210174560547, -78.23661041259766, -80.80587005615234, -83.38500213623047, -85.95026397705078, -88.392578125, -90.68785095214844, -92.96864318847656, -95.2093505859375, -97.35236358642578, -99.36150360107422, -101.18042755126953, -102.92134857177734, -104.60369110107422, -106.27859497070312, -107.93692779541016, -109.50454711914062, -110.95790100097656, -112.26480102539062, -113.4476318359375, -114.55032348632812, -115.59841918945312, -116.59353637695312, -117.56787872314453, -118.43424987792969, -119.07018280029297, -119.529541015625, -119.9432144165039, -120.33118438720703, -120.70291137695312, -121.06876373291016, -121.57264709472656, -122.14915466308594, -122.72602844238281, -123.31329345703125, -123.84371948242188, -124.38484191894531, -124.94699096679688, -125.50639343261719, -126.06773376464844, -126.62725067138672, -127.21639251708984, -127.76771545410156, -128.14712524414062, -128.24986267089844, -128.0001220703125, -127.45743560791016, -126.70941925048828, -125.85266876220703, -124.98062133789062, -124.1561508178711, -123.36287689208984, -122.56819915771484, -121.65084838867188, -120.66740417480469, -119.70370483398438, -118.76301574707031, -117.76809692382812, -116.55887603759766, -115.09596252441406, -113.52935028076172, -111.99527740478516, -110.50000762939453, -108.9967041015625, -107.39553833007812, -105.7052001953125, -103.86796569824219, -101.89085388183594, -99.83897399902344, -97.75530242919922, -95.71993255615234, -93.73746490478516, -91.82310485839844, -89.95047760009766, -88.10604858398438, -86.26592254638672, -84.39051818847656, -82.42990112304688, -80.4601821899414, -78.54206085205078, -76.67953491210938, -74.87965393066406, -73.13782501220703, -71.447998046875, -69.79700469970703, -68.07174682617188, -66.20356750488281, -64.17756652832031, -62.02452850341797, -59.78955841064453, -57.599979400634766, -55.49079895019531, -53.38170623779297, -51.32799530029297, -49.24906539916992, -47.25999069213867, -45.2713508605957, -43.23389434814453, -41.17817687988281, -39.17205047607422, -37.22850799560547, -35.21967697143555, -33.25495910644531, -31.328039169311523, -29.30510902404785, -27.14748191833496, -24.93663215637207, -22.68917465209961, -20.511201858520508, -18.440406799316406, -16.442750930786133, -14.476696014404297, -12.49740982055664, -10.538829803466797, -8.549440383911133, -6.5612688064575195, -4.653802394866943, -2.830416679382324, -1.0931862592697144)
					Xmap = [-215.266 -214.266 -213.266 -212.266 -211.266 -210.266 -209.266 -208.266 -207.266 -206.266 -205.266 -204.266 -203.266 -202.266 -201.266 -200.266 -199.266 -198.266 -197.266 -196.266 -195.266 -194.266 -193.266 -192.266 -191.266 -190.266 -189.266 -188.266 -187.266 -186.266 -185.266 -184.266 -183.266 -182.266 -181.266 -180.266 -179.266 -178.266 -177.266 -176.266 -175.266 -174.266 -173.266 -172.266 -171.266 -170.266 -169.266 -168.266 -167.266 -166.266 -165.266 -164.266 -163.266 -162.266 -161.266 -160.266 -159.266 -158.266 -157.266 -156.266 -155.266 -154.266 -153.266 -152.266 -151.266 -150.266 -149.266 -148.266 -147.266 -146.266 -145.266 -144.266 -143.266 -142.266 -141.266 -140.266 -139.266 -138.266 -137.266 -136.266 -135.266 -134.266 -133.266 -132.266 -131.266 -130.266 -129.266 -128.266 -127.266 -126.266 -125.266 -124.266 -123.266 -122.266 -121.266 -120.266 -119.266 -118.266 -117.266 -116.266 -115.266 -114.266 -113.266 -112.266 -111.266 -110.266 -109.266 -108.266 -107.266 -106.266 -105.266 -104.266 -103.266 -102.266 -101.266 -100.266  -99.266  -98.266  -97.266  -96.266  -95.266  -94.266  -93.266  -92.266  -91.266  -90.266  -89.266  -88.266  -87.266  -86.266  -85.266  -84.266  -83.266  -82.266  -81.266  -80.266  -79.266  -78.266  -77.266  -76.266  -75.266  -74.266  -73.266  -72.266  -71.266  -70.266  -69.266  -68.266  -67.266  -66.266  -65.266  -64.266  -63.266  -62.266  -61.266  -60.266  -59.266  -58.266  -57.266  -56.266  -55.266  -54.266  -53.266  -52.266  -51.266  -50.266  -49.266  -48.266  -47.266  -46.266  -45.266  -44.266  -43.266  -42.266  -41.266  -40.266  -39.266  -38.266  -37.266  -36.266  -35.266  -34.266  -33.266  -32.266  -31.266  -30.266  -29.266  -28.266  -27.266  -26.266  -25.266  -24.266  -23.266  -22.266  -21.266  -20.266  -19.266  -18.266  -17.266  -16.266  -15.266  -14.266  -13.266  -12.266  -11.266  -10.266   -9.266   -8.266   -7.266   -6.266   -5.266   -4.266   -3.266   -2.266   -1.266   -0.266    0.734    1.734    2.734    3.734    4.734    5.734
					    6.734    7.734    8.734    9.734   10.734   11.734   12.734   13.734   14.734   15.734   16.734   17.734   18.734   19.734   20.734   21.734   22.734   23.734   24.734   25.734   26.734   27.734   28.734   29.734   30.734   31.734   32.734   33.734   34.734   35.734   36.734   37.734   38.734   39.734   40.734   41.734   42.734   43.734   44.734   45.734   46.734   47.734   48.734   49.734   50.734   51.734   52.734   53.734   54.734   55.734   56.734   57.734   58.734   59.734   60.734   61.734   62.734   63.734   64.734   65.734   66.734   67.734   68.734   69.734   70.734   71.734   72.734   73.734   74.734   75.734   76.734   77.734   78.734   79.734   80.734   81.734   82.734   83.734   84.734   85.734   86.734   87.734   88.734   89.734   90.734   91.734   92.734   93.734   94.734   95.734   96.734   97.734   98.734   99.734  100.734  101.734  102.734  103.734  104.734  105.734  106.734  107.734  108.734  109.734  110.734  111.734  112.734  113.734  114.734  115.734  116.734  117.734  118.734  119.734  120.734  121.734  122.734  123.734  124.734  125.734  126.734  127.734  128.734  129.734  130.734  131.734  132.734  133.734  134.734  135.734  136.734  137.734  138.734  139.734  140.734  141.734  142.734  143.734  144.734  145.734  146.734  147.734  148.734  149.734  150.734  151.734  152.734  153.734  154.734  155.734  156.734  157.734  158.734  159.734  160.734  161.734  162.734  163.734  164.734  165.734  166.734  167.734  168.734  169.734  170.734  171.734  172.734  173.734  174.734  175.734  176.734  177.734  178.734  179.734  180.734  181.734  182.734  183.734  184.734  185.734  186.734  187.734  188.734  189.734  190.734  191.734  192.734  193.734  194.734  195.734  196.734  197.734  198.734  199.734  200.734  201.734  202.734  203.734  204.734  205.734  206.734  207.734  208.734  209.734]
					Ymap = [-1.782e+02 -1.772e+02 -1.762e+02 -1.752e+02 -1.742e+02 -1.732e+02 -1.722e+02 -1.712e+02 -1.702e+02 -1.692e+02 -1.682e+02 -1.672e+02 -1.662e+02 -1.652e+02 -1.642e+02 -1.632e+02 -1.622e+02 -1.612e+02 -1.602e+02 -1.592e+02 -1.582e+02 -1.572e+02 -1.562e+02 -1.552e+02 -1.542e+02 -1.532e+02 -1.522e+02 -1.512e+02 -1.502e+02 -1.492e+02 -1.482e+02 -1.472e+02 -1.462e+02 -1.452e+02 -1.442e+02 -1.432e+02 -1.422e+02 -1.412e+02 -1.402e+02 -1.392e+02 -1.382e+02 -1.372e+02 -1.362e+02 -1.352e+02 -1.342e+02 -1.332e+02 -1.322e+02 -1.312e+02 -1.302e+02 -1.292e+02 -1.282e+02 -1.272e+02 -1.262e+02 -1.252e+02 -1.242e+02 -1.232e+02 -1.222e+02 -1.212e+02 -1.202e+02 -1.192e+02 -1.182e+02 -1.172e+02 -1.162e+02 -1.152e+02 -1.142e+02 -1.132e+02 -1.122e+02 -1.112e+02 -1.102e+02 -1.092e+02 -1.082e+02 -1.072e+02 -1.062e+02 -1.052e+02 -1.042e+02 -1.032e+02 -1.022e+02 -1.012e+02 -1.002e+02 -9.925e+01 -9.825e+01 -9.725e+01 -9.625e+01 -9.525e+01 -9.425e+01 -9.325e+01 -9.225e+01 -9.125e+01 -9.025e+01 -8.925e+01 -8.825e+01 -8.725e+01 -8.625e+01 -8.525e+01 -8.425e+01 -8.325e+01 -8.225e+01 -8.125e+01 -8.025e+01 -7.925e+01 -7.825e+01 -7.725e+01 -7.625e+01 -7.525e+01 -7.425e+01 -7.325e+01 -7.225e+01 -7.125e+01 -7.025e+01 -6.925e+01 -6.825e+01 -6.725e+01 -6.625e+01 -6.525e+01 -6.425e+01 -6.325e+01 -6.225e+01 -6.125e+01 -6.025e+01 -5.925e+01 -5.825e+01 -5.725e+01 -5.625e+01 -5.525e+01 -5.425e+01 -5.325e+01 -5.225e+01 -5.125e+01 -5.025e+01 -4.925e+01 -4.825e+01 -4.725e+01 -4.625e+01 -4.525e+01 -4.425e+01 -4.325e+01 -4.225e+01 -4.125e+01 -4.025e+01 -3.925e+01 -3.825e+01 -3.725e+01 -3.625e+01 -3.525e+01 -3.425e+01 -3.325e+01 -3.225e+01 -3.125e+01 -3.025e+01 -2.925e+01 -2.825e+01 -2.725e+01 -2.625e+01 -2.525e+01 -2.425e+01 -2.325e+01 -2.225e+01 -2.125e+01 -2.025e+01 -1.925e+01 -1.825e+01 -1.725e+01 -1.625e+01 -1.525e+01 -1.425e+01 -1.325e+01 -1.225e+01 -1.125e+01 -1.025e+01 -9.250e+00 -8.250e+00 -7.250e+00 -6.250e+00 -5.250e+00 -4.250e+00 -3.250e+00 -2.250e+00 -1.250e+00 -2.499e-01  7.501e-01  1.750e+00
					  2.750e+00  3.750e+00  4.750e+00  5.750e+00  6.750e+00  7.750e+00  8.750e+00  9.750e+00  1.075e+01  1.175e+01  1.275e+01  1.375e+01  1.475e+01  1.575e+01  1.675e+01  1.775e+01  1.875e+01  1.975e+01  2.075e+01  2.175e+01  2.275e+01  2.375e+01  2.475e+01  2.575e+01  2.675e+01  2.775e+01  2.875e+01  2.975e+01  3.075e+01  3.175e+01  3.275e+01  3.375e+01  3.475e+01  3.575e+01  3.675e+01  3.775e+01  3.875e+01  3.975e+01  4.075e+01  4.175e+01  4.275e+01  4.375e+01  4.475e+01  4.575e+01  4.675e+01  4.775e+01  4.875e+01  4.975e+01  5.075e+01  5.175e+01  5.275e+01  5.375e+01  5.475e+01  5.575e+01  5.675e+01  5.775e+01  5.875e+01  5.975e+01  6.075e+01  6.175e+01  6.275e+01  6.375e+01  6.475e+01  6.575e+01  6.675e+01  6.775e+01  6.875e+01  6.975e+01  7.075e+01  7.175e+01  7.275e+01  7.375e+01  7.475e+01  7.575e+01  7.675e+01  7.775e+01  7.875e+01  7.975e+01  8.075e+01  8.175e+01  8.275e+01  8.375e+01  8.475e+01  8.575e+01  8.675e+01  8.775e+01  8.875e+01  8.975e+01  9.075e+01  9.175e+01  9.275e+01  9.375e+01  9.475e+01  9.575e+01  9.675e+01  9.775e+01  9.875e+01  9.975e+01  1.008e+02  1.018e+02  1.028e+02  1.038e+02  1.048e+02  1.058e+02  1.068e+02  1.078e+02  1.088e+02  1.098e+02  1.108e+02  1.118e+02  1.128e+02  1.138e+02  1.148e+02  1.158e+02  1.168e+02  1.178e+02  1.188e+02  1.198e+02  1.208e+02  1.218e+02  1.228e+02  1.238e+02  1.248e+02  1.258e+02  1.268e+02  1.278e+02  1.288e+02  1.298e+02  1.308e+02  1.318e+02  1.328e+02  1.338e+02  1.348e+02  1.358e+02  1.368e+02  1.378e+02  1.388e+02  1.398e+02  1.408e+02  1.418e+02  1.428e+02  1.438e+02  1.448e+02  1.458e+02  1.468e+02  1.478e+02  1.488e+02  1.498e+02  1.508e+02  1.518e+02  1.528e+02  1.538e+02  1.548e+02  1.558e+02  1.568e+02  1.578e+02  1.588e+02  1.598e+02  1.608e+02  1.618e+02  1.628e+02  1.638e+02  1.648e+02  1.658e+02  1.668e+02  1.678e+02  1.688e+02  1.698e+02  1.708e+02  1.718e+02  1.728e+02  1.738e+02  1.748e+02  1.758e+02  1.768e+02  1.778e+02  1.788e+02  1.798e+02  1.808e+02  1.818e+02  1.828e+02
					  1.838e+02  1.848e+02  1.858e+02  1.868e+02  1.878e+02  1.888e+02  1.898e+02  1.908e+02  1.918e+02  1.928e+02  1.938e+02  1.948e+02  1.958e+02  1.968e+02  1.978e+02  1.988e+02  1.998e+02  2.008e+02  2.018e+02  2.028e+02  2.038e+02  2.048e+02  2.058e+02  2.068e+02  2.078e+02  2.088e+02  2.098e+02  2.108e+02  2.118e+02  2.128e+02  2.138e+02  2.148e+02  2.158e+02  2.168e+02  2.178e+02  2.188e+02  2.198e+02  2.208e+02  2.218e+02  2.228e+02  2.238e+02  2.248e+02  2.258e+02  2.268e+02  2.278e+02  2.288e+02  2.298e+02  2.308e+02  2.318e+02  2.328e+02  2.338e+02  2.348e+02  2.358e+02  2.368e+02  2.378e+02  2.388e+02  2.398e+02  2.408e+02  2.418e+02  2.428e+02  2.438e+02  2.448e+02  2.458e+02  2.468e+02  2.478e+02  2.488e+02  2.498e+02  2.508e+02  2.518e+02  2.528e+02  2.538e+02  2.548e+02  2.558e+02  2.568e+02  2.578e+02  2.588e+02  2.598e+02  2.608e+02  2.618e+02  2.628e+02]
					Zmap = [-5.894 -4.894 -3.894 -2.894 -1.894 -0.894  0.106  1.106  2.106  3.106  4.106  5.106  6.106  7.106  8.106  9.106 10.106 11.106 12.106 13.106]
					point_map = [[[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]]
					
					 [[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 [[291 291 291 ... 292 292 292]
					  [291 291 291 ... 291 292 292]
					  [291 291 291 ... 291 291 291]
					  ...
					  [162 162 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 ...
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [394 394 394 ... 394 394 394]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]]
					res = 1
					min_point = [-215.266 -178.250   -5.894]
					max_point = [ 209.734  262.750   13.106]
				X = [-215.266 -215.166 -215.066 ...  210.034  210.134  210.234]
				Y = [-178.250 -178.150 -178.050 ...  262.750  262.850  262.950]
				cost_map = [[ 214.381  214.299  214.217 ...  112.184  112.264  112.344]
				 [ 214.324  214.242  214.160 ...  112.124  112.204  112.284]
				 [ 214.267  214.185  214.103 ...  112.064  112.144  112.224]
				 ...
				 [  96.764   96.690   96.616 ...  242.661  242.689  242.717]
				 [  96.831   96.757   96.683 ...  242.757  242.785  242.813]
				 [  96.898   96.824   96.750 ...  242.852  242.881  242.909]]
				res = 0.1
				min_point = [-215.266 -178.250    0.000]
				max_point = [ 210.234  262.950    0.000]
			action_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -1.000]
				high = [ 1.000  1.000  1.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			observation_space = Box(60,) 
				dtype = float32
				shape = (60,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
			max_time = 1000
			time = 0
			idle_timeout = 10
			spec = EnvSpec(CarRacing-v1) 
				id = CarRacing-v1
				entry_point = <class 'src.envs.CarRacing.car_racing.CarRacing'> 
					reset = <function CarRacing.reset at 0x7f6e16e5bdd0>
					get_reward = <function CarRacing.get_reward at 0x7f6e18748b90>
					step = <function CarRacing.step at 0x7f6e16e64ef0>
					render = <function CarRacing.render at 0x7f6e16e64f80>
					observation = <function CarRacing.observation at 0x7f6e16e66050>
					close = <function CarRacing.close at 0x7f6e16e660e0>
					id = 1
				reward_threshold = None
				nondeterministic = False
				max_episode_steps = None
			verbose = 0
		action_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -1.000]
			high = [ 1.000  1.000  1.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		observation_space = Box(60,) 
			dtype = float32
			shape = (60,)
			low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
			high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
			bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': []}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f6d9811bb90> 
			observation_space = Box(60,) 
				dtype = float32
				shape = (60,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
	state_size = (60,)
	action_size = (3,)
	action_space = Box(3,) 
		dtype = float32
		shape = (3,)
		low = [-1.000 -1.000 -1.000]
		high = [ 1.000  1.000  1.000]
		bounded_below = [ True  True  True]
		bounded_above = [ True  True  True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7f6d9811b890> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {9001: <socket.socket fd=76, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59934), raddr=('127.0.0.1', 9001)>, 9002: <socket.socket fd=77, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 52706), raddr=('127.0.0.1', 9002)>, 9003: <socket.socket fd=78, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 55048), raddr=('127.0.0.1', 9003)>, 9004: <socket.socket fd=79, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 58088), raddr=('127.0.0.1', 9004)>, 9005: <socket.socket fd=86, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49912), raddr=('127.0.0.1', 9005)>, 9006: <socket.socket fd=88, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49972), raddr=('127.0.0.1', 9006)>, 9007: <socket.socket fd=89, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 54834), raddr=('127.0.0.1', 9007)>, 9008: <socket.socket fd=90, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 55796), raddr=('127.0.0.1', 9008)>, 9009: <socket.socket fd=91, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 56592), raddr=('127.0.0.1', 9009)>, 9010: <socket.socket fd=92, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49050), raddr=('127.0.0.1', 9010)>, 9011: <socket.socket fd=93, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 33072), raddr=('127.0.0.1', 9011)>, 9012: <socket.socket fd=94, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 44294), raddr=('127.0.0.1', 9012)>, 9013: <socket.socket fd=172, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 39100), raddr=('127.0.0.1', 9013)>, 9014: <socket.socket fd=173, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49954), raddr=('127.0.0.1', 9014)>, 9015: <socket.socket fd=174, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 43744), raddr=('127.0.0.1', 9015)>, 9016: <socket.socket fd=177, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 34988), raddr=('127.0.0.1', 9016)>}
	num_envs = 16
	max_steps = 1000,
agent: <src.models.wrappers.ParallelAgent object at 0x7f6d9811b490> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f6d980cccd0> 
		state_size = (60,)
	agent = <src.models.pytorch.agents.sac.SACAgent object at 0x7f6d980cc9d0> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f6d980cc890> 
			size = (3,)
			dt = 0.2
			action = [-1.000  0.427 -1.000]
			daction_dt = [-0.526  0.533  1.626]
		discrete = False
		action_size = (3,)
		state_size = (60,)
		config = <src.models.Config object at 0x7f6da2a93090> 
			TRIAL_AT = 1000
			SAVE_AT = 10
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.995
			NUM_STEPS = 500
			MAX_BUFFER_SIZE = 100000
			REPLAY_BATCH_SIZE = 32
			TARGET_UPDATE_RATE = 0.0004
			env_name = CarRacing-v1
			rank = 0
			size = 17
			split = 17
			model = sac
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 500000
			render = False
			trial = False
			icm = False
			rs = False
		stats = <src.utils.logger.Stats object at 0x7f6d980cc6d0> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = SACNetwork(
			  (actor_local): SACActor(
			    (layer1): Linear(in_features=60, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=3, bias=True)
			    (action_sig): Linear(in_features=256, out_features=3, bias=True)
			  )
			  (actor_target): SACActor(
			    (layer1): Linear(in_features=60, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=3, bias=True)
			    (action_sig): Linear(in_features=256, out_features=3, bias=True)
			  )
			  (critic_local): SACCritic(
			    (net_state): Linear(in_features=60, out_features=512, bias=True)
			    (net_action): Linear(in_features=3, out_features=512, bias=True)
			    (net_layer1): Linear(in_features=1024, out_features=1024, bias=True)
			    (net_layer2): Linear(in_features=1024, out_features=1024, bias=True)
			    (q_value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			  (critic_target): SACCritic(
			    (net_state): Linear(in_features=60, out_features=512, bias=True)
			    (net_action): Linear(in_features=3, out_features=512, bias=True)
			    (net_layer1): Linear(in_features=1024, out_features=1024, bias=True)
			    (net_layer2): Linear(in_features=1024, out_features=1024, bias=True)
			    (q_value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			) 
			discrete = False
			training = True
			tau = 0.0004
			name = sac
			stats = <src.utils.logger.Stats object at 0x7f6d980cc7d0> 
				mean_dict = {}
				sum_dict = {}
			config = <src.models.Config object at 0x7f6da2a93090> 
				TRIAL_AT = 1000
				SAVE_AT = 10
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.995
				NUM_STEPS = 500
				MAX_BUFFER_SIZE = 100000
				REPLAY_BATCH_SIZE = 32
				TARGET_UPDATE_RATE = 0.0004
				env_name = CarRacing-v1
				rank = 0
				size = 17
				split = 17
				model = sac
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 500000
				render = False
				trial = False
				icm = False
				rs = False
			device = cuda
			src = ['class SACActor(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config, use_discrete=False):\n\t\tsuper().__init__()\n\t\tinput_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN\n\t\tself.discrete = use_discrete and type(action_size) != tuple\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, actor_hidden)\n\t\tself.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)\n\t\tself.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.action_sig = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\t\tself.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)\n\t\t\n\tdef forward(self, state, action=None, sample=True):\n\t\tstate = self.layer1(state).relu()\n\t\tstate = self.layer2(state).relu()\n\t\tstate = self.layer3(state).relu()\n\t\taction_mu = self.action_mu(state)\n\t\taction_sig = self.action_sig(state).clamp(-5,0).exp()\n\t\tdist = torch.distributions.Normal(action_mu, action_sig)\n\t\taction = dist.rsample() if sample else action_mu\n\t\taction_out = gsoftmax(action_mu, hard=False) if self.discrete else action.tanh()\n\t\tlog_prob = torch.log(action_out+1e-6) if self.discrete else dist.log_prob(action)-torch.log(1-action_out.pow(2)+1e-6)\n\t\treturn action_out, log_prob\n', 'class SACCritic(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config):\n\t\tsuper().__init__()\n\t\tinput_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN\n\t\tself.net_state = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.net_action = torch.nn.Linear(action_size[-1], input_layer)\n\t\tself.net_layer1 = torch.nn.Linear(2*input_layer, critic_hidden)\n\t\tself.net_layer2 = torch.nn.Linear(critic_hidden, critic_hidden)\n\t\tself.q_value = torch.nn.Linear(critic_hidden, 1)\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\n\tdef forward(self, state, action):\n\t\tstate = self.net_state(state).relu()\n\t\tnet_action = self.net_action(action).relu()\n\t\tnet_layer = torch.cat([state, net_action], dim=-1)\n\t\tnet_layer = self.net_layer1(net_layer).relu()\n\t\tnet_layer = self.net_layer2(net_layer).relu()\n\t\tq_value = self.q_value(net_layer)\n\t\treturn q_value\n']
			actor_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			critic_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			alpha_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 0
			)
			target_entropy = -3
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f6d980cc510> 
			buffer = deque([], maxlen=100000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f6d980cc250> 
		size = (3,)
		dt = 0.2
		action = [ 0.266 -0.359 -1.000]
		daction_dt = [ 0.545  0.256  0.133]
	discrete = False
	action_size = (3,)
	state_size = (60,)
	config = <src.models.Config object at 0x7f6da2a93090> 
		TRIAL_AT = 1000
		SAVE_AT = 10
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.995
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		env_name = CarRacing-v1
		rank = 0
		size = 17
		split = 17
		model = sac
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 500000
		render = False
		trial = False
		icm = False
		rs = False
	stats = <src.utils.logger.Stats object at 0x7f6d980cc390> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import torch
import numpy as np
from .base import PTACNetwork, PTAgent, PTCritic, Conv, gsoftmax
from src.utils.rand import ReplayBuffer

class SACActor(torch.nn.Module):
	def __init__(self, state_size, action_size, config, use_discrete=False):
		super().__init__()
		input_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN
		self.discrete = use_discrete and type(action_size) != tuple
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, actor_hidden)
		self.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)
		self.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])
		self.action_sig = torch.nn.Linear(actor_hidden, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).clamp(-5,0).exp()
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.rsample() if sample else action_mu
		action_out = gsoftmax(action_mu, hard=False) if self.discrete else action.tanh()
		log_prob = torch.log(action_out+1e-6) if self.discrete else dist.log_prob(action)-torch.log(1-action_out.pow(2)+1e-6)
		return action_out, log_prob

class SACCritic(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		input_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN
		self.net_state = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.net_action = torch.nn.Linear(action_size[-1], input_layer)
		self.net_layer1 = torch.nn.Linear(2*input_layer, critic_hidden)
		self.net_layer2 = torch.nn.Linear(critic_hidden, critic_hidden)
		self.q_value = torch.nn.Linear(critic_hidden, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class SACNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, config, actor=SACActor, critic=SACCritic, gpu=True, load=None, name="sac", use_discrete=False):
		self.discrete = use_discrete and critic==SACCritic and type(action_size)!=tuple
		super().__init__(state_size, action_size, config, actor, critic if not self.discrete else lambda s,a,c: PTCritic(s,a,c), gpu=gpu, load=load, name=name)
		self.log_alpha = torch.nn.Parameter(torch.zeros(1, requires_grad=True).to(self.device))
		self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=config.LEARN_RATE)
		self.target_entropy = -np.product(action_size)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob = self.actor_local(state.to(self.device), action_in, sample)
			return [x.cpu().numpy() if numpy else x for x in [action, log_prob]]

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False, probs=False):
		with torch.enable_grad() if grad else torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			q_value = critic(state) if self.discrete else critic(state, action)
			return q_value.cpu().numpy() if numpy else q_value
	
	def optimize(self, states, actions, targets, next_log_probs, dones, config):
		alpha = self.log_alpha.clamp(-5, 0).detach().exp()
		if not self.discrete: next_log_probs = next_log_probs.sum(-1, keepdim=True)
		q_targets = targets - config.DISCOUNT_RATE*alpha*next_log_probs*(1-dones.view(-1,*[1]*(len(targets.shape)-1)))
		q_targets = (actions*q_targets).mean(-1, keepdim=True) if self.discrete else q_targets

		q_values = self.get_q_value(states, actions, grad=True)
		q_values = q_values.gather(-1, actions.argmax(-1, keepdim=True)) if self.discrete else q_values
		critic_loss = (q_values - q_targets.detach()).pow(2).mean()
		self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		actor_action, log_prob = self.actor_local(states)
		q_actions = self.get_q_value(states, actor_action, grad=True)
		q_baseline = q_targets if self.discrete else q_values
		actor_loss = alpha*log_prob - (q_actions - q_baseline.detach())
		actor_loss = actor_action*actor_loss if self.discrete else actor_loss
		self.step(self.actor_optimizer, actor_loss.mean(), self.actor_local.parameters())
		
		log_prob = (actor_action*log_prob).sum(-1) if self.discrete else log_prob
		alpha_loss = -(self.log_alpha * (log_prob.detach() + self.target_entropy)).mean()
		self.step(self.alpha_optimizer, alpha_loss, [self.log_alpha])
		self.stats.mean(critic_loss=critic_loss, actor_loss=actor_loss.mean(), alpha_loss=alpha_loss)

class SACAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, SACNetwork, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.config.NUM_STEPS:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			states = torch.cat([states, self.to_tensor(next_state).unsqueeze(0)], dim=0)
			next_action, next_log_prob = self.network.get_action_probs(states[-1])
			actions = torch.cat([actions, next_action.unsqueeze(0)], dim=0)
			log_probs = torch.cat([log_probs, next_log_prob.unsqueeze(0)], dim=0)
			values = self.network.get_q_value(states, actions, use_target=True)
			targets = self.compute_gae(values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), values[:-1])[0]
			states, actions, targets, next_log_probs, dones = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states[:-1], actions[:-1], targets, log_probs[1:], dones)]
			self.replay_buffer.extend(list(zip(states, actions, targets, next_log_probs, dones)), shuffle=False)	
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE:
			states, actions, targets, next_log_probs, dones = self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, targets, next_log_probs, dones, config=self.config)


Step:       0, Reward:  -177.694 [   0.000], Avg:  -177.694 (1.000) <0-00:00:00> ({'r_t':    -0.1736, 'eps':     1.0000, 'eps_e':     1.0000})
Step:    1000, Reward:  3729.689 [   0.000], Avg:  1775.997 (1.000) <0-00:00:44> ({'r_t':  -664.9604, 'eps':     1.0000, 'critic_loss':    11.0099, 'actor_loss':    -0.6729, 'alpha_loss':    -0.1244, 'eps_e':     1.0000})
Step:    2000, Reward:  7460.188 [   0.000], Avg:  3670.728 (1.000) <0-00:01:18> ({'r_t':  -506.4631, 'eps':     1.0000, 'critic_loss':     6.0441, 'actor_loss':    -0.6667, 'alpha_loss':    -0.4284, 'eps_e':     1.0000})
Step:    3000, Reward:    -9.699 [   0.000], Avg:  2750.621 (1.000) <0-00:01:54> ({'r_t':  -498.7004, 'eps':     1.0000, 'critic_loss':     4.8407, 'actor_loss':    -0.5809, 'alpha_loss':    -0.7902, 'eps_e':     1.0000})
Step:    4000, Reward:    22.577 [   0.000], Avg:  2205.012 (1.000) <0-00:02:31> ({'r_t':  -295.9543, 'eps':     1.0000, 'critic_loss':     4.8987, 'actor_loss':    -0.5367, 'alpha_loss':    -1.1413, 'eps_e':     1.0000})
Step:    5000, Reward:   -29.950 [   0.000], Avg:  1832.518 (1.000) <0-00:03:08> ({'r_t':  -318.3095, 'eps':     1.0000, 'critic_loss':     4.7501, 'actor_loss':    -0.5034, 'alpha_loss':    -1.4767, 'eps_e':     1.0000})
Step:    6000, Reward:    17.016 [   0.000], Avg:  1573.161 (1.000) <0-00:03:44> ({'r_t':  -326.6909, 'eps':     1.0000, 'critic_loss':     4.8665, 'actor_loss':    -0.5123, 'alpha_loss':    -1.7785, 'eps_e':     1.0000})
Step:    7000, Reward:    15.313 [   0.000], Avg:  1378.430 (1.000) <0-00:04:20> ({'r_t':  -119.1227, 'eps':     1.0000, 'critic_loss':     5.2322, 'actor_loss':    -0.5519, 'alpha_loss':    -2.0206, 'eps_e':     1.0000})
Step:    8000, Reward:    20.981 [   0.000], Avg:  1227.602 (1.000) <0-00:04:58> ({'r_t':   -20.2320, 'eps':     1.0000, 'critic_loss':     5.5844, 'actor_loss':    -0.5457, 'alpha_loss':    -2.2816, 'eps_e':     1.0000})
Step:    9000, Reward:   -17.609 [   0.000], Avg:  1103.081 (1.000) <0-00:05:58> ({'r_t':   -69.5690, 'eps':     1.0000, 'critic_loss':     5.2815, 'actor_loss':    -0.5173, 'alpha_loss':    -2.5621, 'eps_e':     1.0000})
Step:   10000, Reward:   -75.249 [   0.000], Avg:   995.960 (1.000) <0-00:06:48> ({'r_t':   -92.2304, 'eps':     1.0000, 'critic_loss':     4.7894, 'actor_loss':    -0.4622, 'alpha_loss':    -2.8905, 'eps_e':     1.0000})
Step:   11000, Reward:  -258.212 [   0.000], Avg:   891.446 (1.000) <0-00:07:54> ({'r_t':   -71.8113, 'eps':     1.0000, 'critic_loss':     4.0955, 'actor_loss':    -0.4094, 'alpha_loss':    -3.2461, 'eps_e':     1.0000})
Step:   12000, Reward:   -10.498 [   0.000], Avg:   822.066 (1.000) <0-00:08:36> ({'r_t':   -66.4208, 'eps':     1.0000, 'critic_loss':     3.1959, 'actor_loss':    -0.3243, 'alpha_loss':    -3.6182, 'eps_e':     1.0000})
Step:   13000, Reward:    23.416 [   0.000], Avg:   765.019 (1.000) <0-00:09:13> ({'r_t':     3.1721, 'eps':     1.0000, 'critic_loss':     2.8287, 'actor_loss':    -0.2985, 'alpha_loss':    -3.9153, 'eps_e':     1.0000})
Step:   14000, Reward:    10.891 [   0.000], Avg:   714.744 (1.000) <0-00:09:57> ({'r_t':    18.6228, 'eps':     1.0000, 'critic_loss':     2.1548, 'actor_loss':    -0.2761, 'alpha_loss':    -4.1417, 'eps_e':     1.0000})
Step:   15000, Reward:    14.623 [   0.000], Avg:   670.986 (1.000) <0-00:10:51> ({'r_t':    16.3561, 'eps':     1.0000, 'critic_loss':     1.6524, 'actor_loss':    -0.2709, 'alpha_loss':    -4.3504, 'eps_e':     1.0000})
Step:   16000, Reward:    20.276 [   0.000], Avg:   632.709 (1.000) <0-00:11:36> ({'r_t':     8.3576, 'eps':     1.0000, 'critic_loss':     1.3758, 'actor_loss':    -0.2882, 'alpha_loss':    -4.6272, 'eps_e':     1.0000})
Step:   17000, Reward:    29.818 [   0.000], Avg:   599.215 (1.000) <0-00:12:14> ({'r_t':    10.6229, 'eps':     1.0000, 'critic_loss':     1.2234, 'actor_loss':    -0.2615, 'alpha_loss':    -5.0452, 'eps_e':     1.0000})
Step:   18000, Reward:    23.813 [   0.000], Avg:   568.931 (1.000) <0-00:13:20> ({'r_t':    20.5025, 'eps':     1.0000, 'critic_loss':     1.0165, 'actor_loss':    -0.2239, 'alpha_loss':    -5.4378, 'eps_e':     1.0000})
Step:   19000, Reward:   -82.342 [   0.000], Avg:   536.367 (1.000) <0-00:14:26> ({'r_t':    21.8401, 'eps':     1.0000, 'critic_loss':     0.7764, 'actor_loss':    -0.1794, 'alpha_loss':    -5.7929, 'eps_e':     1.0000})
Step:   20000, Reward:     1.870 [   0.000], Avg:   510.915 (1.000) <0-00:15:32> ({'r_t':  -105.8380, 'eps':     1.0000, 'critic_loss':     1.0310, 'actor_loss':    -0.1916, 'alpha_loss':    -5.9279, 'eps_e':     1.0000})
Step:   21000, Reward:     5.097 [   0.000], Avg:   487.923 (1.000) <0-00:16:38> ({'r_t':    11.4209, 'eps':     1.0000, 'critic_loss':     1.0112, 'actor_loss':    -0.2165, 'alpha_loss':    -6.0679, 'eps_e':     1.0000})
Step:   22000, Reward:    18.122 [   0.000], Avg:   467.497 (1.000) <0-00:17:45> ({'r_t':    19.6895, 'eps':     1.0000, 'critic_loss':     0.8998, 'actor_loss':    -0.1917, 'alpha_loss':    -6.3337, 'eps_e':     1.0000})
Step:   23000, Reward:    19.059 [   0.000], Avg:   448.812 (1.000) <0-00:18:51> ({'r_t':    16.4350, 'eps':     1.0000, 'critic_loss':     0.7663, 'actor_loss':    -0.1475, 'alpha_loss':    -6.6842, 'eps_e':     1.0000})
Step:   24000, Reward:    -0.086 [   0.000], Avg:   430.856 (1.000) <0-00:19:57> ({'r_t':    29.4857, 'eps':     1.0000, 'critic_loss':     0.7164, 'actor_loss':    -0.1352, 'alpha_loss':    -6.9955, 'eps_e':     1.0000})
Step:   25000, Reward:    14.181 [   0.000], Avg:   414.830 (1.000) <0-00:21:03> ({'r_t':    27.7731, 'eps':     1.0000, 'critic_loss':     0.7109, 'actor_loss':    -0.1734, 'alpha_loss':    -7.2228, 'eps_e':     1.0000})
Step:   26000, Reward:     3.704 [   0.000], Avg:   399.603 (1.000) <0-00:22:09> ({'r_t':    22.3556, 'eps':     1.0000, 'critic_loss':     0.6270, 'actor_loss':    -0.1921, 'alpha_loss':    -7.4276, 'eps_e':     1.0000})
Step:   27000, Reward:    34.822 [   0.000], Avg:   386.576 (1.000) <0-00:23:01> ({'r_t':    18.6551, 'eps':     1.0000, 'critic_loss':     0.5216, 'actor_loss':    -0.1533, 'alpha_loss':    -7.6478, 'eps_e':     1.0000})
Step:   28000, Reward:    22.386 [   0.000], Avg:   374.017 (1.000) <0-00:24:07> ({'r_t':    36.6577, 'eps':     1.0000, 'critic_loss':     0.5073, 'actor_loss':    -0.2043, 'alpha_loss':    -7.5915, 'eps_e':     1.0000})
Step:   29000, Reward:    42.284 [   0.000], Avg:   362.959 (1.000) <0-00:24:45> ({'r_t':    64.1820, 'eps':     1.0000, 'critic_loss':     0.5675, 'actor_loss':    -0.2719, 'alpha_loss':    -6.3064, 'eps_e':     1.0000})
Step:   30000, Reward:    34.945 [   0.000], Avg:   352.378 (1.000) <0-00:25:21> ({'r_t':   201.3947, 'eps':     1.0000, 'critic_loss':     0.6572, 'actor_loss':    -0.3000, 'alpha_loss':    -4.9966, 'eps_e':     1.0000})
Step:   31000, Reward:    40.665 [   0.000], Avg:   342.637 (1.000) <0-00:25:58> ({'r_t':   248.7747, 'eps':     1.0000, 'critic_loss':     0.7734, 'actor_loss':    -0.3178, 'alpha_loss':    -4.7000, 'eps_e':     1.0000})
Step:   32000, Reward:    15.915 [   0.000], Avg:   332.737 (1.000) <0-00:26:37> ({'r_t':    59.1501, 'eps':     1.0000, 'critic_loss':     1.0513, 'actor_loss':    -0.4086, 'alpha_loss':    -4.0867, 'eps_e':     1.0000})
Step:   33000, Reward:    19.052 [   0.000], Avg:   323.511 (1.000) <0-00:27:16> ({'r_t':    70.8801, 'eps':     1.0000, 'critic_loss':     1.3622, 'actor_loss':    -0.3914, 'alpha_loss':    -3.7640, 'eps_e':     1.0000})
Step:   34000, Reward:    19.233 [   0.000], Avg:   314.817 (1.000) <0-00:27:55> ({'r_t':    89.6715, 'eps':     1.0000, 'critic_loss':     1.7738, 'actor_loss':    -0.3938, 'alpha_loss':    -3.3529, 'eps_e':     1.0000})
Step:   35000, Reward:    14.139 [   0.000], Avg:   306.465 (1.000) <0-00:28:34> ({'r_t':    90.7425, 'eps':     1.0000, 'critic_loss':     1.8255, 'actor_loss':    -0.4028, 'alpha_loss':    -3.1102, 'eps_e':     1.0000})
Step:   36000, Reward:    21.585 [   0.000], Avg:   298.765 (1.000) <0-00:29:14> ({'r_t':    97.9697, 'eps':     1.0000, 'critic_loss':     1.8297, 'actor_loss':    -0.3503, 'alpha_loss':    -3.2506, 'eps_e':     1.0000})
Step:   37000, Reward:    25.711 [   0.000], Avg:   291.580 (1.000) <0-00:29:54> ({'r_t':   126.8826, 'eps':     1.0000, 'critic_loss':     1.9682, 'actor_loss':    -0.3667, 'alpha_loss':    -2.8889, 'eps_e':     1.0000})
Step:   38000, Reward:    18.948 [   0.000], Avg:   284.589 (1.000) <0-00:30:35> ({'r_t':    97.7604, 'eps':     1.0000, 'critic_loss':     2.0764, 'actor_loss':    -0.4303, 'alpha_loss':    -3.0041, 'eps_e':     1.0000})
Step:   39000, Reward:    19.595 [   0.000], Avg:   277.964 (1.000) <0-00:31:17> ({'r_t':   102.0450, 'eps':     1.0000, 'critic_loss':     1.9843, 'actor_loss':    -0.4737, 'alpha_loss':    -2.7463, 'eps_e':     1.0000})
Step:   40000, Reward:    42.721 [   0.000], Avg:   272.227 (1.000) <0-00:31:59> ({'r_t':   101.5802, 'eps':     1.0000, 'critic_loss':     2.1090, 'actor_loss':    -0.5153, 'alpha_loss':    -2.9609, 'eps_e':     1.0000})
Step:   41000, Reward:    40.867 [   0.000], Avg:   266.718 (1.000) <0-00:32:43> ({'r_t':   121.2215, 'eps':     1.0000, 'critic_loss':     2.2332, 'actor_loss':    -0.5797, 'alpha_loss':    -3.3377, 'eps_e':     1.0000})
Step:   42000, Reward:    46.475 [   0.000], Avg:   261.596 (1.000) <0-00:33:28> ({'r_t':   118.4829, 'eps':     1.0000, 'critic_loss':     2.1049, 'actor_loss':    -0.6665, 'alpha_loss':    -3.1407, 'eps_e':     1.0000})
Step:   43000, Reward:    30.875 [   0.000], Avg:   256.353 (1.000) <0-00:34:13> ({'r_t':    95.9617, 'eps':     1.0000, 'critic_loss':     2.1660, 'actor_loss':    -0.6529, 'alpha_loss':    -3.1716, 'eps_e':     1.0000})
Step:   44000, Reward:     1.339 [   0.000], Avg:   250.686 (1.000) <0-00:35:08> ({'r_t':    75.5045, 'eps':     1.0000, 'critic_loss':     2.1532, 'actor_loss':    -0.5957, 'alpha_loss':    -3.2314, 'eps_e':     1.0000})
Step:   45000, Reward:    36.505 [   0.000], Avg:   246.029 (1.000) <0-00:35:54> ({'r_t':    92.5455, 'eps':     1.0000, 'critic_loss':     2.0382, 'actor_loss':    -0.5279, 'alpha_loss':    -3.3452, 'eps_e':     1.0000})
Step:   46000, Reward:    46.885 [   0.000], Avg:   241.792 (1.000) <0-00:36:40> ({'r_t':   127.8028, 'eps':     1.0000, 'critic_loss':     1.8451, 'actor_loss':    -0.4850, 'alpha_loss':    -2.4922, 'eps_e':     1.0000})
Step:   47000, Reward:    60.645 [   0.000], Avg:   238.018 (1.000) <0-00:37:22> ({'r_t':   209.9028, 'eps':     1.0000, 'critic_loss':     1.6904, 'actor_loss':    -0.5343, 'alpha_loss':    -1.9338, 'eps_e':     1.0000})
Step:   48000, Reward:    53.515 [   0.000], Avg:   234.253 (1.000) <0-00:38:03> ({'r_t':   243.9380, 'eps':     1.0000, 'critic_loss':     1.5884, 'actor_loss':    -0.5207, 'alpha_loss':    -1.0971, 'eps_e':     1.0000})
Step:   49000, Reward:    40.260 [   0.000], Avg:   230.373 (1.000) <0-00:38:50> ({'r_t':   236.9809, 'eps':     1.0000, 'critic_loss':     1.2226, 'actor_loss':    -0.5298, 'alpha_loss':    -0.3020, 'eps_e':     1.0000})
Step:   50000, Reward:    46.188 [   0.000], Avg:   226.762 (1.000) <0-00:39:33> ({'r_t':   237.5285, 'eps':     1.0000, 'critic_loss':     1.0468, 'actor_loss':    -0.5161, 'alpha_loss':     0.4872, 'eps_e':     1.0000})
Step:   51000, Reward:    53.504 [   0.000], Avg:   223.430 (1.000) <0-00:40:13> ({'r_t':   194.6069, 'eps':     1.0000, 'critic_loss':     0.8704, 'actor_loss':    -0.4234, 'alpha_loss':     0.4777, 'eps_e':     1.0000})
Step:   52000, Reward:    12.148 [   0.000], Avg:   219.443 (1.000) <0-00:41:00> ({'r_t':   242.2848, 'eps':     1.0000, 'critic_loss':     0.7832, 'actor_loss':    -0.3597, 'alpha_loss':     0.4293, 'eps_e':     1.0000})
Step:   53000, Reward:    64.559 [   0.000], Avg:   216.575 (1.000) <0-00:41:40> ({'r_t':   219.0027, 'eps':     1.0000, 'critic_loss':     0.7463, 'actor_loss':    -0.3613, 'alpha_loss':     0.5332, 'eps_e':     1.0000})
Step:   54000, Reward:    61.249 [   0.000], Avg:   213.751 (1.000) <0-00:42:18> ({'r_t':   282.4123, 'eps':     1.0000, 'critic_loss':     0.7945, 'actor_loss':    -0.4206, 'alpha_loss':     0.6393, 'eps_e':     1.0000})
Step:   55000, Reward:     0.176 [   0.000], Avg:   209.937 (1.000) <0-00:42:59> ({'r_t':   225.4687, 'eps':     1.0000, 'critic_loss':     1.1051, 'actor_loss':    -0.4520, 'alpha_loss':     0.4153, 'eps_e':     1.0000})
Step:   56000, Reward:    56.780 [   0.000], Avg:   207.250 (1.000) <0-00:43:38> ({'r_t':   258.3869, 'eps':     1.0000, 'critic_loss':     1.5630, 'actor_loss':    -0.4790, 'alpha_loss':     0.2371, 'eps_e':     1.0000})
Step:   57000, Reward:    42.751 [   0.000], Avg:   204.414 (1.000) <0-00:44:16> ({'r_t':   309.3333, 'eps':     1.0000, 'critic_loss':     1.3992, 'actor_loss':    -0.6345, 'alpha_loss':     0.2795, 'eps_e':     1.0000})
Step:   58000, Reward:    43.535 [   0.000], Avg:   201.687 (1.000) <0-00:44:54> ({'r_t':   319.6695, 'eps':     1.0000, 'critic_loss':     1.2989, 'actor_loss':    -0.7131, 'alpha_loss':    -0.0625, 'eps_e':     1.0000})
Step:   59000, Reward:    55.654 [   0.000], Avg:   199.253 (1.000) <0-00:45:33> ({'r_t':   298.7964, 'eps':     1.0000, 'critic_loss':     1.2581, 'actor_loss':    -0.6417, 'alpha_loss':    -0.3508, 'eps_e':     1.0000})
Step:   60000, Reward:    56.731 [   0.000], Avg:   196.917 (1.000) <0-00:46:11> ({'r_t':   295.6459, 'eps':     1.0000, 'critic_loss':     1.2103, 'actor_loss':    -0.5478, 'alpha_loss':    -0.5759, 'eps_e':     1.0000})
Step:   61000, Reward:    44.183 [   0.000], Avg:   194.454 (1.000) <0-00:46:49> ({'r_t':   293.9766, 'eps':     1.0000, 'critic_loss':     1.1375, 'actor_loss':    -0.4748, 'alpha_loss':    -1.3364, 'eps_e':     1.0000})
Step:   62000, Reward:    53.100 [   0.000], Avg:   192.210 (1.000) <0-00:47:28> ({'r_t':   241.1604, 'eps':     1.0000, 'critic_loss':     0.9497, 'actor_loss':    -0.3228, 'alpha_loss':    -1.4271, 'eps_e':     1.0000})
Step:   63000, Reward:    54.736 [   0.000], Avg:   190.062 (1.000) <0-00:48:12> ({'r_t':   200.7154, 'eps':     1.0000, 'critic_loss':     0.8812, 'actor_loss':    -0.2452, 'alpha_loss':    -1.0870, 'eps_e':     1.0000})
Step:   64000, Reward:    67.085 [   0.000], Avg:   188.170 (1.000) <0-00:48:52> ({'r_t':    44.6179, 'eps':     1.0000, 'critic_loss':     1.0167, 'actor_loss':    -0.3396, 'alpha_loss':    -0.6356, 'eps_e':     1.0000})
Step:   65000, Reward:    85.389 [   0.000], Avg:   186.613 (1.000) <0-00:49:58> ({'r_t':   199.5074, 'eps':     1.0000, 'critic_loss':     1.1183, 'actor_loss':    -0.5557, 'alpha_loss':    -0.4705, 'eps_e':     1.0000})
Step:   66000, Reward:    64.888 [   0.000], Avg:   184.796 (1.000) <0-00:50:37> ({'r_t':   -29.4713, 'eps':     1.0000, 'critic_loss':     1.3661, 'actor_loss':    -0.8237, 'alpha_loss':    -0.0116, 'eps_e':     1.0000})
Step:   67000, Reward:    65.362 [   0.000], Avg:   183.039 (1.000) <0-00:51:16> ({'r_t':   313.4032, 'eps':     1.0000, 'critic_loss':     1.5510, 'actor_loss':    -0.9025, 'alpha_loss':     0.3769, 'eps_e':     1.0000})
Step:   68000, Reward:    55.317 [   0.000], Avg:   181.188 (1.000) <0-00:51:55> ({'r_t':   321.1538, 'eps':     1.0000, 'critic_loss':     1.3500, 'actor_loss':    -0.8476, 'alpha_loss':     0.1351, 'eps_e':     1.0000})
Step:   69000, Reward:    62.512 [   0.000], Avg:   179.493 (1.000) <0-00:52:34> ({'r_t':   373.7773, 'eps':     1.0000, 'critic_loss':     1.3114, 'actor_loss':    -0.8262, 'alpha_loss':     0.0395, 'eps_e':     1.0000})
Step:   70000, Reward:    61.839 [   0.000], Avg:   177.836 (1.000) <0-00:53:16> ({'r_t':   342.5797, 'eps':     1.0000, 'critic_loss':     1.2188, 'actor_loss':    -0.7923, 'alpha_loss':     0.3723, 'eps_e':     1.0000})
Step:   71000, Reward:    68.384 [   0.000], Avg:   176.316 (1.000) <0-00:53:55> ({'r_t':   284.1696, 'eps':     1.0000, 'critic_loss':     1.1466, 'actor_loss':    -0.5917, 'alpha_loss':     0.2350, 'eps_e':     1.0000})
Step:   72000, Reward:    64.181 [   0.000], Avg:   174.780 (1.000) <0-00:54:38> ({'r_t':   237.7865, 'eps':     1.0000, 'critic_loss':     1.0723, 'actor_loss':    -0.6029, 'alpha_loss':     0.2394, 'eps_e':     1.0000})
Step:   73000, Reward:    19.940 [   0.000], Avg:   172.687 (1.000) <0-00:55:40> ({'r_t':    79.5486, 'eps':     1.0000, 'critic_loss':     0.7592, 'actor_loss':    -0.5484, 'alpha_loss':     0.0025, 'eps_e':     1.0000})
Step:   74000, Reward:    45.219 [   0.000], Avg:   170.988 (1.000) <0-00:56:41> ({'r_t':    73.5836, 'eps':     1.0000, 'critic_loss':     0.5889, 'actor_loss':    -0.5162, 'alpha_loss':    -0.1920, 'eps_e':     1.0000})
Step:   75000, Reward:    60.204 [   0.000], Avg:   169.530 (1.000) <0-00:57:34> ({'r_t':   129.4100, 'eps':     1.0000, 'critic_loss':     0.5778, 'actor_loss':    -0.6598, 'alpha_loss':     0.5668, 'eps_e':     1.0000})
Step:   76000, Reward:    87.675 [   0.000], Avg:   168.467 (1.000) <0-00:58:18> ({'r_t':   248.6172, 'eps':     1.0000, 'critic_loss':     0.5531, 'actor_loss':    -0.7741, 'alpha_loss':     1.1490, 'eps_e':     1.0000})
Step:   77000, Reward:    93.744 [   0.000], Avg:   167.509 (1.000) <0-00:59:01> ({'r_t':   286.1376, 'eps':     1.0000, 'critic_loss':     0.5358, 'actor_loss':    -1.0587, 'alpha_loss':     2.2570, 'eps_e':     1.0000})
Step:   78000, Reward:    91.555 [   0.000], Avg:   166.547 (1.000) <0-00:59:42> ({'r_t':   318.1156, 'eps':     1.0000, 'critic_loss':     0.5416, 'actor_loss':    -1.4106, 'alpha_loss':     2.1686, 'eps_e':     1.0000})
Step:   79000, Reward:    84.254 [   0.000], Avg:   165.519 (1.000) <0-01:00:23> ({'r_t':   331.2608, 'eps':     1.0000, 'critic_loss':     0.4852, 'actor_loss':    -1.3599, 'alpha_loss':     1.4897, 'eps_e':     1.0000})
Step:   80000, Reward:    96.291 [   0.000], Avg:   164.664 (1.000) <0-01:01:04> ({'r_t':   371.6097, 'eps':     1.0000, 'critic_loss':     0.4862, 'actor_loss':    -1.0625, 'alpha_loss':     0.6256, 'eps_e':     1.0000})
Step:   81000, Reward:    94.139 [   0.000], Avg:   163.804 (1.000) <0-01:01:45> ({'r_t':   372.6221, 'eps':     1.0000, 'critic_loss':     0.5243, 'actor_loss':    -0.8212, 'alpha_loss':     0.2883, 'eps_e':     1.0000})
Step:   82000, Reward:    95.172 [   0.000], Avg:   162.977 (1.000) <0-01:02:26> ({'r_t':   389.4593, 'eps':     1.0000, 'critic_loss':     0.5622, 'actor_loss':    -0.7423, 'alpha_loss':     0.1361, 'eps_e':     1.0000})
Step:   83000, Reward:    96.155 [   0.000], Avg:   162.182 (1.000) <0-01:03:06> ({'r_t':   419.3156, 'eps':     1.0000, 'critic_loss':     0.5691, 'actor_loss':    -0.7145, 'alpha_loss':    -0.0218, 'eps_e':     1.0000})
Step:   84000, Reward:    92.927 [   0.000], Avg:   161.367 (1.000) <0-01:03:46> ({'r_t':   434.6919, 'eps':     1.0000, 'critic_loss':     0.5739, 'actor_loss':    -0.7265, 'alpha_loss':     0.1008, 'eps_e':     1.0000})
Step:   85000, Reward:    93.327 [   0.000], Avg:   160.576 (1.000) <0-01:04:25> ({'r_t':   420.8083, 'eps':     1.0000, 'critic_loss':     0.6067, 'actor_loss':    -0.7613, 'alpha_loss':     0.0350, 'eps_e':     1.0000})
Step:   86000, Reward:    83.299 [   0.000], Avg:   159.687 (1.000) <0-01:05:04> ({'r_t':   406.7324, 'eps':     1.0000, 'critic_loss':     0.6402, 'actor_loss':    -0.6643, 'alpha_loss':    -0.1659, 'eps_e':     1.0000})
Step:   87000, Reward:    26.486 [   0.000], Avg:   158.174 (1.000) <0-01:05:43> ({'r_t':   340.1762, 'eps':     1.0000, 'critic_loss':     0.7363, 'actor_loss':    -0.5679, 'alpha_loss':    -0.0998, 'eps_e':     1.0000})
Step:   88000, Reward:    83.426 [   0.000], Avg:   157.334 (1.000) <0-01:06:23> ({'r_t':   330.1438, 'eps':     1.0000, 'critic_loss':     0.7752, 'actor_loss':    -0.5625, 'alpha_loss':    -0.1069, 'eps_e':     1.0000})
Step:   89000, Reward:    81.601 [   0.000], Avg:   156.492 (1.000) <0-01:07:02> ({'r_t':   345.5970, 'eps':     1.0000, 'critic_loss':     0.8144, 'actor_loss':    -0.5061, 'alpha_loss':    -0.0232, 'eps_e':     1.0000})
Step:   90000, Reward:    80.761 [   0.000], Avg:   155.660 (1.000) <0-01:07:40> ({'r_t':   376.0052, 'eps':     1.0000, 'critic_loss':     1.0099, 'actor_loss':    -0.5112, 'alpha_loss':    -0.1105, 'eps_e':     1.0000})
Step:   91000, Reward:    39.975 [   0.000], Avg:   154.403 (1.000) <0-01:08:20> ({'r_t':   393.1132, 'eps':     1.0000, 'critic_loss':     1.2050, 'actor_loss':    -0.4892, 'alpha_loss':    -0.3195, 'eps_e':     1.0000})
Step:   92000, Reward:    86.082 [   0.000], Avg:   153.668 (1.000) <0-01:08:59> ({'r_t':   395.5582, 'eps':     1.0000, 'critic_loss':     1.7342, 'actor_loss':    -0.5361, 'alpha_loss':    -0.0775, 'eps_e':     1.0000})
Step:   93000, Reward:    92.097 [   0.000], Avg:   153.013 (1.000) <0-01:09:39> ({'r_t':   418.3139, 'eps':     1.0000, 'critic_loss':     1.8173, 'actor_loss':    -0.6089, 'alpha_loss':     0.1592, 'eps_e':     1.0000})
Step:   94000, Reward:    49.496 [   0.000], Avg:   151.924 (1.000) <0-01:10:21> ({'r_t':   400.2371, 'eps':     1.0000, 'critic_loss':     1.9263, 'actor_loss':    -0.7482, 'alpha_loss':     0.6339, 'eps_e':     1.0000})
Step:   95000, Reward:    94.745 [   0.000], Avg:   151.328 (1.000) <0-01:11:03> ({'r_t':   219.5078, 'eps':     1.0000, 'critic_loss':     2.2510, 'actor_loss':    -0.9198, 'alpha_loss':     2.2727, 'eps_e':     1.0000})
Step:   96000, Reward:    58.144 [   0.000], Avg:   150.367 (1.000) <0-01:11:43> ({'r_t':   312.6806, 'eps':     1.0000, 'critic_loss':     2.1865, 'actor_loss':    -0.9288, 'alpha_loss':     1.9650, 'eps_e':     1.0000})
Step:   97000, Reward:    95.608 [   0.000], Avg:   149.808 (1.000) <0-01:12:34> ({'r_t':   282.9615, 'eps':     1.0000, 'critic_loss':     2.4016, 'actor_loss':    -0.8834, 'alpha_loss':     1.7085, 'eps_e':     1.0000})
Step:   98000, Reward:   116.160 [   0.000], Avg:   149.469 (1.000) <0-01:13:15> ({'r_t':   176.6945, 'eps':     1.0000, 'critic_loss':     2.3563, 'actor_loss':    -0.7553, 'alpha_loss':     1.2179, 'eps_e':     1.0000})
Step:   99000, Reward:   120.394 [   0.000], Avg:   149.178 (1.000) <0-01:13:56> ({'r_t':   439.1999, 'eps':     1.0000, 'critic_loss':     2.3199, 'actor_loss':    -0.8298, 'alpha_loss':     0.8615, 'eps_e':     1.0000})
Step:  100000, Reward:   105.765 [   0.000], Avg:   148.748 (1.000) <0-01:14:37> ({'r_t':   394.9920, 'eps':     1.0000, 'critic_loss':     2.1771, 'actor_loss':    -0.9131, 'alpha_loss':     0.8597, 'eps_e':     1.0000})
Step:  101000, Reward:   110.690 [   0.000], Avg:   148.375 (1.000) <0-01:15:17> ({'r_t':   426.8176, 'eps':     1.0000, 'critic_loss':     2.2371, 'actor_loss':    -0.9094, 'alpha_loss':     0.6215, 'eps_e':     1.0000})
Step:  102000, Reward:   126.405 [   0.000], Avg:   148.162 (1.000) <0-01:16:00> ({'r_t':   408.4706, 'eps':     1.0000, 'critic_loss':     2.3187, 'actor_loss':    -0.7009, 'alpha_loss':     0.0255, 'eps_e':     1.0000})
Step:  103000, Reward:   100.812 [   0.000], Avg:   147.706 (1.000) <0-01:16:42> ({'r_t':   382.9204, 'eps':     1.0000, 'critic_loss':     2.2648, 'actor_loss':    -0.6108, 'alpha_loss':    -0.3672, 'eps_e':     1.0000})
Step:  104000, Reward:   100.437 [   0.000], Avg:   147.256 (1.000) <0-01:17:36> ({'r_t':   324.6220, 'eps':     1.0000, 'critic_loss':     1.9626, 'actor_loss':    -0.6267, 'alpha_loss':    -0.6205, 'eps_e':     1.0000})
Step:  105000, Reward:    62.825 [   0.000], Avg:   146.460 (1.000) <0-01:18:42> ({'r_t':    95.2407, 'eps':     1.0000, 'critic_loss':     1.9045, 'actor_loss':    -0.5770, 'alpha_loss':    -0.7456, 'eps_e':     1.0000})
Step:  106000, Reward:    98.291 [   0.000], Avg:   146.009 (1.000) <0-01:19:40> ({'r_t':   179.9076, 'eps':     1.0000, 'critic_loss':     1.7664, 'actor_loss':    -0.5001, 'alpha_loss':    -1.2256, 'eps_e':     1.0000})
Step:  107000, Reward:   105.577 [   0.000], Avg:   145.635 (1.000) <0-01:20:32> ({'r_t':   202.4128, 'eps':     1.0000, 'critic_loss':     1.5032, 'actor_loss':    -0.5137, 'alpha_loss':    -1.3495, 'eps_e':     1.0000})
Step:  108000, Reward:    84.652 [   0.000], Avg:   145.076 (1.000) <0-01:21:38> ({'r_t':   100.9741, 'eps':     1.0000, 'critic_loss':     1.2798, 'actor_loss':    -0.5217, 'alpha_loss':    -1.4895, 'eps_e':     1.0000})
Step:  109000, Reward:    88.453 [   0.000], Avg:   144.561 (1.000) <0-01:22:38> ({'r_t':   142.8027, 'eps':     1.0000, 'critic_loss':     0.7716, 'actor_loss':    -0.6490, 'alpha_loss':    -1.1143, 'eps_e':     1.0000})
Step:  110000, Reward:   101.610 [   0.000], Avg:   144.174 (1.000) <0-01:23:28> ({'r_t':   159.2759, 'eps':     1.0000, 'critic_loss':     0.6859, 'actor_loss':    -0.8189, 'alpha_loss':     0.8271, 'eps_e':     1.0000})
Step:  111000, Reward:   111.549 [   0.000], Avg:   143.883 (1.000) <0-01:24:34> ({'r_t':    95.6547, 'eps':     1.0000, 'critic_loss':     0.5906, 'actor_loss':    -0.7778, 'alpha_loss':     1.0535, 'eps_e':     1.0000})
Step:  112000, Reward:    93.312 [   0.000], Avg:   143.435 (1.000) <0-01:25:28> ({'r_t':    57.3301, 'eps':     1.0000, 'critic_loss':     0.6297, 'actor_loss':    -0.5972, 'alpha_loss':    -0.2682, 'eps_e':     1.0000})
Step:  113000, Reward:   104.528 [   0.000], Avg:   143.094 (1.000) <0-01:26:12> ({'r_t':   120.3664, 'eps':     1.0000, 'critic_loss':     0.8042, 'actor_loss':    -0.5510, 'alpha_loss':    -1.0627, 'eps_e':     1.0000})
Step:  114000, Reward:   103.053 [   0.000], Avg:   142.746 (1.000) <0-01:26:56> ({'r_t':   261.6854, 'eps':     1.0000, 'critic_loss':     0.7999, 'actor_loss':    -0.6216, 'alpha_loss':    -0.8543, 'eps_e':     1.0000})
Step:  115000, Reward:   108.860 [   0.000], Avg:   142.453 (1.000) <0-01:27:41> ({'r_t':   348.4656, 'eps':     1.0000, 'critic_loss':     0.8207, 'actor_loss':    -0.6050, 'alpha_loss':    -0.6426, 'eps_e':     1.0000})
Step:  116000, Reward:   106.717 [   0.000], Avg:   142.148 (1.000) <0-01:28:24> ({'r_t':   280.7520, 'eps':     1.0000, 'critic_loss':     0.8422, 'actor_loss':    -0.5133, 'alpha_loss':    -0.5537, 'eps_e':     1.0000})
Step:  117000, Reward:   103.223 [   0.000], Avg:   141.818 (1.000) <0-01:29:09> ({'r_t':   288.6467, 'eps':     1.0000, 'critic_loss':     0.8286, 'actor_loss':    -0.4643, 'alpha_loss':    -0.4778, 'eps_e':     1.0000})
Step:  118000, Reward:   103.083 [   0.000], Avg:   141.493 (1.000) <0-01:29:56> ({'r_t':   230.6304, 'eps':     1.0000, 'critic_loss':     0.8885, 'actor_loss':    -0.3972, 'alpha_loss':     0.3331, 'eps_e':     1.0000})
Step:  119000, Reward:   108.040 [   0.000], Avg:   141.214 (1.000) <0-01:30:42> ({'r_t':   187.9349, 'eps':     1.0000, 'critic_loss':     1.0201, 'actor_loss':    -0.3128, 'alpha_loss':     0.3953, 'eps_e':     1.0000})
Step:  120000, Reward:    68.842 [   0.000], Avg:   140.616 (1.000) <0-01:31:48> ({'r_t':   181.5666, 'eps':     1.0000, 'critic_loss':     0.9513, 'actor_loss':    -0.3916, 'alpha_loss':     0.4096, 'eps_e':     1.0000})
Step:  121000, Reward:   113.965 [   0.000], Avg:   140.397 (1.000) <0-01:32:35> ({'r_t':   198.8716, 'eps':     1.0000, 'critic_loss':     0.9423, 'actor_loss':    -0.3145, 'alpha_loss':    -0.2825, 'eps_e':     1.0000})
Step:  122000, Reward:    94.884 [   0.000], Avg:   140.027 (1.000) <0-01:33:41> ({'r_t':   240.4356, 'eps':     1.0000, 'critic_loss':     0.9691, 'actor_loss':    -0.3257, 'alpha_loss':    -0.7524, 'eps_e':     1.0000})
Step:  123000, Reward:   106.050 [   0.000], Avg:   139.753 (1.000) <0-01:34:47> ({'r_t':   167.5365, 'eps':     1.0000, 'critic_loss':     0.9570, 'actor_loss':    -0.4157, 'alpha_loss':    -1.0919, 'eps_e':     1.0000})
Step:  124000, Reward:    97.456 [   0.000], Avg:   139.415 (1.000) <0-01:35:29> ({'r_t':   317.7598, 'eps':     1.0000, 'critic_loss':     0.9070, 'actor_loss':    -0.4209, 'alpha_loss':    -1.3910, 'eps_e':     1.0000})
Step:  125000, Reward:    97.457 [   0.000], Avg:   139.082 (1.000) <0-01:36:11> ({'r_t':   343.4507, 'eps':     1.0000, 'critic_loss':     0.8567, 'actor_loss':    -0.3806, 'alpha_loss':    -0.8988, 'eps_e':     1.0000})
Step:  126000, Reward:   109.692 [   0.000], Avg:   138.850 (1.000) <0-01:36:53> ({'r_t':   400.5829, 'eps':     1.0000, 'critic_loss':     1.0172, 'actor_loss':    -0.3502, 'alpha_loss':    -0.3554, 'eps_e':     1.0000})
Step:  127000, Reward:   114.529 [   0.000], Avg:   138.660 (1.000) <0-01:37:36> ({'r_t':   436.5047, 'eps':     1.0000, 'critic_loss':     1.1560, 'actor_loss':    -0.3274, 'alpha_loss':     0.0107, 'eps_e':     1.0000})
Step:  128000, Reward:    95.668 [   0.000], Avg:   138.327 (1.000) <0-01:38:16> ({'r_t':   429.0482, 'eps':     1.0000, 'critic_loss':     1.2789, 'actor_loss':    -0.3571, 'alpha_loss':     0.6517, 'eps_e':     1.0000})
Step:  129000, Reward:   105.712 [   0.000], Avg:   138.076 (1.000) <0-01:38:56> ({'r_t':   469.7066, 'eps':     1.0000, 'critic_loss':     1.2871, 'actor_loss':    -0.3840, 'alpha_loss':     1.0245, 'eps_e':     1.0000})
Step:  130000, Reward:   111.543 [   0.000], Avg:   137.874 (1.000) <0-01:39:36> ({'r_t':   436.4791, 'eps':     1.0000, 'critic_loss':     1.2940, 'actor_loss':    -0.4117, 'alpha_loss':     1.4947, 'eps_e':     1.0000})
Step:  131000, Reward:   103.410 [   0.000], Avg:   137.613 (1.000) <0-01:40:18> ({'r_t':   459.0775, 'eps':     1.0000, 'critic_loss':     1.3314, 'actor_loss':    -0.3702, 'alpha_loss':     1.4358, 'eps_e':     1.0000})
Step:  132000, Reward:    87.966 [   0.000], Avg:   137.239 (1.000) <0-01:41:12> ({'r_t':   249.9207, 'eps':     1.0000, 'critic_loss':     1.2037, 'actor_loss':    -0.3647, 'alpha_loss':     1.1969, 'eps_e':     1.0000})
Step:  133000, Reward:    48.110 [   0.000], Avg:   136.574 (1.000) <0-01:42:11> ({'r_t':   117.8473, 'eps':     1.0000, 'critic_loss':     1.0900, 'actor_loss':    -0.3805, 'alpha_loss':     0.7775, 'eps_e':     1.0000})
Step:  134000, Reward:    93.001 [   0.000], Avg:   136.251 (1.000) <0-01:43:06> ({'r_t':   181.6244, 'eps':     1.0000, 'critic_loss':     1.0067, 'actor_loss':    -0.3891, 'alpha_loss':     0.0686, 'eps_e':     1.0000})
Step:  135000, Reward:   100.389 [   0.000], Avg:   135.988 (1.000) <0-01:43:59> ({'r_t':   185.5905, 'eps':     1.0000, 'critic_loss':     0.9418, 'actor_loss':    -0.4065, 'alpha_loss':    -0.5027, 'eps_e':     1.0000})
Step:  136000, Reward:    36.656 [   0.000], Avg:   135.263 (1.000) <0-01:45:05> ({'r_t':    65.7719, 'eps':     1.0000, 'critic_loss':     0.8209, 'actor_loss':    -0.4079, 'alpha_loss':    -1.1974, 'eps_e':     1.0000})
Step:  137000, Reward:   117.967 [   0.000], Avg:   135.137 (1.000) <0-01:45:47> ({'r_t':   252.1587, 'eps':     1.0000, 'critic_loss':     0.7161, 'actor_loss':    -0.3579, 'alpha_loss':    -1.2869, 'eps_e':     1.0000})
Step:  138000, Reward:   110.805 [   0.000], Avg:   134.962 (1.000) <0-01:46:34> ({'r_t':   397.9269, 'eps':     1.0000, 'critic_loss':     0.7001, 'actor_loss':    -0.4540, 'alpha_loss':    -0.7855, 'eps_e':     1.0000})
Step:  139000, Reward:   119.302 [   0.000], Avg:   134.850 (1.000) <0-01:47:19> ({'r_t':   312.6748, 'eps':     1.0000, 'critic_loss':     0.6934, 'actor_loss':    -0.4642, 'alpha_loss':     0.0608, 'eps_e':     1.0000})
Step:  140000, Reward:    52.420 [   0.000], Avg:   134.266 (1.000) <0-01:48:02> ({'r_t':   198.4732, 'eps':     1.0000, 'critic_loss':     1.3490, 'actor_loss':    -0.4143, 'alpha_loss':     1.1186, 'eps_e':     1.0000})
Step:  141000, Reward:    69.396 [   0.000], Avg:   133.809 (1.000) <0-01:49:08> ({'r_t':   -14.8317, 'eps':     1.0000, 'critic_loss':     1.6751, 'actor_loss':    -0.3099, 'alpha_loss':     1.1751, 'eps_e':     1.0000})
Step:  142000, Reward:    96.645 [   0.000], Avg:   133.549 (1.000) <0-01:50:14> ({'r_t':    48.7047, 'eps':     1.0000, 'critic_loss':     1.6727, 'actor_loss':    -0.3703, 'alpha_loss':     1.3203, 'eps_e':     1.0000})
Step:  143000, Reward:    94.648 [   0.000], Avg:   133.279 (1.000) <0-01:51:20> ({'r_t':    93.7098, 'eps':     1.0000, 'critic_loss':     1.5370, 'actor_loss':    -0.3733, 'alpha_loss':     0.5934, 'eps_e':     1.0000})
Step:  144000, Reward:   109.925 [   0.000], Avg:   133.118 (1.000) <0-01:52:26> ({'r_t':   103.0527, 'eps':     1.0000, 'critic_loss':     1.6201, 'actor_loss':    -0.4011, 'alpha_loss':     0.0253, 'eps_e':     1.0000})
Step:  145000, Reward:   103.303 [   0.000], Avg:   132.914 (1.000) <0-01:53:32> ({'r_t':    55.3026, 'eps':     1.0000, 'critic_loss':     1.4912, 'actor_loss':    -0.4254, 'alpha_loss':    -0.7067, 'eps_e':     1.0000})
Step:  146000, Reward:   112.478 [   0.000], Avg:   132.775 (1.000) <0-01:54:21> ({'r_t':   234.6946, 'eps':     1.0000, 'critic_loss':     1.5129, 'actor_loss':    -0.4654, 'alpha_loss':    -1.6318, 'eps_e':     1.0000})
Step:  147000, Reward:   115.654 [   0.000], Avg:   132.659 (1.000) <0-01:55:06> ({'r_t':   250.4401, 'eps':     1.0000, 'critic_loss':     0.9910, 'actor_loss':    -0.4645, 'alpha_loss':    -1.3891, 'eps_e':     1.0000})
Step:  148000, Reward:   112.482 [   0.000], Avg:   132.524 (1.000) <0-01:55:52> ({'r_t':   276.6978, 'eps':     1.0000, 'critic_loss':     0.7435, 'actor_loss':    -0.5027, 'alpha_loss':    -0.6040, 'eps_e':     1.0000})
Step:  149000, Reward:   129.564 [   0.000], Avg:   132.504 (1.000) <0-01:56:35> ({'r_t':   332.1385, 'eps':     1.0000, 'critic_loss':     0.7570, 'actor_loss':    -0.4135, 'alpha_loss':    -0.1947, 'eps_e':     1.0000})
Step:  150000, Reward:   127.624 [   0.000], Avg:   132.472 (1.000) <0-01:57:19> ({'r_t':   371.4191, 'eps':     1.0000, 'critic_loss':     0.7305, 'actor_loss':    -0.3174, 'alpha_loss':     0.3985, 'eps_e':     1.0000})
Step:  151000, Reward:   130.033 [   0.000], Avg:   132.456 (1.000) <0-01:58:02> ({'r_t':   388.1169, 'eps':     1.0000, 'critic_loss':     0.7414, 'actor_loss':    -0.2421, 'alpha_loss':     0.8175, 'eps_e':     1.0000})
Step:  152000, Reward:   127.011 [   0.000], Avg:   132.420 (1.000) <0-01:58:48> ({'r_t':   394.7791, 'eps':     1.0000, 'critic_loss':     0.6972, 'actor_loss':    -0.2554, 'alpha_loss':     1.2152, 'eps_e':     1.0000})
Step:  153000, Reward:   101.446 [   0.000], Avg:   132.219 (1.000) <0-01:59:54> ({'r_t':   242.3695, 'eps':     1.0000, 'critic_loss':     0.7030, 'actor_loss':    -0.2839, 'alpha_loss':     1.0508, 'eps_e':     1.0000})
Step:  154000, Reward:   106.514 [   0.000], Avg:   132.053 (1.000) <0-02:01:00> ({'r_t':    -7.1760, 'eps':     1.0000, 'critic_loss':     0.7981, 'actor_loss':    -0.3155, 'alpha_loss':     0.1457, 'eps_e':     1.0000})
Step:  155000, Reward:    90.479 [   0.000], Avg:   131.786 (1.000) <0-02:01:59> ({'r_t':    89.2406, 'eps':     1.0000, 'critic_loss':     1.0376, 'actor_loss':    -0.3305, 'alpha_loss':    -0.3456, 'eps_e':     1.0000})
Step:  156000, Reward:   134.217 [   0.000], Avg:   131.802 (1.000) <0-02:02:44> ({'r_t':   247.3190, 'eps':     1.0000, 'critic_loss':     1.0908, 'actor_loss':    -0.4881, 'alpha_loss':     0.2282, 'eps_e':     1.0000})
Step:  157000, Reward:   131.676 [   0.000], Avg:   131.801 (1.000) <0-02:03:34> ({'r_t':   242.3888, 'eps':     1.0000, 'critic_loss':     1.0462, 'actor_loss':    -0.6555, 'alpha_loss':     0.7167, 'eps_e':     1.0000})
Step:  158000, Reward:   110.410 [   0.000], Avg:   131.667 (1.000) <0-02:04:20> ({'r_t':   255.7970, 'eps':     1.0000, 'critic_loss':     1.1172, 'actor_loss':    -0.6026, 'alpha_loss':     0.5676, 'eps_e':     1.0000})
Step:  159000, Reward:   136.097 [   0.000], Avg:   131.694 (1.000) <0-02:05:06> ({'r_t':   369.3344, 'eps':     1.0000, 'critic_loss':     1.2931, 'actor_loss':    -0.4965, 'alpha_loss':     0.2329, 'eps_e':     1.0000})
Step:  160000, Reward:   147.612 [   0.000], Avg:   131.793 (1.000) <0-02:05:51> ({'r_t':   434.1231, 'eps':     1.0000, 'critic_loss':     1.1471, 'actor_loss':    -0.3459, 'alpha_loss':    -0.2571, 'eps_e':     1.0000})
Step:  161000, Reward:   143.696 [   0.000], Avg:   131.867 (1.000) <0-02:06:34> ({'r_t':   310.5975, 'eps':     1.0000, 'critic_loss':     1.6709, 'actor_loss':    -0.2279, 'alpha_loss':    -0.2204, 'eps_e':     1.0000})
Step:  162000, Reward:   130.181 [   0.000], Avg:   131.856 (1.000) <0-02:07:20> ({'r_t':   392.3986, 'eps':     1.0000, 'critic_loss':     1.9758, 'actor_loss':    -0.1836, 'alpha_loss':    -0.1079, 'eps_e':     1.0000})
Step:  163000, Reward:   136.944 [   0.000], Avg:   131.887 (1.000) <0-02:08:11> ({'r_t':   363.4806, 'eps':     1.0000, 'critic_loss':     1.9336, 'actor_loss':    -0.1558, 'alpha_loss':     0.2433, 'eps_e':     1.0000})
Step:  164000, Reward:   129.575 [   0.000], Avg:   131.873 (1.000) <0-02:08:57> ({'r_t':   324.1456, 'eps':     1.0000, 'critic_loss':     2.2528, 'actor_loss':    -0.1487, 'alpha_loss':     0.1215, 'eps_e':     1.0000})
Step:  165000, Reward:   135.786 [   0.000], Avg:   131.897 (1.000) <0-02:09:42> ({'r_t':   398.5859, 'eps':     1.0000, 'critic_loss':     2.0825, 'actor_loss':    -0.1259, 'alpha_loss':     0.5068, 'eps_e':     1.0000})
Step:  166000, Reward:   144.565 [   0.000], Avg:   131.973 (1.000) <0-02:10:27> ({'r_t':   355.6605, 'eps':     1.0000, 'critic_loss':     1.8728, 'actor_loss':    -0.1067, 'alpha_loss':     0.1917, 'eps_e':     1.0000})
Step:  167000, Reward:   132.836 [   0.000], Avg:   131.978 (1.000) <0-02:11:12> ({'r_t':   330.5917, 'eps':     1.0000, 'critic_loss':     1.5107, 'actor_loss':    -0.0910, 'alpha_loss':     0.0713, 'eps_e':     1.0000})
Step:  168000, Reward:   136.739 [   0.000], Avg:   132.006 (1.000) <0-02:11:57> ({'r_t':   291.9910, 'eps':     1.0000, 'critic_loss':     0.4989, 'actor_loss':    -0.0645, 'alpha_loss':     0.0221, 'eps_e':     1.0000})
Step:  169000, Reward:   108.894 [   0.000], Avg:   131.870 (1.000) <0-02:12:47> ({'r_t':   289.6571, 'eps':     1.0000, 'critic_loss':     0.5042, 'actor_loss':    -0.0998, 'alpha_loss':     0.0063, 'eps_e':     1.0000})
Step:  170000, Reward:   124.297 [   0.000], Avg:   131.826 (1.000) <0-02:13:32> ({'r_t':   343.7121, 'eps':     1.0000, 'critic_loss':     0.8270, 'actor_loss':    -0.0937, 'alpha_loss':    -0.1364, 'eps_e':     1.0000})
Step:  171000, Reward:   142.624 [   0.000], Avg:   131.889 (1.000) <0-02:14:14> ({'r_t':   374.2338, 'eps':     1.0000, 'critic_loss':     0.7654, 'actor_loss':    -0.0889, 'alpha_loss':    -0.0768, 'eps_e':     1.0000})
Step:  172000, Reward:   130.062 [   0.000], Avg:   131.878 (1.000) <0-02:14:55> ({'r_t':   470.8544, 'eps':     1.0000, 'critic_loss':     0.5964, 'actor_loss':    -0.0891, 'alpha_loss':     0.2597, 'eps_e':     1.0000})
Step:  173000, Reward:   129.178 [   0.000], Avg:   131.863 (1.000) <0-02:15:35> ({'r_t':   415.5758, 'eps':     1.0000, 'critic_loss':     0.6755, 'actor_loss':    -0.0720, 'alpha_loss':     0.3794, 'eps_e':     1.0000})
Step:  174000, Reward:   139.735 [   0.000], Avg:   131.908 (1.000) <0-02:16:22> ({'r_t':   381.1243, 'eps':     1.0000, 'critic_loss':     0.7054, 'actor_loss':    -0.0772, 'alpha_loss':     0.3554, 'eps_e':     1.0000})
Step:  175000, Reward:   134.702 [   0.000], Avg:   131.923 (1.000) <0-02:17:05> ({'r_t':   379.1017, 'eps':     1.0000, 'critic_loss':     0.7774, 'actor_loss':    -0.1072, 'alpha_loss':     0.0488, 'eps_e':     1.0000})
Step:  176000, Reward:   136.627 [   0.000], Avg:   131.950 (1.000) <0-02:17:53> ({'r_t':   317.4471, 'eps':     1.0000, 'critic_loss':     0.7594, 'actor_loss':    -0.0731, 'alpha_loss':     0.1151, 'eps_e':     1.0000})
Step:  177000, Reward:   144.802 [   0.000], Avg:   132.022 (1.000) <0-02:18:41> ({'r_t':   299.9473, 'eps':     1.0000, 'critic_loss':     0.7115, 'actor_loss':    -0.0606, 'alpha_loss':     0.0797, 'eps_e':     1.0000})
Step:  178000, Reward:   134.401 [   0.000], Avg:   132.035 (1.000) <0-02:19:37> ({'r_t':   215.4668, 'eps':     1.0000, 'critic_loss':     0.7452, 'actor_loss':    -0.0556, 'alpha_loss':    -0.0995, 'eps_e':     1.0000})
Step:  179000, Reward:   149.416 [   0.000], Avg:   132.132 (1.000) <0-02:20:25> ({'r_t':   270.6191, 'eps':     1.0000, 'critic_loss':     0.6445, 'actor_loss':    -0.0469, 'alpha_loss':    -0.2084, 'eps_e':     1.0000})
Step:  180000, Reward:   151.243 [   0.000], Avg:   132.238 (1.000) <0-02:21:08> ({'r_t':   385.9814, 'eps':     1.0000, 'critic_loss':     0.5300, 'actor_loss':    -0.0468, 'alpha_loss':    -0.0997, 'eps_e':     1.0000})
Step:  181000, Reward:   145.717 [   0.000], Avg:   132.312 (1.000) <0-02:21:52> ({'r_t':   362.1984, 'eps':     1.0000, 'critic_loss':     0.5577, 'actor_loss':    -0.0569, 'alpha_loss':    -0.0661, 'eps_e':     1.0000})
Step:  182000, Reward:   152.995 [   0.000], Avg:   132.425 (1.000) <0-02:22:41> ({'r_t':   415.2135, 'eps':     1.0000, 'critic_loss':     0.5466, 'actor_loss':    -0.0560, 'alpha_loss':    -0.0845, 'eps_e':     1.0000})
Step:  183000, Reward:   143.205 [   0.000], Avg:   132.483 (1.000) <0-02:23:27> ({'r_t':   398.4735, 'eps':     1.0000, 'critic_loss':     0.5567, 'actor_loss':    -0.0493, 'alpha_loss':    -0.3458, 'eps_e':     1.0000})
Step:  184000, Reward:   139.662 [   0.000], Avg:   132.522 (1.000) <0-02:24:19> ({'r_t':   362.8208, 'eps':     1.0000, 'critic_loss':     0.5644, 'actor_loss':    -0.0759, 'alpha_loss':    -0.1752, 'eps_e':     1.0000})
Step:  185000, Reward:   143.468 [   0.000], Avg:   132.581 (1.000) <0-02:25:00> ({'r_t':   328.2393, 'eps':     1.0000, 'critic_loss':     0.6030, 'actor_loss':    -0.0948, 'alpha_loss':    -0.1478, 'eps_e':     1.0000})
Step:  186000, Reward:   138.448 [   0.000], Avg:   132.612 (1.000) <0-02:25:48> ({'r_t':   282.1545, 'eps':     1.0000, 'critic_loss':     0.6020, 'actor_loss':    -0.0864, 'alpha_loss':    -0.2716, 'eps_e':     1.0000})
Step:  187000, Reward:   131.483 [   0.000], Avg:   132.606 (1.000) <0-02:26:36> ({'r_t':   238.3360, 'eps':     1.0000, 'critic_loss':     0.5653, 'actor_loss':    -0.0947, 'alpha_loss':    -0.2934, 'eps_e':     1.0000})
Step:  188000, Reward:   156.764 [   0.000], Avg:   132.734 (1.000) <0-02:27:28> ({'r_t':   288.9040, 'eps':     1.0000, 'critic_loss':     0.6070, 'actor_loss':    -0.1613, 'alpha_loss':    -0.7421, 'eps_e':     1.0000})
Step:  189000, Reward:   156.508 [   0.000], Avg:   132.859 (1.000) <0-02:28:20> ({'r_t':   313.2215, 'eps':     1.0000, 'critic_loss':     0.6742, 'actor_loss':    -0.2769, 'alpha_loss':    -0.2676, 'eps_e':     1.0000})
Step:  190000, Reward:   172.761 [   0.000], Avg:   133.068 (1.000) <0-02:29:14> ({'r_t':   305.9562, 'eps':     1.0000, 'critic_loss':     0.7548, 'actor_loss':    -0.2277, 'alpha_loss':    -0.4629, 'eps_e':     1.0000})
Step:  191000, Reward:   156.442 [   0.000], Avg:   133.190 (1.000) <0-02:30:06> ({'r_t':   306.3385, 'eps':     1.0000, 'critic_loss':     0.8064, 'actor_loss':    -0.2089, 'alpha_loss':    -0.3667, 'eps_e':     1.0000})
Step:  192000, Reward:   171.038 [   0.000], Avg:   133.386 (1.000) <0-02:30:56> ({'r_t':   267.0686, 'eps':     1.0000, 'critic_loss':     0.8482, 'actor_loss':    -0.2353, 'alpha_loss':    -0.7188, 'eps_e':     1.0000})
Step:  193000, Reward:   166.456 [   0.000], Avg:   133.556 (1.000) <0-02:31:54> ({'r_t':   273.8772, 'eps':     1.0000, 'critic_loss':     0.9374, 'actor_loss':    -0.2474, 'alpha_loss':    -0.5387, 'eps_e':     1.0000})
Step:  194000, Reward:   170.633 [   0.000], Avg:   133.747 (1.000) <0-02:32:52> ({'r_t':   215.4640, 'eps':     1.0000, 'critic_loss':     0.9417, 'actor_loss':    -0.2807, 'alpha_loss':    -0.4412, 'eps_e':     1.0000})
Step:  195000, Reward:   177.105 [   0.000], Avg:   133.968 (1.000) <0-02:33:48> ({'r_t':   220.1886, 'eps':     1.0000, 'critic_loss':     0.8954, 'actor_loss':    -0.2909, 'alpha_loss':    -0.4784, 'eps_e':     1.0000})
Step:  196000, Reward:   167.078 [   0.000], Avg:   134.136 (1.000) <0-02:34:54> ({'r_t':   199.3659, 'eps':     1.0000, 'critic_loss':     0.8106, 'actor_loss':    -0.3395, 'alpha_loss':    -0.5707, 'eps_e':     1.0000})
Step:  197000, Reward:   143.375 [   0.000], Avg:   134.183 (1.000) <0-02:36:00> ({'r_t':   169.4777, 'eps':     1.0000, 'critic_loss':     0.7076, 'actor_loss':    -0.3557, 'alpha_loss':    -0.8590, 'eps_e':     1.0000})
Step:  198000, Reward:   145.418 [   0.000], Avg:   134.239 (1.000) <0-02:37:02> ({'r_t':   199.5313, 'eps':     1.0000, 'critic_loss':     0.5010, 'actor_loss':    -0.3536, 'alpha_loss':    -0.9558, 'eps_e':     1.0000})
Step:  199000, Reward:    90.446 [   0.000], Avg:   134.020 (1.000) <0-02:37:59> ({'r_t':   132.2175, 'eps':     1.0000, 'critic_loss':     0.3760, 'actor_loss':    -0.3404, 'alpha_loss':    -0.3178, 'eps_e':     1.0000})
Step:  200000, Reward:    86.635 [   0.000], Avg:   133.784 (1.000) <0-02:39:05> ({'r_t':   177.0108, 'eps':     1.0000, 'critic_loss':     0.4817, 'actor_loss':    -0.2623, 'alpha_loss':    -0.1335, 'eps_e':     1.0000})
Step:  201000, Reward:    88.564 [   0.000], Avg:   133.560 (1.000) <0-02:40:11> ({'r_t':   102.8351, 'eps':     1.0000, 'critic_loss':     0.6581, 'actor_loss':    -0.2091, 'alpha_loss':    -0.6517, 'eps_e':     1.0000})
Step:  202000, Reward:    65.447 [   0.000], Avg:   133.225 (1.000) <0-02:41:13> ({'r_t':    96.6160, 'eps':     1.0000, 'critic_loss':     0.7703, 'actor_loss':    -0.1971, 'alpha_loss':    -0.6352, 'eps_e':     1.0000})
Step:  203000, Reward:   138.225 [   0.000], Avg:   133.249 (1.000) <0-02:42:19> ({'r_t':   104.4149, 'eps':     1.0000, 'critic_loss':     0.8080, 'actor_loss':    -0.2010, 'alpha_loss':    -0.4044, 'eps_e':     1.0000})
Step:  204000, Reward:   131.581 [   0.000], Avg:   133.241 (1.000) <0-02:43:26> ({'r_t':   135.9495, 'eps':     1.0000, 'critic_loss':     0.6598, 'actor_loss':    -0.2495, 'alpha_loss':    -0.0978, 'eps_e':     1.0000})
Step:  205000, Reward:   114.569 [   0.000], Avg:   133.151 (1.000) <0-02:44:32> ({'r_t':   117.6178, 'eps':     1.0000, 'critic_loss':     0.8146, 'actor_loss':    -0.2318, 'alpha_loss':    -0.4323, 'eps_e':     1.0000})
Step:  206000, Reward:   150.851 [   0.000], Avg:   133.236 (1.000) <0-02:45:38> ({'r_t':   132.7424, 'eps':     1.0000, 'critic_loss':     1.0621, 'actor_loss':    -0.2129, 'alpha_loss':    -1.0166, 'eps_e':     1.0000})
Step:  207000, Reward:   158.545 [   0.000], Avg:   133.358 (1.000) <0-02:46:44> ({'r_t':   151.8551, 'eps':     1.0000, 'critic_loss':     0.6251, 'actor_loss':    -0.2166, 'alpha_loss':    -1.2970, 'eps_e':     1.0000})
Step:  208000, Reward:   154.125 [   0.000], Avg:   133.457 (1.000) <0-02:47:50> ({'r_t':   157.6065, 'eps':     1.0000, 'critic_loss':     0.4654, 'actor_loss':    -0.2148, 'alpha_loss':    -1.3902, 'eps_e':     1.0000})
Step:  209000, Reward:   131.162 [   0.000], Avg:   133.446 (1.000) <0-02:48:56> ({'r_t':   149.3388, 'eps':     1.0000, 'critic_loss':     0.5038, 'actor_loss':    -0.2143, 'alpha_loss':    -1.7624, 'eps_e':     1.0000})
Step:  210000, Reward:   154.454 [   0.000], Avg:   133.546 (1.000) <0-02:50:02> ({'r_t':   153.8610, 'eps':     1.0000, 'critic_loss':     0.9497, 'actor_loss':    -0.2187, 'alpha_loss':    -2.0756, 'eps_e':     1.0000})
Step:  211000, Reward:   140.951 [   0.000], Avg:   133.581 (1.000) <0-02:51:08> ({'r_t':   147.4261, 'eps':     1.0000, 'critic_loss':     0.7148, 'actor_loss':    -0.2330, 'alpha_loss':    -2.2836, 'eps_e':     1.0000})
Step:  212000, Reward:   146.118 [   0.000], Avg:   133.640 (1.000) <0-02:52:14> ({'r_t':   147.6490, 'eps':     1.0000, 'critic_loss':     0.8244, 'actor_loss':    -0.2547, 'alpha_loss':    -2.5829, 'eps_e':     1.0000})
Step:  213000, Reward:   161.216 [   0.000], Avg:   133.768 (1.000) <0-02:53:20> ({'r_t':   179.0735, 'eps':     1.0000, 'critic_loss':     1.3432, 'actor_loss':    -0.3223, 'alpha_loss':    -2.7127, 'eps_e':     1.0000})
Step:  214000, Reward:    63.260 [   0.000], Avg:   133.440 (1.000) <0-02:54:26> ({'r_t':   137.8464, 'eps':     1.0000, 'critic_loss':     1.7345, 'actor_loss':    -0.3042, 'alpha_loss':    -2.6168, 'eps_e':     1.0000})
Step:  215000, Reward:    46.936 [   0.000], Avg:   133.040 (1.000) <0-02:55:32> ({'r_t':    90.3943, 'eps':     1.0000, 'critic_loss':     1.5366, 'actor_loss':    -0.2903, 'alpha_loss':    -2.7765, 'eps_e':     1.0000})
Step:  216000, Reward:   174.500 [   0.000], Avg:   133.231 (1.000) <0-02:56:18> ({'r_t':   248.0795, 'eps':     1.0000, 'critic_loss':     1.3607, 'actor_loss':    -0.3900, 'alpha_loss':    -2.4181, 'eps_e':     1.0000})
Step:  217000, Reward:   123.085 [   0.000], Avg:   133.185 (1.000) <0-02:57:18> ({'r_t':   221.7311, 'eps':     1.0000, 'critic_loss':     1.2434, 'actor_loss':    -0.3274, 'alpha_loss':    -2.1467, 'eps_e':     1.0000})
Step:  218000, Reward:   138.441 [   0.000], Avg:   133.209 (1.000) <0-02:58:24> ({'r_t':   138.0226, 'eps':     1.0000, 'critic_loss':     1.0905, 'actor_loss':    -0.2725, 'alpha_loss':    -1.8704, 'eps_e':     1.0000})
Step:  219000, Reward:   152.002 [   0.000], Avg:   133.294 (1.000) <0-02:59:30> ({'r_t':   150.9885, 'eps':     1.0000, 'critic_loss':     0.8545, 'actor_loss':    -0.3106, 'alpha_loss':    -1.5879, 'eps_e':     1.0000})
Step:  220000, Reward:   152.126 [   0.000], Avg:   133.379 (1.000) <0-03:00:36> ({'r_t':   148.8484, 'eps':     1.0000, 'critic_loss':     0.5026, 'actor_loss':    -0.3278, 'alpha_loss':    -1.6579, 'eps_e':     1.0000})
Step:  221000, Reward:   159.704 [   0.000], Avg:   133.498 (1.000) <0-03:01:42> ({'r_t':   155.6751, 'eps':     1.0000, 'critic_loss':     0.4863, 'actor_loss':    -0.3015, 'alpha_loss':    -1.5470, 'eps_e':     1.0000})
Step:  222000, Reward:   160.955 [   0.000], Avg:   133.621 (1.000) <0-03:02:48> ({'r_t':   158.7762, 'eps':     1.0000, 'critic_loss':     0.4622, 'actor_loss':    -0.3300, 'alpha_loss':    -1.3752, 'eps_e':     1.0000})
Step:  223000, Reward:   165.792 [   0.000], Avg:   133.765 (1.000) <0-03:03:54> ({'r_t':   165.6493, 'eps':     1.0000, 'critic_loss':     0.4318, 'actor_loss':    -0.3094, 'alpha_loss':    -1.0918, 'eps_e':     1.0000})
Step:  224000, Reward:   177.193 [   0.000], Avg:   133.958 (1.000) <0-03:05:00> ({'r_t':   177.2546, 'eps':     1.0000, 'critic_loss':     0.4099, 'actor_loss':    -0.3591, 'alpha_loss':    -0.5307, 'eps_e':     1.0000})
Step:  225000, Reward:    71.793 [   0.000], Avg:   133.682 (1.000) <0-03:06:01> ({'r_t':   200.9975, 'eps':     1.0000, 'critic_loss':     0.4785, 'actor_loss':    -0.4194, 'alpha_loss':    -0.0708, 'eps_e':     1.0000})
Step:  226000, Reward:   133.429 [   0.000], Avg:   133.681 (1.000) <0-03:06:50> ({'r_t':   184.9474, 'eps':     1.0000, 'critic_loss':     2.0215, 'actor_loss':    -0.5272, 'alpha_loss':     0.6126, 'eps_e':     1.0000})
Step:  227000, Reward:   126.466 [   0.000], Avg:   133.650 (1.000) <0-03:07:40> ({'r_t':   228.7683, 'eps':     1.0000, 'critic_loss':     2.7711, 'actor_loss':    -0.5323, 'alpha_loss':     0.5047, 'eps_e':     1.0000})
Step:  228000, Reward:   -45.780 [   0.000], Avg:   132.866 (1.000) <0-03:08:37> ({'r_t':   166.4680, 'eps':     1.0000, 'critic_loss':     7.0517, 'actor_loss':    -0.6801, 'alpha_loss':     2.0508, 'eps_e':     1.0000})
Step:  229000, Reward:    50.350 [   0.000], Avg:   132.507 (1.000) <0-03:09:30> ({'r_t':    74.1644, 'eps':     1.0000, 'critic_loss':    10.5703, 'actor_loss':    -0.7179, 'alpha_loss':     3.2332, 'eps_e':     1.0000})
Step:  230000, Reward:    29.648 [   0.000], Avg:   132.062 (1.000) <0-03:10:32> ({'r_t':   294.3061, 'eps':     1.0000, 'critic_loss':    14.0961, 'actor_loss':    -1.1251, 'alpha_loss':     5.1126, 'eps_e':     1.0000})
Step:  231000, Reward:  -222.798 [   0.000], Avg:   130.533 (1.000) <0-03:11:38> ({'r_t':   -40.6076, 'eps':     1.0000, 'critic_loss':    18.3876, 'actor_loss':    -1.4333, 'alpha_loss':     8.3510, 'eps_e':     1.0000})
Step:  232000, Reward:   -95.092 [   0.000], Avg:   129.564 (1.000) <0-03:12:44> ({'r_t':   -78.2037, 'eps':     1.0000, 'critic_loss':    26.7717, 'actor_loss':    -1.3923, 'alpha_loss':    10.5068, 'eps_e':     1.0000})
Step:  233000, Reward:    19.345 [   0.000], Avg:   129.093 (1.000) <0-03:13:23> ({'r_t':  -165.8132, 'eps':     1.0000, 'critic_loss':    25.4379, 'actor_loss':    -1.2975, 'alpha_loss':     9.7253, 'eps_e':     1.0000})
Step:  234000, Reward:    85.246 [   0.000], Avg:   128.907 (1.000) <0-03:14:29> ({'r_t':   -15.9714, 'eps':     1.0000, 'critic_loss':    26.8118, 'actor_loss':    -1.0775, 'alpha_loss':    10.4392, 'eps_e':     1.0000})
Step:  235000, Reward:    -4.434 [   0.000], Avg:   128.342 (1.000) <0-03:15:27> ({'r_t':   -38.0390, 'eps':     1.0000, 'critic_loss':    25.9452, 'actor_loss':    -0.9868, 'alpha_loss':     9.3930, 'eps_e':     1.0000})
Step:  236000, Reward:    -4.760 [   0.000], Avg:   127.780 (1.000) <0-03:16:29> ({'r_t':    62.1083, 'eps':     1.0000, 'critic_loss':    23.9342, 'actor_loss':    -1.0495, 'alpha_loss':    11.3174, 'eps_e':     1.0000})
Step:  237000, Reward:   -27.259 [   0.000], Avg:   127.129 (1.000) <0-03:17:35> ({'r_t':  -163.1240, 'eps':     1.0000, 'critic_loss':    25.9497, 'actor_loss':    -1.0078, 'alpha_loss':    11.4484, 'eps_e':     1.0000})
Step:  238000, Reward:  -150.220 [   0.000], Avg:   125.968 (1.000) <0-03:18:39> ({'r_t':   -43.2801, 'eps':     1.0000, 'critic_loss':    23.1784, 'actor_loss':    -1.2034, 'alpha_loss':    12.0345, 'eps_e':     1.0000})
Step:  239000, Reward:    69.315 [   0.000], Avg:   125.732 (1.000) <0-03:19:46> ({'r_t':   -13.7800, 'eps':     1.0000, 'critic_loss':    20.6608, 'actor_loss':    -0.9993, 'alpha_loss':     8.5048, 'eps_e':     1.0000})
Step:  240000, Reward:    40.357 [   0.000], Avg:   125.378 (1.000) <0-03:20:52> ({'r_t':   148.3379, 'eps':     1.0000, 'critic_loss':    16.8629, 'actor_loss':    -1.0882, 'alpha_loss':     7.5870, 'eps_e':     1.0000})
Step:  241000, Reward:   194.812 [   0.000], Avg:   125.665 (1.000) <0-03:21:58> ({'r_t':   146.3215, 'eps':     1.0000, 'critic_loss':    12.3793, 'actor_loss':    -1.0339, 'alpha_loss':     7.6832, 'eps_e':     1.0000})
Step:  242000, Reward:   174.008 [   0.000], Avg:   125.864 (1.000) <0-03:23:04> ({'r_t':   193.4440, 'eps':     1.0000, 'critic_loss':    10.6405, 'actor_loss':    -0.9256, 'alpha_loss':     6.4774, 'eps_e':     1.0000})
Step:  243000, Reward:   164.510 [   0.000], Avg:   126.022 (1.000) <0-03:24:10> ({'r_t':   180.8091, 'eps':     1.0000, 'critic_loss':     6.1313, 'actor_loss':    -0.7426, 'alpha_loss':     5.3955, 'eps_e':     1.0000})
Step:  244000, Reward:   180.204 [   0.000], Avg:   126.243 (1.000) <0-03:25:16> ({'r_t':   162.1202, 'eps':     1.0000, 'critic_loss':     3.9492, 'actor_loss':    -0.6207, 'alpha_loss':     4.2534, 'eps_e':     1.0000})
Step:  245000, Reward:   193.680 [   0.000], Avg:   126.517 (1.000) <0-03:26:22> ({'r_t':   201.4846, 'eps':     1.0000, 'critic_loss':     3.3333, 'actor_loss':    -0.6081, 'alpha_loss':     3.3480, 'eps_e':     1.0000})
Step:  246000, Reward:   180.368 [   0.000], Avg:   126.735 (1.000) <0-03:27:28> ({'r_t':   192.4444, 'eps':     1.0000, 'critic_loss':     2.9468, 'actor_loss':    -0.4747, 'alpha_loss':     3.2960, 'eps_e':     1.0000})
Step:  247000, Reward:   186.836 [   0.000], Avg:   126.978 (1.000) <0-03:28:34> ({'r_t':   183.0429, 'eps':     1.0000, 'critic_loss':     2.4170, 'actor_loss':    -0.3638, 'alpha_loss':     2.4466, 'eps_e':     1.0000})
Step:  248000, Reward:   188.446 [   0.000], Avg:   127.225 (1.000) <0-03:29:40> ({'r_t':   184.8919, 'eps':     1.0000, 'critic_loss':     2.4276, 'actor_loss':    -0.2470, 'alpha_loss':     1.7240, 'eps_e':     1.0000})
Step:  249000, Reward:   180.807 [   0.000], Avg:   127.439 (1.000) <0-03:30:46> ({'r_t':   191.9222, 'eps':     1.0000, 'critic_loss':     2.0607, 'actor_loss':    -0.2197, 'alpha_loss':     1.2522, 'eps_e':     1.0000})
Step:  250000, Reward:   141.411 [   0.000], Avg:   127.495 (1.000) <0-03:31:48> ({'r_t':   191.9546, 'eps':     1.0000, 'critic_loss':     1.7395, 'actor_loss':    -0.2287, 'alpha_loss':     0.7736, 'eps_e':     1.0000})
Step:  251000, Reward:   168.877 [   0.000], Avg:   127.659 (1.000) <0-03:32:52> ({'r_t':   218.0207, 'eps':     1.0000, 'critic_loss':     1.5250, 'actor_loss':    -0.2807, 'alpha_loss':     0.3836, 'eps_e':     1.0000})
Step:  252000, Reward:   186.001 [   0.000], Avg:   127.889 (1.000) <0-03:33:53> ({'r_t':   169.0755, 'eps':     1.0000, 'critic_loss':     1.4125, 'actor_loss':    -0.2910, 'alpha_loss':     0.3691, 'eps_e':     1.0000})
Step:  253000, Reward:   184.940 [   0.000], Avg:   128.114 (1.000) <0-03:34:59> ({'r_t':   190.2127, 'eps':     1.0000, 'critic_loss':     1.2465, 'actor_loss':    -0.3159, 'alpha_loss':     0.0842, 'eps_e':     1.0000})
Step:  254000, Reward:   181.579 [   0.000], Avg:   128.324 (1.000) <0-03:35:50> ({'r_t':   173.1535, 'eps':     1.0000, 'critic_loss':     1.2929, 'actor_loss':    -0.3487, 'alpha_loss':     0.0161, 'eps_e':     1.0000})
Step:  255000, Reward:   213.445 [   0.000], Avg:   128.656 (1.000) <0-03:36:50> ({'r_t':   274.0688, 'eps':     1.0000, 'critic_loss':     1.5596, 'actor_loss':    -0.3611, 'alpha_loss':     0.1605, 'eps_e':     1.0000})
Step:  256000, Reward:   215.720 [   0.000], Avg:   128.995 (1.000) <0-03:37:41> ({'r_t':   326.9311, 'eps':     1.0000, 'critic_loss':     1.6686, 'actor_loss':    -0.3843, 'alpha_loss':     0.2038, 'eps_e':     1.0000})
Step:  257000, Reward:   205.344 [   0.000], Avg:   129.291 (1.000) <0-03:38:31> ({'r_t':   402.4065, 'eps':     1.0000, 'critic_loss':     1.3568, 'actor_loss':    -0.3801, 'alpha_loss':     0.1343, 'eps_e':     1.0000})
Step:  258000, Reward:   207.891 [   0.000], Avg:   129.594 (1.000) <0-03:39:23> ({'r_t':   344.0113, 'eps':     1.0000, 'critic_loss':     1.2299, 'actor_loss':    -0.3396, 'alpha_loss':     0.2912, 'eps_e':     1.0000})
Step:  259000, Reward:   189.947 [   0.000], Avg:   129.826 (1.000) <0-03:40:23> ({'r_t':   273.6026, 'eps':     1.0000, 'critic_loss':     1.1704, 'actor_loss':    -0.2885, 'alpha_loss':     0.6542, 'eps_e':     1.0000})
Step:  260000, Reward:   201.657 [   0.000], Avg:   130.102 (1.000) <0-03:41:17> ({'r_t':   262.1185, 'eps':     1.0000, 'critic_loss':     1.2327, 'actor_loss':    -0.2385, 'alpha_loss':     0.5034, 'eps_e':     1.0000})
Step:  261000, Reward:   212.349 [   0.000], Avg:   130.416 (1.000) <0-03:42:10> ({'r_t':   328.5585, 'eps':     1.0000, 'critic_loss':     1.0337, 'actor_loss':    -0.2632, 'alpha_loss':     0.5552, 'eps_e':     1.0000})
Step:  262000, Reward:   197.907 [   0.000], Avg:   130.672 (1.000) <0-03:42:55> ({'r_t':   416.6419, 'eps':     1.0000, 'critic_loss':     0.9874, 'actor_loss':    -0.2669, 'alpha_loss':     0.4358, 'eps_e':     1.0000})
Step:  263000, Reward:   203.841 [   0.000], Avg:   130.949 (1.000) <0-03:43:41> ({'r_t':   469.3518, 'eps':     1.0000, 'critic_loss':     1.0705, 'actor_loss':    -0.2483, 'alpha_loss':     0.0520, 'eps_e':     1.0000})
Step:  264000, Reward:   202.938 [   0.000], Avg:   131.221 (1.000) <0-03:44:31> ({'r_t':   403.7086, 'eps':     1.0000, 'critic_loss':     1.3139, 'actor_loss':    -0.2292, 'alpha_loss':     0.2724, 'eps_e':     1.0000})
Step:  265000, Reward:   215.706 [   0.000], Avg:   131.539 (1.000) <0-03:45:20> ({'r_t':   407.3858, 'eps':     1.0000, 'critic_loss':     1.4020, 'actor_loss':    -0.2405, 'alpha_loss':     0.4988, 'eps_e':     1.0000})
Step:  266000, Reward:   200.899 [   0.000], Avg:   131.798 (1.000) <0-03:46:12> ({'r_t':   427.7260, 'eps':     1.0000, 'critic_loss':     1.3968, 'actor_loss':    -0.2496, 'alpha_loss':     0.5744, 'eps_e':     1.0000})
Step:  267000, Reward:   229.300 [   0.000], Avg:   132.162 (1.000) <0-03:47:02> ({'r_t':   385.9745, 'eps':     1.0000, 'critic_loss':     1.4021, 'actor_loss':    -0.2367, 'alpha_loss':     0.5767, 'eps_e':     1.0000})
Step:  268000, Reward:   213.301 [   0.000], Avg:   132.464 (1.000) <0-03:47:50> ({'r_t':   436.8719, 'eps':     1.0000, 'critic_loss':     1.8858, 'actor_loss':    -0.2242, 'alpha_loss':     0.4907, 'eps_e':     1.0000})
Step:  269000, Reward:   218.663 [   0.000], Avg:   132.783 (1.000) <0-03:48:37> ({'r_t':   488.2223, 'eps':     1.0000, 'critic_loss':     1.5347, 'actor_loss':    -0.1616, 'alpha_loss':     0.2837, 'eps_e':     1.0000})
Step:  270000, Reward:   215.810 [   0.000], Avg:   133.089 (1.000) <0-03:49:22> ({'r_t':   495.7178, 'eps':     1.0000, 'critic_loss':     1.5664, 'actor_loss':    -0.0904, 'alpha_loss':     0.1324, 'eps_e':     1.0000})
Step:  271000, Reward:   209.573 [   0.000], Avg:   133.371 (1.000) <0-03:50:07> ({'r_t':   546.4339, 'eps':     1.0000, 'critic_loss':     1.6269, 'actor_loss':    -0.0183, 'alpha_loss':    -0.0756, 'eps_e':     1.0000})
Step:  272000, Reward:   201.843 [   0.000], Avg:   133.621 (1.000) <0-03:50:52> ({'r_t':   537.0939, 'eps':     1.0000, 'critic_loss':     2.1288, 'actor_loss':    -0.0039, 'alpha_loss':     0.0341, 'eps_e':     1.0000})
Step:  273000, Reward:   199.493 [   0.000], Avg:   133.862 (1.000) <0-03:51:37> ({'r_t':   585.7341, 'eps':     1.0000, 'critic_loss':     2.0228, 'actor_loss':    -0.0080, 'alpha_loss':    -0.0078, 'eps_e':     1.0000})
Step:  274000, Reward:   210.827 [   0.000], Avg:   134.142 (1.000) <0-03:52:25> ({'r_t':   507.4627, 'eps':     1.0000, 'critic_loss':     2.1112, 'actor_loss':    -0.0155, 'alpha_loss':    -0.0514, 'eps_e':     1.0000})
Step:  275000, Reward:   230.155 [   0.000], Avg:   134.490 (1.000) <0-03:53:12> ({'r_t':   535.7303, 'eps':     1.0000, 'critic_loss':     1.6588, 'actor_loss':    -0.0429, 'alpha_loss':    -0.1128, 'eps_e':     1.0000})
Step:  276000, Reward:   231.096 [   0.000], Avg:   134.838 (1.000) <0-03:53:58> ({'r_t':   516.9565, 'eps':     1.0000, 'critic_loss':     1.6094, 'actor_loss':    -0.0366, 'alpha_loss':    -0.0388, 'eps_e':     1.0000})
Step:  277000, Reward:   219.083 [   0.000], Avg:   135.141 (1.000) <0-03:54:44> ({'r_t':   550.5851, 'eps':     1.0000, 'critic_loss':     1.4774, 'actor_loss':    -0.0386, 'alpha_loss':    -0.0621, 'eps_e':     1.0000})
Step:  278000, Reward:   226.274 [   0.000], Avg:   135.468 (1.000) <0-03:55:30> ({'r_t':   599.7370, 'eps':     1.0000, 'critic_loss':     1.2308, 'actor_loss':    -0.0975, 'alpha_loss':     0.0121, 'eps_e':     1.0000})
Step:  279000, Reward:   249.982 [   0.000], Avg:   135.877 (1.000) <0-03:56:16> ({'r_t':   517.0159, 'eps':     1.0000, 'critic_loss':     1.0949, 'actor_loss':    -0.1120, 'alpha_loss':    -0.0437, 'eps_e':     1.0000})
Step:  280000, Reward:   258.789 [   0.000], Avg:   136.314 (1.000) <0-03:57:02> ({'r_t':   648.0627, 'eps':     1.0000, 'critic_loss':     1.0790, 'actor_loss':    -0.1548, 'alpha_loss':    -0.1939, 'eps_e':     1.0000})
Step:  281000, Reward:   252.777 [   0.000], Avg:   136.727 (1.000) <0-03:57:49> ({'r_t':   592.6324, 'eps':     1.0000, 'critic_loss':     1.0655, 'actor_loss':    -0.2835, 'alpha_loss':    -0.4656, 'eps_e':     1.0000})
Step:  282000, Reward:   153.368 [   0.000], Avg:   136.786 (1.000) <0-03:58:48> ({'r_t':   638.5018, 'eps':     1.0000, 'critic_loss':     0.9641, 'actor_loss':    -0.3799, 'alpha_loss':    -0.4575, 'eps_e':     1.0000})
Step:  283000, Reward:   157.169 [   0.000], Avg:   136.858 (1.000) <0-03:59:35> ({'r_t':   393.8183, 'eps':     1.0000, 'critic_loss':     0.9865, 'actor_loss':    -0.4103, 'alpha_loss':    -0.3742, 'eps_e':     1.0000})
Step:  284000, Reward:   161.669 [   0.000], Avg:   136.945 (1.000) <0-04:00:23> ({'r_t':   312.8080, 'eps':     1.0000, 'critic_loss':     4.0240, 'actor_loss':    -0.3509, 'alpha_loss':    -0.4347, 'eps_e':     1.0000})
Step:  285000, Reward:   184.766 [   0.000], Avg:   137.112 (1.000) <0-04:01:09> ({'r_t':   276.9110, 'eps':     1.0000, 'critic_loss':     4.5494, 'actor_loss':    -0.2965, 'alpha_loss':    -0.3303, 'eps_e':     1.0000})
Step:  286000, Reward:   162.890 [   0.000], Avg:   137.202 (1.000) <0-04:01:59> ({'r_t':   438.0515, 'eps':     1.0000, 'critic_loss':     5.0656, 'actor_loss':    -0.3116, 'alpha_loss':    -0.1897, 'eps_e':     1.0000})
Step:  287000, Reward:   228.329 [   0.000], Avg:   137.518 (1.000) <0-04:02:43> ({'r_t':   437.3346, 'eps':     1.0000, 'critic_loss':     5.0601, 'actor_loss':    -0.3876, 'alpha_loss':     0.1186, 'eps_e':     1.0000})
Step:  288000, Reward:   228.048 [   0.000], Avg:   137.832 (1.000) <0-04:03:28> ({'r_t':   583.3169, 'eps':     1.0000, 'critic_loss':     5.1214, 'actor_loss':    -0.4821, 'alpha_loss':     0.2192, 'eps_e':     1.0000})
Step:  289000, Reward:   226.917 [   0.000], Avg:   138.139 (1.000) <0-04:04:12> ({'r_t':   631.2163, 'eps':     1.0000, 'critic_loss':     4.4956, 'actor_loss':    -0.5580, 'alpha_loss':     0.2874, 'eps_e':     1.0000})
Step:  290000, Reward:   243.892 [   0.000], Avg:   138.502 (1.000) <0-04:04:58> ({'r_t':   569.1843, 'eps':     1.0000, 'critic_loss':     3.0565, 'actor_loss':    -0.6034, 'alpha_loss':     0.3668, 'eps_e':     1.0000})
Step:  291000, Reward:   240.631 [   0.000], Avg:   138.852 (1.000) <0-04:05:51> ({'r_t':   407.9182, 'eps':     1.0000, 'critic_loss':     2.8907, 'actor_loss':    -0.5809, 'alpha_loss':     0.5652, 'eps_e':     1.0000})
Step:  292000, Reward:   147.506 [   0.000], Avg:   138.882 (1.000) <0-04:06:49> ({'r_t':   324.2828, 'eps':     1.0000, 'critic_loss':     3.3744, 'actor_loss':    -0.5114, 'alpha_loss':     0.6892, 'eps_e':     1.0000})
Step:  293000, Reward:   171.486 [   0.000], Avg:   138.993 (1.000) <0-04:07:55> ({'r_t':   257.3202, 'eps':     1.0000, 'critic_loss':     3.5467, 'actor_loss':    -0.4490, 'alpha_loss':     0.4110, 'eps_e':     1.0000})
Step:  294000, Reward:   233.436 [   0.000], Avg:   139.313 (1.000) <0-04:09:01> ({'r_t':   226.6878, 'eps':     1.0000, 'critic_loss':     3.6485, 'actor_loss':    -0.3010, 'alpha_loss':     0.0842, 'eps_e':     1.0000})
Step:  295000, Reward:   215.798 [   0.000], Avg:   139.571 (1.000) <0-04:10:07> ({'r_t':   223.9161, 'eps':     1.0000, 'critic_loss':     3.2066, 'actor_loss':    -0.2393, 'alpha_loss':    -0.3201, 'eps_e':     1.0000})
Step:  296000, Reward:   199.743 [   0.000], Avg:   139.774 (1.000) <0-04:11:13> ({'r_t':   216.9697, 'eps':     1.0000, 'critic_loss':     2.8157, 'actor_loss':    -0.2352, 'alpha_loss':    -0.5461, 'eps_e':     1.0000})
Step:  297000, Reward:   140.590 [   0.000], Avg:   139.776 (1.000) <0-04:12:19> ({'r_t':   212.2758, 'eps':     1.0000, 'critic_loss':     1.9241, 'actor_loss':    -0.3286, 'alpha_loss':    -0.7169, 'eps_e':     1.0000})
Step:  298000, Reward:   225.802 [   0.000], Avg:   140.064 (1.000) <0-04:13:04> ({'r_t':   176.6558, 'eps':     1.0000, 'critic_loss':     1.1785, 'actor_loss':    -0.5028, 'alpha_loss':    -0.5310, 'eps_e':     1.0000})
Step:  299000, Reward:   153.283 [   0.000], Avg:   140.108 (1.000) <0-04:14:02> ({'r_t':   278.8396, 'eps':     1.0000, 'critic_loss':     1.2193, 'actor_loss':    -0.6662, 'alpha_loss':    -0.3244, 'eps_e':     1.0000})
Step:  300000, Reward:   162.265 [   0.000], Avg:   140.182 (1.000) <0-04:14:54> ({'r_t':   191.7337, 'eps':     1.0000, 'critic_loss':     1.6457, 'actor_loss':    -0.7536, 'alpha_loss':    -0.1902, 'eps_e':     1.0000})
Step:  301000, Reward:    64.025 [   0.000], Avg:   139.930 (1.000) <0-04:15:53> ({'r_t':   326.5388, 'eps':     1.0000, 'critic_loss':     2.9784, 'actor_loss':    -0.6942, 'alpha_loss':     0.0004, 'eps_e':     1.0000})
Step:  302000, Reward:   142.706 [   0.000], Avg:   139.939 (1.000) <0-04:16:53> ({'r_t':     3.7306, 'eps':     1.0000, 'critic_loss':     3.7453, 'actor_loss':    -0.6783, 'alpha_loss':    -0.0558, 'eps_e':     1.0000})
Step:  303000, Reward:    11.415 [   0.000], Avg:   139.516 (1.000) <0-04:17:52> ({'r_t':   211.7892, 'eps':     1.0000, 'critic_loss':     4.1799, 'actor_loss':    -0.5928, 'alpha_loss':     0.0533, 'eps_e':     1.0000})
Step:  304000, Reward:    75.944 [   0.000], Avg:   139.308 (1.000) <0-04:18:49> ({'r_t':    24.5526, 'eps':     1.0000, 'critic_loss':     5.1172, 'actor_loss':    -0.4907, 'alpha_loss':     0.0858, 'eps_e':     1.0000})
Step:  305000, Reward:   213.722 [   0.000], Avg:   139.551 (1.000) <0-04:19:42> ({'r_t':   214.6582, 'eps':     1.0000, 'critic_loss':     6.7712, 'actor_loss':    -0.4354, 'alpha_loss':     0.4874, 'eps_e':     1.0000})
Step:  306000, Reward:   202.933 [   0.000], Avg:   139.757 (1.000) <0-04:20:35> ({'r_t':   390.5918, 'eps':     1.0000, 'critic_loss':     6.9425, 'actor_loss':    -0.5329, 'alpha_loss':     0.4765, 'eps_e':     1.0000})
Step:  307000, Reward:   168.808 [   0.000], Avg:   139.852 (1.000) <0-04:21:29> ({'r_t':   264.0875, 'eps':     1.0000, 'critic_loss':     6.6788, 'actor_loss':    -0.7933, 'alpha_loss':     0.4672, 'eps_e':     1.0000})
Step:  308000, Reward:   183.180 [   0.000], Avg:   139.992 (1.000) <0-04:22:21> ({'r_t':   470.5026, 'eps':     1.0000, 'critic_loss':     6.6678, 'actor_loss':    -0.8649, 'alpha_loss':     0.3865, 'eps_e':     1.0000})
Step:  309000, Reward:   189.860 [   0.000], Avg:   140.153 (1.000) <0-04:23:13> ({'r_t':   506.4491, 'eps':     1.0000, 'critic_loss':   268.9300, 'actor_loss':    -0.8378, 'alpha_loss':     0.3585, 'eps_e':     1.0000})
Step:  310000, Reward:   617.874 [   0.000], Avg:   141.689 (1.000) <0-04:24:19> ({'r_t':   797.5708, 'eps':     1.0000, 'critic_loss':   410.1707, 'actor_loss':    -0.7190, 'alpha_loss':     0.2540, 'eps_e':     1.0000})
Step:  311000, Reward:   500.647 [   0.000], Avg:   142.839 (1.000) <0-04:25:25> ({'r_t':  1092.5651, 'eps':     1.0000, 'critic_loss':   817.9478, 'actor_loss':    -0.5520, 'alpha_loss':     0.4437, 'eps_e':     1.0000})
Step:  312000, Reward:   223.851 [   0.000], Avg:   143.098 (1.000) <0-04:26:17> ({'r_t':   682.6747, 'eps':     1.0000, 'critic_loss':   768.4429, 'actor_loss':    -0.6190, 'alpha_loss':     0.7785, 'eps_e':     1.0000})
Step:  313000, Reward:   244.324 [   0.000], Avg:   143.420 (1.000) <0-04:27:11> ({'r_t':  1081.8356, 'eps':     1.0000, 'critic_loss':  1737.1210, 'actor_loss':    -0.7075, 'alpha_loss':     0.9709, 'eps_e':     1.0000})
Step:  314000, Reward:   253.684 [   0.000], Avg:   143.770 (1.000) <0-04:28:03> ({'r_t':   692.5028, 'eps':     1.0000, 'critic_loss':  1893.0542, 'actor_loss':    -0.6563, 'alpha_loss':     0.9694, 'eps_e':     1.0000})
Step:  315000, Reward:   222.474 [   0.000], Avg:   144.020 (1.000) <0-04:28:54> ({'r_t':   815.1761, 'eps':     1.0000, 'critic_loss':  2146.2737, 'actor_loss':    -0.7489, 'alpha_loss':     0.5720, 'eps_e':     1.0000})
Step:  316000, Reward:   -88.240 [   0.000], Avg:   143.287 (1.000) <0-04:29:30> ({'r_t':   695.3561, 'eps':     1.0000, 'critic_loss':  2069.7947, 'actor_loss':    -0.7519, 'alpha_loss':     0.4225, 'eps_e':     1.0000})
Step:  317000, Reward:    44.948 [   0.000], Avg:   142.978 (1.000) <0-04:30:12> ({'r_t':    94.5387, 'eps':     1.0000, 'critic_loss':  1924.1095, 'actor_loss':    -0.9435, 'alpha_loss':     0.3394, 'eps_e':     1.0000})
Step:  318000, Reward:   -36.589 [   0.000], Avg:   142.415 (1.000) <0-04:30:48> ({'r_t':    95.4921, 'eps':     1.0000, 'critic_loss':  1675.7848, 'actor_loss':    -0.9621, 'alpha_loss':     0.3450, 'eps_e':     1.0000})
Step:  319000, Reward:   -37.372 [   0.000], Avg:   141.853 (1.000) <0-04:31:24> ({'r_t':   384.4246, 'eps':     1.0000, 'critic_loss':  1169.3938, 'actor_loss':    -0.8875, 'alpha_loss':     0.1590, 'eps_e':     1.0000})
Step:  320000, Reward:   139.157 [   0.000], Avg:   141.844 (1.000) <0-04:32:13> ({'r_t':  1047.4444, 'eps':     1.0000, 'critic_loss':  1059.2085, 'actor_loss':    -1.0171, 'alpha_loss':     0.1040, 'eps_e':     1.0000})
Step:  321000, Reward:    62.097 [   0.000], Avg:   141.597 (1.000) <0-04:32:54> ({'r_t':   464.0412, 'eps':     1.0000, 'critic_loss':  1308.1622, 'actor_loss':    -1.0230, 'alpha_loss':     0.1898, 'eps_e':     1.0000})
Step:  322000, Reward:    70.719 [   0.000], Avg:   141.377 (1.000) <0-04:33:36> ({'r_t':   296.3572, 'eps':     1.0000, 'critic_loss':   971.3441, 'actor_loss':    -1.0230, 'alpha_loss':    -0.0594, 'eps_e':     1.0000})
Step:  323000, Reward:   126.740 [   0.000], Avg:   141.332 (1.000) <0-04:34:24> ({'r_t':   668.8120, 'eps':     1.0000, 'critic_loss':   828.2983, 'actor_loss':    -1.0632, 'alpha_loss':     0.0746, 'eps_e':     1.0000})
Step:  324000, Reward:   132.525 [   0.000], Avg:   141.305 (1.000) <0-04:35:10> ({'r_t':   611.2840, 'eps':     1.0000, 'critic_loss':  1085.4915, 'actor_loss':    -1.0701, 'alpha_loss':     0.1800, 'eps_e':     1.0000})
Step:  325000, Reward:   132.136 [   0.000], Avg:   141.277 (1.000) <0-04:35:56> ({'r_t':  1000.2841, 'eps':     1.0000, 'critic_loss':  1379.2599, 'actor_loss':    -1.1006, 'alpha_loss':     0.0800, 'eps_e':     1.0000})
Step:  326000, Reward:   793.803 [   0.000], Avg:   143.272 (1.000) <0-04:36:56> ({'r_t':  1090.8795, 'eps':     1.0000, 'critic_loss':  1833.7277, 'actor_loss':    -1.1121, 'alpha_loss':     0.0929, 'eps_e':     1.0000})
Step:  327000, Reward:   143.264 [   0.000], Avg:   143.272 (1.000) <0-04:37:42> ({'r_t':   945.7227, 'eps':     1.0000, 'critic_loss':  2096.5796, 'actor_loss':    -1.0315, 'alpha_loss':     0.3122, 'eps_e':     1.0000})
Step:  328000, Reward:   136.969 [   0.000], Avg:   143.253 (1.000) <0-04:38:28> ({'r_t':  1495.7291, 'eps':     1.0000, 'critic_loss':  2562.3540, 'actor_loss':    -0.8686, 'alpha_loss':     0.3790, 'eps_e':     1.0000})
Step:  329000, Reward:    -4.669 [   0.000], Avg:   142.805 (1.000) <0-04:39:09> ({'r_t':  1517.4992, 'eps':     1.0000, 'critic_loss':  3173.3840, 'actor_loss':    -0.9295, 'alpha_loss':     0.6336, 'eps_e':     1.0000})
Step:  330000, Reward:    81.004 [   0.000], Avg:   142.618 (1.000) <0-04:39:52> ({'r_t':  1050.0205, 'eps':     1.0000, 'critic_loss':  3813.6294, 'actor_loss':    -0.9072, 'alpha_loss':     0.5828, 'eps_e':     1.0000})
Step:  331000, Reward:  1821.886 [   0.000], Avg:   147.676 (1.000) <0-04:40:58> ({'r_t':   506.7477, 'eps':     1.0000, 'critic_loss':  3821.9399, 'actor_loss':    -0.8357, 'alpha_loss':     0.3332, 'eps_e':     1.0000})
Step:  332000, Reward:   -31.977 [   0.000], Avg:   147.137 (1.000) <0-04:41:35> ({'r_t':  1764.2123, 'eps':     1.0000, 'critic_loss':  3424.7927, 'actor_loss':    -1.0660, 'alpha_loss':     0.3440, 'eps_e':     1.0000})
Step:  333000, Reward:   -70.436 [   0.000], Avg:   146.485 (1.000) <0-04:42:15> ({'r_t':  1490.0566, 'eps':     1.0000, 'critic_loss':  4124.2920, 'actor_loss':    -1.2262, 'alpha_loss':     0.6403, 'eps_e':     1.0000})
Step:  334000, Reward:   149.588 [   0.000], Avg:   146.495 (1.000) <0-04:43:02> ({'r_t':   -59.2817, 'eps':     1.0000, 'critic_loss':  3464.5654, 'actor_loss':    -1.4956, 'alpha_loss':     0.4614, 'eps_e':     1.0000})
Step:  335000, Reward:   120.629 [   0.000], Avg:   146.418 (1.000) <0-04:43:48> ({'r_t':   653.0223, 'eps':     1.0000, 'critic_loss':  2630.8789, 'actor_loss':    -1.5894, 'alpha_loss':     0.2156, 'eps_e':     1.0000})
Step:  336000, Reward:   134.940 [   0.000], Avg:   146.384 (1.000) <0-04:44:34> ({'r_t':   803.6441, 'eps':     1.0000, 'critic_loss':  2289.2568, 'actor_loss':    -1.6629, 'alpha_loss':    -0.0229, 'eps_e':     1.0000})
Step:  337000, Reward:   829.122 [   0.000], Avg:   148.404 (1.000) <0-04:45:36> ({'r_t':  1088.1196, 'eps':     1.0000, 'critic_loss':  1765.3907, 'actor_loss':    -1.7955, 'alpha_loss':    -0.0308, 'eps_e':     1.0000})
Step:  338000, Reward:   130.886 [   0.000], Avg:   148.352 (1.000) <0-04:46:23> ({'r_t':   463.6914, 'eps':     1.0000, 'critic_loss':  1746.4823, 'actor_loss':    -1.9918, 'alpha_loss':     0.0153, 'eps_e':     1.0000})
Step:  339000, Reward:   106.561 [   0.000], Avg:   148.229 (1.000) <0-04:47:08> ({'r_t':   261.5218, 'eps':     1.0000, 'critic_loss':  1232.5065, 'actor_loss':    -1.6107, 'alpha_loss':    -0.0838, 'eps_e':     1.0000})
Step:  340000, Reward:   114.301 [   0.000], Avg:   148.130 (1.000) <0-04:47:52> ({'r_t':   315.2578, 'eps':     1.0000, 'critic_loss':   832.5124, 'actor_loss':    -1.1627, 'alpha_loss':    -0.1358, 'eps_e':     1.0000})
Step:  341000, Reward:   126.119 [   0.000], Avg:   148.065 (1.000) <0-04:48:37> ({'r_t':   699.7376, 'eps':     1.0000, 'critic_loss':   782.5569, 'actor_loss':    -0.6787, 'alpha_loss':    -0.2018, 'eps_e':     1.0000})
Step:  342000, Reward:   487.473 [   0.000], Avg:   149.055 (1.000) <0-04:49:43> ({'r_t':   476.0691, 'eps':     1.0000, 'critic_loss':   735.8912, 'actor_loss':    -0.6310, 'alpha_loss':    -0.2376, 'eps_e':     1.0000})
Step:  343000, Reward:   109.893 [   0.000], Avg:   148.941 (1.000) <0-04:50:28> ({'r_t':   491.8606, 'eps':     1.0000, 'critic_loss':   552.3262, 'actor_loss':    -0.6371, 'alpha_loss':    -0.2255, 'eps_e':     1.0000})
Step:  344000, Reward:   105.181 [   0.000], Avg:   148.814 (1.000) <0-04:51:13> ({'r_t':   499.0000, 'eps':     1.0000, 'critic_loss':   501.3609, 'actor_loss':    -0.5836, 'alpha_loss':    -0.2343, 'eps_e':     1.0000})
Step:  345000, Reward:    71.365 [   0.000], Avg:   148.590 (1.000) <0-04:51:58> ({'r_t':  1070.2494, 'eps':     1.0000, 'critic_loss':   490.8553, 'actor_loss':    -0.4917, 'alpha_loss':    -0.2146, 'eps_e':     1.0000})
Step:  346000, Reward:  1150.468 [   0.000], Avg:   151.477 (1.000) <0-04:52:54> ({'r_t':   934.0366, 'eps':     1.0000, 'critic_loss':   892.6175, 'actor_loss':    -0.3941, 'alpha_loss':    -0.0482, 'eps_e':     1.0000})
Step:  347000, Reward:  2359.132 [   0.000], Avg:   157.821 (1.000) <0-04:54:00> ({'r_t':  1446.4217, 'eps':     1.0000, 'critic_loss':  1389.2565, 'actor_loss':    -0.4721, 'alpha_loss':     0.0514, 'eps_e':     1.0000})
Step:  348000, Reward:    60.982 [   0.000], Avg:   157.544 (1.000) <0-04:54:45> ({'r_t':  1297.3148, 'eps':     1.0000, 'critic_loss':  2159.7417, 'actor_loss':    -0.6816, 'alpha_loss':     0.1550, 'eps_e':     1.0000})
Step:  349000, Reward:    68.891 [   0.000], Avg:   157.290 (1.000) <0-04:55:30> ({'r_t':   503.6417, 'eps':     1.0000, 'critic_loss':  2476.8484, 'actor_loss':    -0.9371, 'alpha_loss':     0.2001, 'eps_e':     1.0000})
Step:  350000, Reward:    81.674 [   0.000], Avg:   157.075 (1.000) <0-04:56:15> ({'r_t':   449.2375, 'eps':     1.0000, 'critic_loss':  2847.0286, 'actor_loss':    -0.8801, 'alpha_loss':     0.0808, 'eps_e':     1.0000})
Step:  351000, Reward:    80.753 [   0.000], Avg:   156.858 (1.000) <0-04:56:59> ({'r_t':   614.0582, 'eps':     1.0000, 'critic_loss':  2663.9741, 'actor_loss':    -0.7981, 'alpha_loss':     0.0877, 'eps_e':     1.0000})
Step:  352000, Reward:    54.746 [   0.000], Avg:   156.569 (1.000) <0-04:57:40> ({'r_t':  1355.7245, 'eps':     1.0000, 'critic_loss':  2196.5149, 'actor_loss':    -0.6744, 'alpha_loss':     0.0154, 'eps_e':     1.0000})
Step:  353000, Reward:    47.842 [   0.000], Avg:   156.262 (1.000) <0-04:58:24> ({'r_t':  1191.7531, 'eps':     1.0000, 'critic_loss':  2457.7256, 'actor_loss':    -0.6102, 'alpha_loss':     0.0269, 'eps_e':     1.0000})
Step:  354000, Reward:  2823.522 [   0.000], Avg:   163.775 (1.000) <0-04:59:30> ({'r_t':  1114.2448, 'eps':     1.0000, 'critic_loss':  2494.3936, 'actor_loss':    -0.7090, 'alpha_loss':     0.0776, 'eps_e':     1.0000})
Step:  355000, Reward:  2676.590 [   0.000], Avg:   170.834 (1.000) <0-05:00:36> ({'r_t':  1776.4290, 'eps':     1.0000, 'critic_loss':  2507.3242, 'actor_loss':    -0.7352, 'alpha_loss':     0.1142, 'eps_e':     1.0000})
Step:  356000, Reward:    46.064 [   0.000], Avg:   170.484 (1.000) <0-05:01:23> ({'r_t':  1790.3840, 'eps':     1.0000, 'critic_loss':  3183.3777, 'actor_loss':    -0.9719, 'alpha_loss':     0.3099, 'eps_e':     1.0000})
Step:  357000, Reward:    52.536 [   0.000], Avg:   170.155 (1.000) <0-05:02:07> ({'r_t':  1499.5005, 'eps':     1.0000, 'critic_loss':  4102.5435, 'actor_loss':    -1.5158, 'alpha_loss':     0.2316, 'eps_e':     1.0000})
Step:  358000, Reward:   -28.318 [   0.000], Avg:   169.602 (1.000) <0-05:02:53> ({'r_t':  1340.7526, 'eps':     1.0000, 'critic_loss':  4327.4321, 'actor_loss':    -2.2834, 'alpha_loss':     0.2705, 'eps_e':     1.0000})
Step:  359000, Reward:   -32.035 [   0.000], Avg:   169.042 (1.000) <0-05:03:42> ({'r_t':   427.0835, 'eps':     1.0000, 'critic_loss':  4478.8745, 'actor_loss':    -2.7541, 'alpha_loss':     0.2323, 'eps_e':     1.0000})
Step:  360000, Reward:    88.281 [   0.000], Avg:   168.818 (1.000) <0-05:04:29> ({'r_t':   202.0197, 'eps':     1.0000, 'critic_loss':  4128.9722, 'actor_loss':    -3.2703, 'alpha_loss':     0.0970, 'eps_e':     1.0000})
Step:  361000, Reward:    74.768 [   0.000], Avg:   168.558 (1.000) <0-05:05:15> ({'r_t':   195.9055, 'eps':     1.0000, 'critic_loss':  3420.2144, 'actor_loss':    -2.7851, 'alpha_loss':     0.0950, 'eps_e':     1.0000})
Step:  362000, Reward:  2110.232 [   0.000], Avg:   173.907 (1.000) <0-05:06:11> ({'r_t':  2320.2958, 'eps':     1.0000, 'critic_loss':  3175.5220, 'actor_loss':    -2.4617, 'alpha_loss':     0.1465, 'eps_e':     1.0000})
Step:  363000, Reward:  2281.610 [   0.000], Avg:   179.698 (1.000) <0-05:07:17> ({'r_t':  3195.5762, 'eps':     1.0000, 'critic_loss':  4480.8413, 'actor_loss':    -2.6880, 'alpha_loss':     0.2471, 'eps_e':     1.0000})
Step:  364000, Reward:  2089.973 [   0.000], Avg:   184.931 (1.000) <0-05:08:18> ({'r_t':  3311.8075, 'eps':     1.0000, 'critic_loss':  4653.9399, 'actor_loss':    -2.3726, 'alpha_loss':     0.1309, 'eps_e':     1.0000})
Step:  365000, Reward:  2091.292 [   0.000], Avg:   190.140 (1.000) <0-05:09:19> ({'r_t':  2469.5373, 'eps':     1.0000, 'critic_loss':  4563.1348, 'actor_loss':    -2.3688, 'alpha_loss':     0.1419, 'eps_e':     1.0000})
Step:  366000, Reward:    57.087 [   0.000], Avg:   189.777 (1.000) <0-05:10:03> ({'r_t':  1207.9011, 'eps':     1.0000, 'critic_loss':  4940.8950, 'actor_loss':    -2.5840, 'alpha_loss':     0.1803, 'eps_e':     1.0000})
Step:  367000, Reward:    34.944 [   0.000], Avg:   189.357 (1.000) <0-05:10:44> ({'r_t':   580.0257, 'eps':     1.0000, 'critic_loss':  4469.1611, 'actor_loss':    -2.5596, 'alpha_loss':     0.2555, 'eps_e':     1.0000})
Step:  368000, Reward:     5.419 [   0.000], Avg:   188.858 (1.000) <0-05:11:29> ({'r_t':   102.2667, 'eps':     1.0000, 'critic_loss':  4470.2085, 'actor_loss':    -2.7865, 'alpha_loss':     0.2440, 'eps_e':     1.0000})
Step:  369000, Reward:    -0.260 [   0.000], Avg:   188.347 (1.000) <0-05:12:11> ({'r_t':  2424.4127, 'eps':     1.0000, 'critic_loss':  2669.9368, 'actor_loss':    -2.4158, 'alpha_loss':     0.1233, 'eps_e':     1.0000})
Step:  370000, Reward:  3400.298 [   0.000], Avg:   197.005 (1.000) <0-05:13:04> ({'r_t':  3188.5261, 'eps':     1.0000, 'critic_loss':  2557.7905, 'actor_loss':    -2.0020, 'alpha_loss':     0.0038, 'eps_e':     1.0000})
Step:  371000, Reward:  1548.632 [   0.000], Avg:   200.638 (1.000) <0-05:14:10> ({'r_t':  3404.8689, 'eps':     1.0000, 'critic_loss':  4785.2158, 'actor_loss':    -2.0902, 'alpha_loss':    -0.0388, 'eps_e':     1.0000})
Step:  372000, Reward:   962.703 [   0.000], Avg:   202.681 (1.000) <0-05:15:02> ({'r_t':  2272.4055, 'eps':     1.0000, 'critic_loss':  3785.7212, 'actor_loss':    -2.0003, 'alpha_loss':    -0.0203, 'eps_e':     1.0000})
Step:  373000, Reward:   -59.200 [   0.000], Avg:   201.981 (1.000) <0-05:15:38> ({'r_t':  3948.5252, 'eps':     1.0000, 'critic_loss':  6285.9219, 'actor_loss':    -2.1553, 'alpha_loss':     0.0518, 'eps_e':     1.0000})
Step:  374000, Reward:   -59.952 [   0.000], Avg:   201.282 (1.000) <0-05:16:13> ({'r_t':   593.8846, 'eps':     1.0000, 'critic_loss':  9149.4648, 'actor_loss':    -2.4590, 'alpha_loss':     0.3212, 'eps_e':     1.0000})
Step:  375000, Reward:   -60.723 [   0.000], Avg:   200.585 (1.000) <0-05:16:48> ({'r_t':   520.9375, 'eps':     1.0000, 'critic_loss':  8286.2031, 'actor_loss':    -2.6895, 'alpha_loss':     0.2617, 'eps_e':     1.0000})
Step:  376000, Reward:  2085.031 [   0.000], Avg:   205.584 (1.000) <0-05:17:44> ({'r_t':  2142.0157, 'eps':     1.0000, 'critic_loss':  9504.9219, 'actor_loss':    -2.2371, 'alpha_loss':     0.1046, 'eps_e':     1.0000})
Step:  377000, Reward:     9.913 [   0.000], Avg:   205.066 (1.000) <0-05:18:26> ({'r_t':  1292.5316, 'eps':     1.0000, 'critic_loss':  9191.0430, 'actor_loss':    -3.0305, 'alpha_loss':     0.0041, 'eps_e':     1.0000})
Step:  378000, Reward:    31.340 [   0.000], Avg:   204.608 (1.000) <0-05:19:10> ({'r_t':  1035.4614, 'eps':     1.0000, 'critic_loss':  8584.3037, 'actor_loss':    -2.6442, 'alpha_loss':    -0.1070, 'eps_e':     1.0000})
Step:  379000, Reward:    25.629 [   0.000], Avg:   204.137 (1.000) <0-05:19:53> ({'r_t':  1850.7490, 'eps':     1.0000, 'critic_loss':  5671.1006, 'actor_loss':    -2.7850, 'alpha_loss':    -0.1527, 'eps_e':     1.0000})
Step:  380000, Reward:   -36.903 [   0.000], Avg:   203.504 (1.000) <0-05:20:30> ({'r_t':  1056.5247, 'eps':     1.0000, 'critic_loss':  4525.6362, 'actor_loss':    -2.2345, 'alpha_loss':    -0.2388, 'eps_e':     1.0000})
Step:  381000, Reward:   -83.028 [   0.000], Avg:   202.754 (1.000) <0-05:21:10> ({'r_t':  3054.4646, 'eps':     1.0000, 'critic_loss':  3911.0659, 'actor_loss':    -2.2856, 'alpha_loss':    -0.2284, 'eps_e':     1.0000})
Step:  382000, Reward:   -37.994 [   0.000], Avg:   202.126 (1.000) <0-05:21:49> ({'r_t':  3243.2937, 'eps':     1.0000, 'critic_loss':  5229.8071, 'actor_loss':    -2.1091, 'alpha_loss':    -0.1023, 'eps_e':     1.0000})
Step:  383000, Reward:  5677.536 [   0.000], Avg:   216.385 (1.000) <0-05:22:55> ({'r_t':  3659.1595, 'eps':     1.0000, 'critic_loss':  4345.4321, 'actor_loss':    -2.7285, 'alpha_loss':     0.0248, 'eps_e':     1.0000})
Step:  384000, Reward:  3048.147 [   0.000], Avg:   223.740 (1.000) <0-05:23:59> ({'r_t':  3270.3955, 'eps':     1.0000, 'critic_loss':  4711.5688, 'actor_loss':    -3.3311, 'alpha_loss':     0.1114, 'eps_e':     1.0000})
Step:  385000, Reward:   -67.907 [   0.000], Avg:   222.984 (1.000) <0-05:24:35> ({'r_t':  3251.8952, 'eps':     1.0000, 'critic_loss':  5109.9619, 'actor_loss':    -3.1887, 'alpha_loss':     0.1907, 'eps_e':     1.0000})
Step:  386000, Reward:  5147.117 [   0.000], Avg:   235.708 (1.000) <0-05:25:41> ({'r_t':  4360.1812, 'eps':     1.0000, 'critic_loss':  5208.0918, 'actor_loss':    -3.2838, 'alpha_loss':     0.1688, 'eps_e':     1.0000})
Step:  387000, Reward:  4103.705 [   0.000], Avg:   245.677 (1.000) <0-05:26:47> ({'r_t':  4859.9426, 'eps':     1.0000, 'critic_loss':  6567.9575, 'actor_loss':    -3.0129, 'alpha_loss':     0.1893, 'eps_e':     1.0000})
Step:  388000, Reward:  3987.527 [   0.000], Avg:   255.296 (1.000) <0-05:27:37> ({'r_t':  3606.7619, 'eps':     1.0000, 'critic_loss':  7492.9331, 'actor_loss':    -2.2455, 'alpha_loss':     0.1477, 'eps_e':     1.0000})
Step:  389000, Reward:   -76.459 [   0.000], Avg:   254.446 (1.000) <0-05:28:13> ({'r_t':  7339.4851, 'eps':     1.0000, 'critic_loss':  9066.8516, 'actor_loss':    -2.1454, 'alpha_loss':     0.1361, 'eps_e':     1.0000})
Step:  390000, Reward:   -75.315 [   0.000], Avg:   253.602 (1.000) <0-05:28:49> ({'r_t':  9342.0455, 'eps':     1.0000, 'critic_loss': 14577.3291, 'actor_loss':    -2.4935, 'alpha_loss':     0.1793, 'eps_e':     1.0000})
Step:  391000, Reward:   -72.056 [   0.000], Avg:   252.772 (1.000) <0-05:29:24> ({'r_t':  8868.9801, 'eps':     1.0000, 'critic_loss': 20835.4043, 'actor_loss':    -2.9794, 'alpha_loss':     0.2057, 'eps_e':     1.0000})
Step:  392000, Reward: 14505.116 [   0.000], Avg:   289.037 (1.000) <0-05:30:30> ({'r_t':  7229.2561, 'eps':     1.0000, 'critic_loss': 25629.1016, 'actor_loss':    -3.1717, 'alpha_loss':     0.2368, 'eps_e':     1.0000})
Step:  393000, Reward:  7461.042 [   0.000], Avg:   307.240 (1.000) <0-05:31:36> ({'r_t':  8128.9373, 'eps':     1.0000, 'critic_loss': 32481.1465, 'actor_loss':    -3.0977, 'alpha_loss':     0.2159, 'eps_e':     1.0000})
Step:  394000, Reward:  -122.673 [   0.000], Avg:   306.152 (1.000) <0-05:32:25> ({'r_t':  6745.6315, 'eps':     1.0000, 'critic_loss': 44793.5742, 'actor_loss':    -5.3183, 'alpha_loss':     0.2217, 'eps_e':     1.0000})
Step:  395000, Reward:   -61.564 [   0.000], Avg:   305.223 (1.000) <0-05:33:01> ({'r_t':   251.2642, 'eps':     1.0000, 'critic_loss': 56529.0000, 'actor_loss':    -3.9876, 'alpha_loss':     0.1738, 'eps_e':     1.0000})
Step:  396000, Reward:   -49.420 [   0.000], Avg:   304.330 (1.000) <0-05:33:37> ({'r_t':  -577.0681, 'eps':     1.0000, 'critic_loss': 51149.4414, 'actor_loss':    -3.3108, 'alpha_loss':     0.0430, 'eps_e':     1.0000})
Step:  397000, Reward:   -52.657 [   0.000], Avg:   303.433 (1.000) <0-05:34:16> ({'r_t':  -359.9162, 'eps':     1.0000, 'critic_loss': 46231.2891, 'actor_loss':    -3.8060, 'alpha_loss':    -0.0555, 'eps_e':     1.0000})
Step:  398000, Reward:   -59.570 [   0.000], Avg:   302.523 (1.000) <0-05:34:57> ({'r_t':  -300.2920, 'eps':     1.0000, 'critic_loss': 34345.9453, 'actor_loss':    -4.1506, 'alpha_loss':    -0.2057, 'eps_e':     1.0000})
Step:  399000, Reward:   -57.196 [   0.000], Avg:   301.624 (1.000) <0-05:35:33> ({'r_t':  -194.2826, 'eps':     1.0000, 'critic_loss': 23128.1621, 'actor_loss':    -5.0789, 'alpha_loss':    -0.3279, 'eps_e':     1.0000})
Step:  400000, Reward:   -22.695 [   0.000], Avg:   300.815 (1.000) <0-05:36:17> ({'r_t':  -244.3800, 'eps':     1.0000, 'critic_loss': 21196.8242, 'actor_loss':    -4.5630, 'alpha_loss':    -0.4962, 'eps_e':     1.0000})
Step:  401000, Reward:   -63.010 [   0.000], Avg:   299.910 (1.000) <0-05:36:56> ({'r_t':  -153.8118, 'eps':     1.0000, 'critic_loss':  2940.9478, 'actor_loss':    -3.2185, 'alpha_loss':    -0.7009, 'eps_e':     1.0000})
Step:  402000, Reward:  1434.517 [   0.000], Avg:   302.725 (1.000) <0-05:37:49> ({'r_t':   -50.8818, 'eps':     1.0000, 'critic_loss':   184.9036, 'actor_loss':    -1.3646, 'alpha_loss':    -0.8502, 'eps_e':     1.0000})
Step:  403000, Reward:  1973.666 [   0.000], Avg:   306.861 (1.000) <0-05:38:42> ({'r_t':  1551.4222, 'eps':     1.0000, 'critic_loss':  1415.5012, 'actor_loss':    -1.6872, 'alpha_loss':    -0.7764, 'eps_e':     1.0000})
Step:  404000, Reward:   -62.527 [   0.000], Avg:   305.949 (1.000) <0-05:39:36> ({'r_t':  2615.0550, 'eps':     1.0000, 'critic_loss':  3954.7363, 'actor_loss':    -2.5041, 'alpha_loss':    -0.4613, 'eps_e':     1.0000})
Step:  405000, Reward:  2013.176 [   0.000], Avg:   310.154 (1.000) <0-05:40:25> ({'r_t':  1718.8553, 'eps':     1.0000, 'critic_loss':  9683.3223, 'actor_loss':    -3.4042, 'alpha_loss':    -0.2028, 'eps_e':     1.0000})
Step:  406000, Reward:   -52.501 [   0.000], Avg:   309.263 (1.000) <0-05:41:05> ({'r_t':  2229.4375, 'eps':     1.0000, 'critic_loss': 11073.8301, 'actor_loss':    -4.2161, 'alpha_loss':     0.0372, 'eps_e':     1.0000})
Step:  407000, Reward:  1973.604 [   0.000], Avg:   313.342 (1.000) <0-05:41:55> ({'r_t':  3432.3477, 'eps':     1.0000, 'critic_loss': 13039.5684, 'actor_loss':    -5.1075, 'alpha_loss':     0.1219, 'eps_e':     1.0000})
Step:  408000, Reward:  1953.115 [   0.000], Avg:   317.352 (1.000) <0-05:42:44> ({'r_t':  3903.4182, 'eps':     1.0000, 'critic_loss': 16705.2266, 'actor_loss':    -6.2857, 'alpha_loss':     0.2250, 'eps_e':     1.0000})
Step:  409000, Reward:   -26.045 [   0.000], Avg:   316.514 (1.000) <0-05:43:25> ({'r_t':  4015.9818, 'eps':     1.0000, 'critic_loss': 17823.8457, 'actor_loss':    -7.9509, 'alpha_loss':     0.2928, 'eps_e':     1.0000})
Step:  410000, Reward:  2484.678 [   0.000], Avg:   321.790 (1.000) <0-05:44:12> ({'r_t':  4506.5477, 'eps':     1.0000, 'critic_loss': 16258.8369, 'actor_loss':    -7.1449, 'alpha_loss':     0.1630, 'eps_e':     1.0000})
Step:  411000, Reward:   -59.639 [   0.000], Avg:   320.864 (1.000) <0-05:44:49> ({'r_t':  4725.1988, 'eps':     1.0000, 'critic_loss': 12527.4268, 'actor_loss':    -7.3746, 'alpha_loss':     0.2119, 'eps_e':     1.0000})
Step:  412000, Reward:   -75.225 [   0.000], Avg:   319.905 (1.000) <0-05:45:31> ({'r_t':   677.1507, 'eps':     1.0000, 'critic_loss': 13119.2656, 'actor_loss':    -6.5393, 'alpha_loss':     0.1331, 'eps_e':     1.0000})
Step:  413000, Reward:   -55.975 [   0.000], Avg:   318.997 (1.000) <0-05:46:07> ({'r_t':  1023.5919, 'eps':     1.0000, 'critic_loss': 12977.2568, 'actor_loss':    -5.2287, 'alpha_loss':     0.0670, 'eps_e':     1.0000})
Step:  414000, Reward:   -66.384 [   0.000], Avg:   318.068 (1.000) <0-05:46:48> ({'r_t':  1062.2888, 'eps':     1.0000, 'critic_loss': 12554.7402, 'actor_loss':    -6.1695, 'alpha_loss':     0.0673, 'eps_e':     1.0000})
Step:  415000, Reward:   -52.065 [   0.000], Avg:   317.178 (1.000) <0-05:47:27> ({'r_t':   148.1774, 'eps':     1.0000, 'critic_loss': 11363.8838, 'actor_loss':    -4.5304, 'alpha_loss':    -0.0685, 'eps_e':     1.0000})
Step:  416000, Reward: 15783.938 [   0.000], Avg:   354.269 (1.000) <0-05:48:28> ({'r_t':   167.2160, 'eps':     1.0000, 'critic_loss':  7509.6636, 'actor_loss':    -3.0699, 'alpha_loss':    -0.2440, 'eps_e':     1.0000})
Step:  417000, Reward:   -58.581 [   0.000], Avg:   353.281 (1.000) <0-05:49:03> ({'r_t':  7249.7692, 'eps':     1.0000, 'critic_loss': 18270.9238, 'actor_loss':    -2.3341, 'alpha_loss':    -0.2924, 'eps_e':     1.0000})
Step:  418000, Reward: 18369.733 [   0.000], Avg:   396.280 (1.000) <0-05:49:51> ({'r_t':  6960.3061, 'eps':     1.0000, 'critic_loss': 50099.3398, 'actor_loss':    -2.1840, 'alpha_loss':    -0.2351, 'eps_e':     1.0000})
Step:  419000, Reward:   427.288 [   0.000], Avg:   396.354 (1.000) <0-05:50:39> ({'r_t': 10249.4937, 'eps':     1.0000, 'critic_loss': 104037.0000, 'actor_loss':    -3.6068, 'alpha_loss':    -0.0187, 'eps_e':     1.0000})
Step:  420000, Reward:   -40.988 [   0.000], Avg:   395.315 (1.000) <0-05:51:19> ({'r_t':  1933.2032, 'eps':     1.0000, 'critic_loss': 143083.2500, 'actor_loss':    -5.5571, 'alpha_loss':     0.0242, 'eps_e':     1.0000})
Step:  421000, Reward:   -58.774 [   0.000], Avg:   394.239 (1.000) <0-05:51:54> ({'r_t':  1280.9451, 'eps':     1.0000, 'critic_loss': 143376.5312, 'actor_loss':    -6.2970, 'alpha_loss':     0.0441, 'eps_e':     1.0000})
Step:  422000, Reward:   -48.887 [   0.000], Avg:   393.191 (1.000) <0-05:52:33> ({'r_t':  -275.8261, 'eps':     1.0000, 'critic_loss': 139999.4375, 'actor_loss':    -7.4180, 'alpha_loss':     0.0656, 'eps_e':     1.0000})
Step:  423000, Reward:   -44.005 [   0.000], Avg:   392.160 (1.000) <0-05:53:12> ({'r_t':   122.3267, 'eps':     1.0000, 'critic_loss': 134981.4375, 'actor_loss':    -7.5211, 'alpha_loss':    -0.0777, 'eps_e':     1.0000})
Step:  424000, Reward:   -47.044 [   0.000], Avg:   391.127 (1.000) <0-05:53:53> ({'r_t':    39.7833, 'eps':     1.0000, 'critic_loss': 105564.5938, 'actor_loss':    -5.2380, 'alpha_loss':    -0.4004, 'eps_e':     1.0000})
Step:  425000, Reward:   -61.159 [   0.000], Avg:   390.065 (1.000) <0-05:54:39> ({'r_t':  -192.2139, 'eps':     1.0000, 'critic_loss': 63218.7891, 'actor_loss':    -1.9882, 'alpha_loss':    -0.7365, 'eps_e':     1.0000})
Step:  426000, Reward:   -60.490 [   0.000], Avg:   389.010 (1.000) <0-05:55:18> ({'r_t':    99.5365, 'eps':     1.0000, 'critic_loss':  7435.1572, 'actor_loss':    -1.5636, 'alpha_loss':    -0.9053, 'eps_e':     1.0000})
Step:  427000, Reward:   -36.984 [   0.000], Avg:   388.015 (1.000) <0-05:55:57> ({'r_t':  1320.0997, 'eps':     1.0000, 'critic_loss':  1772.5277, 'actor_loss':    -1.0573, 'alpha_loss':    -0.9626, 'eps_e':     1.0000})
Step:  428000, Reward:   -42.056 [   0.000], Avg:   387.012 (1.000) <0-05:56:36> ({'r_t':  2751.2434, 'eps':     1.0000, 'critic_loss':  1908.4298, 'actor_loss':    -1.3943, 'alpha_loss':    -0.8413, 'eps_e':     1.0000})
Step:  429000, Reward:   -34.677 [   0.000], Avg:   386.031 (1.000) <0-05:57:18> ({'r_t':  3859.8622, 'eps':     1.0000, 'critic_loss':  4130.2202, 'actor_loss':    -1.5958, 'alpha_loss':    -0.5382, 'eps_e':     1.0000})
Step:  430000, Reward:   -47.532 [   0.000], Avg:   385.025 (1.000) <0-05:57:57> ({'r_t':  1782.0664, 'eps':     1.0000, 'critic_loss':  6279.4404, 'actor_loss':    -2.2186, 'alpha_loss':    -0.2150, 'eps_e':     1.0000})
Step:  431000, Reward:   -52.521 [   0.000], Avg:   384.013 (1.000) <0-05:58:40> ({'r_t':   861.4060, 'eps':     1.0000, 'critic_loss':  6946.9912, 'actor_loss':    -2.4270, 'alpha_loss':    -0.1972, 'eps_e':     1.0000})
Step:  432000, Reward:   -24.878 [   0.000], Avg:   383.068 (1.000) <0-05:59:23> ({'r_t':   813.1243, 'eps':     1.0000, 'critic_loss':  7476.8267, 'actor_loss':    -1.9070, 'alpha_loss':    -0.0760, 'eps_e':     1.0000})
Step:  433000, Reward:   -47.222 [   0.000], Avg:   382.077 (1.000) <0-06:00:01> ({'r_t':  2909.3040, 'eps':     1.0000, 'critic_loss': 10141.1240, 'actor_loss':    -1.6694, 'alpha_loss':    -0.0255, 'eps_e':     1.0000})
Step:  434000, Reward:   -71.379 [   0.000], Avg:   381.034 (1.000) <0-06:00:38> ({'r_t':  2924.8594, 'eps':     1.0000, 'critic_loss': 11001.0518, 'actor_loss':    -1.7078, 'alpha_loss':    -0.0499, 'eps_e':     1.0000})
Step:  435000, Reward:   -79.095 [   0.000], Avg:   379.979 (1.000) <0-06:01:15> ({'r_t':    17.8529, 'eps':     1.0000, 'critic_loss': 14361.0078, 'actor_loss':    -1.8092, 'alpha_loss':    -0.1529, 'eps_e':     1.0000})
Step:  436000, Reward:   -61.968 [   0.000], Avg:   378.968 (1.000) <0-06:01:59> ({'r_t':   651.4144, 'eps':     1.0000, 'critic_loss': 10428.6572, 'actor_loss':    -1.3733, 'alpha_loss':    -0.2521, 'eps_e':     1.0000})
Step:  437000, Reward:   -48.427 [   0.000], Avg:   377.992 (1.000) <0-06:02:34> ({'r_t':  3384.5354, 'eps':     1.0000, 'critic_loss': 16059.5537, 'actor_loss':    -1.4896, 'alpha_loss':    -0.1180, 'eps_e':     1.0000})
Step:  438000, Reward:   412.890 [   0.000], Avg:   378.071 (1.000) <0-06:03:31> ({'r_t':  1863.5544, 'eps':     1.0000, 'critic_loss': 16472.2422, 'actor_loss':    -1.5013, 'alpha_loss':    -0.1304, 'eps_e':     1.0000})
Step:  439000, Reward:   -44.428 [   0.000], Avg:   377.111 (1.000) <0-06:04:15> ({'r_t':  8822.7554, 'eps':     1.0000, 'critic_loss': 22419.7461, 'actor_loss':    -1.6403, 'alpha_loss':    -0.0602, 'eps_e':     1.0000})
Step:  440000, Reward:   -32.930 [   0.000], Avg:   376.181 (1.000) <0-06:04:53> ({'r_t': 12449.6803, 'eps':     1.0000, 'critic_loss': 62788.4766, 'actor_loss':    -1.9321, 'alpha_loss':     0.2052, 'eps_e':     1.0000})
Step:  441000, Reward:   -65.840 [   0.000], Avg:   375.181 (1.000) <0-06:05:36> ({'r_t':  1438.1890, 'eps':     1.0000, 'critic_loss': 77356.8828, 'actor_loss':    -2.3898, 'alpha_loss':     0.4137, 'eps_e':     1.0000})
Step:  442000, Reward:  -138.548 [   0.000], Avg:   374.022 (1.000) <0-06:06:23> ({'r_t':  -303.7312, 'eps':     1.0000, 'critic_loss': 75828.9141, 'actor_loss':    -2.1679, 'alpha_loss':     0.3237, 'eps_e':     1.0000})
Step:  443000, Reward:   -27.913 [   0.000], Avg:   373.116 (1.000) <0-06:07:07> ({'r_t':  -229.2394, 'eps':     1.0000, 'critic_loss': 77504.1094, 'actor_loss':    -2.1017, 'alpha_loss':     0.1652, 'eps_e':     1.0000})
Step:  444000, Reward:  -115.089 [   0.000], Avg:   372.019 (1.000) <0-06:07:53> ({'r_t':  -171.1871, 'eps':     1.0000, 'critic_loss': 57132.1719, 'actor_loss':    -2.3421, 'alpha_loss':     0.0370, 'eps_e':     1.0000})
Step:  445000, Reward:   -44.201 [   0.000], Avg:   371.086 (1.000) <0-06:08:31> ({'r_t':  -122.6194, 'eps':     1.0000, 'critic_loss': 51812.9609, 'actor_loss':    -2.1202, 'alpha_loss':     0.0063, 'eps_e':     1.0000})
Step:  446000, Reward:   -28.675 [   0.000], Avg:   370.192 (1.000) <0-06:09:11> ({'r_t':  -203.5444, 'eps':     1.0000, 'critic_loss': 26285.7109, 'actor_loss':    -1.5200, 'alpha_loss':    -0.2805, 'eps_e':     1.0000})
Step:  447000, Reward:   -41.124 [   0.000], Avg:   369.274 (1.000) <0-06:09:48> ({'r_t':  -200.9581, 'eps':     1.0000, 'critic_loss':  3359.0842, 'actor_loss':    -1.2984, 'alpha_loss':    -0.5052, 'eps_e':     1.0000})
Step:  448000, Reward:   -56.334 [   0.000], Avg:   368.326 (1.000) <0-06:10:30> ({'r_t':  -133.6612, 'eps':     1.0000, 'critic_loss':   759.3948, 'actor_loss':    -1.2112, 'alpha_loss':    -0.5257, 'eps_e':     1.0000})
Step:  449000, Reward: 10870.729 [   0.000], Avg:   391.665 (1.000) <0-06:11:36> ({'r_t':   701.8989, 'eps':     1.0000, 'critic_loss':  1430.8466, 'actor_loss':    -2.0540, 'alpha_loss':    -0.2533, 'eps_e':     1.0000})
Step:  450000, Reward:   -76.634 [   0.000], Avg:   390.626 (1.000) <0-06:12:13> ({'r_t':  1623.5850, 'eps':     1.0000, 'critic_loss':  5031.5859, 'actor_loss':    -3.1808, 'alpha_loss':     0.1987, 'eps_e':     1.0000})
Step:  451000, Reward:   -43.306 [   0.000], Avg:   389.666 (1.000) <0-06:12:53> ({'r_t':   200.9875, 'eps':     1.0000, 'critic_loss':  5019.9399, 'actor_loss':    -3.2865, 'alpha_loss':     0.1202, 'eps_e':     1.0000})
Step:  452000, Reward:   -52.099 [   0.000], Avg:   388.691 (1.000) <0-06:13:33> ({'r_t':   585.7667, 'eps':     1.0000, 'critic_loss':  6808.5752, 'actor_loss':    -3.8178, 'alpha_loss':     0.0037, 'eps_e':     1.0000})
Step:  453000, Reward:   -38.326 [   0.000], Avg:   387.750 (1.000) <0-06:14:12> ({'r_t':   177.9306, 'eps':     1.0000, 'critic_loss':  7966.2993, 'actor_loss':    -3.6550, 'alpha_loss':     0.0646, 'eps_e':     1.0000})
Step:  454000, Reward:  -162.542 [   0.000], Avg:   386.541 (1.000) <0-06:14:57> ({'r_t':  -271.6193, 'eps':     1.0000, 'critic_loss':  7645.6211, 'actor_loss':    -3.0625, 'alpha_loss':    -0.1875, 'eps_e':     1.0000})
Step:  455000, Reward:   -53.443 [   0.000], Avg:   385.576 (1.000) <0-06:15:38> ({'r_t':   -10.7676, 'eps':     1.0000, 'critic_loss':  8365.5088, 'actor_loss':    -2.0594, 'alpha_loss':    -0.1851, 'eps_e':     1.0000})
Step:  456000, Reward:   -97.410 [   0.000], Avg:   384.519 (1.000) <0-06:16:18> ({'r_t':  5464.1939, 'eps':     1.0000, 'critic_loss': 13188.7539, 'actor_loss':    -1.3397, 'alpha_loss':    -0.4299, 'eps_e':     1.0000})
Step:  457000, Reward:   -61.780 [   0.000], Avg:   383.545 (1.000) <0-06:16:58> ({'r_t':  -329.7903, 'eps':     1.0000, 'critic_loss': 25920.7148, 'actor_loss':    -1.7371, 'alpha_loss':    -0.3809, 'eps_e':     1.0000})
Step:  458000, Reward:   -41.194 [   0.000], Avg:   382.619 (1.000) <0-06:17:36> ({'r_t':  -265.9879, 'eps':     1.0000, 'critic_loss': 24676.0312, 'actor_loss':    -1.8713, 'alpha_loss':    -0.4088, 'eps_e':     1.0000})
Step:  459000, Reward:   -49.221 [   0.000], Avg:   381.681 (1.000) <0-06:18:13> ({'r_t':  -292.3371, 'eps':     1.0000, 'critic_loss': 22770.5449, 'actor_loss':    -1.9843, 'alpha_loss':    -0.2044, 'eps_e':     1.0000})
Step:  460000, Reward:   -78.967 [   0.000], Avg:   380.681 (1.000) <0-06:18:53> ({'r_t':  -228.3164, 'eps':     1.0000, 'critic_loss': 19058.9082, 'actor_loss':    -2.2184, 'alpha_loss':    -0.1845, 'eps_e':     1.0000})
Step:  461000, Reward:   -49.846 [   0.000], Avg:   379.749 (1.000) <0-06:19:30> ({'r_t':  -356.3685, 'eps':     1.0000, 'critic_loss': 20569.7969, 'actor_loss':    -2.5483, 'alpha_loss':    -0.0640, 'eps_e':     1.0000})
Step:  462000, Reward:   -44.876 [   0.000], Avg:   378.832 (1.000) <0-06:20:09> ({'r_t':   159.2052, 'eps':     1.0000, 'critic_loss': 17293.8555, 'actor_loss':    -2.5490, 'alpha_loss':    -0.1009, 'eps_e':     1.0000})
Step:  463000, Reward:   -42.387 [   0.000], Avg:   377.925 (1.000) <0-06:20:47> ({'r_t':  -347.3677, 'eps':     1.0000, 'critic_loss':  8428.9121, 'actor_loss':    -2.5456, 'alpha_loss':    -0.0941, 'eps_e':     1.0000})
Step:  464000, Reward:   -20.351 [   0.000], Avg:   377.068 (1.000) <0-06:21:28> ({'r_t':   -79.2769, 'eps':     1.0000, 'critic_loss':  4251.1890, 'actor_loss':    -2.3516, 'alpha_loss':    -0.0407, 'eps_e':     1.0000})
Step:  465000, Reward:   -50.309 [   0.000], Avg:   376.151 (1.000) <0-06:22:08> ({'r_t':   821.6750, 'eps':     1.0000, 'critic_loss':  9313.6816, 'actor_loss':    -1.8734, 'alpha_loss':    -0.1650, 'eps_e':     1.0000})
Step:  466000, Reward:   -93.020 [   0.000], Avg:   375.146 (1.000) <0-06:22:51> ({'r_t':   551.2050, 'eps':     1.0000, 'critic_loss': 18316.6895, 'actor_loss':    -1.3536, 'alpha_loss':    -0.1829, 'eps_e':     1.0000})
Step:  467000, Reward:  2930.091 [   0.000], Avg:   380.606 (1.000) <0-06:23:37> ({'r_t':   -84.6388, 'eps':     1.0000, 'critic_loss': 19287.2539, 'actor_loss':    -1.6431, 'alpha_loss':     0.1434, 'eps_e':     1.0000})
Step:  468000, Reward:   -82.330 [   0.000], Avg:   379.619 (1.000) <0-06:24:18> ({'r_t':     3.3360, 'eps':     1.0000, 'critic_loss': 21221.5234, 'actor_loss':    -1.8482, 'alpha_loss':     0.3912, 'eps_e':     1.0000})
Step:  469000, Reward:   -43.178 [   0.000], Avg:   378.719 (1.000) <0-06:24:59> ({'r_t':  -265.6355, 'eps':     1.0000, 'critic_loss': 19281.3594, 'actor_loss':    -1.7549, 'alpha_loss':     0.2767, 'eps_e':     1.0000})
Step:  470000, Reward:  -954.551 [   0.000], Avg:   375.888 (1.000) <0-06:26:05> ({'r_t':  6720.8981, 'eps':     1.0000, 'critic_loss': 25708.0762, 'actor_loss':    -1.8108, 'alpha_loss':     0.4835, 'eps_e':     1.0000})
Step:  471000, Reward:   -52.862 [   0.000], Avg:   374.980 (1.000) <0-06:26:46> ({'r_t': 11980.3380, 'eps':     1.0000, 'critic_loss': 75805.8438, 'actor_loss':    -2.1499, 'alpha_loss':     0.7056, 'eps_e':     1.0000})
Step:  472000, Reward:  1024.983 [   0.000], Avg:   376.354 (1.000) <0-06:27:52> ({'r_t': 16448.2994, 'eps':     1.0000, 'critic_loss': 125120.4922, 'actor_loss':    -3.2563, 'alpha_loss':     1.2179, 'eps_e':     1.0000})
Step:  473000, Reward:  1511.083 [   0.000], Avg:   378.748 (1.000) <0-06:28:58> ({'r_t':  2089.8690, 'eps':     1.0000, 'critic_loss': 300938.7188, 'actor_loss':    -4.4840, 'alpha_loss':     1.5291, 'eps_e':     1.0000})
Step:  474000, Reward:   -59.770 [   0.000], Avg:   377.825 (1.000) <0-06:29:33> ({'r_t': 36404.7119, 'eps':     1.0000, 'critic_loss': 387439.0000, 'actor_loss':    -5.7239, 'alpha_loss':     1.7699, 'eps_e':     1.0000})
Step:  475000, Reward:   -50.167 [   0.000], Avg:   376.926 (1.000) <0-06:30:09> ({'r_t':  9105.9632, 'eps':     1.0000, 'critic_loss': 434069.7500, 'actor_loss':    -9.0352, 'alpha_loss':     1.8331, 'eps_e':     1.0000})
Step:  476000, Reward:   -50.458 [   0.000], Avg:   376.030 (1.000) <0-06:30:44> ({'r_t':  -575.6905, 'eps':     1.0000, 'critic_loss': 434159.5312, 'actor_loss':   -13.0596, 'alpha_loss':     1.5887, 'eps_e':     1.0000})
Step:  477000, Reward:  -149.114 [   0.000], Avg:   374.931 (1.000) <0-06:31:24> ({'r_t':  -641.9536, 'eps':     1.0000, 'critic_loss': 348451.9375, 'actor_loss':   -15.4873, 'alpha_loss':     1.2248, 'eps_e':     1.0000})
Step:  478000, Reward:  -114.545 [   0.000], Avg:   373.909 (1.000) <0-06:32:02> ({'r_t':  -736.7488, 'eps':     1.0000, 'critic_loss': 242285.5625, 'actor_loss':   -10.8125, 'alpha_loss':     0.4718, 'eps_e':     1.0000})
Step:  479000, Reward:  -156.650 [   0.000], Avg:   372.804 (1.000) <0-06:32:44> ({'r_t':  -747.4172, 'eps':     1.0000, 'critic_loss': 152159.9375, 'actor_loss':   -10.9184, 'alpha_loss':     0.0813, 'eps_e':     1.0000})
Step:  480000, Reward:   -60.037 [   0.000], Avg:   371.904 (1.000) <0-06:33:22> ({'r_t':  -453.8313, 'eps':     1.0000, 'critic_loss': 118913.6172, 'actor_loss':    -9.0926, 'alpha_loss':    -0.3080, 'eps_e':     1.0000})
Step:  481000, Reward:   -52.613 [   0.000], Avg:   371.023 (1.000) <0-06:34:00> ({'r_t':  -458.4939, 'eps':     1.0000, 'critic_loss': 33910.1406, 'actor_loss':    -5.1619, 'alpha_loss':    -0.8579, 'eps_e':     1.0000})
Step:  482000, Reward:   -62.989 [   0.000], Avg:   370.125 (1.000) <0-06:34:41> ({'r_t':  -278.5205, 'eps':     1.0000, 'critic_loss':   240.8860, 'actor_loss':    -2.9469, 'alpha_loss':    -0.9752, 'eps_e':     1.0000})
Step:  483000, Reward:   -47.567 [   0.000], Avg:   369.262 (1.000) <0-06:35:21> ({'r_t':  -248.9072, 'eps':     1.0000, 'critic_loss':   151.1939, 'actor_loss':    -2.5585, 'alpha_loss':    -1.1007, 'eps_e':     1.0000})
Step:  484000, Reward:   -95.890 [   0.000], Avg:   368.303 (1.000) <0-06:36:03> ({'r_t':  -310.8519, 'eps':     1.0000, 'critic_loss':    56.5579, 'actor_loss':    -2.1374, 'alpha_loss':    -0.9860, 'eps_e':     1.0000})
Step:  485000, Reward:     0.783 [   0.000], Avg:   367.546 (1.000) <0-06:36:45> ({'r_t':  -209.2185, 'eps':     1.0000, 'critic_loss':    32.7841, 'actor_loss':    -1.5060, 'alpha_loss':    -0.8761, 'eps_e':     1.0000})
Step:  486000, Reward:    -9.445 [   0.000], Avg:   366.772 (1.000) <0-06:37:26> ({'r_t':    -8.4697, 'eps':     1.0000, 'critic_loss':    25.9631, 'actor_loss':    -1.2102, 'alpha_loss':    -0.7602, 'eps_e':     1.0000})
Step:  487000, Reward:   -19.929 [   0.000], Avg:   365.980 (1.000) <0-06:38:07> ({'r_t':   -28.0634, 'eps':     1.0000, 'critic_loss':    19.7896, 'actor_loss':    -1.2193, 'alpha_loss':    -0.5696, 'eps_e':     1.0000})
Step:  488000, Reward:   -40.416 [   0.000], Avg:   365.149 (1.000) <0-06:38:48> ({'r_t':   -74.1531, 'eps':     1.0000, 'critic_loss':    14.6631, 'actor_loss':    -1.3699, 'alpha_loss':    -0.3257, 'eps_e':     1.0000})
Step:  489000, Reward:   -44.758 [   0.000], Avg:   364.312 (1.000) <0-06:39:28> ({'r_t':  -346.6873, 'eps':     1.0000, 'critic_loss':    12.9500, 'actor_loss':    -1.1380, 'alpha_loss':    -0.3898, 'eps_e':     1.0000})
Step:  490000, Reward:   -10.358 [   0.000], Avg:   363.549 (1.000) <0-06:40:09> ({'r_t':  -175.7154, 'eps':     1.0000, 'critic_loss':    12.9848, 'actor_loss':    -0.9843, 'alpha_loss':    -0.2696, 'eps_e':     1.0000})
Step:  491000, Reward:     7.104 [   0.000], Avg:   362.825 (1.000) <0-06:40:50> ({'r_t':  -538.7250, 'eps':     1.0000, 'critic_loss':    13.0795, 'actor_loss':    -0.6339, 'alpha_loss':    -0.4579, 'eps_e':     1.0000})
Step:  492000, Reward:   -51.481 [   0.000], Avg:   361.984 (1.000) <0-06:41:30> ({'r_t':  -151.8171, 'eps':     1.0000, 'critic_loss':    17.3034, 'actor_loss':    -0.5724, 'alpha_loss':    -0.4926, 'eps_e':     1.0000})
Step:  493000, Reward:   -15.278 [   0.000], Avg:   361.221 (1.000) <0-06:42:10> ({'r_t':   -99.2744, 'eps':     1.0000, 'critic_loss':    16.1120, 'actor_loss':    -0.6895, 'alpha_loss':    -0.7842, 'eps_e':     1.0000})
Step:  494000, Reward:     2.630 [   0.000], Avg:   360.496 (1.000) <0-06:42:50> ({'r_t':     7.4403, 'eps':     1.0000, 'critic_loss':    15.0137, 'actor_loss':    -0.7385, 'alpha_loss':    -1.1245, 'eps_e':     1.0000})
Step:  495000, Reward:     9.285 [   0.000], Avg:   359.788 (1.000) <0-06:43:29> ({'r_t':    34.2369, 'eps':     1.0000, 'critic_loss':    12.9999, 'actor_loss':    -0.7840, 'alpha_loss':    -1.0690, 'eps_e':     1.0000})
Step:  496000, Reward:     5.575 [   0.000], Avg:   359.075 (1.000) <0-06:44:08> ({'r_t':    34.9914, 'eps':     1.0000, 'critic_loss':    10.2125, 'actor_loss':    -0.7255, 'alpha_loss':    -0.8346, 'eps_e':     1.0000})
Step:  497000, Reward:     4.867 [   0.000], Avg:   358.364 (1.000) <0-06:44:46> ({'r_t':    19.1478, 'eps':     1.0000, 'critic_loss':     7.8942, 'actor_loss':    -0.6174, 'alpha_loss':    -0.8442, 'eps_e':     1.0000})
Step:  498000, Reward:    -1.242 [   0.000], Avg:   357.643 (1.000) <0-06:45:24> ({'r_t':   -16.3398, 'eps':     1.0000, 'critic_loss':     3.2792, 'actor_loss':    -0.4955, 'alpha_loss':    -1.1486, 'eps_e':     1.0000})
Step:  499000, Reward:     5.780 [   0.000], Avg:   356.940 (1.000) <0-06:46:01> ({'r_t':    25.2236, 'eps':     1.0000, 'critic_loss':     2.0362, 'actor_loss':    -0.3965, 'alpha_loss':    -1.1348, 'eps_e':     1.0000})
Step:  500000, Reward:    16.692 [   0.000], Avg:   356.261 (1.000) <0-06:46:39> ({'r_t':    81.8568, 'eps':     1.0000, 'critic_loss':     1.5657, 'actor_loss':    -0.3773, 'alpha_loss':    -1.3006, 'eps_e':     1.0000})
