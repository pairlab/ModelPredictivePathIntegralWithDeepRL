Model: <class 'src.models.pytorch.agents.sac.SACAgent'>, Env: LunarLanderContinuous-v2, Date: 07/06/2020 00:58:59
CPU: 20 Core, 0.0GHz, 377.59 GB, Linux-4.14.175-llgrid-10ms-x86_64-with-debian-buster-sid
GPU 0: Tesla V100-PCIE-32GB, 32.51 GB (Driver: 440.33.01)
GPU 1: Tesla V100-PCIE-32GB, 32.51 GB (Driver: 440.33.01)
Git URL: https://github.com/shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: a87cc07c1e7349ce4ee6e7dc2370e05e7e4f1f07
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 1000000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   dynamics_size = 8
   state_size = (8,)
   action_size = (2,)
   env_name = LunarLanderContinuous-v2
   rank = 0
   size = 17
   split = 17
   model = sac
   framework = pt
   train_prop = 1.0
   tcp_ports = []
   tcp_rank = 0
   num_envs = 1
   nsteps = 500000
   render = False
   trial = False
   icm = False
   rs = False,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f328ea54ef0> 
	env = <GymEnv<TimeLimit<LunarLanderContinuous<LunarLanderContinuous-v2>>>> 
		env = <TimeLimit<LunarLanderContinuous<LunarLanderContinuous-v2>>> 
			env = <LunarLanderContinuous<LunarLanderContinuous-v2>> 
				np_random = RandomState(MT19937)
				viewer = None
				world = b2World(autoClearForces=True,
				        bodies=[b2Body(active=True,
				                      angle=0.0,
				                      angularDamping=0.0,
				                      angularVelocity=0.0,
				                      awake=True,
				                      bullet=False,
				                      contacts=[],
				                      fixedRotation=False,...  )],
				        bodyCount=4,
				        contactCount=0,
				        contactFilter=None,
				        contactListener=ContactDetector(),
				        contactManager=b2ContactManager(allocator=<Swig Object of type 'b2BlockAllocator *' at 0x7f328eb3b900>,
				                                        broadPhase=proxyCount=14,),
				                                        contactCount=0,
				                                        contactFilter=b2ContactFilter(),
				                                        contactList=None,
				                                        contactListener=b2ContactListener(),
				                                        ),
				        contacts=[],
				        continuousPhysics=True,
				        destructionListener=None,
				        gravity=b2Vec2(0,-10),
				        jointCount=2,
				        joints=[b2RevoluteJoint(active=True,
				                               anchorA=b2Vec2(9.97348,13.3407),
				                               anchorB=b2Vec2(9.97348,13.3407),
				                               angle=0.5427989363670349,
				                               bodyA=b2Body(active=True,...  )],
				        locked=False,
				        proxyCount=14,
				        renderer=None,
				        subStepping=False,
				        warmStarting=True,
				        )
				moon = b2Body(active=True,
				       angle=0.0,
				       angularDamping=0.0,
				       angularVelocity=0.0,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.0,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.0,
				                                      awake=True,...  )],
				       inertia=0.0,
				       joints=[],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(0,0),
				       localCenter=b2Vec2(0,0),
				       mass=0.0,
				       massData=I=0.0,center=b2Vec2(0,0),mass=0.0,),
				       position=b2Vec2(0,0),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3be40> >,angle=0.0,position=b2Vec2(0,0),),
				       type=0,
				       userData=None,
				       worldCenter=b2Vec2(0,0),
				       )
				lander = b2Body(active=True,
				       angle=0.0030797214712947607,
				       angularDamping=0.0,
				       angularVelocity=0.1521202176809311,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.0030797214712947607,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.1521202176809311,...  )],
				       inertia=0.8333148956298828,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.97348,13.3407),
				                                                anchorB=b2Vec2(9.97348,13.3407),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.34314,0.0619453),
				       localCenter=b2Vec2(0,0.101307),
				       mass=4.816666603088379,
				       massData=I=0.8333148956298828,center=b2Vec2(0,0.101307),mass=4.816666603088379,),
				       position=b2Vec2(9.97348,13.3407),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3be10> >,angle=0.0030797214712947607,position=b2Vec2(9.97348,13.3407),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.97317,13.442),
				       )
				particles = []
				prev_reward = None
				observation_space = Box(8,) 
					dtype = float32
					shape = (8,)
					low = [-inf -inf -inf -inf -inf -inf -inf -inf]
					high = [ inf  inf  inf  inf  inf  inf  inf  inf]
					bounded_below = [False False False False False False False False]
					bounded_above = [False False False False False False False False]
					np_random = RandomState(MT19937)
				action_space = Box(2,) 
					dtype = float32
					shape = (2,)
					low = [-1.000 -1.000]
					high = [ 1.000  1.000]
					bounded_below = [ True  True]
					bounded_above = [ True  True]
					np_random = RandomState(MT19937)
				game_over = False
				prev_shaping = -168.2942204930803
				helipad_x1 = 8.0
				helipad_x2 = 12.0
				helipad_y = 3.3333333333333335
				sky_polys = [[(0.0, 3.7485395508496286), (2.0, 3.5506350578723618), (2.0, 13.333333333333334), (0.0, 13.333333333333334)], [(2.0, 3.5506350578723618), (4.0, 4.112523251164764), (4.0, 13.333333333333334), (2.0, 13.333333333333334)], [(4.0, 4.112523251164764), (6.0, 3.72894322737402), (6.0, 13.333333333333334), (4.0, 13.333333333333334)], [(6.0, 3.72894322737402), (8.0, 3.3000000000000003), (8.0, 13.333333333333334), (6.0, 13.333333333333334)], [(8.0, 3.3000000000000003), (10.0, 3.3000000000000003), (10.0, 13.333333333333334), (8.0, 13.333333333333334)], [(10.0, 3.3000000000000003), (12.0, 3.3000000000000003), (12.0, 13.333333333333334), (10.0, 13.333333333333334)], [(12.0, 3.3000000000000003), (14.0, 3.1511661657082444), (14.0, 13.333333333333334), (12.0, 13.333333333333334)], [(14.0, 3.1511661657082444), (16.0, 3.134828024789726), (16.0, 13.333333333333334), (14.0, 13.333333333333334)], [(16.0, 3.134828024789726), (18.0, 4.088307013698859), (18.0, 13.333333333333334), (16.0, 13.333333333333334)], [(18.0, 4.088307013698859), (20.0, 4.8639885683419015), (20.0, 13.333333333333334), (18.0, 13.333333333333334)]]
				legs = [b2Body(active=True,
				       angle=0.4958786368370056,
				       angularDamping=0.0,
				       angularVelocity=0.1521279215812683,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.4958786368370056,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.1521279215812683,
				                                      awake=True,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.97348,13.3407),
				                                                anchorB=b2Vec2(9.97348,13.3407),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.2315,0.15867),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(10.8453,13.1302),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3bf30> >,angle=0.4958786368370056,position=b2Vec2(10.8453,13.1302),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(10.8453,13.1302),
				       ), b2Body(active=True,
				       angle=-0.4881656765937805,
				       angularDamping=0.0,
				       angularVelocity=0.1521216630935669,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=-0.4881656765937805,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.1521216630935669,
				                                      awake=True,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.97348,13.3407),
				                                                anchorB=b2Vec2(9.97348,13.3407),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.2315,-0.0347798),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(9.10328,13.1235),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3bf00> >,angle=-0.4881657063961029,position=b2Vec2(9.10328,13.1235),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.10328,13.1235),
				       )]
				drawlist = [b2Body(active=True,
				       angle=0.0030797214712947607,
				       angularDamping=0.0,
				       angularVelocity=0.1521202176809311,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.0030797214712947607,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.1521202176809311,...  )],
				       inertia=0.8333148956298828,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.97348,13.3407),
				                                                anchorB=b2Vec2(9.97348,13.3407),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.34314,0.0619453),
				       localCenter=b2Vec2(0,0.101307),
				       mass=4.816666603088379,
				       massData=I=0.8333148956298828,center=b2Vec2(0,0.101307),mass=4.816666603088379,),
				       position=b2Vec2(9.97348,13.3407),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3be40> >,angle=0.0030797214712947607,position=b2Vec2(9.97348,13.3407),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.97317,13.442),
				       ), b2Body(active=True,
				       angle=0.4958786368370056,
				       angularDamping=0.0,
				       angularVelocity=0.1521279215812683,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.4958786368370056,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.1521279215812683,
				                                      awake=True,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.97348,13.3407),
				                                                anchorB=b2Vec2(9.97348,13.3407),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.2315,0.15867),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(10.8453,13.1302),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3bea0> >,angle=0.4958786368370056,position=b2Vec2(10.8453,13.1302),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(10.8453,13.1302),
				       ), b2Body(active=True,
				       angle=-0.4881656765937805,
				       angularDamping=0.0,
				       angularVelocity=0.1521216630935669,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=-0.4881656765937805,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.1521216630935669,
				                                      awake=True,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.97348,13.3407),
				                                                anchorB=b2Vec2(9.97348,13.3407),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.2315,-0.0347798),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(9.10328,13.1235),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f328eb3bc00> >,angle=-0.4881657063961029,position=b2Vec2(9.10328,13.1235),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.10328,13.1235),
				       )]
				spec = EnvSpec(LunarLanderContinuous-v2) 
					id = LunarLanderContinuous-v2
					entry_point = gym.envs.box2d:LunarLanderContinuous
					reward_threshold = 200
					nondeterministic = False
					max_episode_steps = 1000
				verbose = 0
			action_space = Box(2,) 
				dtype = float32
				shape = (2,)
				low = [-1.000 -1.000]
				high = [ 1.000  1.000]
				bounded_below = [ True  True]
				bounded_above = [ True  True]
				np_random = RandomState(MT19937)
			observation_space = Box(8,) 
				dtype = float32
				shape = (8,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False]
				bounded_above = [False False False False False False False False]
				np_random = RandomState(MT19937)
			reward_range = (-inf, inf)
			metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}
		action_space = Box(2,) 
			dtype = float32
			shape = (2,)
			low = [-1.000 -1.000]
			high = [ 1.000  1.000]
			bounded_below = [ True  True]
			bounded_above = [ True  True]
			np_random = RandomState(MT19937)
		observation_space = Box(8,) 
			dtype = float32
			shape = (8,)
			low = [-inf -inf -inf -inf -inf -inf -inf -inf]
			high = [ inf  inf  inf  inf  inf  inf  inf  inf]
			bounded_below = [False False False False False False False False]
			bounded_above = [False False False False False False False False]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f328ea544e0> 
			observation_space = Box(8,) 
				dtype = float32
				shape = (8,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False]
				bounded_above = [False False False False False False False False]
				np_random = RandomState(MT19937)
	state_size = (8,)
	action_size = (2,)
	action_space = Box(2,) 
		dtype = float32
		shape = (2,)
		low = [-1.000 -1.000]
		high = [ 1.000  1.000]
		bounded_below = [ True  True]
		bounded_above = [ True  True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.MPIConnection object at 0x7f328eb53fd0> 
		root = 0
		rank = 0
		cluster = <list len=16>
	num_envs = 16
	max_steps = 1000,
agent: <src.models.wrappers.ParallelAgent object at 0x7f328eb53f28> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f328eb53908> 
		state_size = (8,)
	agent = <src.models.pytorch.agents.sac.SACAgent object at 0x7f328eb53940> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f328eb53860> 
			size = (2,)
			dt = 0.2
			action = [-0.594  1.000]
			daction_dt = [-0.425 -1.576]
		discrete = False
		action_size = (2,)
		state_size = (8,)
		config = <src.utils.config.Config object at 0x7f329805fa58> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.998
			NUM_STEPS = 500
			MAX_BUFFER_SIZE = 1000000
			REPLAY_BATCH_SIZE = 32
			TARGET_UPDATE_RATE = 0.0004
			dynamics_size = 8
			state_size = (8,)
			action_size = (2,)
			env_name = LunarLanderContinuous-v2
			rank = 0
			size = 17
			split = 17
			model = sac
			framework = pt
			train_prop = 1.0
			tcp_ports = []
			tcp_rank = 0
			num_envs = 1
			nsteps = 500000
			render = False
			trial = False
			icm = False
			rs = False
		stats = <src.utils.logger.Stats object at 0x7f328eb53898> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = SACNetwork(
			  (actor_local): SACActor(
			    (layer1): Linear(in_features=8, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=2, bias=True)
			    (action_sig): Linear(in_features=256, out_features=2, bias=True)
			  )
			  (actor_target): SACActor(
			    (layer1): Linear(in_features=8, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=2, bias=True)
			    (action_sig): Linear(in_features=256, out_features=2, bias=True)
			  )
			  (critic_local): SACCritic(
			    (net_state): Linear(in_features=8, out_features=512, bias=True)
			    (net_action): Linear(in_features=2, out_features=512, bias=True)
			    (net_layer1): Linear(in_features=1024, out_features=1024, bias=True)
			    (net_layer2): Linear(in_features=1024, out_features=1024, bias=True)
			    (q_value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			  (critic_target): SACCritic(
			    (net_state): Linear(in_features=8, out_features=512, bias=True)
			    (net_action): Linear(in_features=2, out_features=512, bias=True)
			    (net_layer1): Linear(in_features=1024, out_features=1024, bias=True)
			    (net_layer2): Linear(in_features=1024, out_features=1024, bias=True)
			    (q_value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			) 
			discrete = False
			training = True
			tau = 0.0004
			name = sac
			stats = <src.utils.logger.Stats object at 0x7f328eb537f0> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7f329805fa58> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.998
				NUM_STEPS = 500
				MAX_BUFFER_SIZE = 1000000
				REPLAY_BATCH_SIZE = 32
				TARGET_UPDATE_RATE = 0.0004
				dynamics_size = 8
				state_size = (8,)
				action_size = (2,)
				env_name = LunarLanderContinuous-v2
				rank = 0
				size = 17
				split = 17
				model = sac
				framework = pt
				train_prop = 1.0
				tcp_ports = []
				tcp_rank = 0
				num_envs = 1
				nsteps = 500000
				render = False
				trial = False
				icm = False
				rs = False
			device = cuda
			src = ['class SACActor(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config, use_discrete=False):\n\t\tsuper().__init__()\n\t\tinput_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN\n\t\tself.discrete = use_discrete and type(action_size) != tuple\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, actor_hidden)\n\t\tself.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)\n\t\tself.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.action_sig = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\t\tself.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)\n\t\t\n\tdef forward(self, state, action=None, sample=True):\n\t\tstate = self.layer1(state).relu()\n\t\tstate = self.layer2(state).relu()\n\t\tstate = self.layer3(state).relu()\n\t\taction_mu = self.action_mu(state)\n\t\taction_sig = self.action_sig(state).clamp(-5,0).exp()\n\t\tdist = torch.distributions.Normal(action_mu, action_sig)\n\t\taction = dist.rsample() if sample else action_mu\n\t\taction_out = gsoftmax(action_mu, hard=False) if self.discrete else action.tanh()\n\t\tlog_prob = torch.log(action_out+1e-6) if self.discrete else dist.log_prob(action)-torch.log(1-action_out.pow(2)+1e-6)\n\t\treturn action_out, log_prob\n', 'class SACCritic(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config):\n\t\tsuper().__init__()\n\t\tinput_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN\n\t\tself.net_state = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.net_action = torch.nn.Linear(action_size[-1], input_layer)\n\t\tself.net_layer1 = torch.nn.Linear(2*input_layer, critic_hidden)\n\t\tself.net_layer2 = torch.nn.Linear(critic_hidden, critic_hidden)\n\t\tself.q_value = torch.nn.Linear(critic_hidden, 1)\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\n\tdef forward(self, state, action):\n\t\tstate = self.net_state(state).relu()\n\t\tnet_action = self.net_action(action).relu()\n\t\tnet_layer = torch.cat([state, net_action], dim=-1)\n\t\tnet_layer = self.net_layer1(net_layer).relu()\n\t\tnet_layer = self.net_layer2(net_layer).relu()\n\t\tq_value = self.q_value(net_layer)\n\t\treturn q_value\n']
			actor_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			critic_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			alpha_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 0
			)
			target_entropy = -2
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f328eb3ff28> 
			buffer = deque([], maxlen=1000000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f328eb3fc50> 
		size = (2,)
		dt = 0.2
		action = [-0.046  1.000]
		daction_dt = [ 0.794 -0.013]
	discrete = False
	action_size = (2,)
	state_size = (8,)
	config = <src.utils.config.Config object at 0x7f329805fa58> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 1000000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		dynamics_size = 8
		state_size = (8,)
		action_size = (2,)
		env_name = LunarLanderContinuous-v2
		rank = 0
		size = 17
		split = 17
		model = sac
		framework = pt
		train_prop = 1.0
		tcp_ports = []
		tcp_rank = 0
		num_envs = 1
		nsteps = 500000
		render = False
		trial = False
		icm = False
		rs = False
	stats = <src.utils.logger.Stats object at 0x7f328eb3fc88> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import torch
import numpy as np
from .base import PTACNetwork, PTAgent, PTCritic, Conv, gsoftmax
from src.utils.rand import ReplayBuffer

class SACActor(torch.nn.Module):
	def __init__(self, state_size, action_size, config, use_discrete=False):
		super().__init__()
		input_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN
		self.discrete = use_discrete and type(action_size) != tuple
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, actor_hidden)
		self.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)
		self.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])
		self.action_sig = torch.nn.Linear(actor_hidden, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).clamp(-5,0).exp()
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.rsample() if sample else action_mu
		action_out = gsoftmax(action_mu, hard=False) if self.discrete else action.tanh()
		log_prob = torch.log(action_out+1e-6) if self.discrete else dist.log_prob(action)-torch.log(1-action_out.pow(2)+1e-6)
		return action_out, log_prob

class SACCritic(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		input_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN
		self.net_state = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.net_action = torch.nn.Linear(action_size[-1], input_layer)
		self.net_layer1 = torch.nn.Linear(2*input_layer, critic_hidden)
		self.net_layer2 = torch.nn.Linear(critic_hidden, critic_hidden)
		self.q_value = torch.nn.Linear(critic_hidden, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class SACNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, config, actor=SACActor, critic=SACCritic, gpu=True, load=None, name="sac", use_discrete=False):
		self.discrete = use_discrete and critic==SACCritic and type(action_size)!=tuple
		super().__init__(state_size, action_size, config, actor, critic if not self.discrete else lambda s,a,c: PTCritic(s,a,c), gpu=gpu, load=load, name=name)
		self.log_alpha = torch.nn.Parameter(torch.zeros(1, requires_grad=True).to(self.device))
		self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=config.LEARN_RATE)
		self.target_entropy = -np.product(action_size)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob = self.actor_local(state.to(self.device), action_in, sample)
			return [x.cpu().numpy() if numpy else x for x in [action, log_prob]]

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False, probs=False):
		with torch.enable_grad() if grad else torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			q_value = critic(state) if self.discrete else critic(state, action)
			return q_value.cpu().numpy() if numpy else q_value
	
	def optimize(self, states, actions, targets, next_log_probs, dones, config):
		alpha = self.log_alpha.clamp(-5, 0).detach().exp()
		if not self.discrete: next_log_probs = next_log_probs.sum(-1, keepdim=True)
		q_targets = targets - config.DISCOUNT_RATE*alpha*next_log_probs*(1-dones.view(-1,*[1]*(len(targets.shape)-1)))
		q_targets = (actions*q_targets).mean(-1, keepdim=True) if self.discrete else q_targets

		q_values = self.get_q_value(states, actions, grad=True)
		q_values = q_values.gather(-1, actions.argmax(-1, keepdim=True)) if self.discrete else q_values
		critic_loss = (q_values - q_targets.detach()).pow(2).mean()
		self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		actor_action, log_prob = self.actor_local(states)
		q_actions = self.get_q_value(states, actor_action, grad=True)
		q_baseline = q_targets if self.discrete else q_values
		actor_loss = alpha*log_prob - (q_actions - q_baseline.detach())
		actor_loss = actor_action*actor_loss if self.discrete else actor_loss
		self.step(self.actor_optimizer, actor_loss.mean(), self.actor_local.parameters())
		
		log_prob = (actor_action*log_prob).sum(-1) if self.discrete else log_prob
		alpha_loss = -(self.log_alpha * (log_prob.detach() + self.target_entropy)).mean()
		self.step(self.alpha_optimizer, alpha_loss, [self.log_alpha])
		self.stats.mean(critic_loss=critic_loss, actor_loss=actor_loss.mean(), alpha_loss=alpha_loss)

class SACAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, SACNetwork, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.config.NUM_STEPS:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			states = torch.cat([states, self.to_tensor(next_state).unsqueeze(0)], dim=0)
			next_action, next_log_prob = self.network.get_action_probs(states[-1])
			actions = torch.cat([actions, next_action.unsqueeze(0)], dim=0)
			log_probs = torch.cat([log_probs, next_log_prob.unsqueeze(0)], dim=0)
			values = self.network.get_q_value(states, actions, use_target=True)
			targets = self.compute_gae(values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), values[:-1])[0]
			states, actions, targets, next_log_probs, dones = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states[:-1], actions[:-1], targets, log_probs[1:], dones)]
			self.replay_buffer.extend(list(zip(states, actions, targets, next_log_probs, dones)), shuffle=False)	
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE:
			states, actions, targets, next_log_probs, dones = self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, targets, next_log_probs, dones, config=self.config)


Step:       0, Reward:  -153.521 [  81.668], Avg:  -153.521 (1.000) <0-00:00:00> ({'r_t':    -0.1772, 'eps':     1.0000, 'eps_e':     1.0000})
Step:    1000, Reward:  -142.798 [  43.890], Avg:  -148.159 (1.000) <0-00:00:10> ({'r_t': -2311.1120, 'eps':     1.0000, 'critic_loss':   693.4736, 'actor_loss':    -2.4187, 'alpha_loss':    -0.0846, 'eps_e':     1.0000})
Step:    2000, Reward:  -232.822 [  85.855], Avg:  -176.380 (1.000) <0-00:00:22> ({'r_t':  -778.7798, 'eps':     1.0000, 'critic_loss':   322.5723, 'actor_loss':    -4.1229, 'alpha_loss':    -0.2354, 'eps_e':     1.0000})
Step:    3000, Reward:  -168.002 [  44.270], Avg:  -174.286 (1.000) <0-00:00:35> ({'r_t':  -611.9980, 'eps':     1.0000, 'critic_loss':   222.4549, 'actor_loss':    -3.6905, 'alpha_loss':    -0.4021, 'eps_e':     1.0000})
Step:    4000, Reward:   -49.285 [  87.527], Avg:  -149.286 (1.000) <0-00:00:52> ({'r_t':  -366.4989, 'eps':     1.0000, 'critic_loss':   181.7108, 'actor_loss':    -3.4464, 'alpha_loss':    -0.5531, 'eps_e':     1.0000})
Step:    5000, Reward:    15.649 [ 112.599], Avg:  -121.796 (1.000) <0-00:01:12> ({'r_t':  -177.9255, 'eps':     1.0000, 'critic_loss':   152.4180, 'actor_loss':    -3.3787, 'alpha_loss':    -0.6824, 'eps_e':     1.0000})
Step:    6000, Reward:   -65.531 [  28.215], Avg:  -113.759 (1.000) <0-00:01:31> ({'r_t':   -33.2466, 'eps':     1.0000, 'critic_loss':   143.5704, 'actor_loss':    -3.1604, 'alpha_loss':    -0.7879, 'eps_e':     1.0000})
Step:    7000, Reward:   -67.255 [ 104.933], Avg:  -107.946 (1.000) <0-00:01:49> ({'r_t':   -56.3085, 'eps':     1.0000, 'critic_loss':   141.6176, 'actor_loss':    -3.1399, 'alpha_loss':    -0.8434, 'eps_e':     1.0000})
Step:    8000, Reward:  -101.047 [  71.921], Avg:  -107.179 (1.000) <0-00:02:05> ({'r_t':   -77.3392, 'eps':     1.0000, 'critic_loss':   140.4598, 'actor_loss':    -2.9948, 'alpha_loss':    -0.9167, 'eps_e':     1.0000})
Step:    9000, Reward:   -75.192 [  38.284], Avg:  -103.980 (1.000) <0-00:02:18> ({'r_t':  -135.1795, 'eps':     1.0000, 'critic_loss':   136.7754, 'actor_loss':    -2.8831, 'alpha_loss':    -0.9545, 'eps_e':     1.0000})
Step:   10000, Reward:  -171.233 [  19.143], Avg:  -110.094 (1.000) <0-00:02:33> ({'r_t':  -164.3549, 'eps':     1.0000, 'critic_loss':   134.0978, 'actor_loss':    -2.7016, 'alpha_loss':    -1.0247, 'eps_e':     1.0000})
Step:   11000, Reward:    -0.083 [ 102.757], Avg:  -100.927 (1.000) <0-00:02:49> ({'r_t':   -65.1007, 'eps':     1.0000, 'critic_loss':   130.9115, 'actor_loss':    -2.6400, 'alpha_loss':    -1.0911, 'eps_e':     1.0000})
Step:   12000, Reward:    13.670 [ 109.373], Avg:   -92.111 (1.000) <0-00:03:09> ({'r_t':    -2.2760, 'eps':     1.0000, 'critic_loss':   124.2365, 'actor_loss':    -2.5198, 'alpha_loss':    -1.1039, 'eps_e':     1.0000})
Step:   13000, Reward:   -33.271 [ 126.690], Avg:   -87.909 (1.000) <0-00:03:25> ({'r_t':   -69.3882, 'eps':     1.0000, 'critic_loss':   121.8423, 'actor_loss':    -2.5877, 'alpha_loss':    -0.9788, 'eps_e':     1.0000})
Step:   14000, Reward:   -93.472 [  69.787], Avg:   -88.279 (1.000) <0-00:03:40> ({'r_t':   -56.3081, 'eps':     1.0000, 'critic_loss':   125.6438, 'actor_loss':    -2.6303, 'alpha_loss':    -0.8733, 'eps_e':     1.0000})
Step:   15000, Reward:  -171.439 [  22.467], Avg:   -93.477 (1.000) <0-00:03:55> ({'r_t':  -142.7877, 'eps':     1.0000, 'critic_loss':   115.8924, 'actor_loss':    -2.6033, 'alpha_loss':    -0.8359, 'eps_e':     1.0000})
Step:   16000, Reward:    22.252 [ 107.752], Avg:   -86.669 (1.000) <0-00:04:14> ({'r_t':   -43.9599, 'eps':     1.0000, 'critic_loss':   121.4042, 'actor_loss':    -2.3463, 'alpha_loss':    -0.9682, 'eps_e':     1.0000})
Step:   17000, Reward:   -36.172 [  77.673], Avg:   -83.864 (1.000) <0-00:04:33> ({'r_t':    27.3907, 'eps':     1.0000, 'critic_loss':   112.7553, 'actor_loss':    -2.4136, 'alpha_loss':    -0.7619, 'eps_e':     1.0000})
Step:   18000, Reward:   -66.337 [ 134.797], Avg:   -82.941 (1.000) <0-00:04:52> ({'r_t':     9.6755, 'eps':     1.0000, 'critic_loss':   108.8925, 'actor_loss':    -2.2998, 'alpha_loss':    -0.6998, 'eps_e':     1.0000})
Step:   19000, Reward:    38.349 [ 128.337], Avg:   -76.877 (1.000) <0-00:05:10> ({'r_t':   -32.6103, 'eps':     1.0000, 'critic_loss':   112.2890, 'actor_loss':    -2.4124, 'alpha_loss':    -0.7339, 'eps_e':     1.0000})
Step:   20000, Reward:    22.438 [ 110.288], Avg:   -72.148 (1.000) <0-00:05:37> ({'r_t':   -32.4455, 'eps':     1.0000, 'critic_loss':   110.0596, 'actor_loss':    -2.2834, 'alpha_loss':    -0.5103, 'eps_e':     1.0000})
Step:   21000, Reward:    -2.419 [  75.452], Avg:   -68.978 (1.000) <0-00:06:06> ({'r_t':   -11.3286, 'eps':     1.0000, 'critic_loss':   113.9866, 'actor_loss':    -2.3402, 'alpha_loss':    -0.2123, 'eps_e':     1.0000})
Step:   22000, Reward:   -14.855 [ 121.570], Avg:   -66.625 (1.000) <0-00:06:35> ({'r_t':    53.3062, 'eps':     1.0000, 'critic_loss':   106.5853, 'actor_loss':    -2.1853, 'alpha_loss':    -0.2197, 'eps_e':     1.0000})
Step:   23000, Reward:   -44.259 [  99.947], Avg:   -65.693 (1.000) <0-00:07:03> ({'r_t':   -74.6089, 'eps':     1.0000, 'critic_loss':   100.9294, 'actor_loss':    -2.1144, 'alpha_loss':    -0.2167, 'eps_e':     1.0000})
Step:   24000, Reward:   -79.018 [  89.137], Avg:   -66.226 (1.000) <0-00:07:31> ({'r_t':    39.9868, 'eps':     1.0000, 'critic_loss':   101.4674, 'actor_loss':    -2.2100, 'alpha_loss':    -0.1631, 'eps_e':     1.0000})
Step:   25000, Reward:    16.193 [ 118.631], Avg:   -63.056 (1.000) <0-00:07:58> ({'r_t':   -24.8065, 'eps':     1.0000, 'critic_loss':   104.8087, 'actor_loss':    -2.4083, 'alpha_loss':     0.3222, 'eps_e':     1.0000})
Step:   26000, Reward:   -28.232 [ 100.245], Avg:   -61.766 (1.000) <0-00:08:27> ({'r_t':   -10.9438, 'eps':     1.0000, 'critic_loss':    97.0187, 'actor_loss':    -2.2540, 'alpha_loss':     0.0644, 'eps_e':     1.0000})
Step:   27000, Reward:    27.468 [  88.612], Avg:   -58.579 (1.000) <0-00:08:56> ({'r_t':   -53.1999, 'eps':     1.0000, 'critic_loss':   102.9369, 'actor_loss':    -2.1520, 'alpha_loss':     0.0424, 'eps_e':     1.0000})
Step:   28000, Reward:    46.749 [ 120.775], Avg:   -54.947 (1.000) <0-00:09:23> ({'r_t':  -102.0743, 'eps':     1.0000, 'critic_loss':    99.2708, 'actor_loss':    -2.1092, 'alpha_loss':    -0.1514, 'eps_e':     1.0000})
Step:   29000, Reward:  -135.041 [  41.088], Avg:   -57.617 (1.000) <0-00:09:53> ({'r_t':   -47.6213, 'eps':     1.0000, 'critic_loss':    97.7472, 'actor_loss':    -2.0781, 'alpha_loss':    -0.2372, 'eps_e':     1.0000})
Step:   30000, Reward:   -71.846 [  58.833], Avg:   -58.076 (1.000) <0-00:10:22> ({'r_t':   -31.6710, 'eps':     1.0000, 'critic_loss':    91.0367, 'actor_loss':    -2.1591, 'alpha_loss':    -0.1074, 'eps_e':     1.0000})
Step:   31000, Reward:   -64.165 [ 105.530], Avg:   -58.266 (1.000) <0-00:10:53> ({'r_t':   -66.7224, 'eps':     1.0000, 'critic_loss':    92.9879, 'actor_loss':    -2.0679, 'alpha_loss':     0.0514, 'eps_e':     1.0000})
Step:   32000, Reward:     0.499 [ 103.068], Avg:   -56.486 (1.000) <0-00:11:24> ({'r_t':   -26.4642, 'eps':     1.0000, 'critic_loss':    92.6577, 'actor_loss':    -2.0091, 'alpha_loss':    -0.1467, 'eps_e':     1.0000})
Step:   33000, Reward:  -112.298 [  53.936], Avg:   -58.127 (1.000) <0-00:11:53> ({'r_t':    11.3908, 'eps':     1.0000, 'critic_loss':    88.6598, 'actor_loss':    -2.0635, 'alpha_loss':     0.0545, 'eps_e':     1.0000})
Step:   34000, Reward:    27.243 [  83.199], Avg:   -55.688 (1.000) <0-00:12:25> ({'r_t':   -36.4500, 'eps':     1.0000, 'critic_loss':    91.3124, 'actor_loss':    -2.0304, 'alpha_loss':    -0.0415, 'eps_e':     1.0000})
Step:   35000, Reward:   -34.443 [  82.266], Avg:   -55.098 (1.000) <0-00:12:54> ({'r_t':   -47.6640, 'eps':     1.0000, 'critic_loss':    90.1830, 'actor_loss':    -2.0632, 'alpha_loss':    -0.0264, 'eps_e':     1.0000})
Step:   36000, Reward:   -34.181 [  86.298], Avg:   -54.533 (1.000) <0-00:13:24> ({'r_t':   -24.8478, 'eps':     1.0000, 'critic_loss':    83.0327, 'actor_loss':    -1.9172, 'alpha_loss':    -0.2071, 'eps_e':     1.0000})
Step:   37000, Reward:     6.471 [  71.172], Avg:   -52.927 (1.000) <0-00:13:53> ({'r_t':   -30.8481, 'eps':     1.0000, 'critic_loss':    84.3915, 'actor_loss':    -1.9470, 'alpha_loss':    -0.0186, 'eps_e':     1.0000})
Step:   38000, Reward:   -42.828 [  61.826], Avg:   -52.668 (1.000) <0-00:14:22> ({'r_t':   -79.3673, 'eps':     1.0000, 'critic_loss':    85.7015, 'actor_loss':    -1.9360, 'alpha_loss':    -0.1046, 'eps_e':     1.0000})
Step:   39000, Reward:  -183.527 [  51.531], Avg:   -55.940 (1.000) <0-00:14:52> ({'r_t':   -34.5997, 'eps':     1.0000, 'critic_loss':    85.4024, 'actor_loss':    -1.9235, 'alpha_loss':    -0.2340, 'eps_e':     1.0000})
Step:   40000, Reward:   -98.021 [  32.694], Avg:   -56.966 (1.000) <0-00:15:21> ({'r_t':   -32.3071, 'eps':     1.0000, 'critic_loss':    82.4979, 'actor_loss':    -1.8290, 'alpha_loss':    -0.2112, 'eps_e':     1.0000})
Step:   41000, Reward:   -84.567 [  40.789], Avg:   -57.623 (1.000) <0-00:15:52> ({'r_t':   -97.2719, 'eps':     1.0000, 'critic_loss':    80.0532, 'actor_loss':    -1.8110, 'alpha_loss':    -0.1452, 'eps_e':     1.0000})
Step:   42000, Reward:   -78.017 [  26.986], Avg:   -58.097 (1.000) <0-00:16:22> ({'r_t':   -86.1477, 'eps':     1.0000, 'critic_loss':    80.4177, 'actor_loss':    -1.7691, 'alpha_loss':    -0.1857, 'eps_e':     1.0000})
Step:   43000, Reward:   -81.857 [  26.314], Avg:   -58.637 (1.000) <0-00:16:53> ({'r_t':   -55.0807, 'eps':     1.0000, 'critic_loss':    79.2742, 'actor_loss':    -1.7463, 'alpha_loss':    -0.1704, 'eps_e':     1.0000})
Step:   44000, Reward:  -116.227 [  30.990], Avg:   -59.917 (1.000) <0-00:17:23> ({'r_t':   -74.6346, 'eps':     1.0000, 'critic_loss':    78.3256, 'actor_loss':    -1.7776, 'alpha_loss':    -0.0960, 'eps_e':     1.0000})
Step:   45000, Reward:   -90.112 [  50.570], Avg:   -60.574 (1.000) <0-00:17:53> ({'r_t':   -84.2469, 'eps':     1.0000, 'critic_loss':    81.0757, 'actor_loss':    -1.8309, 'alpha_loss':     0.0422, 'eps_e':     1.0000})
Step:   46000, Reward:   -68.893 [  32.151], Avg:   -60.751 (1.000) <0-00:18:24> ({'r_t':   -41.9464, 'eps':     1.0000, 'critic_loss':    78.5062, 'actor_loss':    -1.7319, 'alpha_loss':    -0.0656, 'eps_e':     1.0000})
Step:   47000, Reward:   -60.004 [  20.917], Avg:   -60.735 (1.000) <0-00:18:55> ({'r_t':   -57.3118, 'eps':     1.0000, 'critic_loss':    70.5496, 'actor_loss':    -1.7086, 'alpha_loss':    -0.0234, 'eps_e':     1.0000})
Step:   48000, Reward:   -86.840 [  31.927], Avg:   -61.268 (1.000) <0-00:19:25> ({'r_t':   -66.9549, 'eps':     1.0000, 'critic_loss':    69.0409, 'actor_loss':    -1.7143, 'alpha_loss':     0.0085, 'eps_e':     1.0000})
Step:   49000, Reward:   -50.013 [  49.404], Avg:   -61.043 (1.000) <0-00:19:55> ({'r_t':   -72.3067, 'eps':     1.0000, 'critic_loss':    72.2283, 'actor_loss':    -1.6721, 'alpha_loss':    -0.0183, 'eps_e':     1.0000})
Step:   50000, Reward:   -77.919 [  38.354], Avg:   -61.374 (1.000) <0-00:20:26> ({'r_t':   -85.0394, 'eps':     1.0000, 'critic_loss':    71.5646, 'actor_loss':    -1.7765, 'alpha_loss':     0.1389, 'eps_e':     1.0000})
Step:   51000, Reward:   -76.304 [  34.874], Avg:   -61.661 (1.000) <0-00:20:57> ({'r_t':   -96.3495, 'eps':     1.0000, 'critic_loss':    69.2447, 'actor_loss':    -1.7041, 'alpha_loss':     0.0513, 'eps_e':     1.0000})
Step:   52000, Reward:   -98.574 [  20.219], Avg:   -62.357 (1.000) <0-00:21:29> ({'r_t':   -81.2288, 'eps':     1.0000, 'critic_loss':    64.3527, 'actor_loss':    -1.6711, 'alpha_loss':    -0.0193, 'eps_e':     1.0000})
Step:   53000, Reward:   -91.920 [  20.833], Avg:   -62.905 (1.000) <0-00:22:01> ({'r_t':   -76.6434, 'eps':     1.0000, 'critic_loss':    65.0936, 'actor_loss':    -1.5642, 'alpha_loss':    -0.1495, 'eps_e':     1.0000})
Step:   54000, Reward:   -52.562 [  54.518], Avg:   -62.717 (1.000) <0-00:22:33> ({'r_t':   -74.7124, 'eps':     1.0000, 'critic_loss':    66.8889, 'actor_loss':    -1.6073, 'alpha_loss':     0.0777, 'eps_e':     1.0000})
Step:   55000, Reward:   -50.233 [  51.777], Avg:   -62.494 (1.000) <0-00:23:04> ({'r_t':   -80.3191, 'eps':     1.0000, 'critic_loss':    62.6161, 'actor_loss':    -1.6868, 'alpha_loss':     0.0248, 'eps_e':     1.0000})
Step:   56000, Reward:   -87.056 [  21.627], Avg:   -62.925 (1.000) <0-00:23:36> ({'r_t':   -78.0504, 'eps':     1.0000, 'critic_loss':    60.1206, 'actor_loss':    -1.5642, 'alpha_loss':     0.0558, 'eps_e':     1.0000})
Step:   57000, Reward:  -130.660 [  38.211], Avg:   -64.092 (1.000) <0-00:24:07> ({'r_t':  -114.6647, 'eps':     1.0000, 'critic_loss':    60.9656, 'actor_loss':    -1.5734, 'alpha_loss':    -0.1481, 'eps_e':     1.0000})
Step:   58000, Reward:   -79.514 [  25.959], Avg:   -64.354 (1.000) <0-00:24:37> ({'r_t':   -60.9443, 'eps':     1.0000, 'critic_loss':    61.5059, 'actor_loss':    -1.5250, 'alpha_loss':    -0.0322, 'eps_e':     1.0000})
Step:   59000, Reward:   -82.722 [  29.285], Avg:   -64.660 (1.000) <0-00:25:09> ({'r_t':   -54.3531, 'eps':     1.0000, 'critic_loss':    59.0240, 'actor_loss':    -1.5055, 'alpha_loss':     0.0195, 'eps_e':     1.0000})
Step:   60000, Reward:   -42.696 [  20.289], Avg:   -64.300 (1.000) <0-00:25:43> ({'r_t':   -67.6059, 'eps':     1.0000, 'critic_loss':    58.3733, 'actor_loss':    -1.5006, 'alpha_loss':     0.0280, 'eps_e':     1.0000})
Step:   61000, Reward:  -109.389 [  39.185], Avg:   -65.027 (1.000) <0-00:26:15> ({'r_t':   -71.5097, 'eps':     1.0000, 'critic_loss':    57.2385, 'actor_loss':    -1.4371, 'alpha_loss':    -0.2396, 'eps_e':     1.0000})
Step:   62000, Reward:   -94.065 [  32.583], Avg:   -65.488 (1.000) <0-00:26:47> ({'r_t':   -79.1454, 'eps':     1.0000, 'critic_loss':    57.3340, 'actor_loss':    -1.4328, 'alpha_loss':    -0.0862, 'eps_e':     1.0000})
Step:   63000, Reward:   -43.278 [  27.571], Avg:   -65.141 (1.000) <0-00:27:18> ({'r_t':  -104.6211, 'eps':     1.0000, 'critic_loss':    56.8534, 'actor_loss':    -1.5483, 'alpha_loss':     0.0292, 'eps_e':     1.0000})
Step:   64000, Reward:   -44.823 [  25.378], Avg:   -64.828 (1.000) <0-00:27:49> ({'r_t':   -71.2573, 'eps':     1.0000, 'critic_loss':    47.5115, 'actor_loss':    -1.3148, 'alpha_loss':    -0.0749, 'eps_e':     1.0000})
Step:   65000, Reward:  -125.625 [  38.588], Avg:   -65.750 (1.000) <0-00:28:21> ({'r_t':   -82.0696, 'eps':     1.0000, 'critic_loss':    48.0959, 'actor_loss':    -1.0552, 'alpha_loss':    -0.1276, 'eps_e':     1.0000})
Step:   66000, Reward:   -71.223 [  25.645], Avg:   -65.831 (1.000) <0-00:28:53> ({'r_t':   -64.7649, 'eps':     1.0000, 'critic_loss':    46.3474, 'actor_loss':    -0.9448, 'alpha_loss':    -0.4699, 'eps_e':     1.0000})
Step:   67000, Reward:   -74.906 [  29.830], Avg:   -65.965 (1.000) <0-00:29:26> ({'r_t':   -75.3682, 'eps':     1.0000, 'critic_loss':    44.9426, 'actor_loss':    -0.8542, 'alpha_loss':    -0.2948, 'eps_e':     1.0000})
Step:   68000, Reward:   -37.694 [  46.848], Avg:   -65.555 (1.000) <0-00:29:58> ({'r_t':   -57.3624, 'eps':     1.0000, 'critic_loss':    51.0359, 'actor_loss':    -0.8372, 'alpha_loss':    -0.2187, 'eps_e':     1.0000})
Step:   69000, Reward:   -63.110 [  25.034], Avg:   -65.520 (1.000) <0-00:30:30> ({'r_t':   -60.4945, 'eps':     1.0000, 'critic_loss':    47.1045, 'actor_loss':    -0.7033, 'alpha_loss':    -0.4485, 'eps_e':     1.0000})
Step:   70000, Reward:   -63.622 [  25.674], Avg:   -65.493 (1.000) <0-00:31:01> ({'r_t':   -48.3176, 'eps':     1.0000, 'critic_loss':    44.2323, 'actor_loss':    -0.7072, 'alpha_loss':    -0.2756, 'eps_e':     1.0000})
Step:   71000, Reward:   -89.428 [  25.270], Avg:   -65.826 (1.000) <0-00:31:33> ({'r_t':   -59.8513, 'eps':     1.0000, 'critic_loss':    40.5356, 'actor_loss':    -0.6494, 'alpha_loss':    -0.5715, 'eps_e':     1.0000})
Step:   72000, Reward:   -74.577 [  18.953], Avg:   -65.946 (1.000) <0-00:32:06> ({'r_t':   -45.0250, 'eps':     1.0000, 'critic_loss':    36.2918, 'actor_loss':    -0.5523, 'alpha_loss':    -0.8752, 'eps_e':     1.0000})
Step:   73000, Reward:   -59.573 [  27.432], Avg:   -65.860 (1.000) <0-00:32:39> ({'r_t':   -42.6253, 'eps':     1.0000, 'critic_loss':    38.4841, 'actor_loss':    -0.5123, 'alpha_loss':    -0.6381, 'eps_e':     1.0000})
Step:   74000, Reward:   -54.049 [  20.292], Avg:   -65.702 (1.000) <0-00:33:11> ({'r_t':   -59.2710, 'eps':     1.0000, 'critic_loss':    37.3639, 'actor_loss':    -0.5155, 'alpha_loss':    -0.4160, 'eps_e':     1.0000})
Step:   75000, Reward:   -44.905 [  19.037], Avg:   -65.428 (1.000) <0-00:33:43> ({'r_t':   -32.4477, 'eps':     1.0000, 'critic_loss':    36.7042, 'actor_loss':    -0.4706, 'alpha_loss':    -0.6530, 'eps_e':     1.0000})
Step:   76000, Reward:   -61.410 [  20.836], Avg:   -65.376 (1.000) <0-00:34:15> ({'r_t':   -52.6932, 'eps':     1.0000, 'critic_loss':    30.3311, 'actor_loss':    -0.5077, 'alpha_loss':    -0.9755, 'eps_e':     1.0000})
Step:   77000, Reward:   -45.729 [  18.310], Avg:   -65.124 (1.000) <0-00:34:46> ({'r_t':   -57.6701, 'eps':     1.0000, 'critic_loss':    29.8239, 'actor_loss':    -0.5065, 'alpha_loss':    -0.7432, 'eps_e':     1.0000})
Step:   78000, Reward:   -41.621 [  16.012], Avg:   -64.827 (1.000) <0-00:35:18> ({'r_t':   -34.3474, 'eps':     1.0000, 'critic_loss':    33.6338, 'actor_loss':    -0.5018, 'alpha_loss':    -0.8930, 'eps_e':     1.0000})
Step:   79000, Reward:   -44.437 [  27.876], Avg:   -64.572 (1.000) <0-00:35:50> ({'r_t':   -61.3623, 'eps':     1.0000, 'critic_loss':    32.3369, 'actor_loss':    -0.4554, 'alpha_loss':    -0.2804, 'eps_e':     1.0000})
Step:   80000, Reward:   -28.314 [  22.582], Avg:   -64.124 (1.000) <0-00:36:21> ({'r_t':   -29.7026, 'eps':     1.0000, 'critic_loss':    26.6745, 'actor_loss':    -0.4782, 'alpha_loss':     0.0009, 'eps_e':     1.0000})
Step:   81000, Reward:   -36.857 [  29.864], Avg:   -63.792 (1.000) <0-00:36:53> ({'r_t':   -46.5865, 'eps':     1.0000, 'critic_loss':    27.7660, 'actor_loss':    -0.4375, 'alpha_loss':    -0.4209, 'eps_e':     1.0000})
Step:   82000, Reward:   -33.653 [  25.347], Avg:   -63.429 (1.000) <0-00:37:24> ({'r_t':   -17.3370, 'eps':     1.0000, 'critic_loss':    24.9478, 'actor_loss':    -0.4152, 'alpha_loss':    -0.3350, 'eps_e':     1.0000})
Step:   83000, Reward:   -32.617 [  19.640], Avg:   -63.062 (1.000) <0-00:37:56> ({'r_t':   -30.2646, 'eps':     1.0000, 'critic_loss':    25.5621, 'actor_loss':    -0.4213, 'alpha_loss':     0.0518, 'eps_e':     1.0000})
Step:   84000, Reward:   -63.691 [  42.376], Avg:   -63.069 (1.000) <0-00:38:28> ({'r_t':   -24.8395, 'eps':     1.0000, 'critic_loss':    23.4224, 'actor_loss':    -0.3869, 'alpha_loss':    -0.1367, 'eps_e':     1.0000})
Step:   85000, Reward:     0.370 [  56.659], Avg:   -62.332 (1.000) <0-00:38:59> ({'r_t':   -22.1104, 'eps':     1.0000, 'critic_loss':    20.7254, 'actor_loss':    -0.3898, 'alpha_loss':    -0.7304, 'eps_e':     1.0000})
Step:   86000, Reward:   -33.207 [  15.495], Avg:   -61.997 (1.000) <0-00:39:33> ({'r_t':   -31.6695, 'eps':     1.0000, 'critic_loss':    19.1036, 'actor_loss':    -0.3988, 'alpha_loss':    -0.1484, 'eps_e':     1.0000})
Step:   87000, Reward:   -30.716 [  20.279], Avg:   -61.641 (1.000) <0-00:40:06> ({'r_t':   -26.3893, 'eps':     1.0000, 'critic_loss':    21.6565, 'actor_loss':    -0.3635, 'alpha_loss':    -0.5808, 'eps_e':     1.0000})
Step:   88000, Reward:   -32.689 [  23.274], Avg:   -61.316 (1.000) <0-00:40:40> ({'r_t':   -68.4074, 'eps':     1.0000, 'critic_loss':    20.0333, 'actor_loss':    -0.3706, 'alpha_loss':     0.1788, 'eps_e':     1.0000})
Step:   89000, Reward:   -23.613 [  22.213], Avg:   -60.897 (1.000) <0-00:41:12> ({'r_t':   -25.2504, 'eps':     1.0000, 'critic_loss':    17.7432, 'actor_loss':    -0.3880, 'alpha_loss':     0.0571, 'eps_e':     1.0000})
Step:   90000, Reward:   -22.550 [  14.826], Avg:   -60.476 (1.000) <0-00:41:46> ({'r_t':   -19.4539, 'eps':     1.0000, 'critic_loss':    17.6628, 'actor_loss':    -0.4185, 'alpha_loss':    -0.5008, 'eps_e':     1.0000})
Step:   91000, Reward:   -41.821 [  14.487], Avg:   -60.273 (1.000) <0-00:42:21> ({'r_t':   -20.8223, 'eps':     1.0000, 'critic_loss':    15.6392, 'actor_loss':    -0.3663, 'alpha_loss':    -0.7554, 'eps_e':     1.0000})
Step:   92000, Reward:    27.599 [  59.153], Avg:   -59.328 (1.000) <0-00:42:54> ({'r_t':   -17.6549, 'eps':     1.0000, 'critic_loss':    16.0103, 'actor_loss':    -0.3514, 'alpha_loss':    -0.3366, 'eps_e':     1.0000})
Step:   93000, Reward:   -29.158 [  21.687], Avg:   -59.007 (1.000) <0-00:43:26> ({'r_t':    -8.0145, 'eps':     1.0000, 'critic_loss':    14.7608, 'actor_loss':    -0.3936, 'alpha_loss':     0.2621, 'eps_e':     1.0000})
Step:   94000, Reward:    63.109 [  60.516], Avg:   -57.722 (1.000) <0-00:43:59> ({'r_t':    -3.6731, 'eps':     1.0000, 'critic_loss':    15.7817, 'actor_loss':    -0.3272, 'alpha_loss':    -1.0677, 'eps_e':     1.0000})
Step:   95000, Reward:   -15.324 [  14.073], Avg:   -57.280 (1.000) <0-00:44:30> ({'r_t':    -5.0377, 'eps':     1.0000, 'critic_loss':    13.0848, 'actor_loss':    -0.3712, 'alpha_loss':    -1.1365, 'eps_e':     1.0000})
Step:   96000, Reward:    -6.483 [  24.674], Avg:   -56.756 (1.000) <0-00:45:04> ({'r_t':    -7.0762, 'eps':     1.0000, 'critic_loss':    11.3594, 'actor_loss':    -0.3921, 'alpha_loss':    -0.8946, 'eps_e':     1.0000})
Step:   97000, Reward:   -29.804 [  38.730], Avg:   -56.481 (1.000) <0-00:45:36> ({'r_t':    20.4329, 'eps':     1.0000, 'critic_loss':    12.0111, 'actor_loss':    -0.4011, 'alpha_loss':    -1.0856, 'eps_e':     1.0000})
Step:   98000, Reward:    34.328 [  54.950], Avg:   -55.564 (1.000) <0-00:46:09> ({'r_t':    14.8566, 'eps':     1.0000, 'critic_loss':    12.2227, 'actor_loss':    -0.3375, 'alpha_loss':    -0.5938, 'eps_e':     1.0000})
Step:   99000, Reward:    -8.309 [  21.044], Avg:   -55.092 (1.000) <0-00:46:40> ({'r_t':    66.0994, 'eps':     1.0000, 'critic_loss':    11.1677, 'actor_loss':    -0.4165, 'alpha_loss':    -0.5922, 'eps_e':     1.0000})
Step:  100000, Reward:    -7.495 [  14.661], Avg:   -54.620 (1.000) <0-00:47:12> ({'r_t':    31.6726, 'eps':     1.0000, 'critic_loss':    10.4691, 'actor_loss':    -0.3902, 'alpha_loss':    -1.1160, 'eps_e':     1.0000})
Step:  101000, Reward:     6.463 [  59.770], Avg:   -54.021 (1.000) <0-00:47:44> ({'r_t':    38.9797, 'eps':     1.0000, 'critic_loss':    11.0241, 'actor_loss':    -0.4222, 'alpha_loss':    -0.4556, 'eps_e':     1.0000})
Step:  102000, Reward:   -24.932 [  24.887], Avg:   -53.739 (1.000) <0-00:48:16> ({'r_t':    -2.9494, 'eps':     1.0000, 'critic_loss':    10.6353, 'actor_loss':    -0.4070, 'alpha_loss':    -0.6105, 'eps_e':     1.0000})
Step:  103000, Reward:     1.677 [  63.028], Avg:   -53.206 (1.000) <0-00:48:46> ({'r_t':    22.5021, 'eps':     1.0000, 'critic_loss':     9.5486, 'actor_loss':    -0.4193, 'alpha_loss':    -0.9409, 'eps_e':     1.0000})
Step:  104000, Reward:   -16.095 [  21.270], Avg:   -52.853 (1.000) <0-00:49:17> ({'r_t':   -10.0417, 'eps':     1.0000, 'critic_loss':     9.3255, 'actor_loss':    -0.4401, 'alpha_loss':    -0.4085, 'eps_e':     1.0000})
Step:  105000, Reward:    15.745 [  56.297], Avg:   -52.206 (1.000) <0-00:49:50> ({'r_t':   -22.7779, 'eps':     1.0000, 'critic_loss':     8.4120, 'actor_loss':    -0.4593, 'alpha_loss':    -0.3885, 'eps_e':     1.0000})
Step:  106000, Reward:   -10.786 [  32.220], Avg:   -51.819 (1.000) <0-00:50:21> ({'r_t':    -5.4032, 'eps':     1.0000, 'critic_loss':     8.7619, 'actor_loss':    -0.4953, 'alpha_loss':    -0.3035, 'eps_e':     1.0000})
Step:  107000, Reward:    16.930 [  51.641], Avg:   -51.182 (1.000) <0-00:50:52> ({'r_t':    -6.1675, 'eps':     1.0000, 'critic_loss':     8.3931, 'actor_loss':    -0.4640, 'alpha_loss':    -0.2956, 'eps_e':     1.0000})
Step:  108000, Reward:   135.220 [  71.844], Avg:   -49.472 (1.000) <0-00:51:24> ({'r_t':    59.0877, 'eps':     1.0000, 'critic_loss':     8.9875, 'actor_loss':    -0.4582, 'alpha_loss':    -1.0263, 'eps_e':     1.0000})
Step:  109000, Reward:   -36.200 [  23.190], Avg:   -49.351 (1.000) <0-00:51:56> ({'r_t':    77.1988, 'eps':     1.0000, 'critic_loss':     8.3822, 'actor_loss':    -0.5048, 'alpha_loss':    -0.3790, 'eps_e':     1.0000})
Step:  110000, Reward:    80.184 [  75.514], Avg:   -48.184 (1.000) <0-00:52:27> ({'r_t':    75.3903, 'eps':     1.0000, 'critic_loss':     8.3268, 'actor_loss':    -0.5054, 'alpha_loss':    -0.4645, 'eps_e':     1.0000})
Step:  111000, Reward:   129.373 [  95.350], Avg:   -46.599 (1.000) <0-00:52:56> ({'r_t':   184.6234, 'eps':     1.0000, 'critic_loss':     8.3000, 'actor_loss':    -0.5201, 'alpha_loss':    -0.3303, 'eps_e':     1.0000})
Step:  112000, Reward:   174.075 [  76.838], Avg:   -44.646 (1.000) <0-00:53:25> ({'r_t':   252.8943, 'eps':     1.0000, 'critic_loss':     9.9363, 'actor_loss':    -0.5394, 'alpha_loss':    -0.7650, 'eps_e':     1.0000})
Step:  113000, Reward:    83.799 [ 112.733], Avg:   -43.519 (1.000) <0-00:53:55> ({'r_t':   256.0721, 'eps':     1.0000, 'critic_loss':    12.8788, 'actor_loss':    -0.5213, 'alpha_loss':    -0.6732, 'eps_e':     1.0000})
Step:  114000, Reward:    99.504 [ 110.162], Avg:   -42.276 (1.000) <0-00:54:24> ({'r_t':   347.4543, 'eps':     1.0000, 'critic_loss':    13.9575, 'actor_loss':    -0.5565, 'alpha_loss':    -0.1786, 'eps_e':     1.0000})
Step:  115000, Reward:   203.723 [  54.817], Avg:   -40.155 (1.000) <0-00:54:52> ({'r_t':   374.3455, 'eps':     1.0000, 'critic_loss':    13.7816, 'actor_loss':    -0.6439, 'alpha_loss':    -0.4768, 'eps_e':     1.0000})
Step:  116000, Reward:   182.884 [  88.996], Avg:   -38.249 (1.000) <0-00:55:19> ({'r_t':   482.6248, 'eps':     1.0000, 'critic_loss':    16.0044, 'actor_loss':    -0.6610, 'alpha_loss':     0.0819, 'eps_e':     1.0000})
Step:  117000, Reward:   140.742 [ 130.882], Avg:   -36.732 (1.000) <0-00:55:45> ({'r_t':   430.0909, 'eps':     1.0000, 'critic_loss':    18.5798, 'actor_loss':    -0.7096, 'alpha_loss':     0.1096, 'eps_e':     1.0000})
Step:  118000, Reward:   234.670 [  63.853], Avg:   -34.451 (1.000) <0-00:56:13> ({'r_t':   435.2886, 'eps':     1.0000, 'critic_loss':    19.3123, 'actor_loss':    -0.8417, 'alpha_loss':     0.3291, 'eps_e':     1.0000})
Step:  119000, Reward:   133.105 [ 127.690], Avg:   -33.055 (1.000) <0-00:56:39> ({'r_t':   441.7157, 'eps':     1.0000, 'critic_loss':    22.2420, 'actor_loss':    -0.9102, 'alpha_loss':     0.5777, 'eps_e':     1.0000})
Step:  120000, Reward:    82.915 [ 117.935], Avg:   -32.096 (1.000) <0-00:57:05> ({'r_t':   507.4205, 'eps':     1.0000, 'critic_loss':    26.5012, 'actor_loss':    -1.0265, 'alpha_loss':     0.5610, 'eps_e':     1.0000})
Step:  121000, Reward:   150.246 [ 115.474], Avg:   -30.602 (1.000) <0-00:57:28> ({'r_t':   703.9226, 'eps':     1.0000, 'critic_loss':    25.7103, 'actor_loss':    -1.1251, 'alpha_loss':     1.0898, 'eps_e':     1.0000})
Step:  122000, Reward:   245.670 [  60.968], Avg:   -28.356 (1.000) <0-00:57:49> ({'r_t':   934.9056, 'eps':     1.0000, 'critic_loss':    32.5065, 'actor_loss':    -1.3535, 'alpha_loss':     1.0406, 'eps_e':     1.0000})
Step:  123000, Reward:   192.756 [ 109.394], Avg:   -26.572 (1.000) <0-00:58:08> ({'r_t':   981.4724, 'eps':     1.0000, 'critic_loss':    36.5114, 'actor_loss':    -1.6349, 'alpha_loss':     2.5701, 'eps_e':     1.0000})
Step:  124000, Reward:   143.043 [ 119.642], Avg:   -25.216 (1.000) <0-00:58:28> ({'r_t':   828.0807, 'eps':     1.0000, 'critic_loss':    39.2936, 'actor_loss':    -1.9509, 'alpha_loss':     4.0173, 'eps_e':     1.0000})
Step:  125000, Reward:   204.521 [ 102.060], Avg:   -23.392 (1.000) <0-00:58:48> ({'r_t':   900.9894, 'eps':     1.0000, 'critic_loss':    44.3353, 'actor_loss':    -2.0412, 'alpha_loss':     4.2884, 'eps_e':     1.0000})
Step:  126000, Reward:   174.475 [ 121.142], Avg:   -21.834 (1.000) <0-00:59:08> ({'r_t':   971.2520, 'eps':     1.0000, 'critic_loss':    48.6291, 'actor_loss':    -2.3030, 'alpha_loss':     4.6436, 'eps_e':     1.0000})
Step:  127000, Reward:   187.868 [ 127.878], Avg:   -20.196 (1.000) <0-00:59:28> ({'r_t':   954.6458, 'eps':     1.0000, 'critic_loss':    51.5462, 'actor_loss':    -2.8125, 'alpha_loss':     5.0012, 'eps_e':     1.0000})
Step:  128000, Reward:   128.855 [ 126.366], Avg:   -19.041 (1.000) <0-00:59:48> ({'r_t':   856.0568, 'eps':     1.0000, 'critic_loss':    59.6377, 'actor_loss':    -3.2922, 'alpha_loss':     5.2740, 'eps_e':     1.0000})
Step:  129000, Reward:   171.237 [ 134.193], Avg:   -17.577 (1.000) <0-01:00:08> ({'r_t':   951.1077, 'eps':     1.0000, 'critic_loss':    68.6023, 'actor_loss':    -4.0578, 'alpha_loss':     5.8252, 'eps_e':     1.0000})
Step:  130000, Reward:   176.191 [ 116.718], Avg:   -16.098 (1.000) <0-01:00:27> ({'r_t':   958.6125, 'eps':     1.0000, 'critic_loss':    65.2092, 'actor_loss':    -4.4017, 'alpha_loss':     5.7146, 'eps_e':     1.0000})
Step:  131000, Reward:   223.299 [  97.573], Avg:   -14.284 (1.000) <0-01:00:47> ({'r_t':   800.2239, 'eps':     1.0000, 'critic_loss':    72.3764, 'actor_loss':    -4.9058, 'alpha_loss':     5.8613, 'eps_e':     1.0000})
Step:  132000, Reward:   204.891 [ 104.599], Avg:   -12.636 (1.000) <0-01:01:09> ({'r_t':   895.4167, 'eps':     1.0000, 'critic_loss':    76.8348, 'actor_loss':    -5.5536, 'alpha_loss':     5.8862, 'eps_e':     1.0000})
Step:  133000, Reward:    99.188 [ 131.810], Avg:   -11.802 (1.000) <0-01:01:29> ({'r_t':   762.6648, 'eps':     1.0000, 'critic_loss':    80.3606, 'actor_loss':    -6.0022, 'alpha_loss':     5.8428, 'eps_e':     1.0000})
Step:  134000, Reward:   137.169 [ 149.355], Avg:   -10.698 (1.000) <0-01:01:48> ({'r_t':   832.4978, 'eps':     1.0000, 'critic_loss':    81.8770, 'actor_loss':    -6.3239, 'alpha_loss':     5.6118, 'eps_e':     1.0000})
Step:  135000, Reward:   202.257 [ 101.979], Avg:    -9.132 (1.000) <0-01:02:08> ({'r_t':   875.8763, 'eps':     1.0000, 'critic_loss':    81.1811, 'actor_loss':    -6.5539, 'alpha_loss':     5.3917, 'eps_e':     1.0000})
Step:  136000, Reward:   212.790 [  98.536], Avg:    -7.512 (1.000) <0-01:02:27> ({'r_t':   822.7283, 'eps':     1.0000, 'critic_loss':    91.2373, 'actor_loss':    -6.8726, 'alpha_loss':     5.0979, 'eps_e':     1.0000})
Step:  137000, Reward:   236.343 [  65.520], Avg:    -5.745 (1.000) <0-01:02:46> ({'r_t':   990.7148, 'eps':     1.0000, 'critic_loss':   102.2346, 'actor_loss':    -7.0648, 'alpha_loss':     4.8415, 'eps_e':     1.0000})
Step:  138000, Reward:   243.636 [  71.389], Avg:    -3.951 (1.000) <0-01:03:05> ({'r_t':   982.2229, 'eps':     1.0000, 'critic_loss':   100.1663, 'actor_loss':    -7.3419, 'alpha_loss':     4.6847, 'eps_e':     1.0000})
Step:  139000, Reward:   153.652 [ 139.153], Avg:    -2.826 (1.000) <0-01:03:25> ({'r_t':   919.7440, 'eps':     1.0000, 'critic_loss':   108.6502, 'actor_loss':    -7.3568, 'alpha_loss':     4.3426, 'eps_e':     1.0000})
Step:  140000, Reward:   204.079 [ 112.778], Avg:    -1.358 (1.000) <0-01:03:44> ({'r_t':   911.1052, 'eps':     1.0000, 'critic_loss':   102.9816, 'actor_loss':    -7.5942, 'alpha_loss':     4.0860, 'eps_e':     1.0000})
Step:  141000, Reward:   224.524 [  83.450], Avg:     0.233 (1.000) <0-01:04:03> ({'r_t':   964.3111, 'eps':     1.0000, 'critic_loss':   107.1171, 'actor_loss':    -7.6219, 'alpha_loss':     3.8515, 'eps_e':     1.0000})
Step:  142000, Reward:   205.511 [ 117.413], Avg:     1.668 (1.000) <0-01:04:24> ({'r_t':   878.2348, 'eps':     1.0000, 'critic_loss':   110.8035, 'actor_loss':    -7.6322, 'alpha_loss':     3.5586, 'eps_e':     1.0000})
Step:  143000, Reward:   204.128 [  76.403], Avg:     3.074 (1.000) <0-01:04:43> ({'r_t':   994.1807, 'eps':     1.0000, 'critic_loss':   124.9556, 'actor_loss':    -7.5774, 'alpha_loss':     3.2960, 'eps_e':     1.0000})
Step:  144000, Reward:   151.524 [ 130.903], Avg:     4.098 (1.000) <0-01:05:03> ({'r_t':  1010.9192, 'eps':     1.0000, 'critic_loss':   122.7822, 'actor_loss':    -7.7542, 'alpha_loss':     3.0599, 'eps_e':     1.0000})
Step:  145000, Reward:   209.066 [  85.299], Avg:     5.502 (1.000) <0-01:05:22> ({'r_t':   934.1557, 'eps':     1.0000, 'critic_loss':   128.2146, 'actor_loss':    -7.8968, 'alpha_loss':     2.7924, 'eps_e':     1.0000})
Step:  146000, Reward:   172.916 [ 127.963], Avg:     6.641 (1.000) <0-01:05:42> ({'r_t':  1032.9256, 'eps':     1.0000, 'critic_loss':   127.5306, 'actor_loss':    -7.6542, 'alpha_loss':     2.4889, 'eps_e':     1.0000})
Step:  147000, Reward:   218.453 [  67.575], Avg:     8.072 (1.000) <0-01:06:02> ({'r_t':   959.6760, 'eps':     1.0000, 'critic_loss':   128.5718, 'actor_loss':    -7.6338, 'alpha_loss':     2.1851, 'eps_e':     1.0000})
Step:  148000, Reward:   158.983 [ 111.492], Avg:     9.085 (1.000) <0-01:06:21> ({'r_t':  1016.0923, 'eps':     1.0000, 'critic_loss':   131.8892, 'actor_loss':    -7.5785, 'alpha_loss':     1.9604, 'eps_e':     1.0000})
Step:  149000, Reward:   237.150 [  58.547], Avg:    10.605 (1.000) <0-01:06:41> ({'r_t':   989.9988, 'eps':     1.0000, 'critic_loss':   139.6954, 'actor_loss':    -7.4963, 'alpha_loss':     1.7373, 'eps_e':     1.0000})
Step:  150000, Reward:   198.066 [  86.615], Avg:    11.847 (1.000) <0-01:07:01> ({'r_t':   858.3666, 'eps':     1.0000, 'critic_loss':   139.4382, 'actor_loss':    -7.5753, 'alpha_loss':     1.5346, 'eps_e':     1.0000})
Step:  151000, Reward:   220.591 [  62.395], Avg:    13.220 (1.000) <0-01:07:20> ({'r_t':   931.7667, 'eps':     1.0000, 'critic_loss':   150.3349, 'actor_loss':    -7.4903, 'alpha_loss':     1.3333, 'eps_e':     1.0000})
Step:  152000, Reward:   186.603 [ 101.288], Avg:    14.353 (1.000) <0-01:07:39> ({'r_t':   994.9203, 'eps':     1.0000, 'critic_loss':   147.8847, 'actor_loss':    -7.0562, 'alpha_loss':     1.1174, 'eps_e':     1.0000})
Step:  153000, Reward:   169.788 [ 107.191], Avg:    15.362 (1.000) <0-01:08:01> ({'r_t':   998.4616, 'eps':     1.0000, 'critic_loss':   157.4060, 'actor_loss':    -7.1109, 'alpha_loss':     0.9578, 'eps_e':     1.0000})
Step:  154000, Reward:   236.887 [  71.383], Avg:    16.792 (1.000) <0-01:08:20> ({'r_t':   959.9494, 'eps':     1.0000, 'critic_loss':   149.1886, 'actor_loss':    -7.0394, 'alpha_loss':     0.7840, 'eps_e':     1.0000})
Step:  155000, Reward:   205.368 [  76.682], Avg:    18.000 (1.000) <0-01:08:39> ({'r_t':   983.9925, 'eps':     1.0000, 'critic_loss':   162.4783, 'actor_loss':    -6.4847, 'alpha_loss':     0.6117, 'eps_e':     1.0000})
Step:  156000, Reward:   205.677 [  81.519], Avg:    19.196 (1.000) <0-01:08:59> ({'r_t':  1008.8195, 'eps':     1.0000, 'critic_loss':   167.3363, 'actor_loss':    -6.4738, 'alpha_loss':     0.4772, 'eps_e':     1.0000})
Step:  157000, Reward:   157.642 [ 109.504], Avg:    20.072 (1.000) <0-01:09:18> ({'r_t':  1005.2521, 'eps':     1.0000, 'critic_loss':   166.1519, 'actor_loss':    -6.5103, 'alpha_loss':     0.3611, 'eps_e':     1.0000})
Step:  158000, Reward:   197.287 [  89.516], Avg:    21.187 (1.000) <0-01:09:37> ({'r_t':   964.0344, 'eps':     1.0000, 'critic_loss':   181.7188, 'actor_loss':    -6.0166, 'alpha_loss':     0.2528, 'eps_e':     1.0000})
Step:  159000, Reward:   197.293 [  87.414], Avg:    22.287 (1.000) <0-01:09:56> ({'r_t':  1069.7298, 'eps':     1.0000, 'critic_loss':   180.8232, 'actor_loss':    -5.6470, 'alpha_loss':     0.1785, 'eps_e':     1.0000})
Step:  160000, Reward:   229.075 [  43.210], Avg:    23.572 (1.000) <0-01:10:16> ({'r_t':   940.4410, 'eps':     1.0000, 'critic_loss':   178.2199, 'actor_loss':    -5.4206, 'alpha_loss':     0.1113, 'eps_e':     1.0000})
Step:  161000, Reward:   230.223 [  53.627], Avg:    24.847 (1.000) <0-01:10:36> ({'r_t':  1036.2494, 'eps':     1.0000, 'critic_loss':   186.0874, 'actor_loss':    -5.2976, 'alpha_loss':     0.0674, 'eps_e':     1.0000})
Step:  162000, Reward:   210.628 [  58.904], Avg:    25.987 (1.000) <0-01:10:57> ({'r_t':  1042.2231, 'eps':     1.0000, 'critic_loss':   185.1069, 'actor_loss':    -5.0538, 'alpha_loss':     0.0352, 'eps_e':     1.0000})
Step:  163000, Reward:   233.077 [  25.746], Avg:    27.250 (1.000) <0-01:11:16> ({'r_t':   999.3703, 'eps':     1.0000, 'critic_loss':   188.6044, 'actor_loss':    -4.7883, 'alpha_loss':     0.0132, 'eps_e':     1.0000})
Step:  164000, Reward:   195.220 [  78.248], Avg:    28.268 (1.000) <0-01:11:36> ({'r_t':   961.4112, 'eps':     1.0000, 'critic_loss':   197.4432, 'actor_loss':    -4.3888, 'alpha_loss':     0.0036, 'eps_e':     1.0000})
Step:  165000, Reward:   235.177 [  58.964], Avg:    29.514 (1.000) <0-01:11:56> ({'r_t':  1017.1071, 'eps':     1.0000, 'critic_loss':   197.5176, 'actor_loss':    -4.5496, 'alpha_loss':    -0.0013, 'eps_e':     1.0000})
Step:  166000, Reward:   224.575 [  25.433], Avg:    30.682 (1.000) <0-01:12:15> ({'r_t':  1026.7417, 'eps':     1.0000, 'critic_loss':   198.7777, 'actor_loss':    -4.4745, 'alpha_loss':    -0.0028, 'eps_e':     1.0000})
Step:  167000, Reward:   198.654 [  84.954], Avg:    31.682 (1.000) <0-01:12:35> ({'r_t':  1119.2399, 'eps':     1.0000, 'critic_loss':   199.5880, 'actor_loss':    -4.2983, 'alpha_loss':    -0.0032, 'eps_e':     1.0000})
Step:  168000, Reward:   211.117 [  76.271], Avg:    32.744 (1.000) <0-01:12:56> ({'r_t':   911.2632, 'eps':     1.0000, 'critic_loss':   198.3808, 'actor_loss':    -4.2470, 'alpha_loss':    -0.0019, 'eps_e':     1.0000})
Step:  169000, Reward:   230.205 [  33.987], Avg:    33.905 (1.000) <0-01:13:17> ({'r_t':   916.6465, 'eps':     1.0000, 'critic_loss':   202.4914, 'actor_loss':    -4.2040, 'alpha_loss':    -0.0010, 'eps_e':     1.0000})
Step:  170000, Reward:   193.005 [  94.090], Avg:    34.836 (1.000) <0-01:13:39> ({'r_t':  1069.6288, 'eps':     1.0000, 'critic_loss':   206.7864, 'actor_loss':    -4.1423, 'alpha_loss':     0.0003, 'eps_e':     1.0000})
Step:  171000, Reward:   222.470 [  33.610], Avg:    35.927 (1.000) <0-01:14:00> ({'r_t':   979.8809, 'eps':     1.0000, 'critic_loss':   203.1610, 'actor_loss':    -3.8438, 'alpha_loss':     0.0024, 'eps_e':     1.0000})
Step:  172000, Reward:   224.585 [  76.951], Avg:    37.017 (1.000) <0-01:14:19> ({'r_t':  1139.5424, 'eps':     1.0000, 'critic_loss':   210.1902, 'actor_loss':    -3.6162, 'alpha_loss':     0.0046, 'eps_e':     1.0000})
Step:  173000, Reward:   213.986 [  77.719], Avg:    38.034 (1.000) <0-01:14:41> ({'r_t':   977.6950, 'eps':     1.0000, 'critic_loss':   205.9352, 'actor_loss':    -3.6277, 'alpha_loss':     0.0026, 'eps_e':     1.0000})
Step:  174000, Reward:   200.894 [  84.275], Avg:    38.965 (1.000) <0-01:15:00> ({'r_t':  1175.3523, 'eps':     1.0000, 'critic_loss':   212.2486, 'actor_loss':    -3.2791, 'alpha_loss':    -0.0007, 'eps_e':     1.0000})
Step:  175000, Reward:   225.896 [  58.181], Avg:    40.027 (1.000) <0-01:15:20> ({'r_t':  1268.4296, 'eps':     1.0000, 'critic_loss':   208.4333, 'actor_loss':    -3.3515, 'alpha_loss':    -0.0026, 'eps_e':     1.0000})
Step:  176000, Reward:   215.886 [  82.699], Avg:    41.021 (1.000) <0-01:15:42> ({'r_t':  1086.8345, 'eps':     1.0000, 'critic_loss':   201.7482, 'actor_loss':    -2.9870, 'alpha_loss':    -0.0060, 'eps_e':     1.0000})
Step:  177000, Reward:   216.610 [  75.181], Avg:    42.007 (1.000) <0-01:16:04> ({'r_t':  1149.1360, 'eps':     1.0000, 'critic_loss':   214.6954, 'actor_loss':    -2.9312, 'alpha_loss':    -0.0111, 'eps_e':     1.0000})
Step:  178000, Reward:   218.867 [  50.921], Avg:    42.995 (1.000) <0-01:16:25> ({'r_t':  1167.8142, 'eps':     1.0000, 'critic_loss':   213.2742, 'actor_loss':    -2.9859, 'alpha_loss':    -0.0102, 'eps_e':     1.0000})
Step:  179000, Reward:   230.182 [  63.580], Avg:    44.035 (1.000) <0-01:16:45> ({'r_t':  1006.9696, 'eps':     1.0000, 'critic_loss':   218.2358, 'actor_loss':    -2.7489, 'alpha_loss':    -0.0105, 'eps_e':     1.0000})
Step:  180000, Reward:   239.684 [  58.743], Avg:    45.116 (1.000) <0-01:17:05> ({'r_t':  1027.6992, 'eps':     1.0000, 'critic_loss':   214.4188, 'actor_loss':    -2.6984, 'alpha_loss':    -0.0094, 'eps_e':     1.0000})
Step:  181000, Reward:   234.085 [  56.987], Avg:    46.154 (1.000) <0-01:17:24> ({'r_t':  1146.5623, 'eps':     1.0000, 'critic_loss':   203.9630, 'actor_loss':    -2.4901, 'alpha_loss':    -0.0039, 'eps_e':     1.0000})
Step:  182000, Reward:   255.873 [  15.430], Avg:    47.300 (1.000) <0-01:17:44> ({'r_t':  1099.7642, 'eps':     1.0000, 'critic_loss':   211.9136, 'actor_loss':    -2.5282, 'alpha_loss':    -0.0151, 'eps_e':     1.0000})
Step:  183000, Reward:   253.501 [  15.903], Avg:    48.421 (1.000) <0-01:18:04> ({'r_t':  1136.6588, 'eps':     1.0000, 'critic_loss':   205.3669, 'actor_loss':    -2.3072, 'alpha_loss':    -0.0038, 'eps_e':     1.0000})
Step:  184000, Reward:   256.967 [  20.637], Avg:    49.548 (1.000) <0-01:18:23> ({'r_t':  1192.3243, 'eps':     1.0000, 'critic_loss':   205.0539, 'actor_loss':    -2.1508, 'alpha_loss':    -0.0113, 'eps_e':     1.0000})
Step:  185000, Reward:   243.918 [  25.003], Avg:    50.593 (1.000) <0-01:18:43> ({'r_t':  1122.9075, 'eps':     1.0000, 'critic_loss':   202.2110, 'actor_loss':    -2.3639, 'alpha_loss':    -0.0064, 'eps_e':     1.0000})
Step:  186000, Reward:   217.716 [  99.170], Avg:    51.487 (1.000) <0-01:19:03> ({'r_t':  1209.8124, 'eps':     1.0000, 'critic_loss':   200.5942, 'actor_loss':    -2.1435, 'alpha_loss':    -0.0123, 'eps_e':     1.0000})
Step:  187000, Reward:   246.764 [  22.883], Avg:    52.526 (1.000) <0-01:19:23> ({'r_t':  1197.7352, 'eps':     1.0000, 'critic_loss':   194.3598, 'actor_loss':    -2.1509, 'alpha_loss':    -0.0121, 'eps_e':     1.0000})
Step:  188000, Reward:   264.588 [  20.245], Avg:    53.648 (1.000) <0-01:19:43> ({'r_t':  1202.1653, 'eps':     1.0000, 'critic_loss':   204.3109, 'actor_loss':    -2.3377, 'alpha_loss':    -0.0168, 'eps_e':     1.0000})
Step:  189000, Reward:   256.063 [  19.538], Avg:    54.713 (1.000) <0-01:20:02> ({'r_t':  1302.0812, 'eps':     1.0000, 'critic_loss':   197.5762, 'actor_loss':    -2.3294, 'alpha_loss':    -0.0307, 'eps_e':     1.0000})
Step:  190000, Reward:   261.476 [  15.064], Avg:    55.795 (1.000) <0-01:20:21> ({'r_t':  1347.5573, 'eps':     1.0000, 'critic_loss':   193.3058, 'actor_loss':    -2.2704, 'alpha_loss':    -0.0143, 'eps_e':     1.0000})
Step:  191000, Reward:   255.961 [  24.069], Avg:    56.838 (1.000) <0-01:20:40> ({'r_t':  1243.3736, 'eps':     1.0000, 'critic_loss':   187.6054, 'actor_loss':    -2.4003, 'alpha_loss':    -0.0116, 'eps_e':     1.0000})
Step:  192000, Reward:   252.362 [  19.700], Avg:    57.851 (1.000) <0-01:20:59> ({'r_t':  1315.1461, 'eps':     1.0000, 'critic_loss':   190.8082, 'actor_loss':    -2.4877, 'alpha_loss':    -0.0136, 'eps_e':     1.0000})
Step:  193000, Reward:   237.217 [  60.591], Avg:    58.776 (1.000) <0-01:21:18> ({'r_t':  1384.6854, 'eps':     1.0000, 'critic_loss':   187.4058, 'actor_loss':    -2.2700, 'alpha_loss':    -0.0284, 'eps_e':     1.0000})
Step:  194000, Reward:   230.368 [  52.346], Avg:    59.656 (1.000) <0-01:21:36> ({'r_t':  1365.4192, 'eps':     1.0000, 'critic_loss':   182.5163, 'actor_loss':    -2.2892, 'alpha_loss':    -0.0225, 'eps_e':     1.0000})
Step:  195000, Reward:   242.355 [  56.151], Avg:    60.588 (1.000) <0-01:21:55> ({'r_t':  1212.3161, 'eps':     1.0000, 'critic_loss':   181.6497, 'actor_loss':    -2.2679, 'alpha_loss':    -0.0112, 'eps_e':     1.0000})
Step:  196000, Reward:   256.610 [  21.660], Avg:    61.583 (1.000) <0-01:22:13> ({'r_t':  1360.9435, 'eps':     1.0000, 'critic_loss':   183.4383, 'actor_loss':    -2.0988, 'alpha_loss':    -0.0395, 'eps_e':     1.0000})
Step:  197000, Reward:   247.038 [  20.046], Avg:    62.519 (1.000) <0-01:22:32> ({'r_t':  1287.5402, 'eps':     1.0000, 'critic_loss':   175.8282, 'actor_loss':    -2.3260, 'alpha_loss':    -0.0294, 'eps_e':     1.0000})
Step:  198000, Reward:   238.689 [  50.896], Avg:    63.405 (1.000) <0-01:22:50> ({'r_t':  1339.9661, 'eps':     1.0000, 'critic_loss':   174.3774, 'actor_loss':    -2.3480, 'alpha_loss':    -0.0359, 'eps_e':     1.0000})
Step:  199000, Reward:   218.098 [  73.745], Avg:    64.178 (1.000) <0-01:23:08> ({'r_t':  1197.3070, 'eps':     1.0000, 'critic_loss':   171.4310, 'actor_loss':    -2.2533, 'alpha_loss':    -0.0205, 'eps_e':     1.0000})
Step:  200000, Reward:   241.828 [  67.501], Avg:    65.062 (1.000) <0-01:23:27> ({'r_t':  1315.0350, 'eps':     1.0000, 'critic_loss':   162.2990, 'actor_loss':    -2.5979, 'alpha_loss':    -0.0306, 'eps_e':     1.0000})
Step:  201000, Reward:   245.250 [  55.073], Avg:    65.954 (1.000) <0-01:23:45> ({'r_t':  1100.1580, 'eps':     1.0000, 'critic_loss':   160.9980, 'actor_loss':    -2.6221, 'alpha_loss':    -0.0476, 'eps_e':     1.0000})
Step:  202000, Reward:   237.959 [  59.508], Avg:    66.801 (1.000) <0-01:24:04> ({'r_t':  1270.7061, 'eps':     1.0000, 'critic_loss':   166.7036, 'actor_loss':    -2.8023, 'alpha_loss':    -0.0441, 'eps_e':     1.0000})
Step:  203000, Reward:   267.440 [  27.955], Avg:    67.785 (1.000) <0-01:24:23> ({'r_t':  1051.8510, 'eps':     1.0000, 'critic_loss':   162.8179, 'actor_loss':    -2.6255, 'alpha_loss':    -0.0458, 'eps_e':     1.0000})
Step:  204000, Reward:   244.799 [  36.189], Avg:    68.648 (1.000) <0-01:24:43> ({'r_t':  1264.1080, 'eps':     1.0000, 'critic_loss':   166.5379, 'actor_loss':    -2.6897, 'alpha_loss':    -0.0311, 'eps_e':     1.0000})
Step:  205000, Reward:   210.805 [  71.373], Avg:    69.338 (1.000) <0-01:25:02> ({'r_t':  1088.6337, 'eps':     1.0000, 'critic_loss':   156.8942, 'actor_loss':    -2.5180, 'alpha_loss':    -0.0494, 'eps_e':     1.0000})
Step:  206000, Reward:   238.292 [  34.913], Avg:    70.155 (1.000) <0-01:25:21> ({'r_t':  1105.1412, 'eps':     1.0000, 'critic_loss':   162.4236, 'actor_loss':    -2.5879, 'alpha_loss':    -0.0617, 'eps_e':     1.0000})
Step:  207000, Reward:   259.233 [  36.333], Avg:    71.064 (1.000) <0-01:25:40> ({'r_t':  1185.0455, 'eps':     1.0000, 'critic_loss':   144.8765, 'actor_loss':    -2.6647, 'alpha_loss':    -0.0731, 'eps_e':     1.0000})
Step:  208000, Reward:   226.203 [  54.220], Avg:    71.806 (1.000) <0-01:25:59> ({'r_t':  1276.1716, 'eps':     1.0000, 'critic_loss':   149.8579, 'actor_loss':    -2.6486, 'alpha_loss':    -0.0886, 'eps_e':     1.0000})
Step:  209000, Reward:   239.553 [  61.550], Avg:    72.605 (1.000) <0-01:26:17> ({'r_t':  1178.7093, 'eps':     1.0000, 'critic_loss':   149.9276, 'actor_loss':    -2.8086, 'alpha_loss':    -0.0654, 'eps_e':     1.0000})
Step:  210000, Reward:   243.586 [  28.937], Avg:    73.415 (1.000) <0-01:26:35> ({'r_t':  1196.8460, 'eps':     1.0000, 'critic_loss':   143.1078, 'actor_loss':    -3.1696, 'alpha_loss':    -0.0391, 'eps_e':     1.0000})
Step:  211000, Reward:   228.380 [  79.060], Avg:    74.146 (1.000) <0-01:26:55> ({'r_t':  1115.7190, 'eps':     1.0000, 'critic_loss':   131.0161, 'actor_loss':    -3.3360, 'alpha_loss':    -0.0606, 'eps_e':     1.0000})
Step:  212000, Reward:   253.481 [  40.176], Avg:    74.988 (1.000) <0-01:27:14> ({'r_t':  1279.1168, 'eps':     1.0000, 'critic_loss':   140.9271, 'actor_loss':    -3.3483, 'alpha_loss':    -0.0490, 'eps_e':     1.0000})
Step:  213000, Reward:   255.135 [  30.272], Avg:    75.830 (1.000) <0-01:27:33> ({'r_t':  1231.9869, 'eps':     1.0000, 'critic_loss':   136.0281, 'actor_loss':    -3.3694, 'alpha_loss':    -0.0920, 'eps_e':     1.0000})
Step:  214000, Reward:   268.872 [  29.067], Avg:    76.728 (1.000) <0-01:27:51> ({'r_t':  1205.3164, 'eps':     1.0000, 'critic_loss':   133.3884, 'actor_loss':    -3.3908, 'alpha_loss':    -0.0661, 'eps_e':     1.0000})
Step:  215000, Reward:   257.479 [  28.557], Avg:    77.564 (1.000) <0-01:28:10> ({'r_t':  1282.3354, 'eps':     1.0000, 'critic_loss':   130.5132, 'actor_loss':    -3.4646, 'alpha_loss':    -0.0485, 'eps_e':     1.0000})
Step:  216000, Reward:   237.241 [  62.674], Avg:    78.300 (1.000) <0-01:28:28> ({'r_t':  1262.7247, 'eps':     1.0000, 'critic_loss':   127.8679, 'actor_loss':    -3.8070, 'alpha_loss':    -0.0568, 'eps_e':     1.0000})
Step:  217000, Reward:   263.137 [  24.650], Avg:    79.148 (1.000) <0-01:28:47> ({'r_t':  1078.6635, 'eps':     1.0000, 'critic_loss':   129.0875, 'actor_loss':    -4.2024, 'alpha_loss':    -0.0279, 'eps_e':     1.0000})
Step:  218000, Reward:   228.446 [  59.981], Avg:    79.830 (1.000) <0-01:29:06> ({'r_t':  1194.6887, 'eps':     1.0000, 'critic_loss':   134.8423, 'actor_loss':    -4.0222, 'alpha_loss':     0.0096, 'eps_e':     1.0000})
Step:  219000, Reward:   214.411 [  78.016], Avg:    80.442 (1.000) <0-01:29:24> ({'r_t':  1250.4109, 'eps':     1.0000, 'critic_loss':   125.7005, 'actor_loss':    -3.9527, 'alpha_loss':    -0.0113, 'eps_e':     1.0000})
Step:  220000, Reward:   249.735 [  29.284], Avg:    81.208 (1.000) <0-01:29:43> ({'r_t':  1207.1167, 'eps':     1.0000, 'critic_loss':   118.1573, 'actor_loss':    -4.0098, 'alpha_loss':    -0.0217, 'eps_e':     1.0000})
Step:  221000, Reward:   236.535 [  68.764], Avg:    81.907 (1.000) <0-01:30:02> ({'r_t':  1155.9958, 'eps':     1.0000, 'critic_loss':   118.2998, 'actor_loss':    -4.3194, 'alpha_loss':    -0.0131, 'eps_e':     1.0000})
Step:  222000, Reward:   252.583 [  27.756], Avg:    82.673 (1.000) <0-01:30:21> ({'r_t':  1208.5905, 'eps':     1.0000, 'critic_loss':   124.9265, 'actor_loss':    -4.1232, 'alpha_loss':    -0.0637, 'eps_e':     1.0000})
Step:  223000, Reward:   246.104 [  25.086], Avg:    83.402 (1.000) <0-01:30:40> ({'r_t':  1126.3599, 'eps':     1.0000, 'critic_loss':   117.2443, 'actor_loss':    -4.4681, 'alpha_loss':    -0.0379, 'eps_e':     1.0000})
Step:  224000, Reward:   233.176 [  38.234], Avg:    84.068 (1.000) <0-01:30:59> ({'r_t':  1177.7968, 'eps':     1.0000, 'critic_loss':   118.7169, 'actor_loss':    -4.3168, 'alpha_loss':    -0.0667, 'eps_e':     1.0000})
Step:  225000, Reward:   243.522 [  31.351], Avg:    84.774 (1.000) <0-01:31:18> ({'r_t':  1200.2360, 'eps':     1.0000, 'critic_loss':   116.6133, 'actor_loss':    -4.4027, 'alpha_loss':    -0.0091, 'eps_e':     1.0000})
Step:  226000, Reward:   233.752 [  68.461], Avg:    85.430 (1.000) <0-01:31:38> ({'r_t':  1192.1753, 'eps':     1.0000, 'critic_loss':   111.1185, 'actor_loss':    -4.5022, 'alpha_loss':    -0.0355, 'eps_e':     1.0000})
Step:  227000, Reward:   254.067 [  34.499], Avg:    86.169 (1.000) <0-01:31:56> ({'r_t':  1134.9875, 'eps':     1.0000, 'critic_loss':   112.1779, 'actor_loss':    -4.2662, 'alpha_loss':    -0.0726, 'eps_e':     1.0000})
Step:  228000, Reward:   227.225 [  59.245], Avg:    86.785 (1.000) <0-01:32:16> ({'r_t':   980.2701, 'eps':     1.0000, 'critic_loss':   111.4407, 'actor_loss':    -4.5234, 'alpha_loss':    -0.0619, 'eps_e':     1.0000})
Step:  229000, Reward:   234.111 [  60.681], Avg:    87.426 (1.000) <0-01:32:34> ({'r_t':  1064.9095, 'eps':     1.0000, 'critic_loss':   110.6795, 'actor_loss':    -4.3705, 'alpha_loss':    -0.0309, 'eps_e':     1.0000})
Step:  230000, Reward:   227.776 [  84.578], Avg:    88.034 (1.000) <0-01:32:53> ({'r_t':  1104.9332, 'eps':     1.0000, 'critic_loss':   109.1555, 'actor_loss':    -4.3002, 'alpha_loss':    -0.0361, 'eps_e':     1.0000})
Step:  231000, Reward:   264.167 [  35.948], Avg:    88.793 (1.000) <0-01:33:12> ({'r_t':  1172.2714, 'eps':     1.0000, 'critic_loss':   108.0072, 'actor_loss':    -4.1580, 'alpha_loss':    -0.0283, 'eps_e':     1.0000})
Step:  232000, Reward:   214.884 [  81.771], Avg:    89.334 (1.000) <0-01:33:31> ({'r_t':  1229.5258, 'eps':     1.0000, 'critic_loss':   100.6272, 'actor_loss':    -4.1312, 'alpha_loss':    -0.0158, 'eps_e':     1.0000})
Step:  233000, Reward:   252.978 [  29.771], Avg:    90.033 (1.000) <0-01:33:50> ({'r_t':  1029.0013, 'eps':     1.0000, 'critic_loss':   108.9035, 'actor_loss':    -3.9739, 'alpha_loss':    -0.0454, 'eps_e':     1.0000})
Step:  234000, Reward:   236.192 [  36.699], Avg:    90.655 (1.000) <0-01:34:09> ({'r_t':  1112.9317, 'eps':     1.0000, 'critic_loss':   105.9838, 'actor_loss':    -4.1182, 'alpha_loss':    -0.0246, 'eps_e':     1.0000})
Step:  235000, Reward:   220.893 [  82.345], Avg:    91.207 (1.000) <0-01:34:30> ({'r_t':  1182.4265, 'eps':     1.0000, 'critic_loss':   103.9631, 'actor_loss':    -3.9734, 'alpha_loss':    -0.0276, 'eps_e':     1.0000})
Step:  236000, Reward:   212.929 [  72.417], Avg:    91.721 (1.000) <0-01:34:49> ({'r_t':  1095.7894, 'eps':     1.0000, 'critic_loss':   103.6115, 'actor_loss':    -3.9136, 'alpha_loss':     0.0199, 'eps_e':     1.0000})
Step:  237000, Reward:   208.073 [  85.219], Avg:    92.209 (1.000) <0-01:35:09> ({'r_t':  1122.9521, 'eps':     1.0000, 'critic_loss':   108.8122, 'actor_loss':    -3.8078, 'alpha_loss':    -0.0530, 'eps_e':     1.0000})
Step:  238000, Reward:   235.182 [  57.577], Avg:    92.808 (1.000) <0-01:35:29> ({'r_t':   997.2479, 'eps':     1.0000, 'critic_loss':   111.1890, 'actor_loss':    -3.6297, 'alpha_loss':    -0.0088, 'eps_e':     1.0000})
Step:  239000, Reward:   254.493 [  28.153], Avg:    93.481 (1.000) <0-01:35:47> ({'r_t':  1002.5866, 'eps':     1.0000, 'critic_loss':   103.7246, 'actor_loss':    -3.5758, 'alpha_loss':    -0.0420, 'eps_e':     1.0000})
Step:  240000, Reward:   248.928 [  64.663], Avg:    94.126 (1.000) <0-01:36:07> ({'r_t':  1138.5865, 'eps':     1.0000, 'critic_loss':   111.1447, 'actor_loss':    -3.5057, 'alpha_loss':     0.0161, 'eps_e':     1.0000})
Step:  241000, Reward:   242.235 [  65.474], Avg:    94.738 (1.000) <0-01:36:26> ({'r_t':  1114.4184, 'eps':     1.0000, 'critic_loss':   111.6130, 'actor_loss':    -3.2945, 'alpha_loss':    -0.0174, 'eps_e':     1.0000})
Step:  242000, Reward:   213.933 [  98.389], Avg:    95.229 (1.000) <0-01:36:46> ({'r_t':  1020.1345, 'eps':     1.0000, 'critic_loss':   109.9408, 'actor_loss':    -3.2529, 'alpha_loss':    -0.0405, 'eps_e':     1.0000})
Step:  243000, Reward:   238.404 [  63.837], Avg:    95.816 (1.000) <0-01:37:07> ({'r_t':  1031.4230, 'eps':     1.0000, 'critic_loss':   116.2791, 'actor_loss':    -3.1456, 'alpha_loss':    -0.0506, 'eps_e':     1.0000})
Step:  244000, Reward:   188.764 [ 106.811], Avg:    96.195 (1.000) <0-01:37:26> ({'r_t':   926.3987, 'eps':     1.0000, 'critic_loss':   124.8316, 'actor_loss':    -2.9297, 'alpha_loss':    -0.0154, 'eps_e':     1.0000})
Step:  245000, Reward:   209.409 [  76.650], Avg:    96.655 (1.000) <0-01:37:46> ({'r_t':   973.2467, 'eps':     1.0000, 'critic_loss':   118.2880, 'actor_loss':    -2.8423, 'alpha_loss':    -0.0407, 'eps_e':     1.0000})
Step:  246000, Reward:   255.569 [  35.853], Avg:    97.299 (1.000) <0-01:38:05> ({'r_t':  1009.8903, 'eps':     1.0000, 'critic_loss':   123.4133, 'actor_loss':    -2.9939, 'alpha_loss':    -0.0019, 'eps_e':     1.0000})
Step:  247000, Reward:   229.958 [  76.568], Avg:    97.834 (1.000) <0-01:38:24> ({'r_t':  1088.4090, 'eps':     1.0000, 'critic_loss':   119.6254, 'actor_loss':    -2.9614, 'alpha_loss':     0.0136, 'eps_e':     1.0000})
Step:  248000, Reward:   229.563 [  53.626], Avg:    98.363 (1.000) <0-01:38:44> ({'r_t':  1029.0464, 'eps':     1.0000, 'critic_loss':   118.7930, 'actor_loss':    -2.9055, 'alpha_loss':     0.0239, 'eps_e':     1.0000})
Step:  249000, Reward:   187.607 [  98.514], Avg:    98.720 (1.000) <0-01:39:04> ({'r_t':  1130.6556, 'eps':     1.0000, 'critic_loss':   121.1193, 'actor_loss':    -2.7631, 'alpha_loss':    -0.0058, 'eps_e':     1.0000})
Step:  250000, Reward:   246.676 [  62.915], Avg:    99.309 (1.000) <0-01:39:24> ({'r_t':  1147.4938, 'eps':     1.0000, 'critic_loss':   118.5080, 'actor_loss':    -2.7497, 'alpha_loss':    -0.0014, 'eps_e':     1.0000})
Step:  251000, Reward:   235.331 [  81.120], Avg:    99.849 (1.000) <0-01:39:42> ({'r_t':  1124.9436, 'eps':     1.0000, 'critic_loss':   118.2064, 'actor_loss':    -2.6325, 'alpha_loss':     0.0013, 'eps_e':     1.0000})
Step:  252000, Reward:   222.628 [  84.304], Avg:   100.334 (1.000) <0-01:40:01> ({'r_t':  1259.8947, 'eps':     1.0000, 'critic_loss':   129.0374, 'actor_loss':    -2.7201, 'alpha_loss':    -0.0055, 'eps_e':     1.0000})
Step:  253000, Reward:   212.729 [  88.384], Avg:   100.777 (1.000) <0-01:40:19> ({'r_t':  1093.1480, 'eps':     1.0000, 'critic_loss':   126.6083, 'actor_loss':    -2.5887, 'alpha_loss':    -0.0117, 'eps_e':     1.0000})
Step:  254000, Reward:   243.001 [  60.285], Avg:   101.334 (1.000) <0-01:40:39> ({'r_t':  1090.7388, 'eps':     1.0000, 'critic_loss':   124.3360, 'actor_loss':    -2.4776, 'alpha_loss':    -0.0338, 'eps_e':     1.0000})
Step:  255000, Reward:   198.473 [ 100.955], Avg:   101.714 (1.000) <0-01:40:57> ({'r_t':  1125.9761, 'eps':     1.0000, 'critic_loss':   126.3106, 'actor_loss':    -2.5179, 'alpha_loss':     0.0087, 'eps_e':     1.0000})
Step:  256000, Reward:   212.459 [  97.588], Avg:   102.145 (1.000) <0-01:41:18> ({'r_t':  1088.2264, 'eps':     1.0000, 'critic_loss':   126.6177, 'actor_loss':    -2.3606, 'alpha_loss':    -0.0280, 'eps_e':     1.0000})
Step:  257000, Reward:   254.571 [  25.848], Avg:   102.736 (1.000) <0-01:41:37> ({'r_t':  1217.3754, 'eps':     1.0000, 'critic_loss':   133.6663, 'actor_loss':    -2.3093, 'alpha_loss':     0.0015, 'eps_e':     1.0000})
Step:  258000, Reward:   226.089 [  81.016], Avg:   103.212 (1.000) <0-01:41:56> ({'r_t':  1169.0774, 'eps':     1.0000, 'critic_loss':   134.8123, 'actor_loss':    -2.3184, 'alpha_loss':     0.0371, 'eps_e':     1.0000})
Step:  259000, Reward:   174.949 [ 112.287], Avg:   103.488 (1.000) <0-01:42:18> ({'r_t':  1077.8849, 'eps':     1.0000, 'critic_loss':   134.5480, 'actor_loss':    -2.2594, 'alpha_loss':     0.0304, 'eps_e':     1.0000})
Step:  260000, Reward:   240.810 [  72.973], Avg:   104.014 (1.000) <0-01:42:37> ({'r_t':   897.1155, 'eps':     1.0000, 'critic_loss':   135.5859, 'actor_loss':    -2.1089, 'alpha_loss':     0.0153, 'eps_e':     1.0000})
Step:  261000, Reward:   227.513 [  73.476], Avg:   104.485 (1.000) <0-01:42:56> ({'r_t':  1103.1966, 'eps':     1.0000, 'critic_loss':   139.3327, 'actor_loss':    -1.9756, 'alpha_loss':     0.0194, 'eps_e':     1.0000})
Step:  262000, Reward:   220.933 [  95.541], Avg:   104.928 (1.000) <0-01:43:16> ({'r_t':  1173.7499, 'eps':     1.0000, 'critic_loss':   145.6120, 'actor_loss':    -1.7493, 'alpha_loss':     0.0077, 'eps_e':     1.0000})
Step:  263000, Reward:   159.137 [ 112.654], Avg:   105.133 (1.000) <0-01:43:35> ({'r_t':  1116.6560, 'eps':     1.0000, 'critic_loss':   137.5116, 'actor_loss':    -1.6592, 'alpha_loss':    -0.0195, 'eps_e':     1.0000})
Step:  264000, Reward:   231.991 [  75.605], Avg:   105.612 (1.000) <0-01:43:54> ({'r_t':   973.4777, 'eps':     1.0000, 'critic_loss':   142.5167, 'actor_loss':    -1.7670, 'alpha_loss':     0.0245, 'eps_e':     1.0000})
Step:  265000, Reward:   269.767 [  29.863], Avg:   106.229 (1.000) <0-01:44:13> ({'r_t':  1113.6049, 'eps':     1.0000, 'critic_loss':   143.1610, 'actor_loss':    -1.8463, 'alpha_loss':     0.0349, 'eps_e':     1.0000})
Step:  266000, Reward:   262.044 [  26.862], Avg:   106.813 (1.000) <0-01:44:32> ({'r_t':  1119.6765, 'eps':     1.0000, 'critic_loss':   147.9445, 'actor_loss':    -1.8147, 'alpha_loss':    -0.0048, 'eps_e':     1.0000})
Step:  267000, Reward:   214.941 [  93.412], Avg:   107.216 (1.000) <0-01:44:51> ({'r_t':  1179.0586, 'eps':     1.0000, 'critic_loss':   151.2054, 'actor_loss':    -1.7534, 'alpha_loss':     0.0118, 'eps_e':     1.0000})
Step:  268000, Reward:   261.911 [  22.670], Avg:   107.791 (1.000) <0-01:45:10> ({'r_t':  1222.6527, 'eps':     1.0000, 'critic_loss':   151.1746, 'actor_loss':    -1.6355, 'alpha_loss':    -0.0189, 'eps_e':     1.0000})
Step:  269000, Reward:   259.395 [  27.942], Avg:   108.353 (1.000) <0-01:45:31> ({'r_t':  1126.0884, 'eps':     1.0000, 'critic_loss':   154.8593, 'actor_loss':    -1.7037, 'alpha_loss':     0.0064, 'eps_e':     1.0000})
Step:  270000, Reward:   213.482 [  93.895], Avg:   108.741 (1.000) <0-01:45:49> ({'r_t':  1038.3847, 'eps':     1.0000, 'critic_loss':   155.0003, 'actor_loss':    -1.5841, 'alpha_loss':     0.0225, 'eps_e':     1.0000})
Step:  271000, Reward:   240.328 [  68.508], Avg:   109.224 (1.000) <0-01:46:10> ({'r_t':  1051.6364, 'eps':     1.0000, 'critic_loss':   153.7700, 'actor_loss':    -1.4981, 'alpha_loss':    -0.0119, 'eps_e':     1.0000})
Step:  272000, Reward:   264.466 [  14.559], Avg:   109.793 (1.000) <0-01:46:30> ({'r_t':  1110.2966, 'eps':     1.0000, 'critic_loss':   151.9636, 'actor_loss':    -1.4560, 'alpha_loss':    -0.0167, 'eps_e':     1.0000})
Step:  273000, Reward:   246.934 [  66.971], Avg:   110.294 (1.000) <0-01:46:48> ({'r_t':  1252.6228, 'eps':     1.0000, 'critic_loss':   157.6313, 'actor_loss':    -1.5349, 'alpha_loss':     0.0272, 'eps_e':     1.0000})
Step:  274000, Reward:   215.885 [  93.257], Avg:   110.678 (1.000) <0-01:47:08> ({'r_t':  1230.4511, 'eps':     1.0000, 'critic_loss':   157.5101, 'actor_loss':    -1.6050, 'alpha_loss':     0.0370, 'eps_e':     1.0000})
Step:  275000, Reward:   229.011 [  84.394], Avg:   111.106 (1.000) <0-01:47:28> ({'r_t':  1021.0246, 'eps':     1.0000, 'critic_loss':   161.4193, 'actor_loss':    -1.4699, 'alpha_loss':     0.0146, 'eps_e':     1.0000})
Step:  276000, Reward:   219.305 [  90.294], Avg:   111.497 (1.000) <0-01:47:46> ({'r_t':  1147.1313, 'eps':     1.0000, 'critic_loss':   158.9326, 'actor_loss':    -1.4471, 'alpha_loss':     0.0082, 'eps_e':     1.0000})
Step:  277000, Reward:   238.351 [  66.493], Avg:   111.953 (1.000) <0-01:48:05> ({'r_t':  1139.3874, 'eps':     1.0000, 'critic_loss':   160.4206, 'actor_loss':    -1.4129, 'alpha_loss':    -0.0032, 'eps_e':     1.0000})
Step:  278000, Reward:   208.672 [ 125.591], Avg:   112.300 (1.000) <0-01:48:23> ({'r_t':  1161.4184, 'eps':     1.0000, 'critic_loss':   170.2283, 'actor_loss':    -1.3505, 'alpha_loss':    -0.0216, 'eps_e':     1.0000})
Step:  279000, Reward:   257.269 [  20.308], Avg:   112.818 (1.000) <0-01:48:42> ({'r_t':  1187.1594, 'eps':     1.0000, 'critic_loss':   175.3495, 'actor_loss':    -1.3089, 'alpha_loss':     0.0047, 'eps_e':     1.0000})
Step:  280000, Reward:   231.127 [  71.560], Avg:   113.239 (1.000) <0-01:49:00> ({'r_t':  1138.8980, 'eps':     1.0000, 'critic_loss':   167.8917, 'actor_loss':    -1.2974, 'alpha_loss':  -4.00e-05, 'eps_e':     1.0000})
Step:  281000, Reward:   244.672 [  59.886], Avg:   113.705 (1.000) <0-01:49:20> ({'r_t':  1195.7623, 'eps':     1.0000, 'critic_loss':   169.6998, 'actor_loss':    -1.2767, 'alpha_loss':     0.0083, 'eps_e':     1.0000})
Step:  282000, Reward:   191.482 [ 100.347], Avg:   113.980 (1.000) <0-01:49:39> ({'r_t':  1206.7771, 'eps':     1.0000, 'critic_loss':   177.3884, 'actor_loss':    -1.1991, 'alpha_loss':    -0.0131, 'eps_e':     1.0000})
Step:  283000, Reward:   239.528 [  75.040], Avg:   114.422 (1.000) <0-01:49:57> ({'r_t':  1145.1706, 'eps':     1.0000, 'critic_loss':   171.7145, 'actor_loss':    -1.3379, 'alpha_loss':     0.0023, 'eps_e':     1.0000})
Step:  284000, Reward:   222.295 [  88.811], Avg:   114.800 (1.000) <0-01:50:16> ({'r_t':  1172.7479, 'eps':     1.0000, 'critic_loss':   168.4063, 'actor_loss':    -1.4150, 'alpha_loss':     0.0075, 'eps_e':     1.0000})
Step:  285000, Reward:   260.632 [  19.483], Avg:   115.310 (1.000) <0-01:50:35> ({'r_t':  1264.5828, 'eps':     1.0000, 'critic_loss':   175.4386, 'actor_loss':    -1.3190, 'alpha_loss':     0.0138, 'eps_e':     1.0000})
Step:  286000, Reward:   239.446 [  58.263], Avg:   115.743 (1.000) <0-01:50:54> ({'r_t':  1205.3833, 'eps':     1.0000, 'critic_loss':   167.4844, 'actor_loss':    -1.3559, 'alpha_loss':     0.0556, 'eps_e':     1.0000})
Step:  287000, Reward:   211.581 [ 100.904], Avg:   116.075 (1.000) <0-01:51:14> ({'r_t':  1249.8302, 'eps':     1.0000, 'critic_loss':   175.5431, 'actor_loss':    -1.2190, 'alpha_loss':     0.0118, 'eps_e':     1.0000})
Step:  288000, Reward:   244.257 [  59.024], Avg:   116.519 (1.000) <0-01:51:33> ({'r_t':  1229.1704, 'eps':     1.0000, 'critic_loss':   180.1097, 'actor_loss':    -1.3073, 'alpha_loss':     0.0049, 'eps_e':     1.0000})
Step:  289000, Reward:   243.179 [  60.353], Avg:   116.956 (1.000) <0-01:51:52> ({'r_t':  1115.8440, 'eps':     1.0000, 'critic_loss':   172.4639, 'actor_loss':    -1.4321, 'alpha_loss':     0.0827, 'eps_e':     1.0000})
Step:  290000, Reward:   226.316 [  89.723], Avg:   117.332 (1.000) <0-01:52:12> ({'r_t':  1222.5894, 'eps':     1.0000, 'critic_loss':   167.4758, 'actor_loss':    -1.4480, 'alpha_loss':     0.0619, 'eps_e':     1.0000})
Step:  291000, Reward:   246.541 [  24.177], Avg:   117.774 (1.000) <0-01:52:32> ({'r_t':  1126.6552, 'eps':     1.0000, 'critic_loss':   171.0388, 'actor_loss':    -1.5284, 'alpha_loss':     0.0892, 'eps_e':     1.0000})
Step:  292000, Reward:   242.627 [  57.759], Avg:   118.200 (1.000) <0-01:52:50> ({'r_t':  1125.1818, 'eps':     1.0000, 'critic_loss':   166.3238, 'actor_loss':    -1.4018, 'alpha_loss':     0.0614, 'eps_e':     1.0000})
Step:  293000, Reward:   212.004 [  86.487], Avg:   118.519 (1.000) <0-01:53:10> ({'r_t':  1347.4750, 'eps':     1.0000, 'critic_loss':   173.6576, 'actor_loss':    -1.5625, 'alpha_loss':     0.0467, 'eps_e':     1.0000})
Step:  294000, Reward:   263.462 [  21.696], Avg:   119.011 (1.000) <0-01:53:28> ({'r_t':  1277.8166, 'eps':     1.0000, 'critic_loss':   170.5468, 'actor_loss':    -1.5633, 'alpha_loss':     0.0358, 'eps_e':     1.0000})
Step:  295000, Reward:   239.173 [  52.758], Avg:   119.416 (1.000) <0-01:53:48> ({'r_t':  1333.5481, 'eps':     1.0000, 'critic_loss':   176.9393, 'actor_loss':    -1.3241, 'alpha_loss':   7.00e-05, 'eps_e':     1.0000})
Step:  296000, Reward:   230.885 [  55.174], Avg:   119.792 (1.000) <0-01:54:08> ({'r_t':  1187.7300, 'eps':     1.0000, 'critic_loss':   171.6753, 'actor_loss':    -1.6003, 'alpha_loss':     0.0339, 'eps_e':     1.0000})
Step:  297000, Reward:   250.025 [  52.917], Avg:   120.229 (1.000) <0-01:54:26> ({'r_t':  1217.1613, 'eps':     1.0000, 'critic_loss':   178.2338, 'actor_loss':    -1.6161, 'alpha_loss':     0.0391, 'eps_e':     1.0000})
Step:  298000, Reward:   211.856 [  89.130], Avg:   120.535 (1.000) <0-01:54:45> ({'r_t':  1292.7037, 'eps':     1.0000, 'critic_loss':   171.5094, 'actor_loss':    -1.4716, 'alpha_loss':     0.0021, 'eps_e':     1.0000})
Step:  299000, Reward:   249.041 [  30.678], Avg:   120.964 (1.000) <0-01:55:07> ({'r_t':  1252.4296, 'eps':     1.0000, 'critic_loss':   159.9617, 'actor_loss':    -1.6991, 'alpha_loss':     0.0589, 'eps_e':     1.0000})
Step:  300000, Reward:   198.890 [  89.969], Avg:   121.222 (1.000) <0-01:55:28> ({'r_t':  1351.9899, 'eps':     1.0000, 'critic_loss':   178.3618, 'actor_loss':    -1.7529, 'alpha_loss':     0.0895, 'eps_e':     1.0000})
Step:  301000, Reward:   242.148 [  68.351], Avg:   121.623 (1.000) <0-01:55:46> ({'r_t':  1319.1965, 'eps':     1.0000, 'critic_loss':   165.7866, 'actor_loss':    -1.8798, 'alpha_loss':     0.0698, 'eps_e':     1.0000})
Step:  302000, Reward:   254.971 [  17.327], Avg:   122.063 (1.000) <0-01:56:03> ({'r_t':  1482.7514, 'eps':     1.0000, 'critic_loss':   172.7348, 'actor_loss':    -1.8596, 'alpha_loss':     0.0515, 'eps_e':     1.0000})
Step:  303000, Reward:   241.510 [  53.555], Avg:   122.456 (1.000) <0-01:56:15> ({'r_t':  1300.2748, 'eps':     1.0000, 'critic_loss':   178.0092, 'actor_loss':    -1.9757, 'alpha_loss':     0.0351, 'eps_e':     1.0000})
Step:  304000, Reward:   243.576 [  54.073], Avg:   122.853 (1.000) <0-01:56:26> ({'r_t':  1345.2441, 'eps':     1.0000, 'critic_loss':   181.5623, 'actor_loss':    -1.8875, 'alpha_loss':    -0.0114, 'eps_e':     1.0000})
Step:  305000, Reward:   208.884 [  81.322], Avg:   123.134 (1.000) <0-01:56:38> ({'r_t':  1270.0064, 'eps':     1.0000, 'critic_loss':   160.0374, 'actor_loss':    -2.0071, 'alpha_loss':     0.0073, 'eps_e':     1.0000})
Step:  306000, Reward:   227.373 [  42.823], Avg:   123.474 (1.000) <0-01:56:52> ({'r_t':  1272.9421, 'eps':     1.0000, 'critic_loss':   164.5665, 'actor_loss':    -2.0737, 'alpha_loss':     0.0419, 'eps_e':     1.0000})
Step:  307000, Reward:   263.040 [  18.034], Avg:   123.927 (1.000) <0-01:57:05> ({'r_t':  1092.0200, 'eps':     1.0000, 'critic_loss':   173.9470, 'actor_loss':    -1.9765, 'alpha_loss':     0.0027, 'eps_e':     1.0000})
Step:  308000, Reward:   246.041 [  65.233], Avg:   124.322 (1.000) <0-01:57:17> ({'r_t':  1307.2011, 'eps':     1.0000, 'critic_loss':   155.7248, 'actor_loss':    -2.0886, 'alpha_loss':    -0.0037, 'eps_e':     1.0000})
Step:  309000, Reward:   270.557 [  22.865], Avg:   124.794 (1.000) <0-01:57:28> ({'r_t':  1390.1202, 'eps':     1.0000, 'critic_loss':   162.1543, 'actor_loss':    -2.2041, 'alpha_loss':     0.0203, 'eps_e':     1.0000})
Step:  310000, Reward:   268.869 [  19.360], Avg:   125.257 (1.000) <0-01:57:41> ({'r_t':  1463.7440, 'eps':     1.0000, 'critic_loss':   153.5283, 'actor_loss':    -2.2660, 'alpha_loss':     0.0090, 'eps_e':     1.0000})
Step:  311000, Reward:   266.655 [  45.731], Avg:   125.710 (1.000) <0-01:57:55> ({'r_t':  1354.3298, 'eps':     1.0000, 'critic_loss':   168.0342, 'actor_loss':    -2.2965, 'alpha_loss':     0.0392, 'eps_e':     1.0000})
Step:  312000, Reward:   274.583 [  20.805], Avg:   126.186 (1.000) <0-01:58:06> ({'r_t':  1362.2779, 'eps':     1.0000, 'critic_loss':   151.9195, 'actor_loss':    -2.4214, 'alpha_loss':     0.0156, 'eps_e':     1.0000})
Step:  313000, Reward:   268.791 [  15.801], Avg:   126.640 (1.000) <0-01:58:18> ({'r_t':  1419.6268, 'eps':     1.0000, 'critic_loss':   156.4933, 'actor_loss':    -2.5462, 'alpha_loss':     0.0756, 'eps_e':     1.0000})
Step:  314000, Reward:   272.356 [  14.664], Avg:   127.103 (1.000) <0-01:58:30> ({'r_t':  1422.0428, 'eps':     1.0000, 'critic_loss':   160.7895, 'actor_loss':    -2.5012, 'alpha_loss':     0.0243, 'eps_e':     1.0000})
Step:  315000, Reward:   267.916 [  35.797], Avg:   127.548 (1.000) <0-01:58:44> ({'r_t':  1502.4959, 'eps':     1.0000, 'critic_loss':   150.6222, 'actor_loss':    -2.3622, 'alpha_loss':     0.0151, 'eps_e':     1.0000})
Step:  316000, Reward:   280.074 [  17.974], Avg:   128.029 (1.000) <0-01:58:56> ({'r_t':  1240.4724, 'eps':     1.0000, 'critic_loss':   160.4698, 'actor_loss':    -2.3316, 'alpha_loss':     0.0013, 'eps_e':     1.0000})
Step:  317000, Reward:   264.492 [  16.925], Avg:   128.459 (1.000) <0-01:59:08> ({'r_t':  1407.5467, 'eps':     1.0000, 'critic_loss':   142.6041, 'actor_loss':    -2.3969, 'alpha_loss':    -0.0214, 'eps_e':     1.0000})
Step:  318000, Reward:   253.858 [  57.657], Avg:   128.852 (1.000) <0-01:59:21> ({'r_t':  1508.4450, 'eps':     1.0000, 'critic_loss':   133.8233, 'actor_loss':    -2.3856, 'alpha_loss':    -0.0441, 'eps_e':     1.0000})
Step:  319000, Reward:   270.438 [  12.881], Avg:   129.294 (1.000) <0-01:59:33> ({'r_t':  1359.4644, 'eps':     1.0000, 'critic_loss':   136.9136, 'actor_loss':    -2.3775, 'alpha_loss':    -0.0028, 'eps_e':     1.0000})
Step:  320000, Reward:   256.213 [  19.270], Avg:   129.689 (1.000) <0-01:59:45> ({'r_t':  1361.9936, 'eps':     1.0000, 'critic_loss':   143.6126, 'actor_loss':    -2.2796, 'alpha_loss':    -0.0251, 'eps_e':     1.0000})
Step:  321000, Reward:   271.015 [  16.710], Avg:   130.128 (1.000) <0-01:59:57> ({'r_t':  1258.1883, 'eps':     1.0000, 'critic_loss':   135.6721, 'actor_loss':    -2.3763, 'alpha_loss':    -0.0213, 'eps_e':     1.0000})
Step:  322000, Reward:   267.503 [  17.484], Avg:   130.554 (1.000) <0-02:00:09> ({'r_t':  1309.5155, 'eps':     1.0000, 'critic_loss':   136.6748, 'actor_loss':    -2.3451, 'alpha_loss':     0.0111, 'eps_e':     1.0000})
Step:  323000, Reward:   268.623 [  21.008], Avg:   130.980 (1.000) <0-02:00:20> ({'r_t':  1303.8565, 'eps':     1.0000, 'critic_loss':   128.1812, 'actor_loss':    -2.4363, 'alpha_loss':    -0.0161, 'eps_e':     1.0000})
Step:  324000, Reward:   254.972 [  28.719], Avg:   131.361 (1.000) <0-02:00:33> ({'r_t':  1433.6855, 'eps':     1.0000, 'critic_loss':   129.0562, 'actor_loss':    -2.2847, 'alpha_loss':    -0.0270, 'eps_e':     1.0000})
Step:  325000, Reward:   266.278 [  22.679], Avg:   131.775 (1.000) <0-02:00:44> ({'r_t':  1280.3479, 'eps':     1.0000, 'critic_loss':   125.7246, 'actor_loss':    -2.2617, 'alpha_loss':    -0.0181, 'eps_e':     1.0000})
Step:  326000, Reward:   273.707 [  19.202], Avg:   132.209 (1.000) <0-02:00:56> ({'r_t':  1446.8313, 'eps':     1.0000, 'critic_loss':   121.2394, 'actor_loss':    -2.1102, 'alpha_loss':    -0.0232, 'eps_e':     1.0000})
Step:  327000, Reward:   283.268 [  15.547], Avg:   132.670 (1.000) <0-02:01:08> ({'r_t':  1381.7993, 'eps':     1.0000, 'critic_loss':   122.3658, 'actor_loss':    -2.0554, 'alpha_loss':    -0.0802, 'eps_e':     1.0000})
Step:  328000, Reward:   278.313 [  21.633], Avg:   133.112 (1.000) <0-02:01:20> ({'r_t':  1540.2453, 'eps':     1.0000, 'critic_loss':   121.2124, 'actor_loss':    -2.0556, 'alpha_loss':    -0.0678, 'eps_e':     1.0000})
Step:  329000, Reward:   268.571 [  18.477], Avg:   133.523 (1.000) <0-02:01:31> ({'r_t':  1465.7129, 'eps':     1.0000, 'critic_loss':   108.3013, 'actor_loss':    -2.1534, 'alpha_loss':    -0.0346, 'eps_e':     1.0000})
Step:  330000, Reward:   257.724 [  27.492], Avg:   133.898 (1.000) <0-02:01:43> ({'r_t':  1504.1289, 'eps':     1.0000, 'critic_loss':   118.8120, 'actor_loss':    -2.0651, 'alpha_loss':    -0.0116, 'eps_e':     1.0000})
Step:  331000, Reward:   266.180 [  20.184], Avg:   134.297 (1.000) <0-02:01:55> ({'r_t':  1504.6237, 'eps':     1.0000, 'critic_loss':   108.8632, 'actor_loss':    -1.9364, 'alpha_loss':     0.0010, 'eps_e':     1.0000})
Step:  332000, Reward:   270.657 [  37.565], Avg:   134.706 (1.000) <0-02:02:08> ({'r_t':  1330.6858, 'eps':     1.0000, 'critic_loss':   109.4712, 'actor_loss':    -1.9819, 'alpha_loss':    -0.0169, 'eps_e':     1.0000})
Step:  333000, Reward:   267.120 [  28.701], Avg:   135.103 (1.000) <0-02:02:20> ({'r_t':  1394.8371, 'eps':     1.0000, 'critic_loss':   111.0959, 'actor_loss':    -1.9694, 'alpha_loss':    -0.0592, 'eps_e':     1.0000})
Step:  334000, Reward:   276.352 [  24.296], Avg:   135.524 (1.000) <0-02:02:31> ({'r_t':  1515.9069, 'eps':     1.0000, 'critic_loss':   100.7106, 'actor_loss':    -1.8920, 'alpha_loss':     0.0018, 'eps_e':     1.0000})
Step:  335000, Reward:   272.317 [  19.404], Avg:   135.931 (1.000) <0-02:02:44> ({'r_t':  1488.2842, 'eps':     1.0000, 'critic_loss':   100.1353, 'actor_loss':    -1.8909, 'alpha_loss':     0.0194, 'eps_e':     1.0000})
Step:  336000, Reward:   276.404 [  28.403], Avg:   136.348 (1.000) <0-02:02:55> ({'r_t':  1471.4050, 'eps':     1.0000, 'critic_loss':   101.6265, 'actor_loss':    -1.8569, 'alpha_loss':    -0.0192, 'eps_e':     1.0000})
Step:  337000, Reward:   276.633 [  22.644], Avg:   136.763 (1.000) <0-02:03:07> ({'r_t':  1546.7223, 'eps':     1.0000, 'critic_loss':    95.4849, 'actor_loss':    -1.9179, 'alpha_loss':     0.0523, 'eps_e':     1.0000})
Step:  338000, Reward:   271.155 [  32.097], Avg:   137.160 (1.000) <0-02:03:19> ({'r_t':  1432.8621, 'eps':     1.0000, 'critic_loss':    99.8293, 'actor_loss':    -1.8752, 'alpha_loss':     0.0133, 'eps_e':     1.0000})
Step:  339000, Reward:   279.729 [  32.798], Avg:   137.579 (1.000) <0-02:03:32> ({'r_t':  1549.1529, 'eps':     1.0000, 'critic_loss':    95.0990, 'actor_loss':    -1.9357, 'alpha_loss':     0.0020, 'eps_e':     1.0000})
Step:  340000, Reward:   282.717 [  15.179], Avg:   138.005 (1.000) <0-02:03:43> ({'r_t':  1383.0256, 'eps':     1.0000, 'critic_loss':    91.1427, 'actor_loss':    -1.8262, 'alpha_loss':     0.0065, 'eps_e':     1.0000})
Step:  341000, Reward:   271.848 [  62.231], Avg:   138.396 (1.000) <0-02:03:54> ({'r_t':  1543.4769, 'eps':     1.0000, 'critic_loss':    97.2893, 'actor_loss':    -1.8264, 'alpha_loss':    -0.0108, 'eps_e':     1.0000})
Step:  342000, Reward:   262.751 [  60.084], Avg:   138.758 (1.000) <0-02:04:06> ({'r_t':  1506.0001, 'eps':     1.0000, 'critic_loss':    93.0495, 'actor_loss':    -1.9450, 'alpha_loss':     0.0512, 'eps_e':     1.0000})
Step:  343000, Reward:   264.962 [  64.711], Avg:   139.125 (1.000) <0-02:04:17> ({'r_t':  1592.6154, 'eps':     1.0000, 'critic_loss':    92.0931, 'actor_loss':    -1.7998, 'alpha_loss':     0.0035, 'eps_e':     1.0000})
Step:  344000, Reward:   284.177 [  12.876], Avg:   139.546 (1.000) <0-02:04:28> ({'r_t':  1512.0251, 'eps':     1.0000, 'critic_loss':    85.6254, 'actor_loss':    -1.7335, 'alpha_loss':     0.0243, 'eps_e':     1.0000})
Step:  345000, Reward:   286.021 [  11.092], Avg:   139.969 (1.000) <0-02:04:40> ({'r_t':  1559.2437, 'eps':     1.0000, 'critic_loss':    87.8122, 'actor_loss':    -1.7806, 'alpha_loss':     0.0287, 'eps_e':     1.0000})
Step:  346000, Reward:   246.431 [  87.523], Avg:   140.276 (1.000) <0-02:04:51> ({'r_t':  1586.3884, 'eps':     1.0000, 'critic_loss':    88.3875, 'actor_loss':    -1.6088, 'alpha_loss':     0.0303, 'eps_e':     1.0000})
Step:  347000, Reward:   275.449 [  19.524], Avg:   140.664 (1.000) <0-02:05:02> ({'r_t':  1632.6251, 'eps':     1.0000, 'critic_loss':    84.8209, 'actor_loss':    -1.4418, 'alpha_loss':    -0.0246, 'eps_e':     1.0000})
Step:  348000, Reward:   274.795 [  19.612], Avg:   141.049 (1.000) <0-02:05:15> ({'r_t':  1515.3387, 'eps':     1.0000, 'critic_loss':    80.2144, 'actor_loss':    -1.5106, 'alpha_loss':    -0.0200, 'eps_e':     1.0000})
Step:  349000, Reward:   262.566 [  64.239], Avg:   141.396 (1.000) <0-02:05:26> ({'r_t':  1563.7404, 'eps':     1.0000, 'critic_loss':    78.4100, 'actor_loss':    -1.6124, 'alpha_loss':    -0.0380, 'eps_e':     1.0000})
Step:  350000, Reward:   288.093 [  15.849], Avg:   141.814 (1.000) <0-02:05:38> ({'r_t':  1415.0752, 'eps':     1.0000, 'critic_loss':    82.8207, 'actor_loss':    -1.5091, 'alpha_loss':    -0.0402, 'eps_e':     1.0000})
Step:  351000, Reward:   203.020 [ 117.447], Avg:   141.988 (1.000) <0-02:05:50> ({'r_t':  1546.0682, 'eps':     1.0000, 'critic_loss':    72.9463, 'actor_loss':    -1.4854, 'alpha_loss':    -0.0426, 'eps_e':     1.0000})
Step:  352000, Reward:   255.717 [  81.124], Avg:   142.310 (1.000) <0-02:06:01> ({'r_t':  1644.6443, 'eps':     1.0000, 'critic_loss':    82.9968, 'actor_loss':    -1.4635, 'alpha_loss':    -0.0213, 'eps_e':     1.0000})
Step:  353000, Reward:   284.128 [  21.247], Avg:   142.710 (1.000) <0-02:06:12> ({'r_t':  1594.4814, 'eps':     1.0000, 'critic_loss':    74.7148, 'actor_loss':    -1.3931, 'alpha_loss':    -0.0295, 'eps_e':     1.0000})
Step:  354000, Reward:   267.568 [  65.930], Avg:   143.062 (1.000) <0-02:06:23> ({'r_t':  1687.7848, 'eps':     1.0000, 'critic_loss':    71.7477, 'actor_loss':    -1.4069, 'alpha_loss':    -0.0703, 'eps_e':     1.0000})
Step:  355000, Reward:   238.060 [ 128.184], Avg:   143.329 (1.000) <0-02:06:34> ({'r_t':  1572.2238, 'eps':     1.0000, 'critic_loss':    80.8870, 'actor_loss':    -1.3916, 'alpha_loss':    -0.0105, 'eps_e':     1.0000})
Step:  356000, Reward:   264.833 [  68.544], Avg:   143.669 (1.000) <0-02:06:45> ({'r_t':  1559.9862, 'eps':     1.0000, 'critic_loss':    78.9749, 'actor_loss':    -1.3477, 'alpha_loss':    -0.0404, 'eps_e':     1.0000})
Step:  357000, Reward:   245.679 [  82.738], Avg:   143.954 (1.000) <0-02:06:57> ({'r_t':  1614.3516, 'eps':     1.0000, 'critic_loss':    84.2669, 'actor_loss':    -1.3220, 'alpha_loss':    -0.0086, 'eps_e':     1.0000})
Step:  358000, Reward:   284.970 [  21.615], Avg:   144.347 (1.000) <0-02:07:08> ({'r_t':  1582.9649, 'eps':     1.0000, 'critic_loss':    71.4867, 'actor_loss':    -1.3930, 'alpha_loss':     0.0202, 'eps_e':     1.0000})
Step:  359000, Reward:   274.552 [  20.055], Avg:   144.709 (1.000) <0-02:07:19> ({'r_t':  1619.4334, 'eps':     1.0000, 'critic_loss':    77.9035, 'actor_loss':    -1.3130, 'alpha_loss':     0.0201, 'eps_e':     1.0000})
Step:  360000, Reward:   285.917 [  21.660], Avg:   145.100 (1.000) <0-02:07:30> ({'r_t':  1606.2398, 'eps':     1.0000, 'critic_loss':    71.3220, 'actor_loss':    -1.2559, 'alpha_loss':    -0.0210, 'eps_e':     1.0000})
Step:  361000, Reward:   284.349 [  17.737], Avg:   145.485 (1.000) <0-02:07:42> ({'r_t':  1659.0702, 'eps':     1.0000, 'critic_loss':    73.6694, 'actor_loss':    -1.2777, 'alpha_loss':    -0.0620, 'eps_e':     1.0000})
Step:  362000, Reward:   285.467 [  18.169], Avg:   145.870 (1.000) <0-02:07:53> ({'r_t':  1566.5410, 'eps':     1.0000, 'critic_loss':    73.6769, 'actor_loss':    -1.1166, 'alpha_loss':    -0.0579, 'eps_e':     1.0000})
Step:  363000, Reward:   274.879 [  14.666], Avg:   146.225 (1.000) <0-02:08:04> ({'r_t':  1567.3895, 'eps':     1.0000, 'critic_loss':    68.2146, 'actor_loss':    -1.3097, 'alpha_loss':    -0.0257, 'eps_e':     1.0000})
Step:  364000, Reward:   265.012 [  59.995], Avg:   146.550 (1.000) <0-02:08:16> ({'r_t':  1488.9949, 'eps':     1.0000, 'critic_loss':    81.3508, 'actor_loss':    -1.2968, 'alpha_loss':     0.0321, 'eps_e':     1.0000})
Step:  365000, Reward:   232.940 [ 144.990], Avg:   146.786 (1.000) <0-02:08:29> ({'r_t':  1592.8163, 'eps':     1.0000, 'critic_loss':    72.3670, 'actor_loss':    -1.4336, 'alpha_loss':     0.0308, 'eps_e':     1.0000})
Step:  366000, Reward:   219.259 [ 182.302], Avg:   146.984 (1.000) <0-02:08:41> ({'r_t':  1487.1063, 'eps':     1.0000, 'critic_loss':    68.7917, 'actor_loss':    -1.3821, 'alpha_loss':    -0.0471, 'eps_e':     1.0000})
Step:  367000, Reward:   259.496 [  85.355], Avg:   147.289 (1.000) <0-02:08:53> ({'r_t':  1676.6834, 'eps':     1.0000, 'critic_loss':    66.5650, 'actor_loss':    -1.3313, 'alpha_loss':    -0.0080, 'eps_e':     1.0000})
Step:  368000, Reward:   284.403 [  21.762], Avg:   147.661 (1.000) <0-02:09:04> ({'r_t':  1591.0078, 'eps':     1.0000, 'critic_loss':    79.0880, 'actor_loss':    -1.2672, 'alpha_loss':     0.0087, 'eps_e':     1.0000})
Step:  369000, Reward:   278.552 [  13.931], Avg:   148.015 (1.000) <0-02:09:15> ({'r_t':  1646.6021, 'eps':     1.0000, 'critic_loss':    67.6345, 'actor_loss':    -1.2384, 'alpha_loss':    -0.0019, 'eps_e':     1.0000})
Step:  370000, Reward:   256.343 [  69.427], Avg:   148.307 (1.000) <0-02:09:29> ({'r_t':  1657.2072, 'eps':     1.0000, 'critic_loss':    68.8016, 'actor_loss':    -1.1662, 'alpha_loss':    -0.0496, 'eps_e':     1.0000})
Step:  371000, Reward:   270.461 [  62.323], Avg:   148.635 (1.000) <0-02:09:41> ({'r_t':  1627.9696, 'eps':     1.0000, 'critic_loss':    74.9476, 'actor_loss':    -1.3157, 'alpha_loss':     0.0141, 'eps_e':     1.0000})
Step:  372000, Reward:   276.214 [  33.781], Avg:   148.977 (1.000) <0-02:09:54> ({'r_t':  1640.8504, 'eps':     1.0000, 'critic_loss':    64.7195, 'actor_loss':    -1.3284, 'alpha_loss':    -0.0231, 'eps_e':     1.0000})
Step:  373000, Reward:   287.587 [  16.251], Avg:   149.348 (1.000) <0-02:10:06> ({'r_t':  1595.8853, 'eps':     1.0000, 'critic_loss':    65.3831, 'actor_loss':    -1.2754, 'alpha_loss':     0.0154, 'eps_e':     1.0000})
Step:  374000, Reward:   226.576 [  90.412], Avg:   149.554 (1.000) <0-02:10:17> ({'r_t':  1546.5130, 'eps':     1.0000, 'critic_loss':    64.5997, 'actor_loss':    -1.3019, 'alpha_loss':    -0.0482, 'eps_e':     1.0000})
Step:  375000, Reward:   277.117 [  16.406], Avg:   149.893 (1.000) <0-02:10:28> ({'r_t':  1632.5558, 'eps':     1.0000, 'critic_loss':    58.6741, 'actor_loss':    -1.2499, 'alpha_loss':    -0.0518, 'eps_e':     1.0000})
Step:  376000, Reward:   260.575 [  33.146], Avg:   150.187 (1.000) <0-02:10:42> ({'r_t':  1561.1193, 'eps':     1.0000, 'critic_loss':    71.0915, 'actor_loss':    -1.2023, 'alpha_loss':    -0.0196, 'eps_e':     1.0000})
Step:  377000, Reward:   262.470 [  59.099], Avg:   150.484 (1.000) <0-02:10:53> ({'r_t':  1608.3574, 'eps':     1.0000, 'critic_loss':    73.8013, 'actor_loss':    -1.2336, 'alpha_loss':    -0.0759, 'eps_e':     1.0000})
Step:  378000, Reward:   266.153 [  59.108], Avg:   150.789 (1.000) <0-02:11:05> ({'r_t':  1576.9519, 'eps':     1.0000, 'critic_loss':    65.5105, 'actor_loss':    -1.1413, 'alpha_loss':    -0.0426, 'eps_e':     1.0000})
Step:  379000, Reward:   277.049 [  22.319], Avg:   151.121 (1.000) <0-02:11:16> ({'r_t':  1610.0094, 'eps':     1.0000, 'critic_loss':    60.8808, 'actor_loss':    -1.1824, 'alpha_loss':    -0.0597, 'eps_e':     1.0000})
Step:  380000, Reward:   282.907 [  21.021], Avg:   151.467 (1.000) <0-02:11:28> ({'r_t':  1504.9406, 'eps':     1.0000, 'critic_loss':    67.1570, 'actor_loss':    -1.2260, 'alpha_loss':    -0.0614, 'eps_e':     1.0000})
Step:  381000, Reward:   271.213 [  24.497], Avg:   151.780 (1.000) <0-02:11:41> ({'r_t':  1409.5514, 'eps':     1.0000, 'critic_loss':    69.1699, 'actor_loss':    -1.1743, 'alpha_loss':    -0.0341, 'eps_e':     1.0000})
Step:  382000, Reward:   242.788 [  95.122], Avg:   152.018 (1.000) <0-02:11:53> ({'r_t':  1369.3157, 'eps':     1.0000, 'critic_loss':    66.6059, 'actor_loss':    -1.2412, 'alpha_loss':    -0.0167, 'eps_e':     1.0000})
Step:  383000, Reward:   227.752 [  81.784], Avg:   152.215 (1.000) <0-02:12:05> ({'r_t':  1596.9953, 'eps':     1.0000, 'critic_loss':    66.2831, 'actor_loss':    -1.0728, 'alpha_loss':    -0.0436, 'eps_e':     1.0000})
Step:  384000, Reward:   273.165 [  18.981], Avg:   152.529 (1.000) <0-02:12:16> ({'r_t':  1584.2395, 'eps':     1.0000, 'critic_loss':    78.2805, 'actor_loss':    -1.1162, 'alpha_loss':    -0.0759, 'eps_e':     1.0000})
Step:  385000, Reward:   272.336 [  21.838], Avg:   152.840 (1.000) <0-02:12:27> ({'r_t':  1648.0210, 'eps':     1.0000, 'critic_loss':    73.6959, 'actor_loss':    -1.0791, 'alpha_loss':    -0.0428, 'eps_e':     1.0000})
Step:  386000, Reward:   260.756 [  63.430], Avg:   153.119 (1.000) <0-02:12:39> ({'r_t':  1464.2802, 'eps':     1.0000, 'critic_loss':    70.0615, 'actor_loss':    -1.0981, 'alpha_loss':    -0.0133, 'eps_e':     1.0000})
Step:  387000, Reward:   282.029 [  18.950], Avg:   153.451 (1.000) <0-02:12:50> ({'r_t':  1559.8951, 'eps':     1.0000, 'critic_loss':    80.3815, 'actor_loss':    -1.0201, 'alpha_loss':    -0.0562, 'eps_e':     1.0000})
Step:  388000, Reward:   252.973 [  65.402], Avg:   153.707 (1.000) <0-02:13:02> ({'r_t':  1574.1820, 'eps':     1.0000, 'critic_loss':    79.8040, 'actor_loss':    -0.9947, 'alpha_loss':    -0.0651, 'eps_e':     1.0000})
Step:  389000, Reward:   262.097 [  67.103], Avg:   153.985 (1.000) <0-02:13:13> ({'r_t':  1576.2228, 'eps':     1.0000, 'critic_loss':    81.8304, 'actor_loss':    -1.0139, 'alpha_loss':    -0.0510, 'eps_e':     1.0000})
Step:  390000, Reward:   249.302 [  88.318], Avg:   154.228 (1.000) <0-02:13:25> ({'r_t':  1490.3951, 'eps':     1.0000, 'critic_loss':    86.8641, 'actor_loss':    -0.9372, 'alpha_loss':    -0.0644, 'eps_e':     1.0000})
Step:  391000, Reward:   264.765 [  62.239], Avg:   154.510 (1.000) <0-02:13:37> ({'r_t':  1508.5254, 'eps':     1.0000, 'critic_loss':    83.5440, 'actor_loss':    -0.9821, 'alpha_loss':     0.0197, 'eps_e':     1.0000})
Step:  392000, Reward:   245.493 [  87.237], Avg:   154.742 (1.000) <0-02:13:49> ({'r_t':  1553.7213, 'eps':     1.0000, 'critic_loss':    76.1350, 'actor_loss':    -0.9737, 'alpha_loss':    -0.0110, 'eps_e':     1.0000})
Step:  393000, Reward:   276.741 [  74.044], Avg:   155.052 (1.000) <0-02:14:01> ({'r_t':  1585.3660, 'eps':     1.0000, 'critic_loss':    84.5137, 'actor_loss':    -1.0444, 'alpha_loss':     0.0538, 'eps_e':     1.0000})
Step:  394000, Reward:   274.842 [  19.469], Avg:   155.355 (1.000) <0-02:14:12> ({'r_t':  1628.3195, 'eps':     1.0000, 'critic_loss':    81.6785, 'actor_loss':    -0.9918, 'alpha_loss':     0.0308, 'eps_e':     1.0000})
Step:  395000, Reward:   260.857 [  62.675], Avg:   155.621 (1.000) <0-02:14:24> ({'r_t':  1568.4480, 'eps':     1.0000, 'critic_loss':    75.5416, 'actor_loss':    -0.9147, 'alpha_loss':    -0.0409, 'eps_e':     1.0000})
Step:  396000, Reward:   244.826 [  79.311], Avg:   155.846 (1.000) <0-02:14:36> ({'r_t':  1528.5883, 'eps':     1.0000, 'critic_loss':    96.0539, 'actor_loss':    -0.8837, 'alpha_loss':    -0.0388, 'eps_e':     1.0000})
Step:  397000, Reward:   281.457 [  26.414], Avg:   156.162 (1.000) <0-02:14:47> ({'r_t':  1572.6007, 'eps':     1.0000, 'critic_loss':    82.7292, 'actor_loss':    -0.8536, 'alpha_loss':    -0.0746, 'eps_e':     1.0000})
Step:  398000, Reward:   284.237 [  15.118], Avg:   156.483 (1.000) <0-02:14:59> ({'r_t':  1592.0378, 'eps':     1.0000, 'critic_loss':    79.3308, 'actor_loss':    -0.9437, 'alpha_loss':    -0.0270, 'eps_e':     1.0000})
Step:  399000, Reward:   257.362 [  60.595], Avg:   156.735 (1.000) <0-02:15:12> ({'r_t':  1644.1935, 'eps':     1.0000, 'critic_loss':    84.3611, 'actor_loss':    -0.9505, 'alpha_loss':    -0.0317, 'eps_e':     1.0000})
Step:  400000, Reward:   263.118 [  61.988], Avg:   157.000 (1.000) <0-02:15:23> ({'r_t':  1629.5024, 'eps':     1.0000, 'critic_loss':    78.0168, 'actor_loss':    -0.8697, 'alpha_loss':     0.0539, 'eps_e':     1.0000})
Step:  401000, Reward:   283.233 [  16.367], Avg:   157.314 (1.000) <0-02:15:35> ({'r_t':  1646.4172, 'eps':     1.0000, 'critic_loss':    86.7375, 'actor_loss':    -0.8856, 'alpha_loss':    -0.0725, 'eps_e':     1.0000})
Step:  402000, Reward:   274.697 [  18.315], Avg:   157.605 (1.000) <0-02:15:46> ({'r_t':  1733.0156, 'eps':     1.0000, 'critic_loss':    86.8811, 'actor_loss':    -0.9386, 'alpha_loss':    -0.1134, 'eps_e':     1.0000})
Step:  403000, Reward:   264.049 [  63.992], Avg:   157.869 (1.000) <0-02:15:58> ({'r_t':  1613.3836, 'eps':     1.0000, 'critic_loss':    89.8973, 'actor_loss':    -0.8761, 'alpha_loss':    -0.0661, 'eps_e':     1.0000})
Step:  404000, Reward:   275.970 [  24.633], Avg:   158.160 (1.000) <0-02:16:10> ({'r_t':  1541.9615, 'eps':     1.0000, 'critic_loss':    85.8329, 'actor_loss':    -0.8830, 'alpha_loss':    -0.0962, 'eps_e':     1.0000})
Step:  405000, Reward:   279.979 [  17.621], Avg:   158.460 (1.000) <0-02:16:22> ({'r_t':  1533.9148, 'eps':     1.0000, 'critic_loss':    96.3624, 'actor_loss':    -0.8275, 'alpha_loss':    -0.0772, 'eps_e':     1.0000})
Step:  406000, Reward:   282.490 [  21.622], Avg:   158.765 (1.000) <0-02:16:34> ({'r_t':  1592.6108, 'eps':     1.0000, 'critic_loss':    85.8191, 'actor_loss':    -0.8747, 'alpha_loss':    -0.1167, 'eps_e':     1.0000})
Step:  407000, Reward:   270.135 [  20.541], Avg:   159.038 (1.000) <0-02:16:45> ({'r_t':  1686.3283, 'eps':     1.0000, 'critic_loss':    84.8204, 'actor_loss':    -0.8433, 'alpha_loss':    -0.0624, 'eps_e':     1.0000})
Step:  408000, Reward:   264.257 [  61.650], Avg:   159.295 (1.000) <0-02:16:56> ({'r_t':  1694.9404, 'eps':     1.0000, 'critic_loss':    89.0987, 'actor_loss':    -0.9432, 'alpha_loss':     0.0602, 'eps_e':     1.0000})
Step:  409000, Reward:   245.826 [  82.744], Avg:   159.506 (1.000) <0-02:17:07> ({'r_t':  1619.7288, 'eps':     1.0000, 'critic_loss':    85.8541, 'actor_loss':    -0.8767, 'alpha_loss':    -0.0451, 'eps_e':     1.0000})
Step:  410000, Reward:   277.850 [  20.890], Avg:   159.794 (1.000) <0-02:17:19> ({'r_t':  1673.3880, 'eps':     1.0000, 'critic_loss':    89.8938, 'actor_loss':    -0.8578, 'alpha_loss':     0.0369, 'eps_e':     1.0000})
Step:  411000, Reward:   271.929 [  20.668], Avg:   160.067 (1.000) <0-02:17:30> ({'r_t':  1698.5124, 'eps':     1.0000, 'critic_loss':    84.2989, 'actor_loss':    -0.8153, 'alpha_loss':    -0.0067, 'eps_e':     1.0000})
Step:  412000, Reward:   279.027 [  17.929], Avg:   160.355 (1.000) <0-02:17:41> ({'r_t':  1541.6680, 'eps':     1.0000, 'critic_loss':    79.1833, 'actor_loss':    -0.7283, 'alpha_loss':    -0.0783, 'eps_e':     1.0000})
Step:  413000, Reward:   262.648 [  55.703], Avg:   160.602 (1.000) <0-02:17:53> ({'r_t':  1538.4411, 'eps':     1.0000, 'critic_loss':    79.3944, 'actor_loss':    -0.7514, 'alpha_loss':    -0.0734, 'eps_e':     1.0000})
Step:  414000, Reward:   248.541 [  55.574], Avg:   160.814 (1.000) <0-02:18:05> ({'r_t':  1687.1750, 'eps':     1.0000, 'critic_loss':    84.6549, 'actor_loss':    -0.8046, 'alpha_loss':    -0.0194, 'eps_e':     1.0000})
Step:  415000, Reward:   274.313 [  14.560], Avg:   161.086 (1.000) <0-02:18:16> ({'r_t':  1567.9617, 'eps':     1.0000, 'critic_loss':    87.6343, 'actor_loss':    -0.7131, 'alpha_loss':    -0.0589, 'eps_e':     1.0000})
Step:  416000, Reward:   252.129 [  56.779], Avg:   161.305 (1.000) <0-02:18:27> ({'r_t':  1636.1853, 'eps':     1.0000, 'critic_loss':    87.3276, 'actor_loss':    -0.7404, 'alpha_loss':    -0.0419, 'eps_e':     1.0000})
Step:  417000, Reward:   262.994 [  55.970], Avg:   161.548 (1.000) <0-02:18:39> ({'r_t':  1611.5513, 'eps':     1.0000, 'critic_loss':    91.2190, 'actor_loss':    -0.7986, 'alpha_loss':    -0.0181, 'eps_e':     1.0000})
Step:  418000, Reward:   279.845 [  24.137], Avg:   161.830 (1.000) <0-02:18:50> ({'r_t':  1686.0307, 'eps':     1.0000, 'critic_loss':    88.0378, 'actor_loss':    -0.7046, 'alpha_loss':    -0.1057, 'eps_e':     1.0000})
Step:  419000, Reward:   257.645 [  65.646], Avg:   162.058 (1.000) <0-02:19:02> ({'r_t':  1657.9706, 'eps':     1.0000, 'critic_loss':    83.3639, 'actor_loss':    -0.7692, 'alpha_loss':    -0.0665, 'eps_e':     1.0000})
Step:  420000, Reward:   272.239 [  23.162], Avg:   162.320 (1.000) <0-02:19:13> ({'r_t':  1724.1926, 'eps':     1.0000, 'critic_loss':    81.4115, 'actor_loss':    -0.6597, 'alpha_loss':    -0.1221, 'eps_e':     1.0000})
Step:  421000, Reward:   282.803 [  15.478], Avg:   162.606 (1.000) <0-02:19:24> ({'r_t':  1646.9786, 'eps':     1.0000, 'critic_loss':    80.3967, 'actor_loss':    -0.6574, 'alpha_loss':    -0.0445, 'eps_e':     1.0000})
Step:  422000, Reward:   269.339 [  25.830], Avg:   162.858 (1.000) <0-02:19:38> ({'r_t':  1595.0851, 'eps':     1.0000, 'critic_loss':    84.3267, 'actor_loss':    -0.6690, 'alpha_loss':     0.0130, 'eps_e':     1.0000})
Step:  423000, Reward:   270.020 [  30.566], Avg:   163.111 (1.000) <0-02:19:52> ({'r_t':  1531.9406, 'eps':     1.0000, 'critic_loss':    78.8906, 'actor_loss':    -0.6333, 'alpha_loss':    -0.0679, 'eps_e':     1.0000})
Step:  424000, Reward:   281.553 [  16.418], Avg:   163.389 (1.000) <0-02:20:03> ({'r_t':  1646.8477, 'eps':     1.0000, 'critic_loss':    91.9052, 'actor_loss':    -0.6299, 'alpha_loss':    -0.0468, 'eps_e':     1.0000})
Step:  425000, Reward:   281.251 [  35.642], Avg:   163.666 (1.000) <0-02:20:15> ({'r_t':  1711.9885, 'eps':     1.0000, 'critic_loss':    80.3563, 'actor_loss':    -0.5892, 'alpha_loss':    -0.0454, 'eps_e':     1.0000})
Step:  426000, Reward:   273.618 [  17.950], Avg:   163.924 (1.000) <0-02:20:27> ({'r_t':  1684.3791, 'eps':     1.0000, 'critic_loss':    88.2203, 'actor_loss':    -0.6155, 'alpha_loss':     0.0218, 'eps_e':     1.0000})
Step:  427000, Reward:   276.976 [  31.369], Avg:   164.188 (1.000) <0-02:20:40> ({'r_t':  1703.7502, 'eps':     1.0000, 'critic_loss':    88.9852, 'actor_loss':    -0.5753, 'alpha_loss':    -0.0106, 'eps_e':     1.0000})
Step:  428000, Reward:   263.965 [  57.245], Avg:   164.420 (1.000) <0-02:20:51> ({'r_t':  1654.1457, 'eps':     1.0000, 'critic_loss':    75.4389, 'actor_loss':    -0.5489, 'alpha_loss':    -0.0637, 'eps_e':     1.0000})
Step:  429000, Reward:   261.273 [  53.856], Avg:   164.646 (1.000) <0-02:21:02> ({'r_t':  1671.6944, 'eps':     1.0000, 'critic_loss':    79.7735, 'actor_loss':    -0.5225, 'alpha_loss':    -0.1245, 'eps_e':     1.0000})
Step:  430000, Reward:   281.632 [  16.583], Avg:   164.917 (1.000) <0-02:21:14> ({'r_t':  1695.5130, 'eps':     1.0000, 'critic_loss':    80.4861, 'actor_loss':    -0.5170, 'alpha_loss':    -0.0908, 'eps_e':     1.0000})
Step:  431000, Reward:   260.546 [  66.398], Avg:   165.138 (1.000) <0-02:21:26> ({'r_t':  1668.1037, 'eps':     1.0000, 'critic_loss':    78.1224, 'actor_loss':    -0.5209, 'alpha_loss':    -0.1346, 'eps_e':     1.0000})
Step:  432000, Reward:   275.858 [  54.465], Avg:   165.394 (1.000) <0-02:21:38> ({'r_t':  1599.1539, 'eps':     1.0000, 'critic_loss':    77.9170, 'actor_loss':    -0.5058, 'alpha_loss':    -0.0937, 'eps_e':     1.0000})
Step:  433000, Reward:   262.529 [  65.500], Avg:   165.618 (1.000) <0-02:21:49> ({'r_t':  1702.1714, 'eps':     1.0000, 'critic_loss':    78.6368, 'actor_loss':    -0.4877, 'alpha_loss':    -0.0664, 'eps_e':     1.0000})
Step:  434000, Reward:   272.215 [  15.974], Avg:   165.863 (1.000) <0-02:22:00> ({'r_t':  1570.3835, 'eps':     1.0000, 'critic_loss':    81.5216, 'actor_loss':    -0.4904, 'alpha_loss':    -0.0450, 'eps_e':     1.0000})
Step:  435000, Reward:   274.276 [  19.075], Avg:   166.112 (1.000) <0-02:22:12> ({'r_t':  1617.3285, 'eps':     1.0000, 'critic_loss':    71.6705, 'actor_loss':    -0.4556, 'alpha_loss':    -0.0091, 'eps_e':     1.0000})
Step:  436000, Reward:   271.151 [  46.670], Avg:   166.352 (1.000) <0-02:22:23> ({'r_t':  1615.8478, 'eps':     1.0000, 'critic_loss':    76.3841, 'actor_loss':    -0.4901, 'alpha_loss':    -0.0201, 'eps_e':     1.0000})
Step:  437000, Reward:   275.287 [  16.632], Avg:   166.601 (1.000) <0-02:22:34> ({'r_t':  1639.6457, 'eps':     1.0000, 'critic_loss':    78.5727, 'actor_loss':    -0.4583, 'alpha_loss':    -0.1086, 'eps_e':     1.0000})
Step:  438000, Reward:   273.029 [  24.215], Avg:   166.843 (1.000) <0-02:22:48> ({'r_t':  1363.5421, 'eps':     1.0000, 'critic_loss':    75.5827, 'actor_loss':    -0.4519, 'alpha_loss':    -0.0740, 'eps_e':     1.0000})
Step:  439000, Reward:   276.938 [  16.909], Avg:   167.093 (1.000) <0-02:22:59> ({'r_t':  1620.2002, 'eps':     1.0000, 'critic_loss':    79.4395, 'actor_loss':    -0.5028, 'alpha_loss':    -0.0124, 'eps_e':     1.0000})
Step:  440000, Reward:   283.680 [  14.657], Avg:   167.358 (1.000) <0-02:23:10> ({'r_t':  1698.3086, 'eps':     1.0000, 'critic_loss':    76.2920, 'actor_loss':    -0.4724, 'alpha_loss':    -0.0931, 'eps_e':     1.0000})
Step:  441000, Reward:   279.886 [  15.625], Avg:   167.612 (1.000) <0-02:23:22> ({'r_t':  1742.1264, 'eps':     1.0000, 'critic_loss':    77.7208, 'actor_loss':    -0.4879, 'alpha_loss':    -0.0100, 'eps_e':     1.0000})
Step:  442000, Reward:   282.211 [  18.373], Avg:   167.871 (1.000) <0-02:23:33> ({'r_t':  1614.2405, 'eps':     1.0000, 'critic_loss':    74.4303, 'actor_loss':    -0.4710, 'alpha_loss':    -0.0594, 'eps_e':     1.0000})
Step:  443000, Reward:   278.061 [  19.403], Avg:   168.119 (1.000) <0-02:23:44> ({'r_t':  1672.2843, 'eps':     1.0000, 'critic_loss':    70.2813, 'actor_loss':    -0.4698, 'alpha_loss':    -0.1262, 'eps_e':     1.0000})
Step:  444000, Reward:   280.469 [  19.245], Avg:   168.372 (1.000) <0-02:23:56> ({'r_t':  1624.1564, 'eps':     1.0000, 'critic_loss':    71.0751, 'actor_loss':    -0.4966, 'alpha_loss':    -0.1069, 'eps_e':     1.0000})
Step:  445000, Reward:   276.413 [  12.992], Avg:   168.614 (1.000) <0-02:24:07> ({'r_t':  1556.7516, 'eps':     1.0000, 'critic_loss':    69.9275, 'actor_loss':    -0.5024, 'alpha_loss':    -0.0555, 'eps_e':     1.0000})
Step:  446000, Reward:   261.550 [  53.040], Avg:   168.822 (1.000) <0-02:24:19> ({'r_t':  1693.4516, 'eps':     1.0000, 'critic_loss':    73.7517, 'actor_loss':    -0.4672, 'alpha_loss':     0.0362, 'eps_e':     1.0000})
Step:  447000, Reward:   253.717 [  53.480], Avg:   169.011 (1.000) <0-02:24:31> ({'r_t':  1649.9656, 'eps':     1.0000, 'critic_loss':    68.6152, 'actor_loss':    -0.4593, 'alpha_loss':     0.0691, 'eps_e':     1.0000})
Step:  448000, Reward:   276.631 [  18.936], Avg:   169.251 (1.000) <0-02:24:42> ({'r_t':  1533.8964, 'eps':     1.0000, 'critic_loss':    68.7318, 'actor_loss':    -0.4459, 'alpha_loss':    -0.0121, 'eps_e':     1.0000})
Step:  449000, Reward:   268.327 [  56.251], Avg:   169.471 (1.000) <0-02:24:54> ({'r_t':  1722.9132, 'eps':     1.0000, 'critic_loss':    71.6218, 'actor_loss':    -0.4648, 'alpha_loss':    -0.0262, 'eps_e':     1.0000})
Step:  450000, Reward:   283.095 [  13.170], Avg:   169.723 (1.000) <0-02:25:05> ({'r_t':  1704.5990, 'eps':     1.0000, 'critic_loss':    69.1696, 'actor_loss':    -0.4018, 'alpha_loss':    -0.0718, 'eps_e':     1.0000})
Step:  451000, Reward:   263.804 [  59.225], Avg:   169.931 (1.000) <0-02:25:16> ({'r_t':  1590.7832, 'eps':     1.0000, 'critic_loss':    65.7962, 'actor_loss':    -0.4809, 'alpha_loss':    -0.0214, 'eps_e':     1.0000})
Step:  452000, Reward:   281.266 [  17.014], Avg:   170.177 (1.000) <0-02:25:28> ({'r_t':  1645.5695, 'eps':     1.0000, 'critic_loss':    69.5148, 'actor_loss':    -0.4420, 'alpha_loss':     0.0242, 'eps_e':     1.0000})
Step:  453000, Reward:   289.171 [  16.298], Avg:   170.439 (1.000) <0-02:25:39> ({'r_t':  1454.9837, 'eps':     1.0000, 'critic_loss':    67.9421, 'actor_loss':    -0.4438, 'alpha_loss':    -0.0519, 'eps_e':     1.0000})
Step:  454000, Reward:   273.881 [  13.155], Avg:   170.666 (1.000) <0-02:25:51> ({'r_t':  1639.3218, 'eps':     1.0000, 'critic_loss':    66.5583, 'actor_loss':    -0.4675, 'alpha_loss':     0.0523, 'eps_e':     1.0000})
Step:  455000, Reward:   280.931 [  15.650], Avg:   170.908 (1.000) <0-02:26:02> ({'r_t':  1615.5736, 'eps':     1.0000, 'critic_loss':    59.0440, 'actor_loss':    -0.4300, 'alpha_loss':     0.0410, 'eps_e':     1.0000})
Step:  456000, Reward:   278.558 [  19.851], Avg:   171.144 (1.000) <0-02:26:14> ({'r_t':  1605.5510, 'eps':     1.0000, 'critic_loss':    64.6519, 'actor_loss':    -0.4302, 'alpha_loss':     0.0257, 'eps_e':     1.0000})
Step:  457000, Reward:   283.960 [  17.293], Avg:   171.390 (1.000) <0-02:26:26> ({'r_t':  1665.0900, 'eps':     1.0000, 'critic_loss':    62.1980, 'actor_loss':    -0.4510, 'alpha_loss':    -0.0653, 'eps_e':     1.0000})
Step:  458000, Reward:   287.555 [  15.008], Avg:   171.643 (1.000) <0-02:26:39> ({'r_t':  1714.6540, 'eps':     1.0000, 'critic_loss':    60.4725, 'actor_loss':    -0.4204, 'alpha_loss':    -0.0008, 'eps_e':     1.0000})
Step:  459000, Reward:   284.463 [  17.311], Avg:   171.888 (1.000) <0-02:26:50> ({'r_t':  1695.8363, 'eps':     1.0000, 'critic_loss':    61.6217, 'actor_loss':    -0.3975, 'alpha_loss':    -0.0302, 'eps_e':     1.0000})
Step:  460000, Reward:   259.895 [  65.635], Avg:   172.079 (1.000) <0-02:27:04> ({'r_t':  1701.3173, 'eps':     1.0000, 'critic_loss':    58.7520, 'actor_loss':    -0.4163, 'alpha_loss':    -0.0432, 'eps_e':     1.0000})
Step:  461000, Reward:   254.508 [  74.844], Avg:   172.258 (1.000) <0-02:27:15> ({'r_t':  1660.7958, 'eps':     1.0000, 'critic_loss':    58.0371, 'actor_loss':    -0.4625, 'alpha_loss':    -0.0417, 'eps_e':     1.0000})
Step:  462000, Reward:   282.993 [  15.525], Avg:   172.497 (1.000) <0-02:27:26> ({'r_t':  1723.0893, 'eps':     1.0000, 'critic_loss':    59.4669, 'actor_loss':    -0.4672, 'alpha_loss':    -0.0237, 'eps_e':     1.0000})
Step:  463000, Reward:   281.482 [  14.811], Avg:   172.732 (1.000) <0-02:27:38> ({'r_t':  1702.7766, 'eps':     1.0000, 'critic_loss':    61.7526, 'actor_loss':    -0.4361, 'alpha_loss':    -0.0046, 'eps_e':     1.0000})
Step:  464000, Reward:   283.471 [  14.180], Avg:   172.970 (1.000) <0-02:27:49> ({'r_t':  1752.9665, 'eps':     1.0000, 'critic_loss':    55.2390, 'actor_loss':    -0.4678, 'alpha_loss':    -0.0403, 'eps_e':     1.0000})
Step:  465000, Reward:   282.957 [  15.304], Avg:   173.206 (1.000) <0-02:28:01> ({'r_t':  1784.1245, 'eps':     1.0000, 'critic_loss':    58.7378, 'actor_loss':    -0.4402, 'alpha_loss':    -0.0174, 'eps_e':     1.0000})
Step:  466000, Reward:   281.760 [  20.727], Avg:   173.438 (1.000) <0-02:28:12> ({'r_t':  1693.3926, 'eps':     1.0000, 'critic_loss':    53.2222, 'actor_loss':    -0.5065, 'alpha_loss':     0.0022, 'eps_e':     1.0000})
Step:  467000, Reward:   271.540 [  35.791], Avg:   173.648 (1.000) <0-02:28:26> ({'r_t':  1727.5366, 'eps':     1.0000, 'critic_loss':    53.7811, 'actor_loss':    -0.5047, 'alpha_loss':    -0.0522, 'eps_e':     1.0000})
Step:  468000, Reward:   288.158 [  19.652], Avg:   173.892 (1.000) <0-02:28:37> ({'r_t':  1682.8958, 'eps':     1.0000, 'critic_loss':    58.5521, 'actor_loss':    -0.5553, 'alpha_loss':     0.0782, 'eps_e':     1.0000})
Step:  469000, Reward:   283.854 [  30.386], Avg:   174.126 (1.000) <0-02:28:50> ({'r_t':  1755.1053, 'eps':     1.0000, 'critic_loss':    55.1645, 'actor_loss':    -0.5034, 'alpha_loss':    -0.0262, 'eps_e':     1.0000})
Step:  470000, Reward:   286.501 [  19.198], Avg:   174.365 (1.000) <0-02:29:02> ({'r_t':  1659.9160, 'eps':     1.0000, 'critic_loss':    51.9274, 'actor_loss':    -0.5879, 'alpha_loss':     0.0810, 'eps_e':     1.0000})
Step:  471000, Reward:   283.305 [  14.564], Avg:   174.596 (1.000) <0-02:29:13> ({'r_t':  1602.1530, 'eps':     1.0000, 'critic_loss':    48.9327, 'actor_loss':    -0.6287, 'alpha_loss':     0.1108, 'eps_e':     1.0000})
Step:  472000, Reward:   283.999 [  17.642], Avg:   174.827 (1.000) <0-02:29:25> ({'r_t':  1634.6761, 'eps':     1.0000, 'critic_loss':    50.7503, 'actor_loss':    -0.5749, 'alpha_loss':     0.0490, 'eps_e':     1.0000})
Step:  473000, Reward:   286.085 [  14.268], Avg:   175.062 (1.000) <0-02:29:36> ({'r_t':  1676.8911, 'eps':     1.0000, 'critic_loss':    45.9430, 'actor_loss':    -0.5964, 'alpha_loss':     0.0376, 'eps_e':     1.0000})
Step:  474000, Reward:   284.835 [  19.344], Avg:   175.293 (1.000) <0-02:29:47> ({'r_t':  1725.1959, 'eps':     1.0000, 'critic_loss':    49.2567, 'actor_loss':    -0.6037, 'alpha_loss':    -0.0274, 'eps_e':     1.0000})
Step:  475000, Reward:   281.507 [  12.573], Avg:   175.516 (1.000) <0-02:29:59> ({'r_t':  1647.4897, 'eps':     1.0000, 'critic_loss':    51.6543, 'actor_loss':    -0.6522, 'alpha_loss':     0.0994, 'eps_e':     1.0000})
Step:  476000, Reward:   291.829 [  19.401], Avg:   175.760 (1.000) <0-02:30:10> ({'r_t':  1706.6160, 'eps':     1.0000, 'critic_loss':    43.6800, 'actor_loss':    -0.7271, 'alpha_loss':     0.0569, 'eps_e':     1.0000})
Step:  477000, Reward:   280.644 [  20.225], Avg:   175.979 (1.000) <0-02:30:22> ({'r_t':  1725.6223, 'eps':     1.0000, 'critic_loss':    44.9683, 'actor_loss':    -0.7380, 'alpha_loss':     0.0018, 'eps_e':     1.0000})
Step:  478000, Reward:   295.310 [  19.803], Avg:   176.228 (1.000) <0-02:30:33> ({'r_t':  1753.8490, 'eps':     1.0000, 'critic_loss':    42.7917, 'actor_loss':    -0.6986, 'alpha_loss':    -0.0520, 'eps_e':     1.0000})
Step:  479000, Reward:   290.506 [  11.003], Avg:   176.466 (1.000) <0-02:30:44> ({'r_t':  1778.9565, 'eps':     1.0000, 'critic_loss':    34.7844, 'actor_loss':    -0.7402, 'alpha_loss':    -0.0766, 'eps_e':     1.0000})
Step:  480000, Reward:   294.823 [  15.287], Avg:   176.712 (1.000) <0-02:30:56> ({'r_t':  1686.2975, 'eps':     1.0000, 'critic_loss':    42.5987, 'actor_loss':    -0.7405, 'alpha_loss':     0.0038, 'eps_e':     1.0000})
Step:  481000, Reward:   286.092 [  15.745], Avg:   176.939 (1.000) <0-02:31:08> ({'r_t':  1764.3020, 'eps':     1.0000, 'critic_loss':    41.0395, 'actor_loss':    -0.8162, 'alpha_loss':     0.0189, 'eps_e':     1.0000})
Step:  482000, Reward:   285.813 [  17.137], Avg:   177.165 (1.000) <0-02:31:20> ({'r_t':  1761.0757, 'eps':     1.0000, 'critic_loss':    32.9182, 'actor_loss':    -0.8064, 'alpha_loss':     0.0364, 'eps_e':     1.0000})
Step:  483000, Reward:   283.057 [  18.348], Avg:   177.384 (1.000) <0-02:31:31> ({'r_t':  1796.2152, 'eps':     1.0000, 'critic_loss':    34.0231, 'actor_loss':    -0.7867, 'alpha_loss':     0.0423, 'eps_e':     1.0000})
Step:  484000, Reward:   289.125 [  15.966], Avg:   177.614 (1.000) <0-02:31:42> ({'r_t':  1754.1725, 'eps':     1.0000, 'critic_loss':    34.8308, 'actor_loss':    -0.9032, 'alpha_loss':     0.2009, 'eps_e':     1.0000})
Step:  485000, Reward:   274.592 [  30.787], Avg:   177.813 (1.000) <0-02:31:55> ({'r_t':  1789.1742, 'eps':     1.0000, 'critic_loss':    35.2047, 'actor_loss':    -0.8709, 'alpha_loss':     0.0939, 'eps_e':     1.0000})
Step:  486000, Reward:   293.213 [  14.285], Avg:   178.050 (1.000) <0-02:32:06> ({'r_t':  1802.5355, 'eps':     1.0000, 'critic_loss':    30.8296, 'actor_loss':    -0.8770, 'alpha_loss':     0.0872, 'eps_e':     1.0000})
Step:  487000, Reward:   277.318 [  19.871], Avg:   178.254 (1.000) <0-02:32:19> ({'r_t':  1767.8020, 'eps':     1.0000, 'critic_loss':    32.2062, 'actor_loss':    -0.9194, 'alpha_loss':     0.2519, 'eps_e':     1.0000})
Step:  488000, Reward:   288.959 [  16.847], Avg:   178.480 (1.000) <0-02:32:30> ({'r_t':  1836.0449, 'eps':     1.0000, 'critic_loss':    29.1779, 'actor_loss':    -0.9188, 'alpha_loss':     0.2235, 'eps_e':     1.0000})
Step:  489000, Reward:   288.437 [  15.660], Avg:   178.705 (1.000) <0-02:32:42> ({'r_t':  1826.8615, 'eps':     1.0000, 'critic_loss':    32.8594, 'actor_loss':    -0.8893, 'alpha_loss':     0.0817, 'eps_e':     1.0000})
Step:  490000, Reward:   286.824 [  19.813], Avg:   178.925 (1.000) <0-02:32:53> ({'r_t':  1800.6416, 'eps':     1.0000, 'critic_loss':    31.5117, 'actor_loss':    -0.9202, 'alpha_loss':     0.1592, 'eps_e':     1.0000})
Step:  491000, Reward:   294.540 [  19.083], Avg:   179.160 (1.000) <0-02:33:04> ({'r_t':  1807.4956, 'eps':     1.0000, 'critic_loss':    28.6618, 'actor_loss':    -0.8607, 'alpha_loss':    -0.0626, 'eps_e':     1.0000})
Step:  492000, Reward:   283.592 [  20.582], Avg:   179.372 (1.000) <0-02:33:15> ({'r_t':  1857.4012, 'eps':     1.0000, 'critic_loss':    29.9472, 'actor_loss':    -0.9403, 'alpha_loss':     0.0521, 'eps_e':     1.0000})
Step:  493000, Reward:   288.499 [  14.600], Avg:   179.593 (1.000) <0-02:33:27> ({'r_t':  1785.8577, 'eps':     1.0000, 'critic_loss':    32.9019, 'actor_loss':    -0.9205, 'alpha_loss':     0.0649, 'eps_e':     1.0000})
Step:  494000, Reward:   288.588 [  14.260], Avg:   179.813 (1.000) <0-02:33:38> ({'r_t':  1722.4819, 'eps':     1.0000, 'critic_loss':    33.7752, 'actor_loss':    -0.9512, 'alpha_loss':     0.1875, 'eps_e':     1.0000})
Step:  495000, Reward:   291.836 [  21.988], Avg:   180.039 (1.000) <0-02:33:49> ({'r_t':  1841.8951, 'eps':     1.0000, 'critic_loss':    31.0809, 'actor_loss':    -0.9607, 'alpha_loss':     0.0366, 'eps_e':     1.0000})
Step:  496000, Reward:   287.180 [  20.328], Avg:   180.254 (1.000) <0-02:34:00> ({'r_t':  1815.6677, 'eps':     1.0000, 'critic_loss':    29.4913, 'actor_loss':    -0.9404, 'alpha_loss':     0.0013, 'eps_e':     1.0000})
Step:  497000, Reward:   273.867 [  79.116], Avg:   180.442 (1.000) <0-02:34:12> ({'r_t':  1836.8241, 'eps':     1.0000, 'critic_loss':    31.3352, 'actor_loss':    -0.9273, 'alpha_loss':    -0.0463, 'eps_e':     1.0000})
Step:  498000, Reward:   288.175 [  22.569], Avg:   180.658 (1.000) <0-02:34:23> ({'r_t':  1832.1131, 'eps':     1.0000, 'critic_loss':    26.4498, 'actor_loss':    -0.9075, 'alpha_loss':    -0.0176, 'eps_e':     1.0000})
Step:  499000, Reward:   296.348 [  14.682], Avg:   180.889 (1.000) <0-02:34:34> ({'r_t':  1875.9772, 'eps':     1.0000, 'critic_loss':    34.1397, 'actor_loss':    -0.8529, 'alpha_loss':    -0.0654, 'eps_e':     1.0000})
Step:  500000, Reward:   288.287 [  16.726], Avg:   181.104 (1.000) <0-02:34:45> ({'r_t':  1888.2627, 'eps':     1.0000, 'critic_loss':    31.5570, 'actor_loss':    -0.8837, 'alpha_loss':    -0.0403, 'eps_e':     1.0000})
