Model: <class 'src.models.pytorch.mpc.envmodel.differential.DifferentialEnv'>, Env: Pendulum-v0, Date: 29/05/2020 14:39:41
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: 6aa19f9a388401cd4695cd69c1022fc87755770a
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   dynamics_size = 3
   state_size = (3,)
   action_size = (1,)
   env_name = Pendulum-v0
   envmodel = dfrntl
   model = None
   nworkers = 0
   epochs = 50
   seq_len = 20
   batch_size = 128
   train_prop = 0.9
   DYN = 
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.001
      TRANSITION_HIDDEN = 256
      BETA_DYN = 1
      BETA_DOT = 1
      BETA_REW = 1,
num_envs: 0,
envs: <__main__.Trainer object at 0x7f59acc11090>,
agent: DifferentialEnv(
	  (reward): RewardModel(
	    (linear): Linear(in_features=6, out_features=1, bias=True)
	  )
	  (dynamics): TransitionModel(
	    (gru): GRUCell(7, 256)
	    (linear1): Linear(in_features=256, out_features=256, bias=True)
	    (linear2): Linear(in_features=256, out_features=256, bias=True)
	    (linear3): Linear(in_features=256, out_features=3, bias=True)
	  )
	) 
	training = True
	tau = 0.0004
	name = dfrntl
	stats = <src.utils.logger.Stats object at 0x7f59a7c7ed90> 
		mean_dict = {}
		sum_dict = {}
	config = <src.utils.config.Config object at 0x7f59afd7d190> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		dynamics_size = 3
		state_size = (3,)
		action_size = (1,)
		env_name = Pendulum-v0
		envmodel = dfrntl
		model = None
		nworkers = 0
		epochs = 50
		seq_len = 20
		batch_size = 128
		train_prop = 0.9
		DYN = <src.utils.config.Config object at 0x7f59afd70890> 
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.001
			TRANSITION_HIDDEN = 256
			BETA_DYN = 1
			BETA_DOT = 1
			BETA_REW = 1
	device = cuda
	state_size = (3,)
	action_size = (1,)
	discrete = False
	dyn_index = 3
	optimizer = Adam (
	Parameter Group 0
	    amsgrad: False
	    betas: (0.9, 0.999)
	    eps: 1e-08
	    lr: 0.001
	    weight_decay: 0
	)
	scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f590f2f3a90>,

import os
import torch
import numpy as np
from src.utils.misc import load_module
from ...agents.base import PTNetwork, one_hot

class TransitionModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.gru = torch.nn.GRUCell(action_size[-1] + 2*state_size[-1], config.DYN.TRANSITION_HIDDEN)
		self.linear1 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear2 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear3 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, state_size[-1])
		self.config = config

	def forward(self, action, state, state_dot):
		inputs = torch.cat([action, state, state_dot],-1)
		self.hidden = self.gru(inputs, self.hidden)
		linear1 = self.linear1(self.hidden).tanh() + self.hidden
		linear2 = self.linear2(linear1).tanh() + linear1
		state_dot = self.linear3(linear2)
		next_state = state + state_dot
		return next_state, state_dot

	def reset(self, device, batch_size=None):
		if batch_size is None: batch_size = self.hidden[0].shape[1] if hasattr(self, "hidden") else 1
		self.hidden = torch.zeros(batch_size, self.config.DYN.TRANSITION_HIDDEN, device=device)

class RewardModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.linear = torch.nn.Linear(2*state_size[-1], 1)
		self.cost = load_module(config.REWARD_MODEL)() if config.get("REWARD_MODEL") else None
		self.dyn_spec = load_module(config.DYNAMICS_SPEC) if config.get("DYNAMICS_SPEC") else None

	def forward(self, next_state, state_dot):
		if self.cost and self.dyn_spec:
			next_state, state_dot = [x.cpu().numpy() for x in [next_state, state_dot]]
			ns_spec, s_spec = map(self.dyn_spec.observation_spec, [next_state, next_state-state_dot])
			reward = -torch.FloatTensor(self.cost.get_cost(ns_spec, s_spec, mpc=True)).unsqueeze(-1)
		else:
			inputs = torch.cat([next_state, state_dot],-1)
			reward = self.linear(inputs)
		return reward

class DifferentialEnv(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="dfrntl"):
		super().__init__(config, gpu, name)
		self.state_size = state_size
		self.action_size = action_size
		self.discrete = type(self.action_size) != tuple
		self.dyn_index = config.get("dynamics_size", state_size[-1])
		self.reward = RewardModel([self.dyn_index], action_size, config)
		self.dynamics = TransitionModel([self.dyn_index], action_size, config)
		self.optimizer = torch.optim.Adam(self.parameters(), lr=config.DYN.LEARN_RATE)
		self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=config.DYN.FACTOR, patience=config.DYN.PATIENCE)
		self.to(self.device)
		if load: self.load_model(load)

	def step(self, action, state=None, numpy=False, grad=False):
		action, state = map(self.to_tensor, [action, state])
		state = (self.state if state is None else state)[:,:self.dyn_index]
		with torch.enable_grad() if grad else torch.no_grad():
			state, action = map(self.to_tensor, [state, action])
			if self.discrete: action = one_hot(action)
			if self.state is None: self.state = state
			self.state, self.state_dot = self.dynamics(action, state, self.state_dot)
			reward = self.reward(self.state.detach(), self.state_dot.detach()).squeeze(-1)
		return [x.cpu().numpy() if numpy else x for x in [self.state, reward]]

	def reset(self, batch_size=None, state=None, **kwargs):
		self.dynamics.reset(self.device, batch_size)
		self.state = self.to_tensor(state)[:,:self.dyn_index] if state is not None else None
		self.state_dot = torch.zeros_like(self.state) if state is not None else None

	def rollout(self, actions, state, grad=False):
		self.reset(batch_size=len(state), state=state)
		actions = self.to_tensor(actions).transpose(0,1)
		next_states = []
		states_dot = []
		rewards = []
		for action in actions:
			next_state, reward = self.step(action, grad=grad)
			next_states.append(next_state)
			states_dot.append(self.state_dot)
			rewards.append(reward)
		next_states, states_dot, rewards = map(lambda x: torch.stack(x,1), [next_states, states_dot, rewards])
		return (next_states, states_dot), rewards

	def get_loss(self, states, actions, next_states, rewards, dones):
		s, a, ns, r = map(self.to_tensor, (states, actions, next_states, rewards))
		s, ns = [x[:,:,:self.dyn_index] for x in [s, ns]]
		(next_states, states_dot), rewards = self.rollout(a, s[:,0], grad=True)
		dyn_loss = (next_states - ns).pow(2).sum(-1).mean()
		dot_loss = (states_dot - (ns-s)).pow(2).sum(-1).mean()
		rew_loss = (rewards - r).pow(2).mean()
		self.stats.mean(dyn_loss=dyn_loss, dot_loss=dot_loss, rew_loss=rew_loss)
		return self.config.DYN.BETA_DYN*dyn_loss + self.config.DYN.BETA_DOT*dot_loss + self.config.DYN.BETA_REW*rew_loss

	def optimize(self, states, actions, next_states, rewards, dones):
		loss = self.get_loss(states, actions, next_states, rewards, dones)
		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		return loss

	def schedule(self, test_loss):
		self.scheduler.step(test_loss)

	def get_stats(self):
		return {**super().get_stats(), "lr": self.optimizer.param_groups[0]["lr"] if self.optimizer else None}

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		os.makedirs(os.path.dirname(filepath), exist_ok=True)
		torch.save(self.state_dict(), filepath)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		if os.path.exists(filepath):
			self.load_state_dict(torch.load(filepath, map_location=self.device))
			print(f"Loaded DFRNTL model at {filepath}")
		return self

Step:       0, Reward:    17.352 [  33.232], Avg:    17.352 (1.000) <0-00:00:00> ({'dyn_loss':     0.5139, 'dot_loss':     0.0234, 'rew_loss':    31.1030, 'lr':     0.0010})
Step:       1, Reward:     6.174 [  10.661], Avg:    11.763 (1.000) <0-00:00:57> ({'dyn_loss':     0.0946, 'dot_loss':     0.0077, 'rew_loss':    10.1088, 'lr':     0.0010})
Step:       2, Reward:     4.435 [   5.003], Avg:     9.321 (1.000) <0-00:01:57> ({'dyn_loss':     0.0889, 'dot_loss':     0.0064, 'rew_loss':     4.8504, 'lr':     0.0010})
Step:       3, Reward:     4.419 [   4.571], Avg:     8.095 (1.000) <0-00:03:00> ({'dyn_loss':     0.0723, 'dot_loss':     0.0053, 'rew_loss':     4.4778, 'lr':     0.0010})
Step:       4, Reward:     4.448 [   4.537], Avg:     7.366 (1.000) <0-00:04:03> ({'dyn_loss':     0.0649, 'dot_loss':     0.0047, 'rew_loss':     4.4581, 'lr':     0.0010})
Step:       5, Reward:     4.358 [   4.521], Avg:     6.864 (1.000) <0-00:05:03> ({'dyn_loss':     0.0625, 'dot_loss':     0.0045, 'rew_loss':     4.4375, 'lr':     0.0010})
Step:       6, Reward:     4.389 [   4.503], Avg:     6.511 (1.000) <0-00:06:03> ({'dyn_loss':     0.0588, 'dot_loss':     0.0041, 'rew_loss':     4.4289, 'lr':     0.0010})
Step:       7, Reward:     4.579 [   4.497], Avg:     6.269 (1.000) <0-00:07:05> ({'dyn_loss':     0.0579, 'dot_loss':     0.0039, 'rew_loss':     4.4431, 'lr':     0.0010})
Step:       8, Reward:     4.467 [   4.488], Avg:     6.069 (1.000) <0-00:08:08> ({'dyn_loss':     0.0565, 'dot_loss':     0.0038, 'rew_loss':     4.4257, 'lr':     0.0010})
Step:       9, Reward:     4.565 [   4.483], Avg:     5.919 (1.000) <0-00:09:13> ({'dyn_loss':     0.0536, 'dot_loss':     0.0036, 'rew_loss':     4.4339, 'lr':     0.0010})
Step:      10, Reward:     4.308 [   4.477], Avg:     5.772 (1.000) <0-00:10:10> ({'dyn_loss':     0.0528, 'dot_loss':     0.0035, 'rew_loss':     4.4043, 'lr':     0.0010})
Step:      11, Reward:     4.369 [   4.476], Avg:     5.655 (1.000) <0-00:11:13> ({'dyn_loss':     0.0537, 'dot_loss':     0.0035, 'rew_loss':     4.4082, 'lr':     0.0010})
Step:      12, Reward:     4.563 [   4.470], Avg:     5.571 (1.000) <0-00:12:14> ({'dyn_loss':     0.0518, 'dot_loss':     0.0034, 'rew_loss':     4.4242, 'lr':     0.0010})
Step:      13, Reward:     4.362 [   4.468], Avg:     5.485 (1.000) <0-00:13:17> ({'dyn_loss':     0.0516, 'dot_loss':     0.0033, 'rew_loss':     4.4028, 'lr':     0.0010})
Step:      14, Reward:     4.620 [   4.462], Avg:     5.427 (1.000) <0-00:14:16> ({'dyn_loss':     0.0505, 'dot_loss':     0.0033, 'rew_loss':     4.4243, 'lr':     0.0010})
Step:      15, Reward:     4.533 [   4.462], Avg:     5.371 (1.000) <0-00:15:19> ({'dyn_loss':     0.0505, 'dot_loss':     0.0033, 'rew_loss':     4.4150, 'lr':     0.0010})
Step:      16, Reward:     4.432 [   4.458], Avg:     5.316 (1.000) <0-00:16:20> ({'dyn_loss':     0.0510, 'dot_loss':     0.0033, 'rew_loss':     4.4015, 'lr':     0.0005})
Step:      17, Reward:     4.349 [   4.435], Avg:     5.262 (1.000) <0-00:17:18> ({'dyn_loss':     0.0439, 'dot_loss':     0.0027, 'rew_loss':     4.3801, 'lr':     0.0005})
Step:      18, Reward:     4.475 [   4.435], Avg:     5.221 (1.000) <0-00:18:16> ({'dyn_loss':     0.0441, 'dot_loss':     0.0028, 'rew_loss':     4.3920, 'lr':     0.0005})
Step:      19, Reward:     4.364 [   4.434], Avg:     5.178 (1.000) <0-00:19:14> ({'dyn_loss':     0.0434, 'dot_loss':     0.0027, 'rew_loss':     4.3806, 'lr':     0.0005})
Step:      20, Reward:     4.450 [   4.432], Avg:     5.143 (1.000) <0-00:20:11> ({'dyn_loss':     0.0433, 'dot_loss':     0.0027, 'rew_loss':     4.3878, 'lr':     0.0005})
Step:      21, Reward:     4.394 [   4.435], Avg:     5.109 (1.000) <0-00:21:08> ({'dyn_loss':     0.0433, 'dot_loss':     0.0027, 'rew_loss':     4.3851, 'lr':     0.0005})
Step:      22, Reward:     4.431 [   4.431], Avg:     5.080 (1.000) <0-00:22:05> ({'dyn_loss':     0.0430, 'dot_loss':     0.0027, 'rew_loss':     4.3856, 'lr':     0.0003})
Step:      23, Reward:     4.363 [   4.420], Avg:     5.050 (1.000) <0-00:23:03> ({'dyn_loss':     0.0404, 'dot_loss':     0.0025, 'rew_loss':     4.3714, 'lr':     0.0003})
Step:      24, Reward:     4.418 [   4.419], Avg:     5.025 (1.000) <0-00:24:01> ({'dyn_loss':     0.0402, 'dot_loss':     0.0025, 'rew_loss':     4.3766, 'lr':     0.0003})
Step:      25, Reward:     4.361 [   4.419], Avg:     4.999 (1.000) <0-00:24:59> ({'dyn_loss':     0.0402, 'dot_loss':     0.0025, 'rew_loss':     4.3706, 'lr':     0.0003})
Step:      26, Reward:     4.349 [   4.419], Avg:     4.975 (1.000) <0-00:25:56> ({'dyn_loss':     0.0400, 'dot_loss':     0.0025, 'rew_loss':     4.3697, 'lr':     0.0003})
Step:      27, Reward:     4.422 [   4.419], Avg:     4.955 (1.000) <0-00:26:54> ({'dyn_loss':     0.0403, 'dot_loss':     0.0025, 'rew_loss':     4.3760, 'lr':     0.0003})
Step:      28, Reward:     4.354 [   4.420], Avg:     4.935 (1.000) <0-00:27:50> ({'dyn_loss':     0.0400, 'dot_loss':     0.0025, 'rew_loss':     4.3707, 'lr':     0.0001})
Step:      29, Reward:     4.358 [   4.413], Avg:     4.915 (1.000) <0-00:28:46> ({'dyn_loss':     0.0384, 'dot_loss':     0.0024, 'rew_loss':     4.3666, 'lr':     0.0001})
Step:      30, Reward:     4.417 [   4.412], Avg:     4.899 (1.000) <0-00:29:44> ({'dyn_loss':     0.0386, 'dot_loss':     0.0024, 'rew_loss':     4.3720, 'lr':     0.0001})
Step:      31, Reward:     4.393 [   4.412], Avg:     4.883 (1.000) <0-00:30:42> ({'dyn_loss':     0.0384, 'dot_loss':     0.0024, 'rew_loss':     4.3694, 'lr':     0.0001})
Step:      32, Reward:     4.401 [   4.413], Avg:     4.869 (1.000) <0-00:31:40> ({'dyn_loss':     0.0386, 'dot_loss':     0.0024, 'rew_loss':     4.3708, 'lr':     0.0001})
Step:      33, Reward:     4.400 [   4.413], Avg:     4.855 (1.000) <0-00:32:38> ({'dyn_loss':     0.0383, 'dot_loss':     0.0024, 'rew_loss':     4.3709, 'lr':     0.0001})
Step:      34, Reward:     4.377 [   4.412], Avg:     4.841 (1.000) <0-00:33:37> ({'dyn_loss':     0.0383, 'dot_loss':     0.0024, 'rew_loss':     4.3680, 'lr':   6.25e-05})
Step:      35, Reward:     4.379 [   4.409], Avg:     4.829 (1.000) <0-00:34:34> ({'dyn_loss':     0.0374, 'dot_loss':     0.0023, 'rew_loss':     4.3659, 'lr':   6.25e-05})
Step:      36, Reward:     4.352 [   4.408], Avg:     4.816 (1.000) <0-00:35:31> ({'dyn_loss':     0.0375, 'dot_loss':     0.0023, 'rew_loss':     4.3622, 'lr':   6.25e-05})
Step:      37, Reward:     4.359 [   4.409], Avg:     4.804 (1.000) <0-00:36:29> ({'dyn_loss':     0.0373, 'dot_loss':     0.0023, 'rew_loss':     4.3640, 'lr':   6.25e-05})
Step:      38, Reward:     4.355 [   4.409], Avg:     4.792 (1.000) <0-00:37:27> ({'dyn_loss':     0.0373, 'dot_loss':     0.0023, 'rew_loss':     4.3639, 'lr':   6.25e-05})
Step:      39, Reward:     4.359 [   4.408], Avg:     4.781 (1.000) <0-00:38:25> ({'dyn_loss':     0.0373, 'dot_loss':     0.0023, 'rew_loss':     4.3637, 'lr':   6.25e-05})
Step:      40, Reward:     4.354 [   4.407], Avg:     4.771 (1.000) <0-00:39:22> ({'dyn_loss':     0.0372, 'dot_loss':     0.0024, 'rew_loss':     4.3625, 'lr':   3.13e-05})
Step:      41, Reward:     4.367 [   4.407], Avg:     4.761 (1.000) <0-00:40:21> ({'dyn_loss':     0.0367, 'dot_loss':     0.0023, 'rew_loss':     4.3640, 'lr':   3.13e-05})
Step:      42, Reward:     4.388 [   4.407], Avg:     4.753 (1.000) <0-00:41:18> ({'dyn_loss':     0.0367, 'dot_loss':     0.0023, 'rew_loss':     4.3658, 'lr':   3.13e-05})
Step:      43, Reward:     4.383 [   4.406], Avg:     4.744 (1.000) <0-00:42:16> ({'dyn_loss':     0.0368, 'dot_loss':     0.0023, 'rew_loss':     4.3646, 'lr':   3.13e-05})
Step:      44, Reward:     4.367 [   4.406], Avg:     4.736 (1.000) <0-00:43:14> ({'dyn_loss':     0.0367, 'dot_loss':     0.0023, 'rew_loss':     4.3633, 'lr':   3.13e-05})
Step:      45, Reward:     4.368 [   4.406], Avg:     4.728 (1.000) <0-00:44:12> ({'dyn_loss':     0.0366, 'dot_loss':     0.0023, 'rew_loss':     4.3634, 'lr':   3.13e-05})
Step:      46, Reward:     4.383 [   4.406], Avg:     4.720 (1.000) <0-00:45:09> ({'dyn_loss':     0.0366, 'dot_loss':     0.0023, 'rew_loss':     4.3647, 'lr':   1.56e-05})
Step:      47, Reward:     4.375 [   4.406], Avg:     4.713 (1.000) <0-00:46:06> ({'dyn_loss':     0.0364, 'dot_loss':     0.0023, 'rew_loss':     4.3641, 'lr':   1.56e-05})
Step:      48, Reward:     4.375 [   4.406], Avg:     4.706 (1.000) <0-00:47:05> ({'dyn_loss':     0.0364, 'dot_loss':     0.0023, 'rew_loss':     4.3639, 'lr':   1.56e-05})
Step:      49, Reward:     4.379 [   4.404], Avg:     4.700 (1.000) <0-00:48:02> ({'dyn_loss':     0.0363, 'dot_loss':     0.0023, 'rew_loss':     4.3631, 'lr':   1.56e-05})
