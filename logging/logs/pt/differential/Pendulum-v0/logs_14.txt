Model: <class 'src.models.pytorch.mpc.envmodel.differential.DifferentialEnv'>, Env: Pendulum-v0, Date: 29/05/2020 20:19:58
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: b0659115fd31a8c0a350fceeb3053fb2955e7f3b
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   dynamics_size = 3
   state_size = (3,)
   action_size = (1,)
   env_name = Pendulum-v0
   envmodel = dfrntl
   model = None
   nworkers = 0
   epochs = 50
   seq_len = 40
   batch_size = 256
   train_prop = 0.9
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.001
      TRANSITION_HIDDEN = 256
      REWARD_HIDDEN = 256
      BETA_DYN = 1
      BETA_DOT = 0.5
      BETA_DDOT = 0,
num_envs: 0,
envs: <__main__.Trainer object at 0x7fa55249d490>,
agent: DifferentialEnv(
	  (reward): RewardModel(
	    (linear1): Linear(in_features=7, out_features=256, bias=True)
	    (linear2): Linear(in_features=256, out_features=256, bias=True)
	    (linear3): Linear(in_features=256, out_features=1, bias=True)
	  )
	  (dynamics): TransitionModel(
	    (gru): GRUCell(7, 256)
	    (linear1): Linear(in_features=256, out_features=256, bias=True)
	    (linear2): Linear(in_features=256, out_features=256, bias=True)
	    (linear3): Linear(in_features=256, out_features=3, bias=True)
	  )
	) 
	training = True
	tau = 0.0004
	name = dfrntl
	stats = <src.utils.logger.Stats object at 0x7fa54d50aa10> 
		mean_dict = {}
		sum_dict = {}
	config = <src.utils.config.Config object at 0x7fa55560b550> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		dynamics_size = 3
		state_size = (3,)
		action_size = (1,)
		env_name = Pendulum-v0
		envmodel = dfrntl
		model = None
		nworkers = 0
		epochs = 50
		seq_len = 40
		batch_size = 256
		train_prop = 0.9
		DYN = <src.utils.config.Config object at 0x7fa5555f4a50> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.001
			TRANSITION_HIDDEN = 256
			REWARD_HIDDEN = 256
			BETA_DYN = 1
			BETA_DOT = 0.5
			BETA_DDOT = 0
	device = cuda
	state_size = (3,)
	action_size = (1,)
	discrete = False
	dyn_index = 3
	optimizer = Adam (
	Parameter Group 0
	    amsgrad: False
	    betas: (0.9, 0.999)
	    eps: 1e-08
	    lr: 0.001
	    weight_decay: 1e-06
	)
	scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa54d519150>,

import os
import torch
import numpy as np
from src.utils.misc import load_module
from ...agents.base import PTNetwork, one_hot

class TransitionModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.config = config
		self.gru = torch.nn.GRUCell(action_size[-1] + 2*state_size[-1], config.DYN.TRANSITION_HIDDEN)
		self.linear1 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear2 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear3 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, state_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, action, state, state_dot):
		inputs = torch.cat([action, state, state_dot],-1)
		self.hidden = self.gru(inputs, self.hidden)
		linear1 = self.linear1(self.hidden).tanh() + self.hidden
		linear2 = self.linear2(linear1).tanh() + linear1
		state_ddot = self.linear3(linear2)
		state_dot += state_ddot
		next_state = state + state_dot
		return next_state, state_dot, state_ddot

	def reset(self, device, batch_size=None):
		if batch_size is None: batch_size = self.hidden[0].shape[1] if hasattr(self, "hidden") else 1
		self.hidden = torch.zeros(batch_size, self.config.DYN.TRANSITION_HIDDEN, device=device)

class RewardModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.cost = load_module(config.REWARD_MODEL)() if config.get("REWARD_MODEL") else None
		self.dyn_spec = load_module(config.DYNAMICS_SPEC) if config.get("DYNAMICS_SPEC") else None
		self.linear1 = torch.nn.Linear(action_size[-1] + 2*state_size[-1], config.DYN.REWARD_HIDDEN)
		self.linear2 = torch.nn.Linear(config.DYN.REWARD_HIDDEN, config.DYN.REWARD_HIDDEN)
		self.linear3 = torch.nn.Linear(config.DYN.REWARD_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, action, state, next_state):
		if self.cost and self.dyn_spec:
			next_state, state = [x.cpu().numpy() for x in [next_state, state]]
			ns_spec, s_spec = map(self.dyn_spec.observation_spec, [next_state, state])
			reward = -torch.FloatTensor(self.cost.get_cost(ns_spec, s_spec, mpc=True))
		else:
			inputs = torch.cat([action, state, next_state],-1)
			layer1 = self.linear1(inputs).tanh()
			layer2 = self.linear2(layer1).tanh() + layer1
			reward = self.linear3(layer2).squeeze(-1)
		return reward

class DifferentialEnv(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="dfrntl"):
		super().__init__(config, gpu, name)
		self.state_size = state_size
		self.action_size = action_size
		self.discrete = type(self.action_size) != tuple
		self.dyn_index = config.get("dynamics_size", state_size[-1])
		self.reward = RewardModel([self.dyn_index], action_size, config)
		self.dynamics = TransitionModel([self.dyn_index], action_size, config)
		self.optimizer = torch.optim.Adam(self.parameters(), lr=config.DYN.LEARN_RATE, weight_decay=config.DYN.REG_LAMBDA)
		self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=config.DYN.FACTOR, patience=config.DYN.PATIENCE)
		self.to(self.device)
		if load: self.load_model(load)

	def step(self, action, state=None, numpy=False, grad=False):
		action, state = map(self.to_tensor, [action, state])
		state = (self.state if state is None else state)[:,:self.dyn_index]
		with torch.enable_grad() if grad else torch.no_grad():
			state, action = map(self.to_tensor, [state, action])
			if self.discrete: action = one_hot(action)
			if self.state is None: self.state = state
			state_dot = self.state_dot
			self.state, self.state_dot, self.state_ddot = self.dynamics(action, state, state_dot)
			reward = self.reward(action, state, self.state.detach())
		return [x.cpu().numpy() if numpy else x for x in [self.state, reward]]

	def reset(self, batch_size=None, state=None, **kwargs):
		self.dynamics.reset(self.device, batch_size)
		self.state = self.to_tensor(state)[:,:self.dyn_index] if state is not None else None
		self.state_dot = torch.zeros_like(self.state) if state is not None else None

	def rollout(self, actions, state, grad=False):
		self.reset(batch_size=len(state), state=state)
		actions = self.to_tensor(actions).transpose(0,1)
		next_states = []
		states_dot = []
		states_ddot = []
		rewards = []
		for action in actions:
			next_state, reward = self.step(action, grad=grad)
			next_states.append(next_state)
			states_dot.append(self.state_dot)
			states_ddot.append(self.state_ddot)
			rewards.append(reward)
		next_states, states_dot, states_ddot, rewards = map(lambda x: torch.stack(x,1), [next_states, states_dot, states_ddot, rewards])
		return (next_states, states_dot, states_ddot), rewards

	def get_loss(self, states, actions, next_states, rewards, dones):
		s, a, ns, r = map(self.to_tensor, (states, actions, next_states, rewards))
		s, ns = [x[:,:,:self.dyn_index] for x in [s, ns]]
		s_dot = (ns-s)
		ns_dot = torch.cat([torch.zeros_like(s_dot[:,0:1,:]), s_dot[:,:-1,:]], -2)
		(next_states, states_dot, states_ddot), rewards = self.rollout(a, s[:,0], grad=True)
		dyn_loss = (next_states - ns).pow(2).sum(-1).mean()
		dot_loss = (states_dot - s_dot).pow(2).sum(-1).mean()
		ddot_loss = (states_ddot - (ns_dot - s_dot)).pow(2).sum(-1).mean()
		rew_loss = (rewards - r).pow(2).mean()
		self.stats.mean(dyn_loss=dyn_loss, dot_loss=dot_loss, ddot_loss=ddot_loss, rew_loss=rew_loss)
		return self.config.DYN.BETA_DYN*dyn_loss + self.config.DYN.BETA_DOT*dot_loss + self.config.DYN.BETA_DDOT*ddot_loss + rew_loss

	def optimize(self, states, actions, next_states, rewards, dones):
		loss = self.get_loss(states, actions, next_states, rewards, dones)
		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		return loss

	def schedule(self, test_loss):
		self.scheduler.step(test_loss)

	def get_stats(self):
		return {**super().get_stats(), "lr": self.optimizer.param_groups[0]["lr"] if self.optimizer else None}

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		os.makedirs(os.path.dirname(filepath), exist_ok=True)
		torch.save(self.state_dict(), filepath)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		if os.path.exists(filepath):
			self.load_state_dict(torch.load(filepath, map_location=self.device))
			print(f"Loaded DFRNTL model at {filepath}")
		return self

Step:       0, Reward:    35.001 [ 324.960], Avg:    35.001 (1.000) <0-00:00:00> ({'dyn_loss':   274.2730, 'dot_loss':    15.1303, 'ddot_loss':     3.3415, 'rew_loss':    14.1257, 'lr':     0.0010})
Step:       1, Reward:    28.659 [  31.041], Avg:    31.830 (1.000) <0-00:01:31> ({'dyn_loss':    17.4750, 'dot_loss':     0.3774, 'ddot_loss':     0.9322, 'rew_loss':    13.1388, 'lr':     0.0010})
Step:       2, Reward:    25.876 [  27.125], Avg:    29.845 (1.000) <0-00:03:02> ({'dyn_loss':    15.2935, 'dot_loss':     0.3670, 'ddot_loss':     0.4608, 'rew_loss':    11.5233, 'lr':     0.0010})
Step:       3, Reward:    25.383 [  25.188], Avg:    28.730 (1.000) <0-00:04:33> ({'dyn_loss':    14.2461, 'dot_loss':     0.3858, 'ddot_loss':     0.2879, 'rew_loss':    10.7687, 'lr':     0.0010})
Step:       4, Reward:    24.060 [  24.964], Avg:    27.796 (1.000) <0-00:06:02> ({'dyn_loss':    13.6374, 'dot_loss':     0.3917, 'ddot_loss':     0.2203, 'rew_loss':    11.0402, 'lr':     0.0010})
Step:       5, Reward:    22.580 [  25.070], Avg:    26.927 (1.000) <0-00:07:33> ({'dyn_loss':    13.2856, 'dot_loss':     0.3891, 'ddot_loss':     0.1864, 'rew_loss':    11.3412, 'lr':     0.0010})
Step:       6, Reward:    22.141 [  24.124], Avg:    26.243 (1.000) <0-00:09:03> ({'dyn_loss':    12.4497, 'dot_loss':     0.3979, 'ddot_loss':     0.1996, 'rew_loss':    11.2770, 'lr':     0.0010})
Step:       7, Reward:     5.312 [   8.775], Avg:    23.627 (1.000) <0-00:10:32> ({'dyn_loss':     3.6623, 'dot_loss':     0.5528, 'ddot_loss':     0.1655, 'rew_loss':     4.4899, 'lr':     0.0010})
Step:       8, Reward:     3.191 [   5.064], Avg:    21.356 (1.000) <0-00:12:01> ({'dyn_loss':     1.8787, 'dot_loss':     0.5520, 'ddot_loss':     0.1573, 'rew_loss':     2.7216, 'lr':     0.0010})
Step:       9, Reward:     2.510 [   3.217], Avg:    19.471 (1.000) <0-00:13:30> ({'dyn_loss':     1.0964, 'dot_loss':     0.5609, 'ddot_loss':     0.1657, 'rew_loss':     1.7699, 'lr':     0.0010})
Step:      10, Reward:     2.409 [   2.802], Avg:    17.920 (1.000) <0-00:14:58> ({'dyn_loss':     0.9302, 'dot_loss':     0.5648, 'ddot_loss':     0.1765, 'rew_loss':     1.5500, 'lr':     0.0010})
Step:      11, Reward:     3.010 [   2.513], Avg:    16.678 (1.000) <0-00:16:27> ({'dyn_loss':     0.8349, 'dot_loss':     0.5699, 'ddot_loss':     0.1856, 'rew_loss':     1.4431, 'lr':     0.0010})
Step:      12, Reward:     2.646 [   2.260], Avg:    15.598 (1.000) <0-00:17:57> ({'dyn_loss':     0.7312, 'dot_loss':     0.5746, 'ddot_loss':     0.1899, 'rew_loss':     1.2803, 'lr':     0.0010})
Step:      13, Reward:     1.697 [   2.059], Avg:    14.605 (1.000) <0-00:19:25> ({'dyn_loss':     0.6207, 'dot_loss':     0.5826, 'ddot_loss':     0.1920, 'rew_loss':     1.1104, 'lr':     0.0010})
Step:      14, Reward:     1.741 [   1.917], Avg:    13.748 (1.000) <0-00:20:54> ({'dyn_loss':     0.5708, 'dot_loss':     0.5832, 'ddot_loss':     0.1869, 'rew_loss':     1.0374, 'lr':     0.0010})
Step:      15, Reward:     1.888 [   1.847], Avg:    13.006 (1.000) <0-00:22:23> ({'dyn_loss':     0.5532, 'dot_loss':     0.5826, 'ddot_loss':     0.1798, 'rew_loss':     1.0069, 'lr':     0.0010})
Step:      16, Reward:     2.146 [   1.744], Avg:    12.368 (1.000) <0-00:23:53> ({'dyn_loss':     0.5195, 'dot_loss':     0.5826, 'ddot_loss':     0.1701, 'rew_loss':     0.9735, 'lr':     0.0010})
Step:      17, Reward:     1.559 [   1.659], Avg:    11.767 (1.000) <0-00:25:20> ({'dyn_loss':     0.4788, 'dot_loss':     0.5827, 'ddot_loss':     0.1611, 'rew_loss':     0.8788, 'lr':     0.0010})
Step:      18, Reward:     1.550 [   1.589], Avg:    11.229 (1.000) <0-00:26:48> ({'dyn_loss':     0.4489, 'dot_loss':     0.5819, 'ddot_loss':     0.1537, 'rew_loss':     0.8455, 'lr':     0.0010})
Step:      19, Reward:     1.559 [   1.571], Avg:    10.746 (1.000) <0-00:28:16> ({'dyn_loss':     0.4432, 'dot_loss':     0.5772, 'ddot_loss':     0.1447, 'rew_loss':     0.8383, 'lr':     0.0010})
Step:      20, Reward:     1.476 [   1.502], Avg:    10.304 (1.000) <0-00:29:43> ({'dyn_loss':     0.4184, 'dot_loss':     0.5768, 'ddot_loss':     0.1415, 'rew_loss':     0.7923, 'lr':     0.0010})
Step:      21, Reward:     1.509 [   1.445], Avg:     9.905 (1.000) <0-00:31:10> ({'dyn_loss':     0.4010, 'dot_loss':     0.5730, 'ddot_loss':     0.1399, 'rew_loss':     0.7642, 'lr':     0.0010})
Step:      22, Reward:     1.744 [   1.447], Avg:     9.550 (1.000) <0-00:32:37> ({'dyn_loss':     0.4099, 'dot_loss':     0.5660, 'ddot_loss':     0.1380, 'rew_loss':     0.7836, 'lr':     0.0010})
Step:      23, Reward:     1.414 [   1.385], Avg:     9.211 (1.000) <0-00:34:04> ({'dyn_loss':     0.3840, 'dot_loss':     0.5550, 'ddot_loss':     0.1379, 'rew_loss':     0.7267, 'lr':     0.0010})
Step:      24, Reward:     1.363 [   1.377], Avg:     8.897 (1.000) <0-00:35:32> ({'dyn_loss':     0.3842, 'dot_loss':     0.5305, 'ddot_loss':     0.1419, 'rew_loss':     0.7258, 'lr':     0.0010})
Step:      25, Reward:     1.309 [   1.295], Avg:     8.605 (1.000) <0-00:36:59> ({'dyn_loss':     0.3835, 'dot_loss':     0.4196, 'ddot_loss':     0.1434, 'rew_loss':     0.7033, 'lr':     0.0010})
Step:      26, Reward:     1.231 [   1.195], Avg:     8.332 (1.000) <0-00:38:27> ({'dyn_loss':     0.3568, 'dot_loss':     0.3502, 'ddot_loss':     0.1474, 'rew_loss':     0.6671, 'lr':     0.0010})
Step:      27, Reward:     1.277 [   1.163], Avg:     8.080 (1.000) <0-00:39:55> ({'dyn_loss':     0.3477, 'dot_loss':     0.3389, 'ddot_loss':     0.1513, 'rew_loss':     0.6571, 'lr':     0.0010})
Step:      28, Reward:     1.104 [   1.137], Avg:     7.840 (1.000) <0-00:41:23> ({'dyn_loss':     0.3374, 'dot_loss':     0.3339, 'ddot_loss':     0.1525, 'rew_loss':     0.6289, 'lr':     0.0010})
Step:      29, Reward:     1.178 [   1.141], Avg:     7.617 (1.000) <0-00:42:51> ({'dyn_loss':     0.3402, 'dot_loss':     0.3315, 'ddot_loss':     0.1532, 'rew_loss':     0.6387, 'lr':     0.0010})
Step:      30, Reward:     1.089 [   1.111], Avg:     7.407 (1.000) <0-00:44:19> ({'dyn_loss':     0.3307, 'dot_loss':     0.3251, 'ddot_loss':     0.1552, 'rew_loss':     0.6159, 'lr':     0.0010})
Step:      31, Reward:     1.349 [   1.126], Avg:     7.218 (1.000) <0-00:45:47> ({'dyn_loss':     0.3419, 'dot_loss':     0.3359, 'ddot_loss':     0.1543, 'rew_loss':     0.6382, 'lr':     0.0010})
Step:      32, Reward:     1.071 [   1.098], Avg:     7.031 (1.000) <0-00:47:14> ({'dyn_loss':     0.3226, 'dot_loss':     0.3278, 'ddot_loss':     0.1547, 'rew_loss':     0.6090, 'lr':     0.0010})
Step:      33, Reward:     1.143 [   1.084], Avg:     6.858 (1.000) <0-00:48:44> ({'dyn_loss':     0.3254, 'dot_loss':     0.3216, 'ddot_loss':     0.1568, 'rew_loss':     0.6041, 'lr':     0.0010})
Step:      34, Reward:     1.118 [   1.081], Avg:     6.694 (1.000) <0-00:50:12> ({'dyn_loss':     0.3201, 'dot_loss':     0.3223, 'ddot_loss':     0.1565, 'rew_loss':     0.6036, 'lr':     0.0010})
Step:      35, Reward:     1.121 [   1.063], Avg:     6.539 (1.000) <0-00:51:40> ({'dyn_loss':     0.3165, 'dot_loss':     0.3217, 'ddot_loss':     0.1575, 'rew_loss':     0.5914, 'lr':     0.0010})
Step:      36, Reward:     1.152 [   1.064], Avg:     6.394 (1.000) <0-00:53:09> ({'dyn_loss':     0.3155, 'dot_loss':     0.3210, 'ddot_loss':     0.1579, 'rew_loss':     0.5965, 'lr':     0.0010})
Step:      37, Reward:     1.225 [   1.059], Avg:     6.258 (1.000) <0-00:54:38> ({'dyn_loss':     0.3164, 'dot_loss':     0.3210, 'ddot_loss':     0.1579, 'rew_loss':     0.5987, 'lr':     0.0010})
Step:      38, Reward:     1.229 [   1.045], Avg:     6.129 (1.000) <0-00:56:07> ({'dyn_loss':     0.3115, 'dot_loss':     0.3212, 'ddot_loss':     0.1586, 'rew_loss':     0.5911, 'lr':     0.0005})
Step:      39, Reward:     1.164 [   0.910], Avg:     6.005 (1.000) <0-00:57:36> ({'dyn_loss':     0.2739, 'dot_loss':     0.3155, 'ddot_loss':     0.1613, 'rew_loss':     0.5035, 'lr':     0.0005})
Step:      40, Reward:     1.288 [   0.920], Avg:     5.890 (1.000) <0-00:59:05> ({'dyn_loss':     0.2767, 'dot_loss':     0.3161, 'ddot_loss':     0.1634, 'rew_loss':     0.5225, 'lr':     0.0005})
Step:      41, Reward:     1.097 [   0.914], Avg:     5.775 (1.000) <0-01:00:34> ({'dyn_loss':     0.2684, 'dot_loss':     0.3163, 'ddot_loss':     0.1643, 'rew_loss':     0.5062, 'lr':     0.0005})
Step:      42, Reward:     1.107 [   0.903], Avg:     5.667 (1.000) <0-01:02:04> ({'dyn_loss':     0.2631, 'dot_loss':     0.3165, 'ddot_loss':     0.1657, 'rew_loss':     0.5021, 'lr':     0.0005})
Step:      43, Reward:     1.122 [   0.886], Avg:     5.564 (1.000) <0-01:03:31> ({'dyn_loss':     0.2574, 'dot_loss':     0.3166, 'ddot_loss':     0.1675, 'rew_loss':     0.4937, 'lr':     0.0005})
Step:      44, Reward:     1.093 [   0.881], Avg:     5.464 (1.000) <0-01:04:59> ({'dyn_loss':     0.2529, 'dot_loss':     0.3168, 'ddot_loss':     0.1680, 'rew_loss':     0.4909, 'lr':     0.0003})
Step:      45, Reward:     1.065 [   0.795], Avg:     5.369 (1.000) <0-01:06:27> ({'dyn_loss':     0.2228, 'dot_loss':     0.3154, 'ddot_loss':     0.1712, 'rew_loss':     0.4415, 'lr':     0.0003})
Step:      46, Reward:     1.096 [   0.793], Avg:     5.278 (1.000) <0-01:07:56> ({'dyn_loss':     0.2203, 'dot_loss':     0.3157, 'ddot_loss':     0.1719, 'rew_loss':     0.4447, 'lr':     0.0003})
Step:      47, Reward:     1.109 [   0.777], Avg:     5.191 (1.000) <0-01:09:23> ({'dyn_loss':     0.2152, 'dot_loss':     0.3159, 'ddot_loss':     0.1738, 'rew_loss':     0.4374, 'lr':     0.0003})
Step:      48, Reward:     1.108 [   0.761], Avg:     5.108 (1.000) <0-01:10:54> ({'dyn_loss':     0.2077, 'dot_loss':     0.3159, 'ddot_loss':     0.1736, 'rew_loss':     0.4300, 'lr':     0.0003})
Step:      49, Reward:     1.109 [   0.755], Avg:     5.028 (1.000) <0-01:12:24> ({'dyn_loss':     0.2018, 'dot_loss':     0.3165, 'ddot_loss':     0.1762, 'rew_loss':     0.4302, 'lr':     0.0003})
