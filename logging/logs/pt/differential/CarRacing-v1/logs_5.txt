Model: <class 'src.models.pytorch.mpc.envmodel.differential.DifferentialEnv'>, Env: CarRacing-v1, Date: 28/05/2020 16:47:50
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: 826d60376c8b418067528f7f08a8f412dd6aca63
Branch: master

config: 
   TRIAL_AT = 5000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   dynamics_size = 10
   state_size = (40,)
   action_size = (3,)
   env_name = CarRacing-v1
   envmodel = dfrntl
   model = None
   nworkers = 0
   epochs = 50
   seq_len = 20
   batch_size = 32
   train_prop = 0.9
   DYN = 
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.001
      TRANSITION_HIDDEN = 256
      BETA_DYN = 1
      BETA_DOT = 1
      BETA_REW = 0,
num_envs: 0,
envs: <__main__.Trainer object at 0x7fb4ce917fd0>,
agent: DifferentialEnv(
	  (reward): RewardModel(
	    (linear): Linear(in_features=20, out_features=1, bias=True)
	  )
	  (dynamics): TransitionModel(
	    (gru): GRUCell(23, 256)
	    (linear1): Linear(in_features=256, out_features=256, bias=True)
	    (linear2): Linear(in_features=256, out_features=256, bias=True)
	    (linear3): Linear(in_features=256, out_features=10, bias=True)
	  )
	) 
	training = True
	tau = 0.0004
	name = dfrntl
	stats = <src.utils.logger.Stats object at 0x7fb56a9e6f50> 
		mean_dict = {}
		sum_dict = {}
	config = <src.utils.config.Config object at 0x7fb5742b0fd0> 
		TRIAL_AT = 5000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		dynamics_size = 10
		state_size = (40,)
		action_size = (3,)
		env_name = CarRacing-v1
		envmodel = dfrntl
		model = None
		nworkers = 0
		epochs = 50
		seq_len = 20
		batch_size = 32
		train_prop = 0.9
		DYN = <src.utils.config.Config object at 0x7fb5742a4f90> 
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.001
			TRANSITION_HIDDEN = 256
			BETA_DYN = 1
			BETA_DOT = 1
			BETA_REW = 0
	device = cuda
	state_size = (40,)
	action_size = (3,)
	discrete = False
	dyn_index = 10
	optimizer = Adam (
	Parameter Group 0
	    amsgrad: False
	    betas: (0.9, 0.999)
	    eps: 1e-08
	    lr: 0.001
	    weight_decay: 0
	)
	scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb56a9e1090>,

import os
import torch
import numpy as np
from src.utils.misc import load_module
from ...agents.base import PTNetwork, one_hot

class TransitionModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.gru = torch.nn.GRUCell(action_size[-1] + 2*state_size[-1], config.DYN.TRANSITION_HIDDEN)
		self.linear1 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear2 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear3 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, state_size[-1])
		self.config = config

	def forward(self, action, state, state_dot):
		inputs = torch.cat([action, state, state_dot],-1)
		self.hidden = self.gru(inputs, self.hidden)
		linear1 = self.linear1(self.hidden).tanh() + self.hidden
		linear2 = self.linear2(linear1).tanh() + linear1
		state_dot = self.linear3(linear2)
		next_state = state + state_dot
		return next_state, state_dot

	def reset(self, device, batch_size=None):
		if batch_size is None: batch_size = self.hidden[0].shape[1] if hasattr(self, "hidden") else 1
		self.hidden = torch.zeros(batch_size, self.config.DYN.TRANSITION_HIDDEN, device=device)

class RewardModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.linear = torch.nn.Linear(2*state_size[-1], 1)
		self.cost = load_module(config.REWARD_MODEL)() if config.get("REWARD_MODEL") else None
		self.dyn_spec = load_module(config.DYNAMICS_SPEC) if config.get("DYNAMICS_SPEC") else None

	def forward(self, next_state, state_dot):
		if self.cost and self.dyn_spec:
			next_state, state_dot = [x.cpu().numpy() for x in [next_state, state_dot]]
			ns_spec, s_spec = map(self.dyn_spec.observation_spec, [next_state, next_state-state_dot])
			reward = -torch.FloatTensor(self.cost.get_cost(ns_spec, s_spec))
		else:
			inputs = torch.cat([next_state, state_dot],-1)
			reward = self.linear(inputs)
		return reward

class DifferentialEnv(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="dfrntl"):
		super().__init__(config, gpu, name)
		self.state_size = state_size
		self.action_size = action_size
		self.discrete = type(self.action_size) != tuple
		self.dyn_index = config.get("dynamics_size", state_size[-1])
		self.reward = RewardModel([self.dyn_index], action_size, config)
		self.dynamics = TransitionModel([self.dyn_index], action_size, config)
		self.optimizer = torch.optim.Adam(self.parameters(), lr=config.DYN.LEARN_RATE)
		self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=config.DYN.FACTOR, patience=config.DYN.PATIENCE)
		self.to(self.device)
		if load: self.load_model(load)

	def step(self, action, state=None, numpy=False, grad=False):
		action, state = map(self.to_tensor, [action, state])
		state = (self.state if state is None else state)[:,:self.dyn_index]
		with torch.enable_grad() if grad else torch.no_grad():
			state, action = map(self.to_tensor, [state, action])
			if self.discrete: action = one_hot(action)
			if self.state is None: self.state = state
			self.state, self.state_dot = self.dynamics(action, state, self.state_dot)
			reward = self.reward(self.state.detach(), self.state_dot.detach()).squeeze(-1)
		return [x.cpu().numpy() if numpy else x for x in [self.state, reward]]

	def reset(self, batch_size=None, state=None, **kwargs):
		self.dynamics.reset(self.device, batch_size)
		self.state = self.to_tensor(state[:,:self.dyn_index]) if state is not None else None
		self.state_dot = torch.zeros_like(self.state) if state is not None else None

	def rollout(self, actions, state):
		actions = self.to_tensor(actions).transpose(0,1)
		next_states = []
		states_dot = []
		rewards = []
		self.reset(batch_size=state.shape[0], state=state)
		for action in actions:
			next_state, reward = self.step(action, self.state, grad=True)
			next_states.append(next_state)
			states_dot.append(self.state_dot)
			rewards.append(reward)
		next_states, states_dot, rewards = map(lambda x: torch.stack(x,1), [next_states, states_dot, rewards])
		return (next_states, states_dot), rewards

	def get_loss(self, states, actions, next_states, rewards, dones):
		s, a, ns, r = map(self.to_tensor, (states, actions, next_states, rewards))
		s, ns = [x[:,:,:self.dyn_index] for x in [s, ns]]
		(next_states, states_dot), rewards = self.rollout(a, s[:,0])
		dyn_loss = (next_states - ns).pow(2).mean(-1).mean()
		dot_loss = (states_dot - (ns-s)).pow(2).sum(-1).mean()
		rew_loss = (rewards - r).pow(2).mean()
		self.stats.mean(dyn_loss=dyn_loss, dot_loss=dot_loss, rew_loss=rew_loss)
		return self.config.DYN.BETA_DYN*dyn_loss + self.config.DYN.BETA_DOT*dot_loss + self.config.DYN.BETA_REW*rew_loss

	def optimize(self, states, actions, next_states, rewards, dones):
		loss = self.get_loss(states, actions, next_states, rewards, dones)
		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		return loss

	def schedule(self, test_loss):
		self.scheduler.step(test_loss)

	def get_stats(self):
		return {**super().get_stats(), "lr": self.optimizer.param_groups[0]["lr"] if self.optimizer else None}

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		os.makedirs(os.path.dirname(filepath), exist_ok=True)
		torch.save(self.state_dict(), filepath)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		if os.path.exists(filepath):
			self.load_state_dict(torch.load(filepath, map_location=self.device))
			print(f"Loaded DFRNTL model at {filepath}")
		return self


Step:       0, Reward:     7.329 [   9.763], Avg:     7.329 (1.000) <0-00:00:00> ({'dyn_loss':     5.9850, 'dot_loss':     3.5244, 'rew_loss':   250.7304, 'lr':     0.0010})
Step:       1, Reward:     5.669 [   5.763], Avg:     6.499 (1.000) <0-00:04:13> ({'dyn_loss':     3.2187, 'dot_loss':     2.5345, 'rew_loss':   251.7940, 'lr':     0.0010})
Step:       2, Reward:     5.581 [   5.273], Avg:     6.193 (1.000) <0-00:08:28> ({'dyn_loss':     2.8865, 'dot_loss':     2.4182, 'rew_loss':   252.0449, 'lr':     0.0010})
Step:       3, Reward:     5.686 [   4.825], Avg:     6.066 (1.000) <0-00:12:48> ({'dyn_loss':     2.6028, 'dot_loss':     2.3122, 'rew_loss':   252.1296, 'lr':     0.0010})
Step:       4, Reward:     7.160 [   4.677], Avg:     6.285 (1.000) <0-00:17:09> ({'dyn_loss':     2.5851, 'dot_loss':     2.3504, 'rew_loss':   252.3821, 'lr':     0.0010})
Step:       5, Reward:     5.426 [   4.511], Avg:     6.142 (1.000) <0-00:21:26> ({'dyn_loss':     2.3595, 'dot_loss':     2.2465, 'rew_loss':   252.1848, 'lr':     0.0010})
Step:       6, Reward:     5.481 [   4.436], Avg:     6.048 (1.000) <0-00:25:39> ({'dyn_loss':     2.2941, 'dot_loss':     2.2510, 'rew_loss':   252.1400, 'lr':     0.0010})
Step:       7, Reward:     5.652 [   4.533], Avg:     5.998 (1.000) <0-00:29:54> ({'dyn_loss':     2.3380, 'dot_loss':     2.3110, 'rew_loss':   252.0388, 'lr':     0.0010})
Step:       8, Reward:     5.871 [   4.521], Avg:     5.984 (1.000) <0-00:34:08> ({'dyn_loss':     2.3354, 'dot_loss':     2.3260, 'rew_loss':   252.2104, 'lr':     0.0010})
Step:       9, Reward:     5.842 [   4.540], Avg:     5.970 (1.000) <0-00:38:23> ({'dyn_loss':     2.3273, 'dot_loss':     2.3482, 'rew_loss':   252.4323, 'lr':     0.0010})
Step:      10, Reward:     6.427 [   4.534], Avg:     6.011 (1.000) <0-00:42:37> ({'dyn_loss':     2.3662, 'dot_loss':     2.3644, 'rew_loss':   252.1800, 'lr':     0.0010})
Step:      11, Reward:    11.705 [   4.870], Avg:     6.486 (1.000) <0-00:46:51> ({'dyn_loss':     2.8051, 'dot_loss':     2.7758, 'rew_loss':   251.9764, 'lr':     0.0005})
Step:      12, Reward:     5.722 [   5.658], Avg:     6.427 (1.000) <0-00:51:05> ({'dyn_loss':     2.6469, 'dot_loss':     3.0175, 'rew_loss':   251.9790, 'lr':     0.0005})
Step:      13, Reward:     5.257 [   4.059], Avg:     6.344 (1.000) <0-00:55:19> ({'dyn_loss':     1.9777, 'dot_loss':     2.2059, 'rew_loss':   252.3110, 'lr':     0.0005})
Step:      14, Reward:     5.316 [   3.788], Avg:     6.275 (1.000) <0-00:59:33> ({'dyn_loss':     1.8606, 'dot_loss':     2.0867, 'rew_loss':   252.3886, 'lr':     0.0005})
Step:      15, Reward:     5.363 [   3.676], Avg:     6.218 (1.000) <0-01:03:48> ({'dyn_loss':     1.8118, 'dot_loss':     2.0393, 'rew_loss':   252.3793, 'lr':     0.0005})
Step:      16, Reward:     5.317 [   3.558], Avg:     6.165 (1.000) <0-01:08:02> ({'dyn_loss':     1.7450, 'dot_loss':     1.9960, 'rew_loss':   252.3697, 'lr':     0.0005})
Step:      17, Reward:     5.372 [   3.489], Avg:     6.121 (1.000) <0-01:12:21> ({'dyn_loss':     1.7124, 'dot_loss':     1.9726, 'rew_loss':   252.4793, 'lr':     0.0005})
Step:      18, Reward:     5.374 [   3.461], Avg:     6.082 (1.000) <0-01:16:37> ({'dyn_loss':     1.6967, 'dot_loss':     1.9635, 'rew_loss':   252.4972, 'lr':     0.0005})
Step:      19, Reward:     5.169 [   3.406], Avg:     6.036 (1.000) <0-01:20:56> ({'dyn_loss':     1.6465, 'dot_loss':     1.9429, 'rew_loss':   252.4473, 'lr':     0.0005})
Step:      20, Reward:     5.132 [   3.359], Avg:     5.993 (1.000) <0-01:25:15> ({'dyn_loss':     1.6226, 'dot_loss':     1.9210, 'rew_loss':   252.4257, 'lr':     0.0005})
Step:      21, Reward:     5.160 [   3.415], Avg:     5.955 (1.000) <0-01:29:06> ({'dyn_loss':     1.6478, 'dot_loss':     1.9486, 'rew_loss':   252.4551, 'lr':     0.0005})
Step:      22, Reward:     5.186 [   3.333], Avg:     5.922 (1.000) <0-01:32:49> ({'dyn_loss':     1.6105, 'dot_loss':     1.9153, 'rew_loss':   252.5237, 'lr':     0.0005})
Step:      23, Reward:     5.173 [   3.374], Avg:     5.890 (1.000) <0-01:36:44> ({'dyn_loss':     1.6235, 'dot_loss':     1.9379, 'rew_loss':   252.5025, 'lr':     0.0005})
Step:      24, Reward:     5.169 [   3.408], Avg:     5.862 (1.000) <0-01:40:38> ({'dyn_loss':     1.6454, 'dot_loss':     1.9458, 'rew_loss':   252.4776, 'lr':     0.0005})
Step:      25, Reward:     5.247 [   3.439], Avg:     5.838 (1.000) <0-01:44:33> ({'dyn_loss':     1.6491, 'dot_loss':     1.9778, 'rew_loss':   252.4935, 'lr':     0.0005})
Step:      26, Reward:     5.576 [   3.332], Avg:     5.828 (1.000) <0-01:48:35> ({'dyn_loss':     1.6322, 'dot_loss':     1.9332, 'rew_loss':   252.4121, 'lr':     0.0003})
Step:      27, Reward:     5.043 [   2.841], Avg:     5.800 (1.000) <0-01:52:38> ({'dyn_loss':     1.3437, 'dot_loss':     1.7263, 'rew_loss':   252.5331, 'lr':     0.0003})
Step:      28, Reward:     5.111 [   2.688], Avg:     5.776 (1.000) <0-01:56:32> ({'dyn_loss':     1.2786, 'dot_loss':     1.6616, 'rew_loss':   252.5603, 'lr':     0.0003})
Step:      29, Reward:     5.082 [   2.614], Avg:     5.753 (1.000) <0-02:00:28> ({'dyn_loss':     1.2408, 'dot_loss':     1.6299, 'rew_loss':   252.5093, 'lr':     0.0003})
Step:      30, Reward:     5.039 [   2.556], Avg:     5.730 (1.000) <0-02:04:24> ({'dyn_loss':     1.2073, 'dot_loss':     1.6067, 'rew_loss':   252.5136, 'lr':     0.0003})
Step:      31, Reward:     4.950 [   2.530], Avg:     5.706 (1.000) <0-02:08:20> ({'dyn_loss':     1.1911, 'dot_loss':     1.5909, 'rew_loss':   252.5745, 'lr':     0.0003})
Step:      32, Reward:     5.090 [   2.502], Avg:     5.687 (1.000) <0-02:12:15> ({'dyn_loss':     1.1851, 'dot_loss':     1.5862, 'rew_loss':   252.6116, 'lr':     0.0003})
Step:      33, Reward:     5.254 [   2.455], Avg:     5.674 (1.000) <0-02:16:10> ({'dyn_loss':     1.1809, 'dot_loss':     1.5658, 'rew_loss':   252.5805, 'lr':     0.0003})
Step:      34, Reward:     4.992 [   2.436], Avg:     5.655 (1.000) <0-02:20:05> ({'dyn_loss':     1.1531, 'dot_loss':     1.5491, 'rew_loss':   252.5738, 'lr':     0.0003})
Step:      35, Reward:     5.126 [   2.381], Avg:     5.640 (1.000) <0-02:24:02> ({'dyn_loss':     1.1387, 'dot_loss':     1.5279, 'rew_loss':   252.5746, 'lr':     0.0003})
Step:      36, Reward:     5.137 [   2.356], Avg:     5.627 (1.000) <0-02:27:59> ({'dyn_loss':     1.1253, 'dot_loss':     1.5202, 'rew_loss':   252.5287, 'lr':     0.0003})
Step:      37, Reward:     5.151 [   2.330], Avg:     5.614 (1.000) <0-02:31:56> ({'dyn_loss':     1.1143, 'dot_loss':     1.5093, 'rew_loss':   252.5768, 'lr':     0.0001})
Step:      38, Reward:     5.078 [   2.121], Avg:     5.600 (1.000) <0-02:35:52> ({'dyn_loss':     1.0116, 'dot_loss':     1.4170, 'rew_loss':   252.7016, 'lr':     0.0001})
Step:      39, Reward:     5.001 [   2.023], Avg:     5.585 (1.000) <0-02:39:55> ({'dyn_loss':     0.9625, 'dot_loss':     1.3699, 'rew_loss':   252.6818, 'lr':     0.0001})
Step:      40, Reward:     5.056 [   1.982], Avg:     5.573 (1.000) <0-02:43:52> ({'dyn_loss':     0.9489, 'dot_loss':     1.3531, 'rew_loss':   252.6090, 'lr':     0.0001})
Step:      41, Reward:     4.917 [   1.952], Avg:     5.557 (1.000) <0-02:47:52> ({'dyn_loss':     0.9266, 'dot_loss':     1.3335, 'rew_loss':   252.5587, 'lr':     0.0001})
Step:      42, Reward:     4.999 [   1.930], Avg:     5.544 (1.000) <0-02:51:47> ({'dyn_loss':     0.9214, 'dot_loss':     1.3276, 'rew_loss':   252.6174, 'lr':     0.0001})
Step:      43, Reward:     4.992 [   1.891], Avg:     5.531 (1.000) <0-02:55:43> ({'dyn_loss':     0.9036, 'dot_loss':     1.3102, 'rew_loss':   252.6204, 'lr':     0.0001})
Step:      44, Reward:     5.070 [   1.866], Avg:     5.521 (1.000) <0-02:59:39> ({'dyn_loss':     0.8985, 'dot_loss':     1.3003, 'rew_loss':   252.7018, 'lr':     0.0001})
Step:      45, Reward:     5.160 [   1.855], Avg:     5.513 (1.000) <0-03:03:36> ({'dyn_loss':     0.9016, 'dot_loss':     1.2976, 'rew_loss':   252.6732, 'lr':     0.0001})
Step:      46, Reward:     5.025 [   1.825], Avg:     5.503 (1.000) <0-03:07:30> ({'dyn_loss':     0.8766, 'dot_loss':     1.2813, 'rew_loss':   252.6362, 'lr':     0.0001})
Step:      47, Reward:     5.101 [   1.800], Avg:     5.495 (1.000) <0-03:11:26> ({'dyn_loss':     0.8761, 'dot_loss':     1.2678, 'rew_loss':   252.6380, 'lr':   6.25e-05})
Step:      48, Reward:     5.038 [   1.704], Avg:     5.485 (1.000) <0-03:15:26> ({'dyn_loss':     0.8252, 'dot_loss':     1.2256, 'rew_loss':   252.6703, 'lr':   6.25e-05})
Step:      49, Reward:     4.984 [   1.670], Avg:     5.475 (1.000) <0-03:19:26> ({'dyn_loss':     0.8084, 'dot_loss':     1.2067, 'rew_loss':   252.6680, 'lr':   6.25e-05})
