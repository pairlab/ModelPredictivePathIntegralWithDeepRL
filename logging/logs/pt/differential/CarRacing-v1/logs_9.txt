Model: <class 'src.models.pytorch.mpc.envmodel.differential.DifferentialEnv'>, Env: CarRacing-v1, Date: 29/05/2020 23:49:05
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: f49760a1503c280235bea170083f10c4af2abbf0
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   dynamics_size = 10
   state_size = (41,)
   action_size = (3,)
   env_name = CarRacing-v1
   envmodel = dfrntl
   model = None
   nworkers = 0
   epochs = 50
   seq_len = 40
   batch_size = 256
   train_prop = 0.9
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.001
      TRANSITION_HIDDEN = 256
      REWARD_HIDDEN = 256
      BETA_DYN = 0
      BETA_DOT = 0
      BETA_DDOT = 1,
num_envs: 0,
envs: <__main__.Trainer object at 0x7fe7c1b15750>,
agent: DifferentialEnv(
	  (reward): RewardModel(
	    (linear1): Linear(in_features=23, out_features=256, bias=True)
	    (linear2): Linear(in_features=256, out_features=256, bias=True)
	    (linear3): Linear(in_features=256, out_features=1, bias=True)
	  )
	  (dynamics): TransitionModel(
	    (gru): GRUCell(23, 256)
	    (linear1): Linear(in_features=256, out_features=256, bias=True)
	    (linear2): Linear(in_features=256, out_features=256, bias=True)
	    (linear3): Linear(in_features=256, out_features=10, bias=True)
	  )
	) 
	training = True
	tau = 0.0004
	name = dfrntl
	stats = <src.utils.logger.Stats object at 0x7fe718591750> 
		mean_dict = {}
		sum_dict = {}
	config = <src.utils.config.Config object at 0x7fe7c6403450> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		dynamics_size = 10
		state_size = (41,)
		action_size = (3,)
		env_name = CarRacing-v1
		envmodel = dfrntl
		model = None
		nworkers = 0
		epochs = 50
		seq_len = 40
		batch_size = 256
		train_prop = 0.9
		DYN = <src.utils.config.Config object at 0x7fe7c63eea10> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.001
			TRANSITION_HIDDEN = 256
			REWARD_HIDDEN = 256
			BETA_DYN = 0
			BETA_DOT = 0
			BETA_DDOT = 1
	device = cuda
	state_size = (41,)
	action_size = (3,)
	discrete = False
	dyn_index = 10
	optimizer = Adam (
	Parameter Group 0
	    amsgrad: False
	    betas: (0.9, 0.999)
	    eps: 1e-08
	    lr: 0.001
	    weight_decay: 1e-06
	)
	scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe71853c750>,

import os
import torch
import numpy as np
from src.utils.misc import load_module
from ...agents.base import PTNetwork, one_hot

class TransitionModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.config = config
		self.gru = torch.nn.GRUCell(action_size[-1] + 2*state_size[-1], config.DYN.TRANSITION_HIDDEN)
		self.linear1 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear2 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, config.DYN.TRANSITION_HIDDEN)
		self.linear3 = torch.nn.Linear(config.DYN.TRANSITION_HIDDEN, state_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, action, state, state_dot):
		inputs = torch.cat([action, state, state_dot],-1)
		self.hidden = self.gru(inputs, self.hidden)
		linear1 = self.linear1(self.hidden).relu() + self.hidden
		linear2 = self.linear2(linear1).relu() + linear1
		state_ddot = self.linear3(linear2)
		state_dot += state_ddot
		next_state = state + state_dot
		return next_state, state_dot, state_ddot

	def reset(self, device, batch_size=None):
		if batch_size is None: batch_size = self.hidden[0].shape[1] if hasattr(self, "hidden") else 1
		self.hidden = torch.zeros(batch_size, self.config.DYN.TRANSITION_HIDDEN, device=device)

class RewardModel(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		self.cost = load_module(config.REWARD_MODEL)() if config.get("REWARD_MODEL") else None
		self.dyn_spec = load_module(config.DYNAMICS_SPEC) if config.get("DYNAMICS_SPEC") else None
		self.linear1 = torch.nn.Linear(action_size[-1] + 2*state_size[-1], config.DYN.REWARD_HIDDEN)
		self.linear2 = torch.nn.Linear(config.DYN.REWARD_HIDDEN, config.DYN.REWARD_HIDDEN)
		self.linear3 = torch.nn.Linear(config.DYN.REWARD_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, action, state, next_state, grad=False):
		if self.cost and self.dyn_spec and not grad:
			next_state, state = [x.cpu().numpy() for x in [next_state, state]]
			ns_spec, s_spec = map(self.dyn_spec.observation_spec, [next_state, state])
			reward = -torch.FloatTensor(self.cost.get_cost(ns_spec, s_spec))
		else:
			inputs = torch.cat([action, state, next_state],-1)
			layer1 = self.linear1(inputs).tanh()
			layer2 = self.linear2(layer1).tanh() + layer1
			reward = self.linear3(layer2).squeeze(-1)
		return reward

class DifferentialEnv(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="dfrntl"):
		super().__init__(config, gpu, name)
		self.state_size = state_size
		self.action_size = action_size
		self.discrete = type(self.action_size) != tuple
		self.dyn_index = config.get("dynamics_size", state_size[-1])
		self.reward = RewardModel([self.dyn_index], action_size, config)
		self.dynamics = TransitionModel([self.dyn_index], action_size, config)
		self.optimizer = torch.optim.Adam(self.parameters(), lr=config.DYN.LEARN_RATE, weight_decay=config.DYN.REG_LAMBDA)
		self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=config.DYN.FACTOR, patience=config.DYN.PATIENCE)
		self.to(self.device)
		if load: self.load_model(load)

	def step(self, action, state=None, numpy=False, grad=False):
		action, state = map(self.to_tensor, [action, state])
		state = (self.state if state is None else state)[:,:self.dyn_index]
		with torch.enable_grad() if grad else torch.no_grad():
			state, action = map(self.to_tensor, [state, action])
			if self.discrete: action = one_hot(action)
			if self.state is None: self.state = state
			state_dot = self.state_dot
			self.state, self.state_dot, self.state_ddot = self.dynamics(action, state, state_dot)
			reward = self.reward(action, state, self.state.detach(), grad=grad)
		return [x.cpu().numpy() if numpy else x for x in [self.state, reward]]

	def reset(self, batch_size=None, state=None, **kwargs):
		self.dynamics.reset(self.device, batch_size)
		self.state = self.to_tensor(state)[:,:self.dyn_index] if state is not None else None
		self.state_dot = torch.zeros_like(self.state) if state is not None else None

	def rollout(self, actions, state, grad=False):
		self.reset(batch_size=len(state), state=state)
		actions = self.to_tensor(actions).transpose(0,1)
		next_states = []
		states_dot = []
		states_ddot = []
		rewards = []
		for action in actions:
			next_state, reward = self.step(action, grad=grad)
			next_states.append(next_state)
			states_dot.append(self.state_dot)
			states_ddot.append(self.state_ddot)
			rewards.append(reward)
		next_states, states_dot, states_ddot, rewards = map(lambda x: torch.stack(x,1), [next_states, states_dot, states_ddot, rewards])
		return (next_states, states_dot, states_ddot), rewards

	def get_loss(self, states, actions, next_states, rewards, dones):
		s, a, ns, r = map(self.to_tensor, (states, actions, next_states, rewards))
		s, ns = [x[:,:,:self.dyn_index] for x in [s, ns]]
		s_dot = (ns-s)
		ns_dot = torch.cat([s_dot[:,1:,:], s_dot[:,-1:,:]], -2)
		# ns_dot = torch.cat([torch.zeros_like(s_dot[:,0:1,:]), s_dot[:,:-1,:]], -2)
		(next_states, states_dot, states_ddot), rewards = self.rollout(a, s[:,0], grad=True)
		dyn_loss = (next_states - ns).pow(2).sum(-1).mean()
		dot_loss = (states_dot - s_dot).pow(2).sum(-1).mean()
		ddot_loss = (states_ddot[:,:-1] - (ns_dot - s_dot)[:,:-1]).pow(2).sum(-1).mean()
		rew_loss = (rewards - r).pow(2).mean()
		self.stats.mean(dyn_loss=dyn_loss, dot_loss=dot_loss, ddot_loss=ddot_loss, rew_loss=rew_loss)
		return self.config.DYN.BETA_DYN*dyn_loss + self.config.DYN.BETA_DOT*dot_loss + self.config.DYN.BETA_DDOT*ddot_loss + rew_loss

	def optimize(self, states, actions, next_states, rewards, dones):
		loss = self.get_loss(states, actions, next_states, rewards, dones)
		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		return loss

	def schedule(self, test_loss):
		self.scheduler.step(test_loss)

	def get_stats(self):
		return {**super().get_stats(), "lr": self.optimizer.param_groups[0]["lr"] if self.optimizer else None}

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		os.makedirs(os.path.dirname(filepath), exist_ok=True)
		torch.save(self.state_dict(), filepath)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		filepath, _ = self.get_checkpoint_path(dirname, name, net)
		if os.path.exists(filepath):
			self.load_state_dict(torch.load(filepath, map_location=self.device))
			print(f"Loaded DFRNTL model at {filepath}")
		return self

Step:       0, Reward:   553.674 [ 457.106], Avg:   553.674 (1.000) <0-00:00:00> ({'dyn_loss': 19647.3223, 'dot_loss':   185.0866, 'ddot_loss':     5.8780, 'rew_loss':   460.5997, 'lr':     0.0010})
Step:       1, Reward:   552.215 [ 451.865], Avg:   552.944 (1.000) <0-00:03:24> ({'dyn_loss': 12452.7539, 'dot_loss':    74.6118, 'ddot_loss':     5.3790, 'rew_loss':   456.2248, 'lr':     0.0010})
Step:       2, Reward:   557.356 [ 452.037], Avg:   554.415 (1.000) <0-00:06:51> ({'dyn_loss': 28600.7852, 'dot_loss':   177.0603, 'ddot_loss':     7.2406, 'rew_loss':   455.0179, 'lr':     0.0010})
Step:       3, Reward:   557.511 [ 447.678], Avg:   555.189 (1.000) <0-00:10:15> ({'dyn_loss':  9740.2109, 'dot_loss':    70.7737, 'ddot_loss':     6.7185, 'rew_loss':   451.6188, 'lr':     0.0010})
Step:       4, Reward:   557.067 [ 446.289], Avg:   555.564 (1.000) <0-00:13:41> ({'dyn_loss':  9024.6357, 'dot_loss':    81.5640, 'ddot_loss':     6.5045, 'rew_loss':   450.5356, 'lr':     0.0010})
Step:       5, Reward:   555.608 [ 448.987], Avg:   555.572 (1.000) <0-00:17:06> ({'dyn_loss': 73944.0547, 'dot_loss':   627.0465, 'ddot_loss':     7.8847, 'rew_loss':   451.4497, 'lr':     0.0010})
Step:       6, Reward:   559.596 [ 440.190], Avg:   556.147 (1.000) <0-00:20:31> ({'dyn_loss':  5821.6587, 'dot_loss':    40.0155, 'ddot_loss':     6.1298, 'rew_loss':   445.6484, 'lr':     0.0010})
Step:       7, Reward:   584.946 [ 419.678], Avg:   559.747 (1.000) <0-00:23:56> ({'dyn_loss':  5560.5596, 'dot_loss':    38.5014, 'ddot_loss':     6.6676, 'rew_loss':   429.0491, 'lr':     0.0005})
Step:       8, Reward:   589.107 [ 362.345], Avg:   563.009 (1.000) <0-00:27:23> ({'dyn_loss':  1491.4514, 'dot_loss':    16.5908, 'ddot_loss':     6.4246, 'rew_loss':   377.9278, 'lr':     0.0005})
Step:       9, Reward:   567.623 [ 329.833], Avg:   563.470 (1.000) <0-00:30:50> ({'dyn_loss':  1445.1876, 'dot_loss':    16.8361, 'ddot_loss':     6.2891, 'rew_loss':   346.6212, 'lr':     0.0005})
Step:      10, Reward:   589.665 [ 301.036], Avg:   565.852 (1.000) <0-00:34:17> ({'dyn_loss':  1487.6898, 'dot_loss':    17.0976, 'ddot_loss':     6.3545, 'rew_loss':   322.6928, 'lr':     0.0005})
Step:      11, Reward:   586.217 [ 283.502], Avg:   567.549 (1.000) <0-00:37:42> ({'dyn_loss':  1609.8889, 'dot_loss':    17.0044, 'ddot_loss':     6.5029, 'rew_loss':   306.3777, 'lr':     0.0005})
Step:      12, Reward:   568.603 [ 268.206], Avg:   567.630 (1.000) <0-00:41:09> ({'dyn_loss':  1500.0730, 'dot_loss':    17.0382, 'ddot_loss':     6.2821, 'rew_loss':   291.0776, 'lr':     0.0005})
Step:      13, Reward:   559.812 [ 259.096], Avg:   567.071 (1.000) <0-00:44:35> ({'dyn_loss':  1410.0194, 'dot_loss':    17.0593, 'ddot_loss':     6.2691, 'rew_loss':   282.0112, 'lr':     0.0003})
Step:      14, Reward:   572.294 [ 240.541], Avg:   567.419 (1.000) <0-00:48:01> ({'dyn_loss':  1419.1619, 'dot_loss':    17.0548, 'ddot_loss':     6.0402, 'rew_loss':   266.6973, 'lr':     0.0003})
Step:      15, Reward:   577.287 [ 233.430], Avg:   568.036 (1.000) <0-00:51:28> ({'dyn_loss':  1360.5917, 'dot_loss':    16.8729, 'ddot_loss':     5.8580, 'rew_loss':   260.9429, 'lr':     0.0003})
Step:      16, Reward:   571.110 [ 228.807], Avg:   568.217 (1.000) <0-00:54:55> ({'dyn_loss':  1389.1986, 'dot_loss':    16.8949, 'ddot_loss':     5.8043, 'rew_loss':   256.2228, 'lr':     0.0003})
Step:      17, Reward:   565.740 [ 220.983], Avg:   568.079 (1.000) <0-00:58:22> ({'dyn_loss':  1577.6479, 'dot_loss':    17.2104, 'ddot_loss':     5.9140, 'rew_loss':   248.5276, 'lr':     0.0003})
Step:      18, Reward:   563.633 [ 214.596], Avg:   567.845 (1.000) <0-01:01:50> ({'dyn_loss':  1524.8894, 'dot_loss':    17.8622, 'ddot_loss':     5.8402, 'rew_loss':   242.6298, 'lr':     0.0003})
Step:      19, Reward:   564.831 [ 208.963], Avg:   567.695 (1.000) <0-01:05:18> ({'dyn_loss':  1469.3694, 'dot_loss':    17.3995, 'ddot_loss':     5.8442, 'rew_loss':   237.6552, 'lr':     0.0001})
Step:      20, Reward:   573.543 [ 198.284], Avg:   567.973 (1.000) <0-01:08:45> ({'dyn_loss':  1449.7158, 'dot_loss':    17.3399, 'ddot_loss':     5.7698, 'rew_loss':   228.9334, 'lr':     0.0001})
Step:      21, Reward:   569.578 [ 188.672], Avg:   568.046 (1.000) <0-01:12:11> ({'dyn_loss':  1416.6052, 'dot_loss':    17.7927, 'ddot_loss':     5.7279, 'rew_loss':   219.9104, 'lr':     0.0001})
Step:      22, Reward:   570.974 [ 183.598], Avg:   568.173 (1.000) <0-01:15:41> ({'dyn_loss':  1488.5375, 'dot_loss':    17.5884, 'ddot_loss':     5.7373, 'rew_loss':   215.4553, 'lr':     0.0001})
Step:      23, Reward:   581.422 [ 179.745], Avg:   568.725 (1.000) <0-01:19:11> ({'dyn_loss':  1443.5933, 'dot_loss':    17.4691, 'ddot_loss':     5.6870, 'rew_loss':   213.0404, 'lr':     0.0001})
Step:      24, Reward:   570.015 [ 175.364], Avg:   568.777 (1.000) <0-01:22:40> ({'dyn_loss':  1453.4746, 'dot_loss':    17.1073, 'ddot_loss':     5.6714, 'rew_loss':   207.9930, 'lr':     0.0001})
Step:      25, Reward:   574.668 [ 169.854], Avg:   569.004 (1.000) <0-01:26:14> ({'dyn_loss':  1384.0168, 'dot_loss':    17.2074, 'ddot_loss':     5.6148, 'rew_loss':   203.5258, 'lr':   6.25e-05})
Step:      26, Reward:   579.015 [ 161.810], Avg:   569.374 (1.000) <0-01:29:47> ({'dyn_loss':  1359.7374, 'dot_loss':    17.1074, 'ddot_loss':     5.5856, 'rew_loss':   196.7135, 'lr':   6.25e-05})
Step:      27, Reward:   579.173 [ 156.740], Avg:   569.724 (1.000) <0-01:33:19> ({'dyn_loss':  1346.7113, 'dot_loss':    17.0819, 'ddot_loss':     5.5850, 'rew_loss':   192.1522, 'lr':   6.25e-05})
Step:      28, Reward:   582.122 [ 151.889], Avg:   570.152 (1.000) <0-01:36:52> ({'dyn_loss':  1297.1134, 'dot_loss':    17.0741, 'ddot_loss':     5.5740, 'rew_loss':   188.0688, 'lr':   6.25e-05})
Step:      29, Reward:   578.772 [ 149.186], Avg:   570.439 (1.000) <0-01:40:24> ({'dyn_loss':  1338.4691, 'dot_loss':    17.0488, 'ddot_loss':     5.6031, 'rew_loss':   185.2743, 'lr':   6.25e-05})
Step:      30, Reward:   579.620 [ 145.850], Avg:   570.735 (1.000) <0-01:43:56> ({'dyn_loss':  1333.1940, 'dot_loss':    16.9457, 'ddot_loss':     5.5633, 'rew_loss':   182.3834, 'lr':   6.25e-05})
Step:      31, Reward:   576.821 [ 143.235], Avg:   570.926 (1.000) <0-01:47:31> ({'dyn_loss':  1352.7750, 'dot_loss':    16.9293, 'ddot_loss':     5.5394, 'rew_loss':   179.7751, 'lr':   3.13e-05})
Step:      32, Reward:   579.433 [ 139.224], Avg:   571.183 (1.000) <0-01:51:05> ({'dyn_loss':  1363.4658, 'dot_loss':    16.9822, 'ddot_loss':     5.5587, 'rew_loss':   176.3874, 'lr':   3.13e-05})
Step:      33, Reward:   581.831 [ 135.779], Avg:   571.497 (1.000) <0-01:54:39> ({'dyn_loss':  1368.7743, 'dot_loss':    16.9990, 'ddot_loss':     5.5461, 'rew_loss':   173.5223, 'lr':   3.13e-05})
Step:      34, Reward:   580.081 [ 133.909], Avg:   571.742 (1.000) <0-01:58:13> ({'dyn_loss':  1349.4536, 'dot_loss':    16.9272, 'ddot_loss':     5.5097, 'rew_loss':   171.7003, 'lr':   3.13e-05})
Step:      35, Reward:   580.754 [ 131.508], Avg:   571.992 (1.000) <0-02:01:49> ({'dyn_loss':  1351.3047, 'dot_loss':    16.9124, 'ddot_loss':     5.4959, 'rew_loss':   169.6111, 'lr':   3.13e-05})
Step:      36, Reward:   579.274 [ 130.071], Avg:   572.189 (1.000) <0-02:05:25> ({'dyn_loss':  1331.5322, 'dot_loss':    16.9099, 'ddot_loss':     5.4839, 'rew_loss':   168.1816, 'lr':   3.13e-05})
Step:      37, Reward:   580.184 [ 129.297], Avg:   572.399 (1.000) <0-02:09:01> ({'dyn_loss':  1318.4076, 'dot_loss':    16.9105, 'ddot_loss':     5.4734, 'rew_loss':   167.5818, 'lr':   1.56e-05})
Step:      38, Reward:   582.463 [ 127.698], Avg:   572.657 (1.000) <0-02:12:37> ({'dyn_loss':  1323.8867, 'dot_loss':    16.8820, 'ddot_loss':     5.4563, 'rew_loss':   166.3764, 'lr':   1.56e-05})
Step:      39, Reward:   578.791 [ 126.670], Avg:   572.811 (1.000) <0-02:16:13> ({'dyn_loss':  1315.7704, 'dot_loss':    16.8392, 'ddot_loss':     5.4402, 'rew_loss':   165.1082, 'lr':   1.56e-05})
Step:      40, Reward:   581.572 [ 125.964], Avg:   573.024 (1.000) <0-02:19:46> ({'dyn_loss':  1335.4556, 'dot_loss':    16.8737, 'ddot_loss':     5.4394, 'rew_loss':   164.7415, 'lr':   1.56e-05})
Step:      41, Reward:   581.485 [ 125.367], Avg:   573.226 (1.000) <0-02:23:19> ({'dyn_loss':  1338.9329, 'dot_loss':    16.8409, 'ddot_loss':     5.4280, 'rew_loss':   164.2050, 'lr':   1.56e-05})
Step:      42, Reward:   581.777 [ 124.938], Avg:   573.425 (1.000) <0-02:26:54> ({'dyn_loss':  1325.4849, 'dot_loss':    16.8202, 'ddot_loss':     5.4116, 'rew_loss':   163.8627, 'lr':   1.56e-05})
Step:      43, Reward:   585.641 [ 124.384], Avg:   573.702 (1.000) <0-02:30:30> ({'dyn_loss':  1315.1028, 'dot_loss':    16.7997, 'ddot_loss':     5.4019, 'rew_loss':   163.7466, 'lr':   7.81e-06})
Step:      44, Reward:   584.481 [ 123.670], Avg:   573.942 (1.000) <0-02:34:06> ({'dyn_loss':  1314.8854, 'dot_loss':    16.7920, 'ddot_loss':     5.3919, 'rew_loss':   163.0000, 'lr':   7.81e-06})
Step:      45, Reward:   583.612 [ 122.822], Avg:   574.152 (1.000) <0-02:37:41> ({'dyn_loss':  1309.5389, 'dot_loss':    16.7949, 'ddot_loss':     5.3880, 'rew_loss':   162.1534, 'lr':   7.81e-06})
Step:      46, Reward:   584.049 [ 121.950], Avg:   574.363 (1.000) <0-02:41:17> ({'dyn_loss':  1302.7388, 'dot_loss':    16.7872, 'ddot_loss':     5.3845, 'rew_loss':   161.4119, 'lr':   7.81e-06})
Step:      47, Reward:   584.563 [ 121.531], Avg:   574.575 (1.000) <0-02:44:48> ({'dyn_loss':  1300.0698, 'dot_loss':    16.7825, 'ddot_loss':     5.3769, 'rew_loss':   161.0910, 'lr':   7.81e-06})
Step:      48, Reward:   583.565 [ 121.200], Avg:   574.759 (1.000) <0-02:48:25> ({'dyn_loss':  1304.4603, 'dot_loss':    16.7775, 'ddot_loss':     5.3743, 'rew_loss':   160.6981, 'lr':   7.81e-06})
Step:      49, Reward:   585.564 [ 120.914], Avg:   574.975 (1.000) <0-02:52:01> ({'dyn_loss':  1303.3943, 'dot_loss':    16.7811, 'ddot_loss':     5.3666, 'rew_loss':   160.6416, 'lr':   3.91e-06})
