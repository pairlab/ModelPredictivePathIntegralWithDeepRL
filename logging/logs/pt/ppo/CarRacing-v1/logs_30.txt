Model: <class 'src.models.pytorch.agents.ppo.PPOAgent'>, Env: CarRacing-v1, Date: 01/06/2020 18:06:40
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: f49760a1503c280235bea170083f10c4af2abbf0
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   BATCH_SIZE = 32
   PPO_EPOCHS = 2
   ENTROPY_WEIGHT = 0.01
   CLIP_PARAM = 0.05
   dynamics_size = 13
   state_size = (80,)
   action_size = (3,)
   env_name = CarRacing-v1
   rank = 0
   size = 17
   split = 17
   model = ppo
   framework = pt
   train_prop = 1.0
   tcp_ports = [11000, 11001, 11002, 11003, 11004, 11005, 11006, 11007, 11008, 11009, 11010, 11011, 11012, 11013, 11014, 11015, 11016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 1000000
   render = False
   trial = False
   icm = False
   rs = False,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7fdcc0960210> 
	env = <GymEnv<CarRacing<CarRacing-v1>>> 
		env = <CarRacing<CarRacing-v1>> 
			channel = <mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel object at 0x7fdcc0956ed0>
			scale_sim = <function CarRacing.__init__.<locals>.<lambda> at 0x7fdcc08afcb0>
			env = <UnityToGymWrapper instance> 
				visual_obs = None
				game_over = False
				name = CarBehavior?team=0
				group_spec = BehaviorSpec(observation_shapes=[(30,)], action_type=<ActionType.CONTINUOUS: 1>, action_shape=3)
				use_visual = False
				uint8_visual = False
			cost_model = <src.envs.CarRacing.objective.cost.CostModel object at 0x7fdcc08aec90> 
				track = <src.envs.CarRacing.objective.track.Track object at 0x7fdcc08ae8d0> 
					track = <list len=500>
					X = (1.540585208684206, 1.5814536064863205, 1.6016383588314056, 1.6350171357393264, 1.6559478223323822, 1.6717498254776002, 1.709812204837799, 1.7354034245014192, 1.7725858569145203, 1.8077154874801635, 1.958074402809143, 2.0178433418273927, 2.1851138830184937, 2.258661150932312, 2.3439700841903686, 2.452700424194336, 2.586679172515869, 2.782884216308594, 3.047244071960449, 3.4783129692077637, 3.9734771251678467, 4.596014499664307, 5.29957389831543, 6.05716609954834, 6.824328422546387, 7.646727561950684, 8.59219741821289, 9.675070762634277, 10.77119255065918, 11.868535041809082, 12.83842658996582, 13.727555274963379, 14.569844245910645, 15.391722679138184, 16.204023361206055, 17.02372169494629, 17.626384735107422, 18.072078704833984, 18.462026596069336, 18.803436279296875, 19.08125877380371, 19.200590133666992, 19.074377059936523, 18.833162307739258, 18.582487106323242, 18.339160919189453, 17.97744369506836, 17.59515380859375, 17.09140968322754, 16.50218391418457, 15.817791938781738, 14.983868598937988, 13.986822128295898, 12.817933082580566, 11.528505325317383, 10.241579055786133, 8.946599960327148, 7.588953971862793, 6.2032341957092285, 4.799948692321777, 3.3720505237579346, 1.9454675912857056, 0.4815756678581238, -0.9242660999298096, -2.3082480430603027, -3.7190709114074707, -5.090760231018066, -6.490819931030273, -7.933252811431885, -9.48039722442627, -11.141877174377441, -12.927711486816406, -14.796602249145508, -16.603300094604492, -18.390233993530273, -20.1385498046875, -21.805997848510742, -23.41408920288086, -25.02754783630371, -26.801597595214844, -28.776451110839844, -30.972705841064453, -33.385520935058594, -35.90762710571289, -38.527618408203125, -41.362369537353516, -44.435585021972656, -47.831398010253906, -51.587188720703125, -55.642662048339844, -59.980804443359375, -64.55036163330078, -69.1060562133789, -73.4732666015625, -77.65788269042969, -81.6474380493164, -85.45370483398438, -89.12055206298828, -92.67816925048828, -96.15220642089844, -99.54827117919922, -102.86875915527344, -106.01786804199219, -109.03597259521484, -111.96282958984375, -114.75870513916016, -117.48453521728516, -120.2335205078125, -123.01750946044922, -125.81232452392578, -128.56246948242188, -131.20936584472656, -133.767333984375, -136.21359252929688, -138.6573486328125, -141.0603485107422, -143.3613739013672, -145.4899444580078, -147.5723114013672, -149.41514587402344, -150.9908905029297, -152.32089233398438, -153.6006622314453, -154.83030700683594, -156.0063018798828, -157.14691162109375, -158.23680114746094, -159.30880737304688, -160.30152893066406, -161.2411651611328, -162.03582763671875, -162.72186279296875, -163.28753662109375, -163.81460571289062, -164.31549072265625, -164.78814697265625, -165.1201171875, -165.26596069335938, -165.24961853027344, -165.20376586914062, -165.07931518554688, -165.0469512939453, -165.03262329101562, -164.86660766601562, -164.62220764160156, -164.3842315673828, -164.145263671875, -163.90011596679688, -163.64981079101562, -163.3218231201172, -162.726318359375, -161.83493041992188, -160.71856689453125, -159.4139862060547, -157.9736328125, -156.54212951660156, -155.10464477539062, -153.63636779785156, -152.13641357421875, -150.6412811279297, -149.1659698486328, -147.64437866210938, -146.01336669921875, -144.21286010742188, -142.3518829345703, -140.49502563476562, -138.6591796875, -136.8135986328125, -134.9413604736328, -132.9547882080078, -130.7132110595703, -128.1597137451172, -125.3279037475586, -122.26266479492188, -118.97386932373047, -115.49871826171875, -111.90750122070312, -108.16539764404297, -104.34297180175781, -100.58757781982422, -96.96247863769531, -93.51396942138672, -90.1981201171875, -86.93607330322266, -83.70171356201172, -80.58210754394531, -77.49177551269531, -74.4620132446289, -71.53809356689453, -68.60317993164062, -65.52932739257812, -62.46957778930664, -59.48895263671875, -56.56187057495117, -53.813289642333984, -51.1711311340332, -48.648197174072266, -46.242332458496094, -43.94118118286133, -41.766075134277344, -39.70472717285156, -37.813140869140625, -36.01365280151367, -34.269657135009766, -32.50520706176758, -30.680166244506836, -28.837051391601562, -27.001256942749023, -25.25333023071289, -23.701873779296875, -22.668081283569336, -22.199195861816406, -22.169893264770508, -22.46630859375, -23.134033203125, -24.32797622680664, -26.001781463623047, -27.869766235351562, -29.80392074584961, -31.775949478149414, -33.793365478515625, -35.771907806396484, -37.70563888549805, -39.61886215209961, -41.516029357910156, -43.41127014160156, -45.27768325805664, -47.11109924316406, -48.94091796875, -50.77583694458008, -52.619163513183594, -54.48332977294922, -56.314815521240234, -58.103755950927734, -59.823333740234375, -61.56585693359375, -63.30061340332031, -64.97642517089844, -66.51130676269531, -67.94270324707031, -69.3357925415039, -70.66708374023438, -71.93402099609375, -73.18978118896484, -74.31753540039062, -75.23255920410156, -75.95966339111328, -76.61920166015625, -77.26768493652344, -77.9359130859375, -78.5946273803711, -79.26289367675781, -79.79534912109375, -80.2015380859375, -80.60335540771484, -81.02714538574219, -81.53772735595703, -82.04193878173828, -82.53047180175781, -83.04158020019531, -83.56088256835938, -84.14714813232422, -84.81393432617188, -85.55133056640625, -86.36656188964844, -87.24837493896484, -88.13751983642578, -88.99240112304688, -89.81124877929688, -90.60415649414062, -91.33631896972656, -92.02133178710938, -92.65229034423828, -93.23121643066406, -93.7853012084961, -94.3372573852539, -94.88070678710938, -95.41710662841797, -95.84803771972656, -96.24778747558594, -96.6568374633789, -97.0496826171875, -97.41992950439453, -97.77052307128906, -97.91485595703125, -97.96147155761719, -97.87026977539062, -97.53227233886719, -96.85386657714844, -95.81302642822266, -94.54135131835938, -93.15739440917969, -91.603271484375, -89.95466613769531, -88.35015106201172, -86.80291748046875, -85.39144134521484, -84.07344055175781, -82.86149597167969, -81.5972671508789, -80.11182403564453, -78.36345672607422, -76.40621948242188, -74.32894134521484, -72.0761489868164, -69.69659423828125, -67.17849731445312, -64.48152160644531, -61.61235046386719, -58.499427795410156, -55.10073471069336, -51.55522918701172, -47.74736785888672, -43.832923889160156, -39.801971435546875, -35.743858337402344, -31.80649757385254, -28.028738021850586, -24.38759994506836, -20.836519241333008, -17.374597549438477, -14.002902030944824, -10.617079734802246, -7.34421443939209, -4.187110424041748, -1.115414023399353, 2.037353277206421, 5.401520252227783, 8.870983123779297, 12.423381805419922, 16.180818557739258, 20.157392501831055, 24.33769989013672, 28.77823829650879, 33.3828010559082, 38.12346267700195, 42.767642974853516, 47.21396255493164, 51.497074127197266, 55.640106201171875, 59.61445999145508, 63.45794677734375, 67.16992950439453, 70.71627044677734, 74.12809753417969, 77.53622436523438, 80.97876739501953, 84.45626068115234, 87.9986572265625, 91.61026000976562, 95.1865234375, 98.68260192871094, 102.08172607421875, 105.37554168701172, 108.5978012084961, 111.72406005859375, 114.72969818115234, 117.6103515625, 120.28418731689453, 122.77039337158203, 125.10813903808594, 127.35991668701172, 129.5707550048828, 131.73577880859375, 133.8451385498047, 135.88076782226562, 137.81361389160156, 139.69195556640625, 141.56494140625, 143.51321411132812, 145.43582153320312, 147.37954711914062, 149.30592346191406, 151.1349334716797, 152.76832580566406, 154.18382263183594, 155.40008544921875, 156.48155212402344, 157.39840698242188, 158.19866943359375, 158.91281127929688, 159.4974822998047, 160.02337646484375, 160.31883239746094, 160.23129272460938, 159.7694854736328, 159.0675506591797, 158.11312866210938, 157.08311462402344, 155.8784942626953, 154.47816467285156, 152.8489990234375, 151.00660705566406, 149.11109924316406, 147.24368286132812, 145.35427856445312, 143.4554443359375, 141.39073181152344, 139.07090759277344, 136.57705688476562, 134.08177185058594, 131.63348388671875, 129.23263549804688, 126.91446685791016, 124.63007354736328, 122.27965545654297, 119.90943145751953, 117.51732635498047, 115.1493148803711, 112.83964538574219, 110.53994750976562, 108.22462463378906, 105.85285949707031, 103.4562759399414, 101.13794708251953, 98.82323455810547, 96.44384765625, 93.94629669189453, 91.3570556640625, 88.73168182373047, 86.05917358398438, 83.26211547851562, 80.25263214111328, 77.10718536376953, 73.97905731201172, 70.96484375, 68.1133804321289, 65.44701385498047, 62.890159606933594, 60.41355514526367, 57.95263671875, 55.59248352050781, 53.20044708251953, 50.7462272644043, 48.28958511352539, 45.88505935668945, 43.5562744140625, 41.31084442138672, 39.171634674072266, 37.183380126953125, 35.43268966674805, 33.800804138183594, 32.20466613769531, 30.66669273376465, 29.13826560974121, 27.552635192871094, 25.97852325439453, 24.294662475585938, 22.565439224243164, 20.874217987060547, 19.30082893371582, 17.831933975219727, 16.408084869384766, 15.044317245483398, 13.766607284545898, 12.577005386352539, 11.475253105163574, 10.496495246887207, 9.622332572937012, 8.769275665283203, 7.927954196929932, 7.112521648406982, 6.322704315185547, 5.563619136810303, 4.829586982727051, 4.113427639007568, 3.3697121143341064, 2.5567243099212646, 1.7977246046066284, 1.0246542692184448, 0.2572939395904541, -0.4480553865432739, -1.1242897510528564, -1.6556841135025024, -2.0525705814361572, -2.214649200439453, -2.169621467590332, -2.035892963409424, -1.9102517366409302, -1.7909443378448486, -1.7162281274795532, -1.651557445526123, -1.5775796175003052, -1.5097243785858154, -1.4451829195022583, -1.3808107376098633, -1.3076838254928589, -1.1195673942565918, -0.8252816200256348, -0.5349398255348206, -0.2580118477344513, 0.009828831069171429, 0.2716897428035736, 0.5349469780921936, 0.7902784943580627, 1.052398443222046, 1.31592857837677, 1.570581078529358, 1.6137370109558105, 1.6365979194641114)
					Z = (-0.8819639682769775, -0.8812801241874695, -0.8804802298545837, -0.8791921734809875, -0.8777425289154053, -0.8758563995361328, -0.873963475227356, -0.8539403676986694, -0.7802032232284546, -0.761174201965332, -0.7716957926750183, -0.8395041823387146, -0.8772552609443665, -0.8344407081604004, -0.788372814655304, -0.80742347240448, -0.8527643084526062, -0.8346409797668457, -0.824370265007019, -0.8134136199951172, -0.7967275381088257, -0.7752544283866882, -0.7417746782302856, -0.6927484273910522, -0.633834719657898, -0.5747796297073364, -0.5113369226455688, -0.4433113932609558, -0.3737497925758362, -0.3008161187171936, -0.2312106341123581, -0.16523221135139465, -0.09990986436605453, -0.033577218651771545, 0.03842548280954361, 0.11881522089242935, 0.1981208622455597, 0.28177762031555176, 0.38250869512557983, 0.5017393231391907, 0.625041127204895, 0.7394312620162964, 0.8367793560028076, 0.9279725551605225, 1.0242633819580078, 1.1258037090301514, 1.2272775173187256, 1.3421326875686646, 1.4506069421768188, 1.561546802520752, 1.6706804037094116, 1.7743912935256958, 1.8515067100524902, 1.9097793102264404, 1.948763370513916, 1.9814872741699219, 2.0233898162841797, 2.07637095451355, 2.132861375808716, 2.17509126663208, 2.2180161476135254, 2.274773597717285, 2.3546767234802246, 2.4420950412750244, 2.5328733921051025, 2.6344215869903564, 2.7358694076538086, 2.8366494178771973, 2.9418249130249023, 3.0620920658111572, 3.1827614307403564, 3.30625581741333, 3.427833080291748, 3.5489587783813477, 3.675954818725586, 3.79117488861084, 3.901960849761963, 4.005653381347656, 4.107993125915527, 4.2158284187316895, 4.328779220581055, 4.445080280303955, 4.569532871246338, 4.690032005310059, 4.799752712249756, 4.872299671173096, 4.92843770980835, 4.985036849975586, 5.057000637054443, 5.13352108001709, 5.213327884674072, 5.295718193054199, 5.3766703605651855, 5.451817512512207, 5.519579887390137, 5.582165718078613, 5.639312267303467, 5.692175388336182, 5.7414727210998535, 5.787367820739746, 5.830183506011963, 5.869744300842285, 5.905086994171143, 5.936120986938477, 5.963281154632568, 5.987318992614746, 6.008669376373291, 6.027542591094971, 6.044310569763184, 6.057828903198242, 6.067286968231201, 6.074985504150391, 6.081448554992676, 6.086737155914307, 6.091536998748779, 6.096595764160156, 6.1012773513793945, 6.104137420654297, 6.10720682144165, 6.105283260345459, 6.09289026260376, 6.069871425628662, 6.042582988739014, 6.011574745178223, 5.977062702178955, 5.945542812347412, 5.9195661544799805, 5.900696277618408, 5.875031471252441, 5.850343227386475, 5.822032451629639, 5.787215232849121, 5.749323844909668, 5.708043575286865, 5.672667503356934, 5.640613079071045, 5.58774995803833, 5.510519504547119, 5.4132280349731445, 5.318352222442627, 5.21757173538208, 5.129578113555908, 5.049224376678467, 4.955892086029053, 4.855170726776123, 4.759181022644043, 4.6699957847595215, 4.590251922607422, 4.507761478424072, 4.420248508453369, 4.298507213592529, 4.1367998123168945, 3.954977035522461, 3.7536673545837402, 3.5393548011779785, 3.336235761642456, 3.13871431350708, 2.941469192504883, 2.743802785873413, 2.5500059127807617, 2.362222671508789, 2.172161817550659, 1.9712504148483276, 1.7527763843536377, 1.5335578918457031, 1.3216581344604492, 1.11974036693573, 0.924856424331665, 0.7362942099571228, 0.548167884349823, 0.3510936498641968, 0.14911779761314392, -0.04503828287124634, -0.22794248163700104, -0.3905165493488312, -0.5209499597549438, -0.6174218654632568, -0.6916936039924622, -0.7458155751228333, -0.7768694162368774, -0.7899942994117737, -0.7893635630607605, -0.7789414525032043, -0.7635725736618042, -0.7461717128753662, -0.7283236980438232, -0.704211413860321, -0.6622856855392456, -0.5993924140930176, -0.5216199159622192, -0.426088809967041, -0.3150973916053772, -0.1974087506532669, -0.07835512608289719, 0.03133012354373932, 0.13556505739688873, 0.24022513628005981, 0.3493971824645996, 0.45991453528404236, 0.5715771317481995, 0.6827750205993652, 0.7940959930419922, 0.907843291759491, 1.025125503540039, 1.148614764213562, 1.2811535596847534, 1.417541265487671, 1.5532535314559937, 1.6824359893798828, 1.7986339330673218, 1.8819316625595093, 1.9304401874542236, 1.9543043375015259, 1.9636659622192383, 1.9588732719421387, 1.916387915611267, 1.8345577716827393, 1.7349056005477905, 1.6296110153198242, 1.5208213329315186, 1.405418872833252, 1.2866981029510498, 1.16438889503479, 1.0394600629806519, 0.9107307195663452, 0.7798608541488647, 0.6512886881828308, 0.5262399315834045, 0.4030036926269531, 0.2815271019935608, 0.16398224234580994, 0.05072043836116791, -0.05590145289897919, -0.15327762067317963, -0.24135041236877441, -0.3243723213672638, -0.3988741636276245, -0.4620799124240875, -0.542617678642273, -0.646656334400177, -0.7287228107452393, -0.7844877243041992, -0.806078314781189, -0.8148013949394226, -0.8116025924682617, -0.8039451837539673, -0.7978506088256836, -0.8006065487861633, -0.8066939115524292, -0.8129818439483643, -0.8215823173522949, -0.8290983438491821, -0.8362972736358643, -0.8428731560707092, -0.8489797711372375, -0.8558133840560913, -0.8626493811607361, -0.8682581186294556, -0.8741699457168579, -0.879978597164154, -0.8859436511993408, -0.8909560441970825, -0.8937748670578003, -0.8939367532730103, -0.8897822499275208, -0.8787690997123718, -0.8593403697013855, -0.8307321667671204, -0.8021003603935242, -0.7821503281593323, -0.7700151801109314, -0.7592963576316833, -0.7492351531982422, -0.7390634417533875, -0.7314242720603943, -0.7212424278259277, -0.7080341577529907, -0.6888165473937988, -0.66937655210495, -0.6463529467582703, -0.6128187775611877, -0.5654257535934448, -0.5037499666213989, -0.42715343832969666, -0.34471648931503296, -0.25006303191185, -0.14578062295913696, -0.03818090260028839, 0.0759134441614151, 0.21288788318634033, 0.35622480511665344, 0.515775203704834, 0.6532223224639893, 0.7738814949989319, 0.8932506442070007, 1.0421302318572998, 1.2146294116973877, 1.385721206665039, 1.5515326261520386, 1.7406084537506104, 1.9566478729248047, 2.214561700820923, 2.5135207176208496, 2.8274102210998535, 3.160696268081665, 3.501220941543579, 3.8431997299194336, 4.200472354888916, 4.574350357055664, 4.894090175628662, 5.0936360359191895, 5.216364860534668, 5.390469074249268, 5.586197853088379, 5.784314155578613, 5.985593795776367, 6.1828765869140625, 6.373883247375488, 6.556783199310303, 6.733740329742432, 6.906088829040527, 7.071183204650879, 7.233142852783203, 7.3868231773376465, 7.530625343322754, 7.665377616882324, 7.797634124755859, 7.930730819702148, 8.059279441833496, 8.180848121643066, 8.296680450439453, 8.406368255615234, 8.505520820617676, 8.589674949645996, 8.655287742614746, 8.70052719116211, 8.722027778625488, 8.70865249633789, 8.652679443359375, 8.560135841369629, 8.443024635314941, 8.307100296020508, 8.149582862854004, 7.971302032470703, 7.780361175537109, 7.575259685516357, 7.355491638183594, 7.124767303466797, 6.885737419128418, 6.638427257537842, 6.395895481109619, 6.166090488433838, 5.953654766082764, 5.738729953765869, 5.529703140258789, 5.342148303985596, 5.179572105407715, 5.024766445159912, 4.851255416870117, 4.646117210388184, 4.430662155151367, 4.217848777770996, 4.0131144523620605, 3.7878849506378174, 3.559556245803833, 3.3353841304779053, 3.1190574169158936, 2.9180359840393066, 2.7267343997955322, 2.5381720066070557, 2.3227102756500244, 2.0959630012512207, 1.8809078931808472, 1.6847819089889526, 1.495663046836853, 1.3055880069732666, 1.1171165704727173, 0.9520562887191772, 0.8042331337928772, 0.681337833404541, 0.5795820951461792, 0.5025584101676941, 0.46133852005004883, 0.4328932762145996, 0.3858243227005005, 0.3234015107154846, 0.2624247372150421, 0.19709435105323792, 0.15313704311847687, 0.11826862394809723, 0.08544927090406418, 0.04712279140949249, 0.0015682056546211243, -0.026410788297653198, -0.03486667573451996, -0.027389593422412872, -0.0065015703439712524, 0.0059362053871154785, 0.002570606768131256, -0.006264716386795044, -0.013282939791679382, -0.018584154546260834, -0.022372961044311523, -0.0232115238904953, -0.02133723348379135, -0.030498042702674866, -0.057736508548259735, -0.09805164486169815, -0.13833804428577423, -0.17615404725074768, -0.21290594339370728, -0.24737012386322021, -0.26589956879615784, -0.2773838937282562, -0.2822290062904358, -0.2861996591091156, -0.2940981388092041, -0.2990141808986664, -0.3035801351070404, -0.3050832152366638, -0.3049992024898529, -0.30373987555503845, -0.3003387153148651, -0.29614898562431335, -0.2985635995864868, -0.31389492750167847, -0.34401920437812805, -0.3844596743583679, -0.4300534129142761, -0.4741150140762329, -0.5105020999908447, -0.5354415774345398, -0.552415132522583, -0.5600359439849854, -0.5654557943344116, -0.5681073665618896, -0.5666967630386353, -0.5622239112854004, -0.5597591996192932, -0.5650179386138916, -0.579081654548645, -0.5969113707542419, -0.6101321578025818, -0.622231125831604, -0.6340838074684143, -0.6458472609519958, -0.657522976398468, -0.6685013771057129, -0.6801296472549438, -0.6912583708763123, -0.7032382488250732, -0.7155491709709167, -0.7265709042549133, -0.7348979115486145, -0.7445682287216187, -0.7536845207214355, -0.761847198009491, -0.7706142067909241, -0.7806366682052612, -0.7898868322372437, -0.7978246212005615, -0.8051745295524597, -0.8114349842071533, -0.8171375393867493, -0.821597158908844, -0.8264663219451904, -0.8312869071960449, -0.8363567590713501, -0.8399266004562378, -0.8434712290763855, -0.8482410907745361, -0.8517320156097412, -0.8557907342910767, -0.8605977296829224, -0.864855170249939, -0.8680832982063293, -0.869952917098999, -0.8720065951347351, -0.8741781711578369, -0.8759156465530396, -0.8775535821914673, -0.8793764710426331, -0.8817098140716553, -0.8832718729972839, -0.8847836852073669, -0.8870889544487, -0.8891378045082092, -0.8896875977516174, -0.8895387649536133, -0.8889559507369995, -0.8881706595420837, -0.8874912261962891, -0.8865614533424377, -0.8851791024208069, -0.8832001686096191, -0.8809881806373596, -0.8781297206878662, -0.8746054172515869, -0.8718098402023315, -0.8688086271286011)
					Y = (0.24426956474781036, 0.4990326166152954, 0.819128692150116, 1.153626799583435, 1.5026447772979736, 1.8859440088272095, 2.373248815536499, 2.968236207962036, 3.61586332321167, 4.355114459991455, 5.173743724822998, 6.038478374481201, 6.951005458831787, 7.899267673492432, 8.918261528015137, 10.051026344299316, 11.312947273254395, 12.90755558013916, 14.871548652648926, 17.198680877685547, 19.908754348754883, 22.898487091064453, 26.10063934326172, 29.397844314575195, 32.636375427246094, 35.74137878417969, 38.707183837890625, 41.484439849853516, 44.07951736450195, 46.60736846923828, 49.15201187133789, 51.65317916870117, 54.06341552734375, 56.4561882019043, 58.852813720703125, 61.29132080078125, 63.84211730957031, 66.49172973632812, 69.07376861572266, 71.62057495117188, 74.08918762207031, 76.49169158935547, 78.78299713134766, 80.95753479003906, 83.06936645507812, 85.1029281616211, 87.12429809570312, 89.12969970703125, 91.03314971923828, 92.87902069091797, 94.55635070800781, 96.09061431884766, 97.33863830566406, 98.26770782470703, 98.91900634765625, 99.34143829345703, 99.79500579833984, 100.22048950195312, 100.46652221679688, 100.50714111328125, 100.43055725097656, 100.3218765258789, 100.27439880371094, 100.24840545654297, 100.22171020507812, 100.19712829589844, 100.16851043701172, 100.09687042236328, 100.02641296386719, 99.95970153808594, 99.8285140991211, 99.58265686035156, 99.25724792480469, 98.94861602783203, 98.7610855102539, 98.6032943725586, 98.43841552734375, 98.27819061279297, 98.11662292480469, 97.93367004394531, 97.72758483886719, 97.4378662109375, 97.10028839111328, 96.74153900146484, 96.36189270019531, 95.95005798339844, 95.50723266601562, 95.01679229736328, 94.47090911865234, 93.8803482055664, 93.24833679199219, 92.5796127319336, 91.90768432617188, 91.14244079589844, 90.31917572021484, 89.48597717285156, 88.64861297607422, 87.82418823242188, 87.01628875732422, 86.22871398925781, 85.56230163574219, 84.96900177001953, 84.57625579833984, 84.36016082763672, 84.20700073242188, 84.08193969726562, 83.97764587402344, 83.87611389160156, 83.92423248291016, 84.14193725585938, 84.41809844970703, 84.70330810546875, 85.00025939941406, 85.29436492919922, 85.68895721435547, 86.27693176269531, 87.06804656982422, 88.0323715209961, 89.15747833251953, 90.61774444580078, 92.43035125732422, 94.46464538574219, 96.57106018066406, 98.82080078125, 101.0973129272461, 103.33666229248047, 105.50848388671875, 107.6570053100586, 109.891357421875, 112.15137481689453, 114.42011260986328, 116.68489074707031, 118.90473175048828, 121.11170959472656, 123.25049591064453, 125.32403564453125, 127.53121185302734, 129.89825439453125, 132.2855987548828, 134.6158905029297, 136.92697143554688, 139.15802001953125, 141.3134002685547, 143.4351806640625, 145.5569305419922, 147.65158081054688, 149.7096405029297, 151.71261596679688, 153.65261840820312, 155.51608276367188, 157.31924438476562, 159.11117553710938, 160.7533416748047, 162.2732696533203, 163.74002075195312, 165.19287109375, 166.6624298095703, 168.05679321289062, 169.36721801757812, 170.6645965576172, 171.94862365722656, 173.23680114746094, 174.46946716308594, 175.60227966308594, 176.68606567382812, 177.7667236328125, 178.8304901123047, 179.89537048339844, 180.9698944091797, 182.1023712158203, 183.38099670410156, 184.83396911621094, 186.4405059814453, 188.17733764648438, 190.03277587890625, 191.99041748046875, 193.9769287109375, 195.76626586914062, 197.2998809814453, 198.64427185058594, 199.84442138671875, 201.0236358642578, 202.19769287109375, 203.31591796875, 204.40118408203125, 205.4407196044922, 206.46392822265625, 207.45944213867188, 208.4150848388672, 209.36993408203125, 210.36520385742188, 211.35165405273438, 212.19497680664062, 212.80360412597656, 212.99081420898438, 212.8595428466797, 212.59893798828125, 212.30372619628906, 211.88113403320312, 211.2249298095703, 210.27505493164062, 209.16802978515625, 207.95042419433594, 206.6737060546875, 205.3536376953125, 203.98805236816406, 202.4827117919922, 200.79603576660156, 198.84075927734375, 196.52613830566406, 193.94662475585938, 191.1892852783203, 188.33187866210938, 185.4967803955078, 182.7758331298828, 180.3319091796875, 178.08534240722656, 175.87472534179688, 173.57350158691406, 171.1052703857422, 168.51658630371094, 165.9554443359375, 163.4188995361328, 160.97314453125, 158.5869903564453, 156.26071166992188, 154.0010223388672, 151.86273193359375, 149.84214782714844, 147.8561553955078, 145.87100219726562, 143.8812255859375, 141.9394073486328, 140.04071044921875, 138.22088623046875, 136.38259887695312, 134.54953002929688, 132.78271484375, 130.9574737548828, 129.08750915527344, 127.25975799560547, 125.4315185546875, 123.64933013916016, 121.882080078125, 120.05531311035156, 118.18463134765625, 116.25498962402344, 114.34269714355469, 112.4908447265625, 110.6985092163086, 108.94164276123047, 107.16153717041016, 105.32911682128906, 103.44462585449219, 101.6138916015625, 99.76459503173828, 97.91300964355469, 96.16510772705078, 94.41311645507812, 92.58258056640625, 90.4946517944336, 88.02781677246094, 85.19628143310547, 82.00907135009766, 78.48986053466797, 74.69635772705078, 70.86166381835938, 67.15168762207031, 63.572113037109375, 60.10674285888672, 56.803375244140625, 53.6189079284668, 50.549373626708984, 47.61164474487305, 44.77302932739258, 41.92876434326172, 39.06986999511719, 36.2219352722168, 33.32758331298828, 30.242610931396484, 26.973918914794922, 23.662368774414062, 20.41046714782715, 17.231449127197266, 14.126823425292969, 11.168815612792969, 8.347853660583496, 5.706920623779297, 3.3018741607666016, 1.2335699796676636, -0.5328974723815918, -2.043576717376709, -3.110535144805908, -3.740983486175537, -4.098943710327148, -4.4906511306762695, -4.8972249031066895, -5.2530198097229, -5.577995777130127, -5.934023857116699, -6.255759239196777, -6.630918025970459, -7.013139724731445, -7.412384033203125, -7.725191116333008, -8.017799377441406, -8.335323333740234, -8.662646293640137, -9.008383750915527, -9.383427619934082, -9.718378067016602, -10.013775825500488, -10.301630973815918, -10.562592506408691, -10.815587997436523, -11.065951347351074, -11.301687240600586, -11.448249816894531, -11.537090301513672, -11.524465560913086, -11.443005561828613, -11.383244514465332, -11.339241981506348, -11.295818328857422, -11.257658004760742, -11.223909378051758, -11.219079971313477, -11.304905891418457, -11.446738243103027, -11.616390228271484, -11.812542915344238, -12.02774429321289, -12.266841888427734, -12.534515380859375, -12.815123558044434, -13.006359100341797, -13.117430686950684, -13.182148933410645, -13.210461616516113, -13.223767280578613, -13.236565589904785, -13.257308006286621, -13.364906311035156, -13.60283374786377, -13.906349182128906, -14.247852325439453, -14.630463600158691, -15.034890174865723, -15.458684921264648, -15.909191131591797, -16.372478485107422, -16.83634376525879, -17.298728942871094, -17.954330444335938, -18.74985694885254, -19.579227447509766, -20.42566680908203, -21.43193817138672, -22.800357818603516, -24.44293212890625, -26.13048553466797, -27.82823944091797, -29.55722427368164, -31.477741241455078, -33.487709045410156, -35.511478424072266, -37.493263244628906, -39.456016540527344, -41.433685302734375, -43.504295349121094, -45.86669158935547, -48.45779037475586, -51.14822006225586, -53.83092498779297, -56.52829360961914, -59.291015625, -62.107452392578125, -64.86852264404297, -67.60960388183594, -70.36067199707031, -73.03939819335938, -75.66210174560547, -78.23661041259766, -80.80587005615234, -83.38500213623047, -85.95026397705078, -88.392578125, -90.68785095214844, -92.96864318847656, -95.2093505859375, -97.35236358642578, -99.36150360107422, -101.18042755126953, -102.92134857177734, -104.60369110107422, -106.27859497070312, -107.93692779541016, -109.50454711914062, -110.95790100097656, -112.26480102539062, -113.4476318359375, -114.55032348632812, -115.59841918945312, -116.59353637695312, -117.56787872314453, -118.43424987792969, -119.07018280029297, -119.529541015625, -119.9432144165039, -120.33118438720703, -120.70291137695312, -121.06876373291016, -121.57264709472656, -122.14915466308594, -122.72602844238281, -123.31329345703125, -123.84371948242188, -124.38484191894531, -124.94699096679688, -125.50639343261719, -126.06773376464844, -126.62725067138672, -127.21639251708984, -127.76771545410156, -128.14712524414062, -128.24986267089844, -128.0001220703125, -127.45743560791016, -126.70941925048828, -125.85266876220703, -124.98062133789062, -124.1561508178711, -123.36287689208984, -122.56819915771484, -121.65084838867188, -120.66740417480469, -119.70370483398438, -118.76301574707031, -117.76809692382812, -116.55887603759766, -115.09596252441406, -113.52935028076172, -111.99527740478516, -110.50000762939453, -108.9967041015625, -107.39553833007812, -105.7052001953125, -103.86796569824219, -101.89085388183594, -99.83897399902344, -97.75530242919922, -95.71993255615234, -93.73746490478516, -91.82310485839844, -89.95047760009766, -88.10604858398438, -86.26592254638672, -84.39051818847656, -82.42990112304688, -80.4601821899414, -78.54206085205078, -76.67953491210938, -74.87965393066406, -73.13782501220703, -71.447998046875, -69.79700469970703, -68.07174682617188, -66.20356750488281, -64.17756652832031, -62.02452850341797, -59.78955841064453, -57.599979400634766, -55.49079895019531, -53.38170623779297, -51.32799530029297, -49.24906539916992, -47.25999069213867, -45.2713508605957, -43.23389434814453, -41.17817687988281, -39.17205047607422, -37.22850799560547, -35.21967697143555, -33.25495910644531, -31.328039169311523, -29.30510902404785, -27.14748191833496, -24.93663215637207, -22.68917465209961, -20.511201858520508, -18.440406799316406, -16.442750930786133, -14.476696014404297, -12.49740982055664, -10.538829803466797, -8.549440383911133, -6.5612688064575195, -4.653802394866943, -2.830416679382324, -1.0931862592697144)
					Xmap = [-215.266 -214.266 -213.266 -212.266 -211.266 -210.266 -209.266 -208.266 -207.266 -206.266 -205.266 -204.266 -203.266 -202.266 -201.266 -200.266 -199.266 -198.266 -197.266 -196.266 -195.266 -194.266 -193.266 -192.266 -191.266 -190.266 -189.266 -188.266 -187.266 -186.266 -185.266 -184.266 -183.266 -182.266 -181.266 -180.266 -179.266 -178.266 -177.266 -176.266 -175.266 -174.266 -173.266 -172.266 -171.266 -170.266 -169.266 -168.266 -167.266 -166.266 -165.266 -164.266 -163.266 -162.266 -161.266 -160.266 -159.266 -158.266 -157.266 -156.266 -155.266 -154.266 -153.266 -152.266 -151.266 -150.266 -149.266 -148.266 -147.266 -146.266 -145.266 -144.266 -143.266 -142.266 -141.266 -140.266 -139.266 -138.266 -137.266 -136.266 -135.266 -134.266 -133.266 -132.266 -131.266 -130.266 -129.266 -128.266 -127.266 -126.266 -125.266 -124.266 -123.266 -122.266 -121.266 -120.266 -119.266 -118.266 -117.266 -116.266 -115.266 -114.266 -113.266 -112.266 -111.266 -110.266 -109.266 -108.266 -107.266 -106.266 -105.266 -104.266 -103.266 -102.266 -101.266 -100.266  -99.266  -98.266  -97.266  -96.266  -95.266  -94.266  -93.266  -92.266  -91.266  -90.266  -89.266  -88.266  -87.266  -86.266  -85.266  -84.266  -83.266  -82.266  -81.266  -80.266  -79.266  -78.266  -77.266  -76.266  -75.266  -74.266  -73.266  -72.266  -71.266  -70.266  -69.266  -68.266  -67.266  -66.266  -65.266  -64.266  -63.266  -62.266  -61.266  -60.266  -59.266  -58.266  -57.266  -56.266  -55.266  -54.266  -53.266  -52.266  -51.266  -50.266  -49.266  -48.266  -47.266  -46.266  -45.266  -44.266  -43.266  -42.266  -41.266  -40.266  -39.266  -38.266  -37.266  -36.266  -35.266  -34.266  -33.266  -32.266  -31.266  -30.266  -29.266  -28.266  -27.266  -26.266  -25.266  -24.266  -23.266  -22.266  -21.266  -20.266  -19.266  -18.266  -17.266  -16.266  -15.266  -14.266  -13.266  -12.266  -11.266  -10.266   -9.266   -8.266   -7.266   -6.266   -5.266   -4.266   -3.266   -2.266   -1.266   -0.266    0.734    1.734    2.734    3.734    4.734    5.734
					    6.734    7.734    8.734    9.734   10.734   11.734   12.734   13.734   14.734   15.734   16.734   17.734   18.734   19.734   20.734   21.734   22.734   23.734   24.734   25.734   26.734   27.734   28.734   29.734   30.734   31.734   32.734   33.734   34.734   35.734   36.734   37.734   38.734   39.734   40.734   41.734   42.734   43.734   44.734   45.734   46.734   47.734   48.734   49.734   50.734   51.734   52.734   53.734   54.734   55.734   56.734   57.734   58.734   59.734   60.734   61.734   62.734   63.734   64.734   65.734   66.734   67.734   68.734   69.734   70.734   71.734   72.734   73.734   74.734   75.734   76.734   77.734   78.734   79.734   80.734   81.734   82.734   83.734   84.734   85.734   86.734   87.734   88.734   89.734   90.734   91.734   92.734   93.734   94.734   95.734   96.734   97.734   98.734   99.734  100.734  101.734  102.734  103.734  104.734  105.734  106.734  107.734  108.734  109.734  110.734  111.734  112.734  113.734  114.734  115.734  116.734  117.734  118.734  119.734  120.734  121.734  122.734  123.734  124.734  125.734  126.734  127.734  128.734  129.734  130.734  131.734  132.734  133.734  134.734  135.734  136.734  137.734  138.734  139.734  140.734  141.734  142.734  143.734  144.734  145.734  146.734  147.734  148.734  149.734  150.734  151.734  152.734  153.734  154.734  155.734  156.734  157.734  158.734  159.734  160.734  161.734  162.734  163.734  164.734  165.734  166.734  167.734  168.734  169.734  170.734  171.734  172.734  173.734  174.734  175.734  176.734  177.734  178.734  179.734  180.734  181.734  182.734  183.734  184.734  185.734  186.734  187.734  188.734  189.734  190.734  191.734  192.734  193.734  194.734  195.734  196.734  197.734  198.734  199.734  200.734  201.734  202.734  203.734  204.734  205.734  206.734  207.734  208.734  209.734]
					Ymap = [-1.782e+02 -1.772e+02 -1.762e+02 -1.752e+02 -1.742e+02 -1.732e+02 -1.722e+02 -1.712e+02 -1.702e+02 -1.692e+02 -1.682e+02 -1.672e+02 -1.662e+02 -1.652e+02 -1.642e+02 -1.632e+02 -1.622e+02 -1.612e+02 -1.602e+02 -1.592e+02 -1.582e+02 -1.572e+02 -1.562e+02 -1.552e+02 -1.542e+02 -1.532e+02 -1.522e+02 -1.512e+02 -1.502e+02 -1.492e+02 -1.482e+02 -1.472e+02 -1.462e+02 -1.452e+02 -1.442e+02 -1.432e+02 -1.422e+02 -1.412e+02 -1.402e+02 -1.392e+02 -1.382e+02 -1.372e+02 -1.362e+02 -1.352e+02 -1.342e+02 -1.332e+02 -1.322e+02 -1.312e+02 -1.302e+02 -1.292e+02 -1.282e+02 -1.272e+02 -1.262e+02 -1.252e+02 -1.242e+02 -1.232e+02 -1.222e+02 -1.212e+02 -1.202e+02 -1.192e+02 -1.182e+02 -1.172e+02 -1.162e+02 -1.152e+02 -1.142e+02 -1.132e+02 -1.122e+02 -1.112e+02 -1.102e+02 -1.092e+02 -1.082e+02 -1.072e+02 -1.062e+02 -1.052e+02 -1.042e+02 -1.032e+02 -1.022e+02 -1.012e+02 -1.002e+02 -9.925e+01 -9.825e+01 -9.725e+01 -9.625e+01 -9.525e+01 -9.425e+01 -9.325e+01 -9.225e+01 -9.125e+01 -9.025e+01 -8.925e+01 -8.825e+01 -8.725e+01 -8.625e+01 -8.525e+01 -8.425e+01 -8.325e+01 -8.225e+01 -8.125e+01 -8.025e+01 -7.925e+01 -7.825e+01 -7.725e+01 -7.625e+01 -7.525e+01 -7.425e+01 -7.325e+01 -7.225e+01 -7.125e+01 -7.025e+01 -6.925e+01 -6.825e+01 -6.725e+01 -6.625e+01 -6.525e+01 -6.425e+01 -6.325e+01 -6.225e+01 -6.125e+01 -6.025e+01 -5.925e+01 -5.825e+01 -5.725e+01 -5.625e+01 -5.525e+01 -5.425e+01 -5.325e+01 -5.225e+01 -5.125e+01 -5.025e+01 -4.925e+01 -4.825e+01 -4.725e+01 -4.625e+01 -4.525e+01 -4.425e+01 -4.325e+01 -4.225e+01 -4.125e+01 -4.025e+01 -3.925e+01 -3.825e+01 -3.725e+01 -3.625e+01 -3.525e+01 -3.425e+01 -3.325e+01 -3.225e+01 -3.125e+01 -3.025e+01 -2.925e+01 -2.825e+01 -2.725e+01 -2.625e+01 -2.525e+01 -2.425e+01 -2.325e+01 -2.225e+01 -2.125e+01 -2.025e+01 -1.925e+01 -1.825e+01 -1.725e+01 -1.625e+01 -1.525e+01 -1.425e+01 -1.325e+01 -1.225e+01 -1.125e+01 -1.025e+01 -9.250e+00 -8.250e+00 -7.250e+00 -6.250e+00 -5.250e+00 -4.250e+00 -3.250e+00 -2.250e+00 -1.250e+00 -2.499e-01  7.501e-01  1.750e+00
					  2.750e+00  3.750e+00  4.750e+00  5.750e+00  6.750e+00  7.750e+00  8.750e+00  9.750e+00  1.075e+01  1.175e+01  1.275e+01  1.375e+01  1.475e+01  1.575e+01  1.675e+01  1.775e+01  1.875e+01  1.975e+01  2.075e+01  2.175e+01  2.275e+01  2.375e+01  2.475e+01  2.575e+01  2.675e+01  2.775e+01  2.875e+01  2.975e+01  3.075e+01  3.175e+01  3.275e+01  3.375e+01  3.475e+01  3.575e+01  3.675e+01  3.775e+01  3.875e+01  3.975e+01  4.075e+01  4.175e+01  4.275e+01  4.375e+01  4.475e+01  4.575e+01  4.675e+01  4.775e+01  4.875e+01  4.975e+01  5.075e+01  5.175e+01  5.275e+01  5.375e+01  5.475e+01  5.575e+01  5.675e+01  5.775e+01  5.875e+01  5.975e+01  6.075e+01  6.175e+01  6.275e+01  6.375e+01  6.475e+01  6.575e+01  6.675e+01  6.775e+01  6.875e+01  6.975e+01  7.075e+01  7.175e+01  7.275e+01  7.375e+01  7.475e+01  7.575e+01  7.675e+01  7.775e+01  7.875e+01  7.975e+01  8.075e+01  8.175e+01  8.275e+01  8.375e+01  8.475e+01  8.575e+01  8.675e+01  8.775e+01  8.875e+01  8.975e+01  9.075e+01  9.175e+01  9.275e+01  9.375e+01  9.475e+01  9.575e+01  9.675e+01  9.775e+01  9.875e+01  9.975e+01  1.008e+02  1.018e+02  1.028e+02  1.038e+02  1.048e+02  1.058e+02  1.068e+02  1.078e+02  1.088e+02  1.098e+02  1.108e+02  1.118e+02  1.128e+02  1.138e+02  1.148e+02  1.158e+02  1.168e+02  1.178e+02  1.188e+02  1.198e+02  1.208e+02  1.218e+02  1.228e+02  1.238e+02  1.248e+02  1.258e+02  1.268e+02  1.278e+02  1.288e+02  1.298e+02  1.308e+02  1.318e+02  1.328e+02  1.338e+02  1.348e+02  1.358e+02  1.368e+02  1.378e+02  1.388e+02  1.398e+02  1.408e+02  1.418e+02  1.428e+02  1.438e+02  1.448e+02  1.458e+02  1.468e+02  1.478e+02  1.488e+02  1.498e+02  1.508e+02  1.518e+02  1.528e+02  1.538e+02  1.548e+02  1.558e+02  1.568e+02  1.578e+02  1.588e+02  1.598e+02  1.608e+02  1.618e+02  1.628e+02  1.638e+02  1.648e+02  1.658e+02  1.668e+02  1.678e+02  1.688e+02  1.698e+02  1.708e+02  1.718e+02  1.728e+02  1.738e+02  1.748e+02  1.758e+02  1.768e+02  1.778e+02  1.788e+02  1.798e+02  1.808e+02  1.818e+02  1.828e+02
					  1.838e+02  1.848e+02  1.858e+02  1.868e+02  1.878e+02  1.888e+02  1.898e+02  1.908e+02  1.918e+02  1.928e+02  1.938e+02  1.948e+02  1.958e+02  1.968e+02  1.978e+02  1.988e+02  1.998e+02  2.008e+02  2.018e+02  2.028e+02  2.038e+02  2.048e+02  2.058e+02  2.068e+02  2.078e+02  2.088e+02  2.098e+02  2.108e+02  2.118e+02  2.128e+02  2.138e+02  2.148e+02  2.158e+02  2.168e+02  2.178e+02  2.188e+02  2.198e+02  2.208e+02  2.218e+02  2.228e+02  2.238e+02  2.248e+02  2.258e+02  2.268e+02  2.278e+02  2.288e+02  2.298e+02  2.308e+02  2.318e+02  2.328e+02  2.338e+02  2.348e+02  2.358e+02  2.368e+02  2.378e+02  2.388e+02  2.398e+02  2.408e+02  2.418e+02  2.428e+02  2.438e+02  2.448e+02  2.458e+02  2.468e+02  2.478e+02  2.488e+02  2.498e+02  2.508e+02  2.518e+02  2.528e+02  2.538e+02  2.548e+02  2.558e+02  2.568e+02  2.578e+02  2.588e+02  2.598e+02  2.608e+02  2.618e+02  2.628e+02]
					Zmap = [-5.894 -4.894 -3.894 -2.894 -1.894 -0.894  0.106  1.106  2.106  3.106  4.106  5.106  6.106  7.106  8.106  9.106 10.106 11.106 12.106 13.106]
					point_map = [[[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]]
					
					 [[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 [[291 291 291 ... 292 292 292]
					  [291 291 291 ... 291 292 292]
					  [291 291 291 ... 291 291 291]
					  ...
					  [162 162 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 ...
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [394 394 394 ... 394 394 394]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]]
					res = 1
					min_point = [-215.266 -178.250   -5.894]
					max_point = [ 209.734  262.750   13.106]
				X = [-215.266 -215.166 -215.066 ...  210.034  210.134  210.234]
				Y = [-178.250 -178.150 -178.050 ...  262.750  262.850  262.950]
				Z = [-0.894  8.722]
				cost_map = [[[ 214.381  214.381]
				  [ 214.299  214.299]
				  [ 214.217  214.217]
				  ...
				  [ 112.184  112.184]
				  [ 112.264  112.264]
				  [ 112.344  112.344]]
				
				 [[ 214.324  214.324]
				  [ 214.242  214.242]
				  [ 214.160  214.160]
				  ...
				  [ 112.124  112.124]
				  [ 112.204  112.204]
				  [ 112.284  112.284]]
				
				 [[ 214.267  214.267]
				  [ 214.185  214.185]
				  [ 214.103  214.103]
				  ...
				  [ 112.064  112.064]
				  [ 112.144  112.144]
				  [ 112.224  112.224]]
				
				 ...
				
				 [[  96.764   96.764]
				  [  96.690   96.690]
				  [  96.616   96.616]
				  ...
				  [ 242.661  242.661]
				  [ 242.689  242.689]
				  [ 242.717  242.717]]
				
				 [[  96.831   96.831]
				  [  96.757   96.757]
				  [  96.683   96.683]
				  ...
				  [ 242.757  242.757]
				  [ 242.785  242.785]
				  [ 242.813  242.813]]
				
				 [[  96.898   96.898]
				  [  96.824   96.824]
				  [  96.750   96.750]
				  ...
				  [ 242.852  242.852]
				  [ 242.881  242.881]
				  [ 242.909  242.909]]]
				res = 0.1
				min_point = [-215.266 -178.250   -0.894]
				max_point = [ 210.234  262.950    8.722]
				src = 
						def get_cost(self, state, prevstate=None):
							prevstate = state if prevstate is None else prevstate
							prevpos = prevstate["pos"][...,[0,2,1]]
							pos = state["pos"][...,[0,2,1]]
							vy = state["vel"][...,-1]
							cost = self.get_point_cost(pos, transform=True)
							progress = self.track.get_progress(prevpos, pos)
							reward = np.minimum(progress,0) + 2*progress + np.tanh(vy/self.vtarget)-np.power(self.vtarget-vy,2)/self.vtarget**2 - cost
							# reward = progress + np.tanh(vy/self.vtarget) - cost
				
				vtarget = 20
			action_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -1.000]
				high = [ 1.000  1.000  1.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			cost_queries = <list len=25>
			dynamics_size = 13
			obs = [ 1.617e-09 -3.908e-03 -7.273e-09  1.777e-12 -1.954e-01  3.555e-13  0.000e+00  0.000e+00  0.000e+00  1.000e+00  9.095e-13 -1.164e-10 -4.547e-12  0.000e+00  2.000e-02  3.657e-01  4.017e-01  4.572e-01  5.260e-01  6.036e-01  2.700e-01  3.171e-01  3.850e-01  4.646e-01  5.509e-01  1.792e-01  2.444e-01  3.277e-01  4.184e-01  5.125e-01  1.063e-01  1.973e-01  2.942e-01  3.927e-01  4.918e-01  1.024e-01  1.953e-01  2.929e-01  3.917e-01  4.910e-01]
			observation_space = Box(80,) 
				dtype = float32
				shape = (80,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
			src = 		return state
				
					def step(self, action):
						self.time += 1
						next_state, reward, done, info = self.env.step(action)
						idle = next_state[29]
						done = done or idle>self.idle_timeout or self.time > self.max_time
						next_state, next_spec = self.observation(next_state)
						terminal = -(1-self.time/self.max_time)*int(done)
						reward = -self.cost_model.get_cost(next_spec, self.spec) + terminal
						self.spec = next_spec
			
			max_time = 500
			time = 0
			idle_timeout = 10
			spec = EnvSpec(CarRacing-v1) 
				id = CarRacing-v1
				entry_point = <class 'src.envs.CarRacing.car_racing.CarRacing'> 
					reset = <function CarRacing.reset at 0x7fdd4086f680>
					step = <function CarRacing.step at 0x7fdd4086f5f0>
					render = <function CarRacing.render at 0x7fdd68cd4b90>
					dynamics_spec = <staticmethod object at 0x7fdd40870c10>
					track_spec = <function CarRacing.track_spec at 0x7fdd68cd4cb0>
					observation = <function CarRacing.observation at 0x7fdd68cd4d40>
					dynamics_keys = <staticmethod object at 0x7fdd40870b10>
					observation_spec = <staticmethod object at 0x7fdd40870b50>
					close = <function CarRacing.close at 0x7fdd68cd4ef0>
					id = 2
				reward_threshold = None
				nondeterministic = False
				max_episode_steps = None
			verbose = 0
		action_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -1.000]
			high = [ 1.000  1.000  1.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		observation_space = Box(80,) 
			dtype = float32
			shape = (80,)
			low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
			high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
			bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': []}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7fdcc08ae7d0> 
			observation_space = Box(80,) 
				dtype = float32
				shape = (80,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
	state_size = (80,)
	action_size = (3,)
	action_space = Box(3,) 
		dtype = float32
		shape = (3,)
		low = [-1.000 -1.000 -1.000]
		high = [ 1.000  1.000  1.000]
		bounded_below = [ True  True  True]
		bounded_above = [ True  True  True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7fdcc08aec10> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {11001: <socket.socket fd=35, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 43604), raddr=('127.0.0.1', 11001)>, 11002: <socket.socket fd=36, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42512), raddr=('127.0.0.1', 11002)>, 11003: <socket.socket fd=37, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 51532), raddr=('127.0.0.1', 11003)>, 11004: <socket.socket fd=38, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 56124), raddr=('127.0.0.1', 11004)>, 11005: <socket.socket fd=39, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 57056), raddr=('127.0.0.1', 11005)>, 11006: <socket.socket fd=40, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48302), raddr=('127.0.0.1', 11006)>, 11007: <socket.socket fd=41, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 34254), raddr=('127.0.0.1', 11007)>, 11008: <socket.socket fd=42, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42072), raddr=('127.0.0.1', 11008)>, 11009: <socket.socket fd=43, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 51122), raddr=('127.0.0.1', 11009)>, 11010: <socket.socket fd=44, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 45712), raddr=('127.0.0.1', 11010)>, 11011: <socket.socket fd=45, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 54640), raddr=('127.0.0.1', 11011)>, 11012: <socket.socket fd=46, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 51526), raddr=('127.0.0.1', 11012)>, 11013: <socket.socket fd=47, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 51066), raddr=('127.0.0.1', 11013)>, 11014: <socket.socket fd=48, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46998), raddr=('127.0.0.1', 11014)>, 11015: <socket.socket fd=49, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42678), raddr=('127.0.0.1', 11015)>, 11016: <socket.socket fd=50, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 50052), raddr=('127.0.0.1', 11016)>}
	num_envs = 16
	max_steps = 1000,
agent: <src.models.wrappers.ParallelAgent object at 0x7fdcc08ae510> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7fdcc090acd0> 
		state_size = (80,)
	agent = <src.models.pytorch.agents.ppo.PPOAgent object at 0x7fdcc090ab10> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7fdcc090ac50> 
			size = (3,)
			dt = 0.2
			action = [ 0.744  0.460  1.000]
			daction_dt = [ 1.705  0.025  0.553]
		discrete = False
		action_size = (3,)
		state_size = (80,)
		config = <src.utils.config.Config object at 0x7fdcc80cc510> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.998
			NUM_STEPS = 500
			MAX_BUFFER_SIZE = 100000
			REPLAY_BATCH_SIZE = 32
			TARGET_UPDATE_RATE = 0.0004
			BATCH_SIZE = 32
			PPO_EPOCHS = 2
			ENTROPY_WEIGHT = 0.01
			CLIP_PARAM = 0.05
			dynamics_size = 13
			state_size = (80,)
			action_size = (3,)
			env_name = CarRacing-v1
			rank = 0
			size = 17
			split = 17
			model = ppo
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 1000000
			render = False
			trial = False
			icm = False
			rs = False
		stats = <src.utils.logger.Stats object at 0x7fdcc08de050> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = PPONetwork(
			  (actor_local): PPOActor(
			    (layer1): Linear(in_features=80, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=3, bias=True)
			  )
			  (actor_target): PPOActor(
			    (layer1): Linear(in_features=80, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=3, bias=True)
			  )
			  (critic_local): PPOCritic(
			    (layer1): Linear(in_features=80, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=1024, bias=True)
			    (layer3): Linear(in_features=1024, out_features=1024, bias=True)
			    (value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			  (critic_target): PPOCritic(
			    (layer1): Linear(in_features=80, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=1024, bias=True)
			    (layer3): Linear(in_features=1024, out_features=1024, bias=True)
			    (value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			) 
			training = True
			tau = 0.0004
			name = ppo
			stats = <src.utils.logger.Stats object at 0x7fdcc806ff90> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7fdcc80cc510> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.998
				NUM_STEPS = 500
				MAX_BUFFER_SIZE = 100000
				REPLAY_BATCH_SIZE = 32
				TARGET_UPDATE_RATE = 0.0004
				BATCH_SIZE = 32
				PPO_EPOCHS = 2
				ENTROPY_WEIGHT = 0.01
				CLIP_PARAM = 0.05
				dynamics_size = 13
				state_size = (80,)
				action_size = (3,)
				env_name = CarRacing-v1
				rank = 0
				size = 17
				split = 17
				model = ppo
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 1000000
				render = False
				trial = False
				icm = False
				rs = False
			device = cuda
			src = ['class PPOActor(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config, use_discrete=False):\n\t\tsuper().__init__()\n\t\tinput_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN\n\t\tself.discrete = use_discrete and type(action_size) != tuple\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, actor_hidden)\n\t\tself.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)\n\t\tself.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\t\tself.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)\n\t\t\n\tdef forward(self, state, action_in=None, sample=True):\n\t\tstate = self.layer1(state).relu()\n\t\tstate = self.layer2(state).relu()\n\t\tstate = self.layer3(state).relu()\n\t\taction_mu = self.action_mu(state)\n\t\taction_sig = self.action_sig.exp().expand_as(action_mu)\n\t\tdist = self.dist(action_mu, action_sig)\n\t\taction = dist.sample() if action_in is None else action_in.argmax(-1) if self.discrete else action_in\n\t\taction_out = one_hot_from_indices(action, action_mu.size(-1)) if self.discrete else action\n\t\tlog_prob = dist.log_prob(action)\n\t\tentropy = dist.entropy()\n\t\treturn action_out, log_prob, entropy\n', 'class PPOCritic(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config):\n\t\tsuper().__init__()\n\t\tinput_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, critic_hidden)\n\t\tself.layer3 = torch.nn.Linear(critic_hidden, critic_hidden)\n\t\tself.value = torch.nn.Linear(critic_hidden, 1)\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\n\tdef forward(self, state):\n\t\tstate = self.layer1(state).relu()\n\t\tstate = self.layer2(state).relu()\n\t\tstate = self.layer3(state).relu()\n\t\tvalue = self.value(state)\n\t\treturn value\n']
			actor_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			critic_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7fdcc0870890> 
			buffer = deque([], maxlen=100000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7fdcc0870810> 
		size = (3,)
		dt = 0.2
		action = [ 0.505 -0.039 -0.102]
		daction_dt = [ 1.103  0.361  2.137]
	discrete = False
	action_size = (3,)
	state_size = (80,)
	config = <src.utils.config.Config object at 0x7fdcc80cc510> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		BATCH_SIZE = 32
		PPO_EPOCHS = 2
		ENTROPY_WEIGHT = 0.01
		CLIP_PARAM = 0.05
		dynamics_size = 13
		state_size = (80,)
		action_size = (3,)
		env_name = CarRacing-v1
		rank = 0
		size = 17
		split = 17
		model = ppo
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 1000000
		render = False
		trial = False
		icm = False
		rs = False
	stats = <src.utils.logger.Stats object at 0x7fdcc0870790> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import torch
import numpy as np
from .base import PTACNetwork, PTAgent, Conv, one_hot_from_indices
from src.utils.rand import ReplayBuffer, PrioritizedReplayBuffer

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size, config, use_discrete=False):
		super().__init__()
		input_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN
		self.discrete = use_discrete and type(action_size) != tuple
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, actor_hidden)
		self.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)
		self.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)
		
	def forward(self, state, action_in=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = self.dist(action_mu, action_sig)
		action = dist.sample() if action_in is None else action_in.argmax(-1) if self.discrete else action_in
		action_out = one_hot_from_indices(action, action_mu.size(-1)) if self.discrete else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_out, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		input_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, critic_hidden)
		self.layer3 = torch.nn.Linear(critic_hidden, critic_hidden)
		self.value = torch.nn.Linear(critic_hidden, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, config, actor=PPOActor, critic=PPOCritic, gpu=True, load=None, name="ppo"):
		super().__init__(state_size, action_size, config, actor=actor, critic=critic, gpu=gpu, load=load, name=name)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			action_or_entropy = action if action_in is None else entropy.mean()
			return (x.cpu().numpy() if numpy else x for x in [action_or_entropy, log_prob])

	def get_value(self, state, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device)).cpu().numpy() if numpy else self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, config):
		values = self.get_value(states, grad=True)
		critic_loss = (values - targets).pow(2).mean()
		self.step(self.critic_optimizer, critic_loss)

		entropy, new_log_probs = self.get_action_probs(states, actions, grad=True)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-config.CLIP_PARAM, 1.0+config.CLIP_PARAM)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + config.ENTROPY_WEIGHT*entropy).mean()
		self.step(self.actor_optimizer, actor_loss)
		self.stats.mean(critic_loss=critic_loss, actor_loss=actor_loss)

class PPOAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, PPONetwork, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		self.action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.config.NUM_STEPS:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			states = torch.cat([states, self.to_tensor(next_state).unsqueeze(0)], dim=0)
			values = self.network.get_value(states)
			targets, advantages = self.compute_gae(values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), values[:-1])
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states[:-1], actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*self.config.PPO_EPOCHS)//self.config.BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(self.config.BATCH_SIZE, torch.stack)
				self.network.optimize(state, action, log_prob, target, advantage, config=self.config)
				

Step:       0, Reward:  -390.013 [ 238.068], Avg:  -390.013 (1.000) <0-00:00:00> ({'r_t':    -1.1339, 'eps':     1.0000, 'eps_e':     1.0000})
Step:    1000, Reward:  -338.309 [ 170.600], Avg:  -364.161 (1.000) <0-00:00:33> ({'r_t': -1302.3133, 'eps':     1.0000, 'critic_loss':    41.6843, 'actor_loss':    17.9518, 'eps_e':     1.0000})
Step:    2000, Reward:  -305.332 [ 201.768], Avg:  -344.551 (1.000) <0-00:01:05> ({'r_t': -1228.6264, 'eps':     1.0000, 'critic_loss':    62.1126, 'actor_loss':     9.6720, 'eps_e':     1.0000})
Step:    3000, Reward:  -269.336 [ 137.302], Avg:  -325.747 (1.000) <0-00:01:35> ({'r_t': -1310.2303, 'eps':     1.0000, 'critic_loss':   117.9338, 'actor_loss':     6.4376, 'eps_e':     1.0000})
Step:    4000, Reward:  -250.005 [  89.468], Avg:  -310.599 (1.000) <0-00:02:07> ({'r_t': -1189.2257, 'eps':     1.0000, 'critic_loss':   159.2007, 'actor_loss':     1.0290, 'eps_e':     1.0000})
Step:    5000, Reward:  -161.083 [  95.468], Avg:  -285.679 (1.000) <0-00:02:36> ({'r_t': -1047.0485, 'eps':     1.0000, 'critic_loss':   138.3125, 'actor_loss':    -1.4385, 'eps_e':     1.0000})
Step:    6000, Reward:   -93.654 [  60.221], Avg:  -258.247 (1.000) <0-00:03:00> ({'r_t':  -999.2829, 'eps':     1.0000, 'critic_loss':    89.7714, 'actor_loss':     0.1330, 'eps_e':     1.0000})
Step:    7000, Reward:   -58.683 [  31.728], Avg:  -233.302 (1.000) <0-00:03:23> ({'r_t':  -885.9804, 'eps':     1.0000, 'critic_loss':    52.0730, 'actor_loss':     0.2402, 'eps_e':     1.0000})
Step:    8000, Reward:   -45.463 [  20.820], Avg:  -212.431 (1.000) <0-00:03:44> ({'r_t':  -770.9242, 'eps':     1.0000, 'critic_loss':    34.9366, 'actor_loss':    -0.2830, 'eps_e':     1.0000})
Step:    9000, Reward:   -32.018 [   6.977], Avg:  -194.389 (1.000) <0-00:04:05> ({'r_t':  -577.1665, 'eps':     1.0000, 'critic_loss':    13.5772, 'actor_loss':    -0.2130, 'eps_e':     1.0000})
Step:   10000, Reward:   -23.204 [   9.710], Avg:  -178.827 (1.000) <0-00:04:27> ({'r_t':  -477.2919, 'eps':     1.0000, 'critic_loss':     9.1757, 'actor_loss':     0.0635, 'eps_e':     1.0000})
Step:   11000, Reward:   -30.568 [  13.195], Avg:  -166.472 (1.000) <0-00:04:48> ({'r_t':  -374.2068, 'eps':     1.0000, 'critic_loss':     8.6485, 'actor_loss':     0.0750, 'eps_e':     1.0000})
Step:   12000, Reward:   -29.293 [  13.329], Avg:  -155.920 (1.000) <0-00:05:09> ({'r_t':  -350.5414, 'eps':     1.0000, 'critic_loss':    10.2021, 'actor_loss':    -0.1034, 'eps_e':     1.0000})
Step:   13000, Reward:   -24.726 [  13.765], Avg:  -146.549 (1.000) <0-00:05:30> ({'r_t':  -326.4839, 'eps':     1.0000, 'critic_loss':     7.2511, 'actor_loss':     0.0483, 'eps_e':     1.0000})
Step:   14000, Reward:   -16.725 [   5.926], Avg:  -137.894 (1.000) <0-00:05:51> ({'r_t':  -270.4126, 'eps':     1.0000, 'critic_loss':     4.3713, 'actor_loss':    -0.2201, 'eps_e':     1.0000})
Step:   15000, Reward:   -18.502 [   9.115], Avg:  -130.432 (1.000) <0-00:06:12> ({'r_t':  -234.0364, 'eps':     1.0000, 'critic_loss':     4.6918, 'actor_loss':     0.2089, 'eps_e':     1.0000})
Step:   16000, Reward:   -15.776 [   8.510], Avg:  -123.688 (1.000) <0-00:06:33> ({'r_t':  -237.4123, 'eps':     1.0000, 'critic_loss':     5.9614, 'actor_loss':    -0.0431, 'eps_e':     1.0000})
Step:   17000, Reward:   -10.747 [   7.403], Avg:  -117.413 (1.000) <0-00:06:54> ({'r_t':  -143.6984, 'eps':     1.0000, 'critic_loss':     4.3698, 'actor_loss':     0.0047, 'eps_e':     1.0000})
Step:   18000, Reward:    -6.150 [  10.440], Avg:  -111.557 (1.000) <0-00:07:16> ({'r_t':   -75.4835, 'eps':     1.0000, 'critic_loss':     5.9093, 'actor_loss':    -0.1220, 'eps_e':     1.0000})
Step:   19000, Reward:     4.284 [  15.080], Avg:  -105.765 (1.000) <0-00:07:37> ({'r_t':    -7.7587, 'eps':     1.0000, 'critic_loss':     6.9101, 'actor_loss':    -0.2466, 'eps_e':     1.0000})
Step:   20000, Reward:    11.715 [   4.460], Avg:  -100.171 (1.000) <0-00:07:58> ({'r_t':    39.5502, 'eps':     1.0000, 'critic_loss':     6.9547, 'actor_loss':     0.0671, 'eps_e':     1.0000})
Step:   21000, Reward:    11.689 [  10.449], Avg:   -95.086 (1.000) <0-00:08:20> ({'r_t':   135.2909, 'eps':     1.0000, 'critic_loss':     5.4929, 'actor_loss':    -0.1023, 'eps_e':     1.0000})
Step:   22000, Reward:    18.531 [   7.486], Avg:   -90.146 (1.000) <0-00:08:41> ({'r_t':   157.5636, 'eps':     1.0000, 'critic_loss':     7.7813, 'actor_loss':     0.0107, 'eps_e':     1.0000})
Step:   23000, Reward:    20.641 [   6.621], Avg:   -85.530 (1.000) <0-00:09:03> ({'r_t':   217.8670, 'eps':     1.0000, 'critic_loss':     6.0597, 'actor_loss':    -0.0145, 'eps_e':     1.0000})
Step:   24000, Reward:    16.707 [   7.746], Avg:   -81.441 (1.000) <0-00:09:24> ({'r_t':   243.4437, 'eps':     1.0000, 'critic_loss':     8.4780, 'actor_loss':    -0.1538, 'eps_e':     1.0000})
Step:   25000, Reward:    18.114 [  13.878], Avg:   -77.612 (1.000) <0-00:09:46> ({'r_t':   224.1890, 'eps':     1.0000, 'critic_loss':     7.9506, 'actor_loss':    -0.0117, 'eps_e':     1.0000})
Step:   26000, Reward:    24.984 [   9.472], Avg:   -73.812 (1.000) <0-00:10:08> ({'r_t':   273.6918, 'eps':     1.0000, 'critic_loss':     8.2719, 'actor_loss':    -0.3794, 'eps_e':     1.0000})
Step:   27000, Reward:    26.299 [   6.511], Avg:   -70.236 (1.000) <0-00:10:29> ({'r_t':   274.3572, 'eps':     1.0000, 'critic_loss':     9.4535, 'actor_loss':     0.1862, 'eps_e':     1.0000})
Step:   28000, Reward:    24.981 [   9.232], Avg:   -66.953 (1.000) <0-00:10:51> ({'r_t':   310.9116, 'eps':     1.0000, 'critic_loss':     7.2625, 'actor_loss':    -0.2591, 'eps_e':     1.0000})
Step:   29000, Reward:    25.071 [  11.760], Avg:   -63.886 (1.000) <0-00:11:13> ({'r_t':   332.8554, 'eps':     1.0000, 'critic_loss':     6.3209, 'actor_loss':     0.0486, 'eps_e':     1.0000})
Step:   30000, Reward:    28.012 [  13.170], Avg:   -60.921 (1.000) <0-00:11:35> ({'r_t':   289.7745, 'eps':     1.0000, 'critic_loss':     7.9642, 'actor_loss':     0.3364, 'eps_e':     1.0000})
Step:   31000, Reward:    34.142 [  13.615], Avg:   -57.950 (1.000) <0-00:11:57> ({'r_t':   309.6135, 'eps':     1.0000, 'critic_loss':    10.5416, 'actor_loss':     0.0995, 'eps_e':     1.0000})
Step:   32000, Reward:    36.162 [  20.259], Avg:   -55.099 (1.000) <0-00:12:19> ({'r_t':   335.5935, 'eps':     1.0000, 'critic_loss':    17.6397, 'actor_loss':     0.2540, 'eps_e':     1.0000})
Step:   33000, Reward:    27.083 [  23.387], Avg:   -52.681 (1.000) <0-00:12:41> ({'r_t':   356.0473, 'eps':     1.0000, 'critic_loss':    21.6878, 'actor_loss':    -0.1548, 'eps_e':     1.0000})
Step:   34000, Reward:    44.155 [  22.423], Avg:   -49.915 (1.000) <0-00:13:03> ({'r_t':   348.8465, 'eps':     1.0000, 'critic_loss':    18.1113, 'actor_loss':    -0.2213, 'eps_e':     1.0000})
Step:   35000, Reward:    39.743 [  19.183], Avg:   -47.424 (1.000) <0-00:13:25> ({'r_t':   383.6763, 'eps':     1.0000, 'critic_loss':    25.1943, 'actor_loss':     0.0815, 'eps_e':     1.0000})
Step:   36000, Reward:    31.294 [  23.959], Avg:   -45.297 (1.000) <0-00:13:47> ({'r_t':   478.0998, 'eps':     1.0000, 'critic_loss':    31.2763, 'actor_loss':     0.2819, 'eps_e':     1.0000})
Step:   37000, Reward:    57.871 [  17.068], Avg:   -42.582 (1.000) <0-00:14:10> ({'r_t':   560.4912, 'eps':     1.0000, 'critic_loss':    23.9903, 'actor_loss':    -0.5492, 'eps_e':     1.0000})
Step:   38000, Reward:    51.107 [  19.991], Avg:   -40.180 (1.000) <0-00:14:32> ({'r_t':   613.5692, 'eps':     1.0000, 'critic_loss':    19.0548, 'actor_loss':     0.0801, 'eps_e':     1.0000})
Step:   39000, Reward:    61.518 [  19.739], Avg:   -37.637 (1.000) <0-00:14:55> ({'r_t':   587.2765, 'eps':     1.0000, 'critic_loss':    27.3749, 'actor_loss':    -0.0897, 'eps_e':     1.0000})
Step:   40000, Reward:    67.448 [  22.129], Avg:   -35.074 (1.000) <0-00:15:17> ({'r_t':   607.4016, 'eps':     1.0000, 'critic_loss':    30.4214, 'actor_loss':     0.1800, 'eps_e':     1.0000})
Step:   41000, Reward:    61.718 [  24.105], Avg:   -32.769 (1.000) <0-00:15:39> ({'r_t':   676.2727, 'eps':     1.0000, 'critic_loss':    31.3630, 'actor_loss':    -0.0913, 'eps_e':     1.0000})
Step:   42000, Reward:    66.074 [  18.196], Avg:   -30.471 (1.000) <0-00:16:02> ({'r_t':   735.5434, 'eps':     1.0000, 'critic_loss':    18.9503, 'actor_loss':    -0.0696, 'eps_e':     1.0000})
Step:   43000, Reward:    43.469 [  12.821], Avg:   -28.790 (1.000) <0-00:16:24> ({'r_t':   734.4121, 'eps':     1.0000, 'critic_loss':    25.9654, 'actor_loss':    -0.0756, 'eps_e':     1.0000})
Step:   44000, Reward:    70.643 [  23.327], Avg:   -26.581 (1.000) <0-00:16:46> ({'r_t':   737.0999, 'eps':     1.0000, 'critic_loss':    23.6884, 'actor_loss':     0.0122, 'eps_e':     1.0000})
Step:   45000, Reward:    71.146 [  17.221], Avg:   -24.456 (1.000) <0-00:17:09> ({'r_t':   705.2361, 'eps':     1.0000, 'critic_loss':    21.6995, 'actor_loss':     0.3422, 'eps_e':     1.0000})
Step:   46000, Reward:    64.144 [  17.138], Avg:   -22.571 (1.000) <0-00:17:31> ({'r_t':   711.4586, 'eps':     1.0000, 'critic_loss':    20.7283, 'actor_loss':     0.0652, 'eps_e':     1.0000})
Step:   47000, Reward:    62.104 [  22.535], Avg:   -20.807 (1.000) <0-00:17:54> ({'r_t':   722.1001, 'eps':     1.0000, 'critic_loss':    24.9905, 'actor_loss':     0.3956, 'eps_e':     1.0000})
Step:   48000, Reward:    63.642 [  29.849], Avg:   -19.084 (1.000) <0-00:18:16> ({'r_t':   739.0924, 'eps':     1.0000, 'critic_loss':    29.0466, 'actor_loss':     0.3447, 'eps_e':     1.0000})
Step:   49000, Reward:    75.457 [  19.407], Avg:   -17.193 (1.000) <0-00:18:39> ({'r_t':   791.9707, 'eps':     1.0000, 'critic_loss':    21.3305, 'actor_loss':     0.0539, 'eps_e':     1.0000})
Step:   50000, Reward:    77.297 [  18.509], Avg:   -15.340 (1.000) <0-00:19:01> ({'r_t':   736.3853, 'eps':     1.0000, 'critic_loss':    21.5589, 'actor_loss':     0.2841, 'eps_e':     1.0000})
Step:   51000, Reward:    74.198 [  16.875], Avg:   -13.618 (1.000) <0-00:19:23> ({'r_t':   831.0520, 'eps':     1.0000, 'critic_loss':    16.4616, 'actor_loss':    -0.1353, 'eps_e':     1.0000})
Step:   52000, Reward:    73.869 [  19.733], Avg:   -11.967 (1.000) <0-00:19:46> ({'r_t':   758.7711, 'eps':     1.0000, 'critic_loss':    20.9994, 'actor_loss':     0.3673, 'eps_e':     1.0000})
Step:   53000, Reward:    85.071 [  12.800], Avg:   -10.170 (1.000) <0-00:20:09> ({'r_t':   807.6045, 'eps':     1.0000, 'critic_loss':    24.8153, 'actor_loss':     0.1105, 'eps_e':     1.0000})
Step:   54000, Reward:    78.651 [  19.411], Avg:    -8.555 (1.000) <0-00:20:31> ({'r_t':   834.7607, 'eps':     1.0000, 'critic_loss':    24.3230, 'actor_loss':     0.1760, 'eps_e':     1.0000})
Step:   55000, Reward:    80.612 [  28.214], Avg:    -6.963 (1.000) <0-00:20:53> ({'r_t':   832.7248, 'eps':     1.0000, 'critic_loss':    16.2808, 'actor_loss':     0.0924, 'eps_e':     1.0000})
Step:   56000, Reward:    72.779 [  30.069], Avg:    -5.564 (1.000) <0-00:21:16> ({'r_t':   897.1567, 'eps':     1.0000, 'critic_loss':    13.9806, 'actor_loss':    -0.0057, 'eps_e':     1.0000})
Step:   57000, Reward:    78.546 [  18.040], Avg:    -4.114 (1.000) <0-00:21:38> ({'r_t':   891.9718, 'eps':     1.0000, 'critic_loss':    15.1381, 'actor_loss':     0.0526, 'eps_e':     1.0000})
Step:   58000, Reward:    84.886 [  20.504], Avg:    -2.606 (1.000) <0-00:22:00> ({'r_t':   899.4004, 'eps':     1.0000, 'critic_loss':    14.6597, 'actor_loss':     0.3105, 'eps_e':     1.0000})
Step:   59000, Reward:    85.030 [  20.094], Avg:    -1.145 (1.000) <0-00:22:23> ({'r_t':   906.7314, 'eps':     1.0000, 'critic_loss':    15.7667, 'actor_loss':     0.3050, 'eps_e':     1.0000})
Step:   60000, Reward:    73.900 [  21.666], Avg:     0.085 (1.000) <0-00:22:45> ({'r_t':   862.3088, 'eps':     1.0000, 'critic_loss':    19.3339, 'actor_loss':     0.3221, 'eps_e':     1.0000})
Step:   61000, Reward:    78.839 [  11.338], Avg:     1.356 (1.000) <0-00:23:08> ({'r_t':   845.3983, 'eps':     1.0000, 'critic_loss':    22.6584, 'actor_loss':     0.6345, 'eps_e':     1.0000})
Step:   62000, Reward:    84.929 [  16.860], Avg:     2.682 (1.000) <0-00:23:30> ({'r_t':   865.6235, 'eps':     1.0000, 'critic_loss':    15.7430, 'actor_loss':    -0.1342, 'eps_e':     1.0000})
Step:   63000, Reward:    73.046 [  16.362], Avg:     3.782 (1.000) <0-00:23:53> ({'r_t':   832.2679, 'eps':     1.0000, 'critic_loss':    21.6237, 'actor_loss':     0.7210, 'eps_e':     1.0000})
Step:   64000, Reward:    79.199 [  22.142], Avg:     4.942 (1.000) <0-00:24:15> ({'r_t':   882.2925, 'eps':     1.0000, 'critic_loss':    20.1973, 'actor_loss':    -0.4057, 'eps_e':     1.0000})
Step:   65000, Reward:    89.595 [  23.836], Avg:     6.224 (1.000) <0-00:24:38> ({'r_t':   943.0217, 'eps':     1.0000, 'critic_loss':    16.3717, 'actor_loss':     0.3901, 'eps_e':     1.0000})
Step:   66000, Reward:    92.841 [  14.098], Avg:     7.517 (1.000) <0-00:25:01> ({'r_t':   946.4091, 'eps':     1.0000, 'critic_loss':    13.6276, 'actor_loss':     0.1427, 'eps_e':     1.0000})
Step:   67000, Reward:    95.193 [  16.746], Avg:     8.807 (1.000) <0-00:25:23> ({'r_t':   985.8539, 'eps':     1.0000, 'critic_loss':    16.6246, 'actor_loss':     0.0307, 'eps_e':     1.0000})
Step:   68000, Reward:    94.818 [  15.578], Avg:    10.053 (1.000) <0-00:25:46> ({'r_t':   903.4070, 'eps':     1.0000, 'critic_loss':    18.2840, 'actor_loss':    -0.2532, 'eps_e':     1.0000})
Step:   69000, Reward:    94.192 [  19.905], Avg:    11.255 (1.000) <0-00:26:08> ({'r_t':   978.9965, 'eps':     1.0000, 'critic_loss':    15.5807, 'actor_loss':     0.2637, 'eps_e':     1.0000})
Step:   70000, Reward:    92.151 [  18.444], Avg:    12.394 (1.000) <0-00:26:31> ({'r_t':   964.8113, 'eps':     1.0000, 'critic_loss':    27.1478, 'actor_loss':     0.1832, 'eps_e':     1.0000})
Step:   71000, Reward:    79.769 [  17.422], Avg:    13.330 (1.000) <0-00:26:53> ({'r_t':   964.5768, 'eps':     1.0000, 'critic_loss':    13.6345, 'actor_loss':     0.0747, 'eps_e':     1.0000})
Step:   72000, Reward:    88.842 [  18.493], Avg:    14.365 (1.000) <0-00:27:16> ({'r_t':   966.0492, 'eps':     1.0000, 'critic_loss':    18.0594, 'actor_loss':    -0.1949, 'eps_e':     1.0000})
Step:   73000, Reward:    80.684 [  12.056], Avg:    15.261 (1.000) <0-00:27:38> ({'r_t':   951.4073, 'eps':     1.0000, 'critic_loss':    18.4401, 'actor_loss':     0.4660, 'eps_e':     1.0000})
Step:   74000, Reward:    87.650 [  19.571], Avg:    16.226 (1.000) <0-00:28:02> ({'r_t':   928.5334, 'eps':     1.0000, 'critic_loss':    21.2044, 'actor_loss':     0.3587, 'eps_e':     1.0000})
Step:   75000, Reward:    86.693 [  19.132], Avg:    17.153 (1.000) <0-00:28:24> ({'r_t':   976.6690, 'eps':     1.0000, 'critic_loss':    12.4950, 'actor_loss':    -0.1414, 'eps_e':     1.0000})
Step:   76000, Reward:    93.858 [  19.509], Avg:    18.149 (1.000) <0-00:28:47> ({'r_t':   974.2147, 'eps':     1.0000, 'critic_loss':    13.2213, 'actor_loss':     0.2439, 'eps_e':     1.0000})
Step:   77000, Reward:    94.341 [  14.288], Avg:    19.126 (1.000) <0-00:29:10> ({'r_t':   998.4804, 'eps':     1.0000, 'critic_loss':    13.1438, 'actor_loss':     0.6936, 'eps_e':     1.0000})
Step:   78000, Reward:    96.875 [  19.096], Avg:    20.110 (1.000) <0-00:29:32> ({'r_t':   983.1893, 'eps':     1.0000, 'critic_loss':    14.1074, 'actor_loss':     0.1822, 'eps_e':     1.0000})
Step:   79000, Reward:    95.737 [  13.546], Avg:    21.056 (1.000) <0-00:29:55> ({'r_t':   970.4630, 'eps':     1.0000, 'critic_loss':    14.9811, 'actor_loss':     0.0647, 'eps_e':     1.0000})
Step:   80000, Reward:    82.471 [  17.254], Avg:    21.814 (1.000) <0-00:30:18> ({'r_t':   868.5177, 'eps':     1.0000, 'critic_loss':    18.4197, 'actor_loss':     0.3859, 'eps_e':     1.0000})
Step:   81000, Reward:    95.590 [  19.954], Avg:    22.714 (1.000) <0-00:30:41> ({'r_t':   912.5756, 'eps':     1.0000, 'critic_loss':    13.5957, 'actor_loss':    -0.1386, 'eps_e':     1.0000})
Step:   82000, Reward:   101.309 [  18.418], Avg:    23.661 (1.000) <0-00:31:04> ({'r_t':  1003.9477, 'eps':     1.0000, 'critic_loss':    11.5403, 'actor_loss':    -0.1616, 'eps_e':     1.0000})
Step:   83000, Reward:   105.129 [  15.617], Avg:    24.630 (1.000) <0-00:31:26> ({'r_t':   997.9419, 'eps':     1.0000, 'critic_loss':    14.9098, 'actor_loss':    -0.0239, 'eps_e':     1.0000})
Step:   84000, Reward:    94.034 [  32.508], Avg:    25.447 (1.000) <0-00:31:50> ({'r_t':  1016.9127, 'eps':     1.0000, 'critic_loss':    12.1937, 'actor_loss':    -0.3894, 'eps_e':     1.0000})
Step:   85000, Reward:   103.209 [  17.111], Avg:    26.351 (1.000) <0-00:32:13> ({'r_t':   977.4049, 'eps':     1.0000, 'critic_loss':    13.4819, 'actor_loss':     0.3324, 'eps_e':     1.0000})
Step:   86000, Reward:   104.875 [  15.410], Avg:    27.254 (1.000) <0-00:32:36> ({'r_t':   968.1637, 'eps':     1.0000, 'critic_loss':    12.0742, 'actor_loss':     0.3408, 'eps_e':     1.0000})
Step:   87000, Reward:    92.260 [  16.789], Avg:    27.992 (1.000) <0-00:32:59> ({'r_t':  1040.0283, 'eps':     1.0000, 'critic_loss':     8.9343, 'actor_loss':    -0.0542, 'eps_e':     1.0000})
Step:   88000, Reward:    94.244 [  16.329], Avg:    28.737 (1.000) <0-00:33:22> ({'r_t':   966.0592, 'eps':     1.0000, 'critic_loss':    20.1990, 'actor_loss':     0.3618, 'eps_e':     1.0000})
Step:   89000, Reward:   115.030 [  11.990], Avg:    29.696 (1.000) <0-00:33:44> ({'r_t':  1034.8178, 'eps':     1.0000, 'critic_loss':    11.5830, 'actor_loss':    -0.1685, 'eps_e':     1.0000})
Step:   90000, Reward:   104.395 [  15.464], Avg:    30.516 (1.000) <0-00:34:07> ({'r_t':  1059.8792, 'eps':     1.0000, 'critic_loss':     9.8172, 'actor_loss':    -0.2177, 'eps_e':     1.0000})
Step:   91000, Reward:   106.137 [  15.446], Avg:    31.338 (1.000) <0-00:34:30> ({'r_t':  1110.7683, 'eps':     1.0000, 'critic_loss':     8.6142, 'actor_loss':    -0.0524, 'eps_e':     1.0000})
Step:   92000, Reward:   106.181 [  16.917], Avg:    32.143 (1.000) <0-00:34:53> ({'r_t':  1076.5607, 'eps':     1.0000, 'critic_loss':     9.0412, 'actor_loss':     0.1497, 'eps_e':     1.0000})
Step:   93000, Reward:   103.137 [  14.784], Avg:    32.898 (1.000) <0-00:35:15> ({'r_t':  1022.2653, 'eps':     1.0000, 'critic_loss':    16.5431, 'actor_loss':     0.5381, 'eps_e':     1.0000})
Step:   94000, Reward:   104.693 [  12.476], Avg:    33.654 (1.000) <0-00:35:38> ({'r_t':  1003.7109, 'eps':     1.0000, 'critic_loss':    16.7419, 'actor_loss':     0.1215, 'eps_e':     1.0000})
Step:   95000, Reward:   105.137 [  14.494], Avg:    34.399 (1.000) <0-00:36:01> ({'r_t':  1042.7101, 'eps':     1.0000, 'critic_loss':    10.2721, 'actor_loss':     0.2834, 'eps_e':     1.0000})
Step:   96000, Reward:   111.942 [  11.984], Avg:    35.198 (1.000) <0-00:36:24> ({'r_t':  1114.8505, 'eps':     1.0000, 'critic_loss':     9.7467, 'actor_loss':    -0.6062, 'eps_e':     1.0000})
Step:   97000, Reward:   103.109 [  14.637], Avg:    35.891 (1.000) <0-00:36:47> ({'r_t':   937.2947, 'eps':     1.0000, 'critic_loss':    27.3412, 'actor_loss':     0.6038, 'eps_e':     1.0000})
Step:   98000, Reward:   110.982 [  15.264], Avg:    36.650 (1.000) <0-00:37:10> ({'r_t':  1067.5661, 'eps':     1.0000, 'critic_loss':    11.2034, 'actor_loss':    -0.2577, 'eps_e':     1.0000})
Step:   99000, Reward:   108.480 [  18.824], Avg:    37.368 (1.000) <0-00:37:33> ({'r_t':  1085.8648, 'eps':     1.0000, 'critic_loss':     9.1806, 'actor_loss':     0.0328, 'eps_e':     1.0000})
Step:  100000, Reward:   104.366 [  12.538], Avg:    38.031 (1.000) <0-00:37:56> ({'r_t':  1077.0783, 'eps':     1.0000, 'critic_loss':    10.0318, 'actor_loss':     0.1413, 'eps_e':     1.0000})
Step:  101000, Reward:   100.356 [  13.618], Avg:    38.642 (1.000) <0-00:38:19> ({'r_t':   991.8794, 'eps':     1.0000, 'critic_loss':    12.4770, 'actor_loss':     0.0419, 'eps_e':     1.0000})
Step:  102000, Reward:  -181.539 [ 786.789], Avg:    36.505 (1.000) <0-00:38:43> ({'r_t':  1034.9622, 'eps':     1.0000, 'critic_loss':    24.5189, 'actor_loss':     0.6254, 'eps_e':     1.0000})
Step:  103000, Reward:    31.017 [  14.709], Avg:    36.452 (1.000) <0-00:39:06> ({'r_t':   559.6796, 'eps':     1.0000, 'critic_loss':  1066.5591, 'actor_loss':     4.3005, 'eps_e':     1.0000})
Step:  104000, Reward:    41.728 [   8.942], Avg:    36.502 (1.000) <0-00:39:28> ({'r_t':   392.2676, 'eps':     1.0000, 'critic_loss':     4.2448, 'actor_loss':    -0.1619, 'eps_e':     1.0000})
Step:  105000, Reward:    53.006 [   5.134], Avg:    36.658 (1.000) <0-00:39:50> ({'r_t':   580.4124, 'eps':     1.0000, 'critic_loss':     4.8403, 'actor_loss':    -0.2687, 'eps_e':     1.0000})
Step:  106000, Reward:    61.218 [   6.147], Avg:    36.887 (1.000) <0-00:40:12> ({'r_t':   597.7076, 'eps':     1.0000, 'critic_loss':     5.2403, 'actor_loss':    -0.1206, 'eps_e':     1.0000})
Step:  107000, Reward:    61.274 [   6.723], Avg:    37.113 (1.000) <0-00:40:34> ({'r_t':   681.7870, 'eps':     1.0000, 'critic_loss':     6.3144, 'actor_loss':    -0.1296, 'eps_e':     1.0000})
Step:  108000, Reward:    59.338 [  13.166], Avg:    37.317 (1.000) <0-00:40:57> ({'r_t':   673.9593, 'eps':     1.0000, 'critic_loss':     9.5962, 'actor_loss':     0.0124, 'eps_e':     1.0000})
Step:  109000, Reward:    60.345 [  15.173], Avg:    37.526 (1.000) <0-00:41:19> ({'r_t':   687.1815, 'eps':     1.0000, 'critic_loss':     7.1191, 'actor_loss':     0.1486, 'eps_e':     1.0000})
Step:  110000, Reward:    76.858 [  13.669], Avg:    37.881 (1.000) <0-00:41:42> ({'r_t':   788.1990, 'eps':     1.0000, 'critic_loss':     7.1375, 'actor_loss':    -0.3453, 'eps_e':     1.0000})
Step:  111000, Reward:    95.869 [   5.161], Avg:    38.399 (1.000) <0-00:42:04> ({'r_t':   908.2871, 'eps':     1.0000, 'critic_loss':     8.4657, 'actor_loss':    -0.5023, 'eps_e':     1.0000})
Step:  112000, Reward:   112.506 [  13.402], Avg:    39.054 (1.000) <0-00:42:27> ({'r_t':  1051.6054, 'eps':     1.0000, 'critic_loss':    16.6686, 'actor_loss':    -0.5373, 'eps_e':     1.0000})
Step:  113000, Reward:   100.739 [  16.584], Avg:    39.595 (1.000) <0-00:42:49> ({'r_t':  1067.2882, 'eps':     1.0000, 'critic_loss':    13.5541, 'actor_loss':     0.1008, 'eps_e':     1.0000})
Step:  114000, Reward:   112.627 [  10.431], Avg:    40.231 (1.000) <0-00:43:12> ({'r_t':  1114.8179, 'eps':     1.0000, 'critic_loss':     9.4481, 'actor_loss':    -0.1939, 'eps_e':     1.0000})
Step:  115000, Reward:    77.435 [   9.052], Avg:    40.551 (1.000) <0-00:43:35> ({'r_t':   547.7202, 'eps':     1.0000, 'critic_loss':  4057.7632, 'actor_loss':     7.8815, 'eps_e':     1.0000})
Step:  116000, Reward:   107.591 [  16.524], Avg:    41.124 (1.000) <0-00:43:57> ({'r_t':   978.8177, 'eps':     1.0000, 'critic_loss':     9.9491, 'actor_loss':    -0.9614, 'eps_e':     1.0000})
Step:  117000, Reward:   109.244 [  17.343], Avg:    41.702 (1.000) <0-00:44:20> ({'r_t':  1124.7568, 'eps':     1.0000, 'critic_loss':    11.7348, 'actor_loss':    -0.6851, 'eps_e':     1.0000})
Step:  118000, Reward:   117.541 [  12.888], Avg:    42.339 (1.000) <0-00:44:43> ({'r_t':  1210.1771, 'eps':     1.0000, 'critic_loss':    10.3175, 'actor_loss':    -0.2726, 'eps_e':     1.0000})
Step:  119000, Reward:   116.023 [  15.272], Avg:    42.953 (1.000) <0-00:45:05> ({'r_t':  1184.4272, 'eps':     1.0000, 'critic_loss':     9.6751, 'actor_loss':     0.5214, 'eps_e':     1.0000})
Step:  120000, Reward:   114.809 [  14.042], Avg:    43.547 (1.000) <0-00:45:28> ({'r_t':  1194.4875, 'eps':     1.0000, 'critic_loss':    18.5059, 'actor_loss':    -0.0082, 'eps_e':     1.0000})
Step:  121000, Reward:   110.788 [  14.533], Avg:    44.098 (1.000) <0-00:45:50> ({'r_t':  1146.5132, 'eps':     1.0000, 'critic_loss':    12.9887, 'actor_loss':     0.1217, 'eps_e':     1.0000})
Step:  122000, Reward:   124.409 [  12.823], Avg:    44.751 (1.000) <0-00:46:13> ({'r_t':  1163.8220, 'eps':     1.0000, 'critic_loss':    11.9466, 'actor_loss':     0.0552, 'eps_e':     1.0000})
Step:  123000, Reward:    90.912 [  11.290], Avg:    45.123 (1.000) <0-00:46:36> ({'r_t':  1202.9623, 'eps':     1.0000, 'critic_loss':    10.4254, 'actor_loss':    -0.1260, 'eps_e':     1.0000})
Step:  124000, Reward:   127.002 [  11.309], Avg:    45.778 (1.000) <0-00:46:59> ({'r_t':  1005.8287, 'eps':     1.0000, 'critic_loss':    12.7899, 'actor_loss':     0.8612, 'eps_e':     1.0000})
Step:  125000, Reward:   114.328 [  20.137], Avg:    46.322 (1.000) <0-00:47:21> ({'r_t':  1138.1933, 'eps':     1.0000, 'critic_loss':    17.7298, 'actor_loss':     0.2264, 'eps_e':     1.0000})
Step:  126000, Reward:   119.490 [  15.343], Avg:    46.898 (1.000) <0-00:47:44> ({'r_t':  1151.1087, 'eps':     1.0000, 'critic_loss':    18.3926, 'actor_loss':    -0.2189, 'eps_e':     1.0000})
Step:  127000, Reward:   118.357 [  17.650], Avg:    47.457 (1.000) <0-00:48:07> ({'r_t':  1181.9503, 'eps':     1.0000, 'critic_loss':    11.7477, 'actor_loss':     0.0057, 'eps_e':     1.0000})
Step:  128000, Reward:   110.018 [  10.452], Avg:    47.942 (1.000) <0-00:48:29> ({'r_t':  1144.0434, 'eps':     1.0000, 'critic_loss':    15.0485, 'actor_loss':    -0.0211, 'eps_e':     1.0000})
Step:  129000, Reward:   115.789 [  18.587], Avg:    48.463 (1.000) <0-00:48:52> ({'r_t':  1206.7417, 'eps':     1.0000, 'critic_loss':     8.6566, 'actor_loss':    -0.2495, 'eps_e':     1.0000})
Step:  130000, Reward:   121.162 [  17.287], Avg:    49.018 (1.000) <0-00:49:15> ({'r_t':  1196.0709, 'eps':     1.0000, 'critic_loss':    10.1782, 'actor_loss':     0.3788, 'eps_e':     1.0000})
Step:  131000, Reward:   123.608 [  18.282], Avg:    49.583 (1.000) <0-00:49:37> ({'r_t':  1213.7526, 'eps':     1.0000, 'critic_loss':    14.3477, 'actor_loss':    -0.0292, 'eps_e':     1.0000})
Step:  132000, Reward:   124.683 [  13.518], Avg:    50.148 (1.000) <0-00:50:00> ({'r_t':  1213.2081, 'eps':     1.0000, 'critic_loss':    11.3498, 'actor_loss':    -0.1081, 'eps_e':     1.0000})
Step:  133000, Reward:    61.141 [  32.233], Avg:    50.230 (1.000) <0-00:50:23> ({'r_t':   826.3011, 'eps':     1.0000, 'critic_loss':    54.1081, 'actor_loss':     1.1321, 'eps_e':     1.0000})
Step:  134000, Reward:   123.194 [  13.397], Avg:    50.771 (1.000) <0-00:50:45> ({'r_t':  1067.5631, 'eps':     1.0000, 'critic_loss':    35.4497, 'actor_loss':    -1.1884, 'eps_e':     1.0000})
Step:  135000, Reward:   127.005 [  15.206], Avg:    51.331 (1.000) <0-00:51:08> ({'r_t':  1208.9463, 'eps':     1.0000, 'critic_loss':     8.7009, 'actor_loss':     0.1148, 'eps_e':     1.0000})
Step:  136000, Reward:   111.042 [  15.914], Avg:    51.767 (1.000) <0-00:51:31> ({'r_t':  1225.1361, 'eps':     1.0000, 'critic_loss':    13.3966, 'actor_loss':    -0.2576, 'eps_e':     1.0000})
Step:  137000, Reward:   132.978 [  15.353], Avg:    52.355 (1.000) <0-00:51:54> ({'r_t':  1217.3346, 'eps':     1.0000, 'critic_loss':    12.0664, 'actor_loss':    -0.1947, 'eps_e':     1.0000})
Step:  138000, Reward:   126.046 [  11.445], Avg:    52.886 (1.000) <0-00:52:17> ({'r_t':  1268.4009, 'eps':     1.0000, 'critic_loss':     9.2727, 'actor_loss':    -0.1004, 'eps_e':     1.0000})
Step:  139000, Reward:   128.964 [  13.074], Avg:    53.429 (1.000) <0-00:52:39> ({'r_t':  1297.0428, 'eps':     1.0000, 'critic_loss':    12.0422, 'actor_loss':    -0.2246, 'eps_e':     1.0000})
Step:  140000, Reward:   133.222 [   9.012], Avg:    53.995 (1.000) <0-00:53:02> ({'r_t':  1311.9632, 'eps':     1.0000, 'critic_loss':     7.1158, 'actor_loss':     0.0907, 'eps_e':     1.0000})
Step:  141000, Reward:   130.978 [  10.681], Avg:    54.537 (1.000) <0-00:53:25> ({'r_t':  1302.1945, 'eps':     1.0000, 'critic_loss':    14.8043, 'actor_loss':     0.5728, 'eps_e':     1.0000})
Step:  142000, Reward:   133.711 [  20.121], Avg:    55.091 (1.000) <0-00:53:47> ({'r_t':  1275.7818, 'eps':     1.0000, 'critic_loss':    14.6384, 'actor_loss':     0.1542, 'eps_e':     1.0000})
Step:  143000, Reward:   124.159 [  21.133], Avg:    55.570 (1.000) <0-00:54:10> ({'r_t':  1308.7507, 'eps':     1.0000, 'critic_loss':    15.7211, 'actor_loss':    -0.0226, 'eps_e':     1.0000})
Step:  144000, Reward:   126.872 [  18.294], Avg:    56.062 (1.000) <0-00:54:33> ({'r_t':  1356.9556, 'eps':     1.0000, 'critic_loss':     6.3341, 'actor_loss':     0.0999, 'eps_e':     1.0000})
Step:  145000, Reward:    24.757 [  33.591], Avg:    55.848 (1.000) <0-00:54:56> ({'r_t':    88.2684, 'eps':     1.0000, 'critic_loss':  5358.2334, 'actor_loss':     7.1113, 'eps_e':     1.0000})
Step:  146000, Reward:    31.523 [  32.749], Avg:    55.682 (1.000) <0-00:55:18> ({'r_t':   363.7136, 'eps':     1.0000, 'critic_loss':   119.2294, 'actor_loss':    -0.3252, 'eps_e':     1.0000})
Step:  147000, Reward:    53.564 [  36.627], Avg:    55.668 (1.000) <0-00:55:40> ({'r_t':   591.2811, 'eps':     1.0000, 'critic_loss':    96.4862, 'actor_loss':    -0.4508, 'eps_e':     1.0000})
Step:  148000, Reward:    82.708 [  20.645], Avg:    55.849 (1.000) <0-00:56:04> ({'r_t':   676.4451, 'eps':     1.0000, 'critic_loss':    62.5620, 'actor_loss':    -0.3400, 'eps_e':     1.0000})
Step:  149000, Reward:   104.487 [  17.454], Avg:    56.174 (1.000) <0-00:56:26> ({'r_t':   924.5100, 'eps':     1.0000, 'critic_loss':    16.3848, 'actor_loss':    -0.8238, 'eps_e':     1.0000})
Step:  150000, Reward:   117.261 [  12.232], Avg:    56.578 (1.000) <0-00:56:49> ({'r_t':  1126.3911, 'eps':     1.0000, 'critic_loss':    23.7640, 'actor_loss':    -0.5731, 'eps_e':     1.0000})
Step:  151000, Reward:  -485.281 [ 366.047], Avg:    53.013 (1.000) <0-00:57:20> ({'r_t':  -164.2595, 'eps':     1.0000, 'critic_loss':  1901.3026, 'actor_loss':     2.8216, 'eps_e':     1.0000})
Step:  152000, Reward:  -346.540 [ 318.993], Avg:    50.402 (1.000) <0-00:57:53> ({'r_t': -1579.8501, 'eps':     1.0000, 'critic_loss':    82.3224, 'actor_loss':    25.6324, 'eps_e':     1.0000})
Step:  153000, Reward:  -467.972 [ 322.716], Avg:    47.036 (1.000) <0-00:58:26> ({'r_t': -1516.2330, 'eps':     1.0000, 'critic_loss':   203.8316, 'actor_loss':    13.7185, 'eps_e':     1.0000})
Step:  154000, Reward:  -329.717 [ 306.756], Avg:    44.605 (1.000) <0-00:58:59> ({'r_t': -1464.2125, 'eps':     1.0000, 'critic_loss':   284.6571, 'actor_loss':     6.3476, 'eps_e':     1.0000})
Step:  155000, Reward:  -246.109 [ 292.161], Avg:    42.742 (1.000) <0-00:59:30> ({'r_t': -1271.1084, 'eps':     1.0000, 'critic_loss':   343.8235, 'actor_loss':     2.4952, 'eps_e':     1.0000})
Step:  156000, Reward:  -226.720 [ 263.166], Avg:    41.025 (1.000) <0-01:00:03> ({'r_t': -1226.0208, 'eps':     1.0000, 'critic_loss':   370.3040, 'actor_loss':     0.7391, 'eps_e':     1.0000})
Step:  157000, Reward:   -97.187 [ 111.286], Avg:    40.151 (1.000) <0-01:00:30> ({'r_t': -1223.7024, 'eps':     1.0000, 'critic_loss':   409.5670, 'actor_loss':     0.4918, 'eps_e':     1.0000})
Step:  158000, Reward:   -94.360 [ 143.329], Avg:    39.305 (1.000) <0-01:00:58> ({'r_t':  -899.2391, 'eps':     1.0000, 'critic_loss':   268.1859, 'actor_loss':    -0.6734, 'eps_e':     1.0000})
Step:  159000, Reward:   -48.126 [  39.464], Avg:    38.758 (1.000) <0-01:01:22> ({'r_t':  -796.9214, 'eps':     1.0000, 'critic_loss':   165.4074, 'actor_loss':    -0.2747, 'eps_e':     1.0000})
Step:  160000, Reward:   -88.721 [  69.129], Avg:    37.966 (1.000) <0-01:01:45> ({'r_t':  -675.7641, 'eps':     1.0000, 'critic_loss':   139.4530, 'actor_loss':    -0.9690, 'eps_e':     1.0000})
Step:  161000, Reward:   -38.769 [  34.347], Avg:    37.493 (1.000) <0-01:02:09> ({'r_t':  -470.8596, 'eps':     1.0000, 'critic_loss':    55.7405, 'actor_loss':    -0.1643, 'eps_e':     1.0000})
Step:  162000, Reward:   -38.646 [  29.510], Avg:    37.026 (1.000) <0-01:02:32> ({'r_t':  -368.8627, 'eps':     1.0000, 'critic_loss':    33.2570, 'actor_loss':    -0.0384, 'eps_e':     1.0000})
Step:  163000, Reward:   -47.685 [  39.846], Avg:    36.509 (1.000) <0-01:02:55> ({'r_t':  -376.7996, 'eps':     1.0000, 'critic_loss':    40.9535, 'actor_loss':     0.6179, 'eps_e':     1.0000})
Step:  164000, Reward:    -7.906 [  15.992], Avg:    36.240 (1.000) <0-01:03:17> ({'r_t':  -248.6233, 'eps':     1.0000, 'critic_loss':    40.4092, 'actor_loss':    -0.0087, 'eps_e':     1.0000})
Step:  165000, Reward:     4.349 [   7.585], Avg:    36.048 (1.000) <0-01:03:39> ({'r_t':     4.1790, 'eps':     1.0000, 'critic_loss':     5.7064, 'actor_loss':    -0.2298, 'eps_e':     1.0000})
Step:  166000, Reward:    24.954 [  15.568], Avg:    35.981 (1.000) <0-01:04:01> ({'r_t':   174.1182, 'eps':     1.0000, 'critic_loss':     7.6914, 'actor_loss':    -0.4283, 'eps_e':     1.0000})
Step:  167000, Reward:    41.380 [  27.884], Avg:    36.013 (1.000) <0-01:04:24> ({'r_t':   409.5535, 'eps':     1.0000, 'critic_loss':    20.1924, 'actor_loss':    -0.6895, 'eps_e':     1.0000})
Step:  168000, Reward:    97.880 [  34.820], Avg:    36.379 (1.000) <0-01:04:46> ({'r_t':   811.3323, 'eps':     1.0000, 'critic_loss':    62.6273, 'actor_loss':    -1.3777, 'eps_e':     1.0000})
Step:  169000, Reward:   100.686 [  14.099], Avg:    36.758 (1.000) <0-01:05:09> ({'r_t':   988.5626, 'eps':     1.0000, 'critic_loss':   355.2519, 'actor_loss':     1.0826, 'eps_e':     1.0000})
Step:  170000, Reward:   101.930 [  22.470], Avg:    37.139 (1.000) <0-01:05:32> ({'r_t':  1109.2988, 'eps':     1.0000, 'critic_loss':    19.9018, 'actor_loss':    -0.0934, 'eps_e':     1.0000})
Step:  171000, Reward:   109.312 [  19.502], Avg:    37.558 (1.000) <0-01:05:54> ({'r_t':   926.3364, 'eps':     1.0000, 'critic_loss':    37.7178, 'actor_loss':     0.7722, 'eps_e':     1.0000})
Step:  172000, Reward:   111.146 [  17.720], Avg:    37.984 (1.000) <0-01:06:17> ({'r_t':  1177.7418, 'eps':     1.0000, 'critic_loss':    23.8867, 'actor_loss':    -0.0862, 'eps_e':     1.0000})
Step:  173000, Reward:   122.464 [  13.350], Avg:    38.469 (1.000) <0-01:06:39> ({'r_t':  1179.6209, 'eps':     1.0000, 'critic_loss':    14.1035, 'actor_loss':     0.0143, 'eps_e':     1.0000})
Step:  174000, Reward:   126.105 [  22.140], Avg:    38.970 (1.000) <0-01:07:02> ({'r_t':  1225.8543, 'eps':     1.0000, 'critic_loss':    16.7579, 'actor_loss':    -0.3188, 'eps_e':     1.0000})
Step:  175000, Reward:   131.314 [  11.864], Avg:    39.495 (1.000) <0-01:07:25> ({'r_t':  1230.9816, 'eps':     1.0000, 'critic_loss':    23.5472, 'actor_loss':     0.3192, 'eps_e':     1.0000})
Step:  176000, Reward:   135.461 [   9.508], Avg:    40.037 (1.000) <0-01:07:48> ({'r_t':  1288.8563, 'eps':     1.0000, 'critic_loss':    19.4254, 'actor_loss':    -0.2000, 'eps_e':     1.0000})
Step:  177000, Reward:   135.760 [   8.082], Avg:    40.575 (1.000) <0-01:08:10> ({'r_t':  1367.5824, 'eps':     1.0000, 'critic_loss':     6.1160, 'actor_loss':     0.0238, 'eps_e':     1.0000})
Step:  178000, Reward:   111.232 [  38.269], Avg:    40.970 (1.000) <0-01:08:33> ({'r_t':  1360.2493, 'eps':     1.0000, 'critic_loss':    13.7958, 'actor_loss':     0.3974, 'eps_e':     1.0000})
Step:  179000, Reward:   129.443 [  16.572], Avg:    41.461 (1.000) <0-01:08:55> ({'r_t':  1233.6296, 'eps':     1.0000, 'critic_loss':    47.5340, 'actor_loss':    -0.2775, 'eps_e':     1.0000})
Step:  180000, Reward:   135.210 [  16.655], Avg:    41.979 (1.000) <0-01:09:18> ({'r_t':  1368.5213, 'eps':     1.0000, 'critic_loss':    13.8055, 'actor_loss':     0.1140, 'eps_e':     1.0000})
Step:  181000, Reward:   133.595 [  14.323], Avg:    42.482 (1.000) <0-01:09:41> ({'r_t':  1390.3390, 'eps':     1.0000, 'critic_loss':     7.1141, 'actor_loss':    -0.0402, 'eps_e':     1.0000})
Step:  182000, Reward:   -74.193 [  71.190], Avg:    41.845 (1.000) <0-01:10:05> ({'r_t':   921.5264, 'eps':     1.0000, 'critic_loss':  1138.0143, 'actor_loss':     5.0976, 'eps_e':     1.0000})
Step:  183000, Reward:  -149.200 [ 152.052], Avg:    40.807 (1.000) <0-01:10:33> ({'r_t':  -813.3798, 'eps':     1.0000, 'critic_loss':   153.8971, 'actor_loss':     5.1259, 'eps_e':     1.0000})
Step:  184000, Reward:   -45.491 [  51.983], Avg:    40.340 (1.000) <0-01:10:56> ({'r_t':  -587.6139, 'eps':     1.0000, 'critic_loss':   144.3051, 'actor_loss':     0.2562, 'eps_e':     1.0000})
Step:  185000, Reward:    -9.687 [   7.944], Avg:    40.071 (1.000) <0-01:11:19> ({'r_t':  -248.2213, 'eps':     1.0000, 'critic_loss':    39.5502, 'actor_loss':    -0.5054, 'eps_e':     1.0000})
Step:  186000, Reward:    -6.806 [   1.773], Avg:    39.820 (1.000) <0-01:11:40> ({'r_t':  -133.1731, 'eps':     1.0000, 'critic_loss':     7.1323, 'actor_loss':    -0.1068, 'eps_e':     1.0000})
Step:  187000, Reward:    -5.306 [   3.731], Avg:    39.580 (1.000) <0-01:12:02> ({'r_t':   -32.9855, 'eps':     1.0000, 'critic_loss':     1.5143, 'actor_loss':    -0.0544, 'eps_e':     1.0000})
Step:  188000, Reward:    -2.668 [   1.712], Avg:    39.357 (1.000) <0-01:12:24> ({'r_t':   -61.5970, 'eps':     1.0000, 'critic_loss':     1.3268, 'actor_loss':    -0.0285, 'eps_e':     1.0000})
Step:  189000, Reward:     6.314 [   3.153], Avg:    39.183 (1.000) <0-01:12:45> ({'r_t':     6.3559, 'eps':     1.0000, 'critic_loss':     1.5885, 'actor_loss':    -0.0873, 'eps_e':     1.0000})
Step:  190000, Reward:    18.898 [   3.845], Avg:    39.077 (1.000) <0-01:13:07> ({'r_t':   182.5974, 'eps':     1.0000, 'critic_loss':     1.7923, 'actor_loss':    -0.2115, 'eps_e':     1.0000})
Step:  191000, Reward:    33.388 [   4.781], Avg:    39.047 (1.000) <0-01:13:29> ({'r_t':   319.9027, 'eps':     1.0000, 'critic_loss':     2.0870, 'actor_loss':    -0.2743, 'eps_e':     1.0000})
Step:  192000, Reward:    41.875 [   5.401], Avg:    39.062 (1.000) <0-01:13:51> ({'r_t':   468.9603, 'eps':     1.0000, 'critic_loss':     2.9865, 'actor_loss':    -0.3310, 'eps_e':     1.0000})
Step:  193000, Reward:    50.519 [   3.629], Avg:    39.121 (1.000) <0-01:14:13> ({'r_t':   546.4467, 'eps':     1.0000, 'critic_loss':     4.6840, 'actor_loss':    -0.1674, 'eps_e':     1.0000})
Step:  194000, Reward:    55.063 [   5.184], Avg:    39.203 (1.000) <0-01:14:35> ({'r_t':   603.6426, 'eps':     1.0000, 'critic_loss':     4.1058, 'actor_loss':    -0.1961, 'eps_e':     1.0000})
Step:  195000, Reward:    62.403 [  19.154], Avg:    39.321 (1.000) <0-01:14:57> ({'r_t':   683.9032, 'eps':     1.0000, 'critic_loss':     2.9968, 'actor_loss':    -0.1356, 'eps_e':     1.0000})
Step:  196000, Reward:    75.091 [  13.711], Avg:    39.502 (1.000) <0-01:15:19> ({'r_t':   811.0300, 'eps':     1.0000, 'critic_loss':     7.2173, 'actor_loss':    -0.2803, 'eps_e':     1.0000})
Step:  197000, Reward:    91.323 [   6.462], Avg:    39.764 (1.000) <0-01:15:41> ({'r_t':   905.2461, 'eps':     1.0000, 'critic_loss':    25.5529, 'actor_loss':     0.4211, 'eps_e':     1.0000})
Step:  198000, Reward:    95.036 [   4.707], Avg:    40.042 (1.000) <0-01:16:04> ({'r_t':  1031.1567, 'eps':     1.0000, 'critic_loss':     6.5838, 'actor_loss':    -0.0835, 'eps_e':     1.0000})
Step:  199000, Reward:    99.182 [   7.837], Avg:    40.338 (1.000) <0-01:16:26> ({'r_t':  1058.8003, 'eps':     1.0000, 'critic_loss':     4.3971, 'actor_loss':    -0.0054, 'eps_e':     1.0000})
Step:  200000, Reward:   101.907 [   5.611], Avg:    40.644 (1.000) <0-01:16:48> ({'r_t':  1079.0497, 'eps':     1.0000, 'critic_loss':     4.4619, 'actor_loss':    -0.0707, 'eps_e':     1.0000})
Step:  201000, Reward:   100.186 [   9.668], Avg:    40.939 (1.000) <0-01:17:10> ({'r_t':  1092.6581, 'eps':     1.0000, 'critic_loss':     4.2242, 'actor_loss':     0.0527, 'eps_e':     1.0000})
Step:  202000, Reward:   105.074 [   9.852], Avg:    41.255 (1.000) <0-01:17:33> ({'r_t':  1099.1182, 'eps':     1.0000, 'critic_loss':     9.7374, 'actor_loss':     0.0729, 'eps_e':     1.0000})
Step:  203000, Reward:   104.767 [   7.956], Avg:    41.566 (1.000) <0-01:17:55> ({'r_t':  1111.4054, 'eps':     1.0000, 'critic_loss':     9.6615, 'actor_loss':    -0.1056, 'eps_e':     1.0000})
Step:  204000, Reward:   106.005 [   8.701], Avg:    41.880 (1.000) <0-01:18:18> ({'r_t':  1164.6353, 'eps':     1.0000, 'critic_loss':     5.7126, 'actor_loss':    -0.1358, 'eps_e':     1.0000})
Step:  205000, Reward:   105.546 [   8.069], Avg:    42.189 (1.000) <0-01:18:40> ({'r_t':  1122.5605, 'eps':     1.0000, 'critic_loss':     5.0847, 'actor_loss':     0.1193, 'eps_e':     1.0000})
Step:  206000, Reward:   110.870 [   6.516], Avg:    42.521 (1.000) <0-01:19:02> ({'r_t':  1158.4956, 'eps':     1.0000, 'critic_loss':     6.6968, 'actor_loss':     0.0109, 'eps_e':     1.0000})
Step:  207000, Reward:    20.116 [  51.203], Avg:    42.413 (1.000) <0-01:19:25> ({'r_t':   880.7819, 'eps':     1.0000, 'critic_loss':  3276.0955, 'actor_loss':     4.3932, 'eps_e':     1.0000})
Step:  208000, Reward:    66.527 [  43.683], Avg:    42.529 (1.000) <0-01:19:47> ({'r_t':   523.9565, 'eps':     1.0000, 'critic_loss':   182.7025, 'actor_loss':    -3.6993, 'eps_e':     1.0000})
Step:  209000, Reward:    86.570 [   5.002], Avg:    42.739 (1.000) <0-01:20:09> ({'r_t':   776.3190, 'eps':     1.0000, 'critic_loss':    56.2986, 'actor_loss':     0.5473, 'eps_e':     1.0000})
Step:  210000, Reward:    92.753 [   7.051], Avg:    42.976 (1.000) <0-01:20:32> ({'r_t':   993.7941, 'eps':     1.0000, 'critic_loss':     8.7420, 'actor_loss':    -0.1291, 'eps_e':     1.0000})
Step:  211000, Reward:    97.242 [   8.051], Avg:    43.232 (1.000) <0-01:20:54> ({'r_t':  1022.6082, 'eps':     1.0000, 'critic_loss':     6.2795, 'actor_loss':     0.0509, 'eps_e':     1.0000})
Step:  212000, Reward:    96.954 [   7.352], Avg:    43.484 (1.000) <0-01:21:17> ({'r_t':  1050.8208, 'eps':     1.0000, 'critic_loss':     5.7074, 'actor_loss':     0.0329, 'eps_e':     1.0000})
Step:  213000, Reward:   103.986 [   5.035], Avg:    43.767 (1.000) <0-01:21:39> ({'r_t':  1083.5615, 'eps':     1.0000, 'critic_loss':     4.0838, 'actor_loss':    -0.1086, 'eps_e':     1.0000})
Step:  214000, Reward:   109.299 [   5.106], Avg:    44.071 (1.000) <0-01:22:01> ({'r_t':  1131.6468, 'eps':     1.0000, 'critic_loss':     4.1667, 'actor_loss':    -0.1445, 'eps_e':     1.0000})
Step:  215000, Reward:    98.342 [   6.998], Avg:    44.323 (1.000) <0-01:22:24> ({'r_t':  1160.0970, 'eps':     1.0000, 'critic_loss':     5.3532, 'actor_loss':     0.3612, 'eps_e':     1.0000})
Step:  216000, Reward:   105.174 [   6.806], Avg:    44.603 (1.000) <0-01:22:46> ({'r_t':  1115.2024, 'eps':     1.0000, 'critic_loss':     5.0038, 'actor_loss':    -0.0248, 'eps_e':     1.0000})
Step:  217000, Reward:   105.746 [   8.327], Avg:    44.883 (1.000) <0-01:23:09> ({'r_t':  1167.9283, 'eps':     1.0000, 'critic_loss':     4.1324, 'actor_loss':    -0.0070, 'eps_e':     1.0000})
Step:  218000, Reward:   105.959 [   8.391], Avg:    45.162 (1.000) <0-01:23:31> ({'r_t':  1176.4907, 'eps':     1.0000, 'critic_loss':     4.1474, 'actor_loss':     0.1610, 'eps_e':     1.0000})
Step:  219000, Reward:   112.552 [   5.706], Avg:    45.469 (1.000) <0-01:23:53> ({'r_t':  1178.5413, 'eps':     1.0000, 'critic_loss':     5.6601, 'actor_loss':     0.1150, 'eps_e':     1.0000})
Step:  220000, Reward:   107.022 [  17.643], Avg:    45.747 (1.000) <0-01:24:16> ({'r_t':  1149.0729, 'eps':     1.0000, 'critic_loss':     8.2231, 'actor_loss':     0.3308, 'eps_e':     1.0000})
Step:  221000, Reward:   111.566 [  11.726], Avg:    46.044 (1.000) <0-01:24:38> ({'r_t':  1176.6596, 'eps':     1.0000, 'critic_loss':    11.4109, 'actor_loss':     0.3613, 'eps_e':     1.0000})
Step:  222000, Reward:   -67.784 [   1.734], Avg:    45.533 (1.000) <0-01:25:00> ({'r_t':   526.4098, 'eps':     1.0000, 'critic_loss':  1160.1594, 'actor_loss':     9.0557, 'eps_e':     1.0000})
Step:  223000, Reward:   -66.867 [   1.509], Avg:    45.031 (1.000) <0-01:25:21> ({'r_t': -1058.2771, 'eps':     1.0000, 'critic_loss':    29.3229, 'actor_loss':     4.1027, 'eps_e':     1.0000})
Step:  224000, Reward:   -64.874 [   1.115], Avg:    44.543 (1.000) <0-01:25:42> ({'r_t': -1016.2735, 'eps':     1.0000, 'critic_loss':     0.9149, 'actor_loss':    -0.0216, 'eps_e':     1.0000})
Step:  225000, Reward:   -62.187 [   1.459], Avg:    44.071 (1.000) <0-01:26:02> ({'r_t':  -987.0816, 'eps':     1.0000, 'critic_loss':     0.6809, 'actor_loss':    -0.0233, 'eps_e':     1.0000})
Step:  226000, Reward:   -62.201 [   5.730], Avg:    43.603 (1.000) <0-01:26:24> ({'r_t':  -940.6101, 'eps':     1.0000, 'critic_loss':     2.0927, 'actor_loss':    -0.1652, 'eps_e':     1.0000})
Step:  227000, Reward:   -57.683 [  25.107], Avg:    43.158 (1.000) <0-01:26:47> ({'r_t':  -828.1733, 'eps':     1.0000, 'critic_loss':    24.4746, 'actor_loss':     0.0978, 'eps_e':     1.0000})
Step:  228000, Reward:   -53.141 [  53.223], Avg:    42.738 (1.000) <0-01:27:11> ({'r_t':  -625.4663, 'eps':     1.0000, 'critic_loss':    85.1634, 'actor_loss':    -0.1658, 'eps_e':     1.0000})
Step:  229000, Reward:   -57.836 [  62.425], Avg:    42.301 (1.000) <0-01:27:35> ({'r_t':  -462.4637, 'eps':     1.0000, 'critic_loss':    93.2269, 'actor_loss':    -0.5861, 'eps_e':     1.0000})
Step:  230000, Reward:   -30.866 [  38.553], Avg:    41.984 (1.000) <0-01:27:59> ({'r_t':  -369.3015, 'eps':     1.0000, 'critic_loss':    81.2267, 'actor_loss':    -0.0888, 'eps_e':     1.0000})
Step:  231000, Reward:   -28.943 [  26.254], Avg:    41.678 (1.000) <0-01:28:23> ({'r_t':  -369.1330, 'eps':     1.0000, 'critic_loss':    69.1749, 'actor_loss':     0.6072, 'eps_e':     1.0000})
Step:  232000, Reward:   -10.920 [   8.905], Avg:    41.452 (1.000) <0-01:28:46> ({'r_t':  -281.2513, 'eps':     1.0000, 'critic_loss':    56.0506, 'actor_loss':    -0.2012, 'eps_e':     1.0000})
Step:  233000, Reward:    -7.807 [   5.366], Avg:    41.242 (1.000) <0-01:29:08> ({'r_t':  -176.3364, 'eps':     1.0000, 'critic_loss':    28.6516, 'actor_loss':    -0.3791, 'eps_e':     1.0000})
Step:  234000, Reward:    -5.574 [   6.119], Avg:    41.043 (1.000) <0-01:29:30> ({'r_t':  -105.1267, 'eps':     1.0000, 'critic_loss':    11.5272, 'actor_loss':     0.0632, 'eps_e':     1.0000})
Step:  235000, Reward:    -1.937 [   3.548], Avg:    40.860 (1.000) <0-01:29:53> ({'r_t':   -27.4609, 'eps':     1.0000, 'critic_loss':    10.9067, 'actor_loss':     0.1197, 'eps_e':     1.0000})
Step:  236000, Reward:    30.962 [  31.765], Avg:    40.819 (1.000) <0-01:30:15> ({'r_t':   102.8344, 'eps':     1.0000, 'critic_loss':    59.0896, 'actor_loss':    -0.6177, 'eps_e':     1.0000})
Step:  237000, Reward:    51.653 [  47.809], Avg:    40.864 (1.000) <0-01:30:38> ({'r_t':   429.7480, 'eps':     1.0000, 'critic_loss':   150.4973, 'actor_loss':    -1.0357, 'eps_e':     1.0000})
Step:  238000, Reward:    58.291 [  44.100], Avg:    40.937 (1.000) <0-01:31:01> ({'r_t':   554.0868, 'eps':     1.0000, 'critic_loss':   144.3288, 'actor_loss':    -0.0871, 'eps_e':     1.0000})
Step:  239000, Reward:    79.632 [  32.424], Avg:    41.098 (1.000) <0-01:31:23> ({'r_t':   615.1157, 'eps':     1.0000, 'critic_loss':   144.1094, 'actor_loss':    -0.6879, 'eps_e':     1.0000})
Step:  240000, Reward:    57.688 [  49.093], Avg:    41.167 (1.000) <0-01:31:47> ({'r_t':   594.2081, 'eps':     1.0000, 'critic_loss':   121.6362, 'actor_loss':     0.5158, 'eps_e':     1.0000})
Step:  241000, Reward:    59.895 [  38.276], Avg:    41.245 (1.000) <0-01:32:11> ({'r_t':   600.9095, 'eps':     1.0000, 'critic_loss':   134.8276, 'actor_loss':    -0.0675, 'eps_e':     1.0000})
Step:  242000, Reward:    62.155 [  43.085], Avg:    41.331 (1.000) <0-01:32:34> ({'r_t':   565.3189, 'eps':     1.0000, 'critic_loss':   161.5371, 'actor_loss':    -0.1157, 'eps_e':     1.0000})
Step:  243000, Reward:    88.929 [  10.862], Avg:    41.526 (1.000) <0-01:32:58> ({'r_t':   560.4034, 'eps':     1.0000, 'critic_loss':   120.2132, 'actor_loss':     0.4004, 'eps_e':     1.0000})
Step:  244000, Reward:    77.351 [  32.922], Avg:    41.672 (1.000) <0-01:33:21> ({'r_t':   658.7859, 'eps':     1.0000, 'critic_loss':    85.7078, 'actor_loss':    -0.0512, 'eps_e':     1.0000})
Step:  245000, Reward:    83.368 [  31.040], Avg:    41.841 (1.000) <0-01:33:44> ({'r_t':   759.8512, 'eps':     1.0000, 'critic_loss':    62.7522, 'actor_loss':    -0.0797, 'eps_e':     1.0000})
Step:  246000, Reward:    64.245 [  41.169], Avg:    41.932 (1.000) <0-01:34:08> ({'r_t':   712.7001, 'eps':     1.0000, 'critic_loss':   104.8705, 'actor_loss':     0.6943, 'eps_e':     1.0000})
Step:  247000, Reward:    66.433 [  59.332], Avg:    42.031 (1.000) <0-01:34:31> ({'r_t':   694.4701, 'eps':     1.0000, 'critic_loss':    79.6400, 'actor_loss':    -0.2123, 'eps_e':     1.0000})
Step:  248000, Reward:    17.874 [  28.617], Avg:    41.934 (1.000) <0-01:34:54> ({'r_t':   162.5454, 'eps':     1.0000, 'critic_loss':  2681.8826, 'actor_loss':     3.8326, 'eps_e':     1.0000})
Step:  249000, Reward:    29.484 [  32.671], Avg:    41.884 (1.000) <0-01:35:17> ({'r_t':   302.0310, 'eps':     1.0000, 'critic_loss':    81.9598, 'actor_loss':    -0.6978, 'eps_e':     1.0000})
Step:  250000, Reward:    52.433 [  37.235], Avg:    41.926 (1.000) <0-01:35:39> ({'r_t':   509.2241, 'eps':     1.0000, 'critic_loss':    64.0432, 'actor_loss':    -0.5204, 'eps_e':     1.0000})
Step:  251000, Reward:    74.724 [  19.270], Avg:    42.056 (1.000) <0-01:36:03> ({'r_t':   646.7256, 'eps':     1.0000, 'critic_loss':    58.1048, 'actor_loss':    -0.3059, 'eps_e':     1.0000})
Step:  252000, Reward:    45.828 [  54.568], Avg:    42.071 (1.000) <0-01:36:26> ({'r_t':   589.4470, 'eps':     1.0000, 'critic_loss':   124.1884, 'actor_loss':     0.3431, 'eps_e':     1.0000})
Step:  253000, Reward:    96.153 [  35.973], Avg:    42.284 (1.000) <0-01:36:50> ({'r_t':   737.5854, 'eps':     1.0000, 'critic_loss':    95.3827, 'actor_loss':    -1.2352, 'eps_e':     1.0000})
Step:  254000, Reward:    84.739 [  36.089], Avg:    42.451 (1.000) <0-01:37:14> ({'r_t':   762.2788, 'eps':     1.0000, 'critic_loss':   146.3938, 'actor_loss':     0.5279, 'eps_e':     1.0000})
Step:  255000, Reward:    83.985 [  48.390], Avg:    42.613 (1.000) <0-01:37:38> ({'r_t':   756.1635, 'eps':     1.0000, 'critic_loss':   105.3670, 'actor_loss':    -0.2389, 'eps_e':     1.0000})
Step:  256000, Reward:    79.035 [  46.549], Avg:    42.755 (1.000) <0-01:38:01> ({'r_t':   802.0066, 'eps':     1.0000, 'critic_loss':   129.7722, 'actor_loss':     0.1105, 'eps_e':     1.0000})
Step:  257000, Reward:    83.645 [  48.870], Avg:    42.913 (1.000) <0-01:38:24> ({'r_t':   797.0861, 'eps':     1.0000, 'critic_loss':   106.8022, 'actor_loss':     0.1580, 'eps_e':     1.0000})
Step:  258000, Reward:    84.502 [  56.960], Avg:    43.074 (1.000) <0-01:38:47> ({'r_t':   799.9660, 'eps':     1.0000, 'critic_loss':    86.0876, 'actor_loss':     0.1192, 'eps_e':     1.0000})
Step:  259000, Reward:    96.960 [  34.238], Avg:    43.281 (1.000) <0-01:39:11> ({'r_t':   763.0046, 'eps':     1.0000, 'critic_loss':   143.4128, 'actor_loss':     0.0734, 'eps_e':     1.0000})
Step:  260000, Reward:    98.062 [  40.195], Avg:    43.491 (1.000) <0-01:39:34> ({'r_t':   899.2142, 'eps':     1.0000, 'critic_loss':    77.6332, 'actor_loss':    -0.6288, 'eps_e':     1.0000})
Step:  261000, Reward:    74.133 [  57.608], Avg:    43.608 (1.000) <0-01:39:58> ({'r_t':   959.7655, 'eps':     1.0000, 'critic_loss':    73.5126, 'actor_loss':     0.3855, 'eps_e':     1.0000})
Step:  262000, Reward:   115.549 [  13.310], Avg:    43.881 (1.000) <0-01:40:20> ({'r_t':   940.1370, 'eps':     1.0000, 'critic_loss':   107.1375, 'actor_loss':    -0.2004, 'eps_e':     1.0000})
Step:  263000, Reward:    99.707 [  35.659], Avg:    44.093 (1.000) <0-01:40:44> ({'r_t':   919.2682, 'eps':     1.0000, 'critic_loss':    71.7545, 'actor_loss':     0.7061, 'eps_e':     1.0000})
Step:  264000, Reward:   115.527 [   9.517], Avg:    44.362 (1.000) <0-01:41:07> ({'r_t':   943.8586, 'eps':     1.0000, 'critic_loss':   112.1731, 'actor_loss':    -0.2069, 'eps_e':     1.0000})
Step:  265000, Reward:    99.082 [  39.914], Avg:    44.568 (1.000) <0-01:41:30> ({'r_t':   945.0903, 'eps':     1.0000, 'critic_loss':    64.2947, 'actor_loss':     0.2968, 'eps_e':     1.0000})
Step:  266000, Reward:    56.303 [  23.213], Avg:    44.612 (1.000) <0-01:41:54> ({'r_t':   418.2143, 'eps':     1.0000, 'critic_loss':   115.0330, 'actor_loss':     2.2560, 'eps_e':     1.0000})
Step:  267000, Reward:    78.561 [  21.857], Avg:    44.739 (1.000) <0-01:42:17> ({'r_t':   646.3813, 'eps':     1.0000, 'critic_loss':    46.1232, 'actor_loss':    -1.4543, 'eps_e':     1.0000})
Step:  268000, Reward:   105.177 [  10.332], Avg:    44.963 (1.000) <0-01:42:40> ({'r_t':   818.7563, 'eps':     1.0000, 'critic_loss':    38.4469, 'actor_loss':    -0.8236, 'eps_e':     1.0000})
Step:  269000, Reward:   115.694 [   8.635], Avg:    45.225 (1.000) <0-01:43:03> ({'r_t':   943.3320, 'eps':     1.0000, 'critic_loss':    36.6572, 'actor_loss':     0.0038, 'eps_e':     1.0000})
Step:  270000, Reward:    99.828 [  47.103], Avg:    45.427 (1.000) <0-01:43:27> ({'r_t':   853.6793, 'eps':     1.0000, 'critic_loss':   120.7570, 'actor_loss':    -0.1487, 'eps_e':     1.0000})
Step:  271000, Reward:    81.796 [  50.768], Avg:    45.561 (1.000) <0-01:43:51> ({'r_t':   929.1353, 'eps':     1.0000, 'critic_loss':    60.8230, 'actor_loss':     0.6064, 'eps_e':     1.0000})
Step:  272000, Reward:    93.108 [  45.158], Avg:    45.735 (1.000) <0-01:44:14> ({'r_t':   936.9938, 'eps':     1.0000, 'critic_loss':    26.1753, 'actor_loss':     0.1442, 'eps_e':     1.0000})
Step:  273000, Reward:    82.166 [   8.477], Avg:    45.868 (1.000) <0-01:44:38> ({'r_t':   751.8434, 'eps':     1.0000, 'critic_loss':    75.8016, 'actor_loss':     1.0573, 'eps_e':     1.0000})
Step:  274000, Reward:    99.630 [  13.483], Avg:    46.063 (1.000) <0-01:45:02> ({'r_t':   744.3417, 'eps':     1.0000, 'critic_loss':    72.4337, 'actor_loss':     0.1235, 'eps_e':     1.0000})
Step:  275000, Reward:    99.661 [  32.859], Avg:    46.257 (1.000) <0-01:45:26> ({'r_t':   863.6680, 'eps':     1.0000, 'critic_loss':    47.0978, 'actor_loss':    -0.0240, 'eps_e':     1.0000})
Step:  276000, Reward:    84.477 [  51.430], Avg:    46.395 (1.000) <0-01:45:49> ({'r_t':   886.1124, 'eps':     1.0000, 'critic_loss':    60.5334, 'actor_loss':    -0.0032, 'eps_e':     1.0000})
Step:  277000, Reward:   112.050 [  23.107], Avg:    46.631 (1.000) <0-01:46:13> ({'r_t':   894.4310, 'eps':     1.0000, 'critic_loss':    83.8942, 'actor_loss':     0.2171, 'eps_e':     1.0000})
Step:  278000, Reward:   106.471 [  29.533], Avg:    46.846 (1.000) <0-01:46:37> ({'r_t':   949.1878, 'eps':     1.0000, 'critic_loss':    34.4030, 'actor_loss':    -0.2452, 'eps_e':     1.0000})
Step:  279000, Reward:    92.976 [  38.654], Avg:    47.011 (1.000) <0-01:47:00> ({'r_t':   977.3544, 'eps':     1.0000, 'critic_loss':    62.1625, 'actor_loss':     0.6033, 'eps_e':     1.0000})
Step:  280000, Reward:   100.315 [  29.716], Avg:    47.200 (1.000) <0-01:47:23> ({'r_t':   880.8456, 'eps':     1.0000, 'critic_loss':    81.6768, 'actor_loss':     0.2751, 'eps_e':     1.0000})
Step:  281000, Reward:    94.563 [  45.028], Avg:    47.368 (1.000) <0-01:47:47> ({'r_t':   940.0668, 'eps':     1.0000, 'critic_loss':   106.3815, 'actor_loss':     0.0077, 'eps_e':     1.0000})
Step:  282000, Reward:    69.749 [  73.968], Avg:    47.447 (1.000) <0-01:48:11> ({'r_t':   904.5442, 'eps':     1.0000, 'critic_loss':    75.6689, 'actor_loss':     0.3623, 'eps_e':     1.0000})
Step:  283000, Reward:   100.297 [  49.991], Avg:    47.634 (1.000) <0-01:48:34> ({'r_t':   894.5470, 'eps':     1.0000, 'critic_loss':   168.4319, 'actor_loss':    -0.2704, 'eps_e':     1.0000})
Step:  284000, Reward:   109.509 [  11.373], Avg:    47.851 (1.000) <0-01:48:58> ({'r_t':   920.4523, 'eps':     1.0000, 'critic_loss':    96.9564, 'actor_loss':     0.2733, 'eps_e':     1.0000})
Step:  285000, Reward:   101.191 [  36.233], Avg:    48.037 (1.000) <0-01:49:21> ({'r_t':   976.3106, 'eps':     1.0000, 'critic_loss':    48.5343, 'actor_loss':     0.1413, 'eps_e':     1.0000})
Step:  286000, Reward:   109.817 [  36.930], Avg:    48.252 (1.000) <0-01:49:44> ({'r_t':   926.0708, 'eps':     1.0000, 'critic_loss':   119.2608, 'actor_loss':    -0.4709, 'eps_e':     1.0000})
Step:  287000, Reward:   110.634 [  20.878], Avg:    48.469 (1.000) <0-01:50:08> ({'r_t':  1013.9602, 'eps':     1.0000, 'critic_loss':    40.7255, 'actor_loss':    -0.0471, 'eps_e':     1.0000})
Step:  288000, Reward:    53.179 [  67.332], Avg:    48.485 (1.000) <0-01:50:32> ({'r_t':   393.4153, 'eps':     1.0000, 'critic_loss':   255.3133, 'actor_loss':     1.0503, 'eps_e':     1.0000})
Step:  289000, Reward:    53.920 [  69.829], Avg:    48.504 (1.000) <0-01:50:55> ({'r_t':   683.7865, 'eps':     1.0000, 'critic_loss':   153.0877, 'actor_loss':    -0.4465, 'eps_e':     1.0000})
Step:  290000, Reward:   101.235 [  33.688], Avg:    48.685 (1.000) <0-01:51:19> ({'r_t':   807.3107, 'eps':     1.0000, 'critic_loss':    81.4595, 'actor_loss':     0.3878, 'eps_e':     1.0000})
Step:  291000, Reward:    87.903 [  46.085], Avg:    48.820 (1.000) <0-01:51:42> ({'r_t':   836.4680, 'eps':     1.0000, 'critic_loss':    84.3318, 'actor_loss':     0.1845, 'eps_e':     1.0000})
Step:  292000, Reward:    92.172 [  50.721], Avg:    48.968 (1.000) <0-01:52:06> ({'r_t':   685.0554, 'eps':     1.0000, 'critic_loss':   169.3053, 'actor_loss':     0.4671, 'eps_e':     1.0000})
Step:  293000, Reward:   105.897 [  34.004], Avg:    49.161 (1.000) <0-01:52:30> ({'r_t':   877.4376, 'eps':     1.0000, 'critic_loss':   104.0188, 'actor_loss':     0.4713, 'eps_e':     1.0000})
Step:  294000, Reward:    79.258 [  42.465], Avg:    49.263 (1.000) <0-01:52:53> ({'r_t':   764.4187, 'eps':     1.0000, 'critic_loss':   132.7618, 'actor_loss':     0.4688, 'eps_e':     1.0000})
Step:  295000, Reward:    69.037 [  60.896], Avg:    49.330 (1.000) <0-01:53:17> ({'r_t':   845.5512, 'eps':     1.0000, 'critic_loss':    69.8152, 'actor_loss':    -0.7928, 'eps_e':     1.0000})
Step:  296000, Reward:    88.052 [  42.024], Avg:    49.460 (1.000) <0-01:53:40> ({'r_t':   861.1628, 'eps':     1.0000, 'critic_loss':    73.8744, 'actor_loss':     0.4028, 'eps_e':     1.0000})
Step:  297000, Reward:   104.361 [  30.987], Avg:    49.645 (1.000) <0-01:54:04> ({'r_t':   967.1469, 'eps':     1.0000, 'critic_loss':    59.2852, 'actor_loss':    -0.0915, 'eps_e':     1.0000})
Step:  298000, Reward:    92.531 [  43.131], Avg:    49.788 (1.000) <0-01:54:27> ({'r_t':   963.3745, 'eps':     1.0000, 'critic_loss':    65.5164, 'actor_loss':     0.2395, 'eps_e':     1.0000})
Step:  299000, Reward:   108.369 [  13.363], Avg:    49.983 (1.000) <0-01:54:51> ({'r_t':   969.5999, 'eps':     1.0000, 'critic_loss':    44.3534, 'actor_loss':     0.0024, 'eps_e':     1.0000})
Step:  300000, Reward:   116.989 [  10.021], Avg:    50.206 (1.000) <0-01:55:14> ({'r_t':  1005.4009, 'eps':     1.0000, 'critic_loss':    41.7150, 'actor_loss':     0.3304, 'eps_e':     1.0000})
Step:  301000, Reward:   108.431 [  30.776], Avg:    50.399 (1.000) <0-01:55:38> ({'r_t':   986.5133, 'eps':     1.0000, 'critic_loss':    83.4768, 'actor_loss':     0.1822, 'eps_e':     1.0000})
Step:  302000, Reward:   119.731 [   8.392], Avg:    50.628 (1.000) <0-01:56:01> ({'r_t':  1044.9857, 'eps':     1.0000, 'critic_loss':    68.1807, 'actor_loss':    -0.4609, 'eps_e':     1.0000})
Step:  303000, Reward:   108.306 [  37.850], Avg:    50.817 (1.000) <0-01:56:24> ({'r_t':  1024.7308, 'eps':     1.0000, 'critic_loss':    41.9204, 'actor_loss':    -0.0909, 'eps_e':     1.0000})
Step:  304000, Reward:    97.234 [  62.406], Avg:    50.969 (1.000) <0-01:56:47> ({'r_t':   897.5272, 'eps':     1.0000, 'critic_loss':    98.1754, 'actor_loss':     1.3164, 'eps_e':     1.0000})
Step:  305000, Reward:   105.618 [  17.080], Avg:    51.148 (1.000) <0-01:57:11> ({'r_t':   963.7039, 'eps':     1.0000, 'critic_loss':    54.3467, 'actor_loss':    -0.0765, 'eps_e':     1.0000})
Step:  306000, Reward:    88.895 [  57.099], Avg:    51.271 (1.000) <0-01:57:34> ({'r_t':   852.6942, 'eps':     1.0000, 'critic_loss':   105.2365, 'actor_loss':     0.8160, 'eps_e':     1.0000})
Step:  307000, Reward:   102.629 [  37.390], Avg:    51.438 (1.000) <0-01:57:57> ({'r_t':   911.2472, 'eps':     1.0000, 'critic_loss':   126.8173, 'actor_loss':    -0.6007, 'eps_e':     1.0000})
Step:  308000, Reward:   108.064 [  30.295], Avg:    51.621 (1.000) <0-01:58:21> ({'r_t':  1012.5049, 'eps':     1.0000, 'critic_loss':    66.1472, 'actor_loss':     0.0761, 'eps_e':     1.0000})
Step:  309000, Reward:   119.944 [   8.770], Avg:    51.841 (1.000) <0-01:58:44> ({'r_t':   926.0428, 'eps':     1.0000, 'critic_loss':    68.2346, 'actor_loss':     0.0958, 'eps_e':     1.0000})
Step:  310000, Reward:   102.062 [  33.470], Avg:    52.003 (1.000) <0-01:59:08> ({'r_t':  1024.3902, 'eps':     1.0000, 'critic_loss':    18.0283, 'actor_loss':     0.1808, 'eps_e':     1.0000})
Step:  311000, Reward:   114.225 [  16.521], Avg:    52.202 (1.000) <0-01:59:32> ({'r_t':   946.3121, 'eps':     1.0000, 'critic_loss':    68.7279, 'actor_loss':     0.5482, 'eps_e':     1.0000})
Step:  312000, Reward:    74.938 [  66.677], Avg:    52.275 (1.000) <0-01:59:55> ({'r_t':   959.4115, 'eps':     1.0000, 'critic_loss':    84.8464, 'actor_loss':     0.8760, 'eps_e':     1.0000})
Step:  313000, Reward:   118.541 [  12.719], Avg:    52.486 (1.000) <0-02:00:19> ({'r_t':   836.7504, 'eps':     1.0000, 'critic_loss':   190.0624, 'actor_loss':     0.1335, 'eps_e':     1.0000})
Step:  314000, Reward:   112.160 [  31.705], Avg:    52.675 (1.000) <0-02:00:42> ({'r_t':   987.4311, 'eps':     1.0000, 'critic_loss':   104.5990, 'actor_loss':    -0.1628, 'eps_e':     1.0000})
Step:  315000, Reward:   108.052 [  13.016], Avg:    52.851 (1.000) <0-02:01:05> ({'r_t':  1034.5431, 'eps':     1.0000, 'critic_loss':    44.4584, 'actor_loss':    -0.1868, 'eps_e':     1.0000})
Step:  316000, Reward:    79.082 [  48.632], Avg:    52.933 (1.000) <0-02:01:29> ({'r_t':   924.8366, 'eps':     1.0000, 'critic_loss':    28.9182, 'actor_loss':     0.8468, 'eps_e':     1.0000})
Step:  317000, Reward:    90.379 [  52.181], Avg:    53.051 (1.000) <0-02:01:52> ({'r_t':   926.4998, 'eps':     1.0000, 'critic_loss':    54.3456, 'actor_loss':     0.0544, 'eps_e':     1.0000})
Step:  318000, Reward:    95.134 [  54.420], Avg:    53.183 (1.000) <0-02:02:15> ({'r_t':   986.8867, 'eps':     1.0000, 'critic_loss':    75.1219, 'actor_loss':    -0.2642, 'eps_e':     1.0000})
Step:  319000, Reward:   122.330 [   9.957], Avg:    53.399 (1.000) <0-02:02:38> ({'r_t':  1009.3226, 'eps':     1.0000, 'critic_loss':    41.9385, 'actor_loss':     0.6191, 'eps_e':     1.0000})
Step:  320000, Reward:   115.449 [  36.947], Avg:    53.592 (1.000) <0-02:03:02> ({'r_t':  1059.5099, 'eps':     1.0000, 'critic_loss':    57.5396, 'actor_loss':     0.1364, 'eps_e':     1.0000})
Step:  321000, Reward:   101.272 [  56.247], Avg:    53.741 (1.000) <0-02:03:26> ({'r_t':  1023.5847, 'eps':     1.0000, 'critic_loss':    85.6425, 'actor_loss':     0.6405, 'eps_e':     1.0000})
Step:  322000, Reward:   111.954 [  49.091], Avg:    53.921 (1.000) <0-02:03:49> ({'r_t':  1013.6771, 'eps':     1.0000, 'critic_loss':   115.3770, 'actor_loss':    -0.0289, 'eps_e':     1.0000})
Step:  323000, Reward:   113.932 [  39.204], Avg:    54.106 (1.000) <0-02:04:12> ({'r_t':  1003.5787, 'eps':     1.0000, 'critic_loss':    80.5666, 'actor_loss':    -0.1758, 'eps_e':     1.0000})
Step:  324000, Reward:   103.305 [  41.062], Avg:    54.257 (1.000) <0-02:04:36> ({'r_t':   917.3064, 'eps':     1.0000, 'critic_loss':   103.0626, 'actor_loss':     0.1139, 'eps_e':     1.0000})
Step:  325000, Reward:    96.024 [  49.494], Avg:    54.385 (1.000) <0-02:05:01> ({'r_t':   970.0911, 'eps':     1.0000, 'critic_loss':    76.4357, 'actor_loss':     0.1837, 'eps_e':     1.0000})
Step:  326000, Reward:   118.690 [  32.644], Avg:    54.582 (1.000) <0-02:05:24> ({'r_t':  1073.5695, 'eps':     1.0000, 'critic_loss':    21.6147, 'actor_loss':    -0.5779, 'eps_e':     1.0000})
Step:  327000, Reward:   129.774 [   9.076], Avg:    54.811 (1.000) <0-02:05:47> ({'r_t':  1013.4173, 'eps':     1.0000, 'critic_loss':   111.5196, 'actor_loss':     0.2062, 'eps_e':     1.0000})
Step:  328000, Reward:   -36.333 [  48.741], Avg:    54.534 (1.000) <0-02:06:11> ({'r_t':   938.9547, 'eps':     1.0000, 'critic_loss':   105.6534, 'actor_loss':     1.7816, 'eps_e':     1.0000})
Step:  329000, Reward:    12.394 [  31.582], Avg:    54.407 (1.000) <0-02:06:35> ({'r_t':   -12.8867, 'eps':     1.0000, 'critic_loss':  2086.6472, 'actor_loss':     2.8816, 'eps_e':     1.0000})
Step:  330000, Reward:    32.102 [  21.991], Avg:    54.339 (1.000) <0-02:06:57> ({'r_t':   220.5154, 'eps':     1.0000, 'critic_loss':    98.6091, 'actor_loss':     0.8513, 'eps_e':     1.0000})
Step:  331000, Reward:    79.718 [  30.064], Avg:    54.416 (1.000) <0-02:07:21> ({'r_t':   408.7021, 'eps':     1.0000, 'critic_loss':    39.7778, 'actor_loss':    -0.7954, 'eps_e':     1.0000})
Step:  332000, Reward:   104.640 [  19.754], Avg:    54.567 (1.000) <0-02:07:45> ({'r_t':   801.1723, 'eps':     1.0000, 'critic_loss':    83.1997, 'actor_loss':    -1.0888, 'eps_e':     1.0000})
Step:  333000, Reward:   104.795 [  30.849], Avg:    54.717 (1.000) <0-02:08:09> ({'r_t':   928.1605, 'eps':     1.0000, 'critic_loss':    62.6126, 'actor_loss':    -0.0340, 'eps_e':     1.0000})
Step:  334000, Reward:   107.628 [  38.137], Avg:    54.875 (1.000) <0-02:08:33> ({'r_t':   938.3277, 'eps':     1.0000, 'critic_loss':    56.2203, 'actor_loss':     0.2530, 'eps_e':     1.0000})
Step:  335000, Reward:   121.009 [  16.221], Avg:    55.072 (1.000) <0-02:08:56> ({'r_t':  1056.0276, 'eps':     1.0000, 'critic_loss':    36.6777, 'actor_loss':    -0.1201, 'eps_e':     1.0000})
Step:  336000, Reward:    94.442 [  62.912], Avg:    55.188 (1.000) <0-02:09:20> ({'r_t':  1057.8269, 'eps':     1.0000, 'critic_loss':    46.3371, 'actor_loss':     0.0688, 'eps_e':     1.0000})
Step:  337000, Reward:   127.094 [  12.286], Avg:    55.401 (1.000) <0-02:09:43> ({'r_t':  1097.0270, 'eps':     1.0000, 'critic_loss':    75.9441, 'actor_loss':     0.0721, 'eps_e':     1.0000})
Step:  338000, Reward:   115.153 [  38.514], Avg:    55.577 (1.000) <0-02:10:07> ({'r_t':  1111.5537, 'eps':     1.0000, 'critic_loss':    41.3305, 'actor_loss':     0.3374, 'eps_e':     1.0000})
Step:  339000, Reward:   113.148 [  44.668], Avg:    55.747 (1.000) <0-02:10:31> ({'r_t':  1059.3021, 'eps':     1.0000, 'critic_loss':    56.5147, 'actor_loss':    -0.0851, 'eps_e':     1.0000})
Step:  340000, Reward:   113.314 [  40.808], Avg:    55.916 (1.000) <0-02:10:54> ({'r_t':  1004.3019, 'eps':     1.0000, 'critic_loss':    92.6392, 'actor_loss':     0.5930, 'eps_e':     1.0000})
Step:  341000, Reward:   125.714 [  34.369], Avg:    56.120 (1.000) <0-02:11:18> ({'r_t':  1074.9693, 'eps':     1.0000, 'critic_loss':    88.2036, 'actor_loss':    -0.1256, 'eps_e':     1.0000})
Step:  342000, Reward:   104.959 [  62.737], Avg:    56.262 (1.000) <0-02:11:41> ({'r_t':  1133.7165, 'eps':     1.0000, 'critic_loss':    57.4702, 'actor_loss':    -0.1697, 'eps_e':     1.0000})
Step:  343000, Reward:   118.524 [  33.094], Avg:    56.443 (1.000) <0-02:12:05> ({'r_t':  1104.4634, 'eps':     1.0000, 'critic_loss':    52.1512, 'actor_loss':    -0.3397, 'eps_e':     1.0000})
Step:  344000, Reward:   100.057 [  49.300], Avg:    56.570 (1.000) <0-02:12:28> ({'r_t':   946.1040, 'eps':     1.0000, 'critic_loss':    74.5856, 'actor_loss':     0.3630, 'eps_e':     1.0000})
Step:  345000, Reward:   114.953 [  38.768], Avg:    56.738 (1.000) <0-02:12:51> ({'r_t':  1071.5294, 'eps':     1.0000, 'critic_loss':    14.8503, 'actor_loss':    -0.2417, 'eps_e':     1.0000})
Step:  346000, Reward:   118.777 [   9.635], Avg:    56.917 (1.000) <0-02:13:15> ({'r_t':  1075.1840, 'eps':     1.0000, 'critic_loss':    26.5024, 'actor_loss':     0.2217, 'eps_e':     1.0000})
Step:  347000, Reward:   131.177 [   6.497], Avg:    57.130 (1.000) <0-02:13:38> ({'r_t':  1063.0322, 'eps':     1.0000, 'critic_loss':    41.9139, 'actor_loss':     0.1873, 'eps_e':     1.0000})
Step:  348000, Reward:   107.189 [  57.143], Avg:    57.274 (1.000) <0-02:14:02> ({'r_t':  1144.1584, 'eps':     1.0000, 'critic_loss':    17.2929, 'actor_loss':    -0.4259, 'eps_e':     1.0000})
Step:  349000, Reward:    94.042 [  25.734], Avg:    57.379 (1.000) <0-02:14:25> ({'r_t':  1063.0004, 'eps':     1.0000, 'critic_loss':    92.5454, 'actor_loss':     0.9889, 'eps_e':     1.0000})
Step:  350000, Reward:    97.870 [  33.982], Avg:    57.494 (1.000) <0-02:14:49> ({'r_t':   933.7183, 'eps':     1.0000, 'critic_loss':    60.9063, 'actor_loss':     0.4868, 'eps_e':     1.0000})
Step:  351000, Reward:   102.783 [  42.153], Avg:    57.623 (1.000) <0-02:15:12> ({'r_t':   963.2688, 'eps':     1.0000, 'critic_loss':    55.1588, 'actor_loss':    -0.1134, 'eps_e':     1.0000})
Step:  352000, Reward:   104.664 [  46.802], Avg:    57.756 (1.000) <0-02:15:35> ({'r_t':  1061.8225, 'eps':     1.0000, 'critic_loss':    36.4208, 'actor_loss':    -0.2009, 'eps_e':     1.0000})
Step:  353000, Reward:   112.951 [  53.106], Avg:    57.912 (1.000) <0-02:15:58> ({'r_t':   968.9833, 'eps':     1.0000, 'critic_loss':   119.9438, 'actor_loss':     0.4465, 'eps_e':     1.0000})
Step:  354000, Reward:    97.544 [  18.043], Avg:    58.024 (1.000) <0-02:16:22> ({'r_t':   847.1739, 'eps':     1.0000, 'critic_loss':  1117.5865, 'actor_loss':     0.5304, 'eps_e':     1.0000})
Step:  355000, Reward:   123.285 [  33.008], Avg:    58.207 (1.000) <0-02:16:45> ({'r_t':   895.2815, 'eps':     1.0000, 'critic_loss':    42.5420, 'actor_loss':    -0.4851, 'eps_e':     1.0000})
Step:  356000, Reward:   127.060 [  15.372], Avg:    58.400 (1.000) <0-02:17:09> ({'r_t':   956.1017, 'eps':     1.0000, 'critic_loss':   100.5605, 'actor_loss':     0.0426, 'eps_e':     1.0000})
Step:  357000, Reward:   122.899 [  34.555], Avg:    58.580 (1.000) <0-02:17:33> ({'r_t':   937.4106, 'eps':     1.0000, 'critic_loss':   179.3713, 'actor_loss':    -0.0792, 'eps_e':     1.0000})
Step:  358000, Reward:   121.480 [  41.531], Avg:    58.755 (1.000) <0-02:17:56> ({'r_t':  1039.3162, 'eps':     1.0000, 'critic_loss':    64.2958, 'actor_loss':    -0.3417, 'eps_e':     1.0000})
Step:  359000, Reward:   116.592 [  41.790], Avg:    58.916 (1.000) <0-02:18:21> ({'r_t':  1021.4491, 'eps':     1.0000, 'critic_loss':    83.8958, 'actor_loss':     0.5645, 'eps_e':     1.0000})
Step:  360000, Reward:   125.972 [   9.524], Avg:    59.102 (1.000) <0-02:18:45> ({'r_t':  1075.6291, 'eps':     1.0000, 'critic_loss':    56.3637, 'actor_loss':    -0.0253, 'eps_e':     1.0000})
Step:  361000, Reward:   117.574 [  47.077], Avg:    59.263 (1.000) <0-02:19:09> ({'r_t':  1105.5448, 'eps':     1.0000, 'critic_loss':    29.2263, 'actor_loss':    -0.4219, 'eps_e':     1.0000})
Step:  362000, Reward:   103.742 [  48.151], Avg:    59.386 (1.000) <0-02:19:32> ({'r_t':  1031.1047, 'eps':     1.0000, 'critic_loss':    56.4797, 'actor_loss':     0.7380, 'eps_e':     1.0000})
Step:  363000, Reward:   104.472 [  61.683], Avg:    59.510 (1.000) <0-02:19:56> ({'r_t':  1078.6690, 'eps':     1.0000, 'critic_loss':    57.9542, 'actor_loss':     0.0397, 'eps_e':     1.0000})
Step:  364000, Reward:   118.938 [  47.540], Avg:    59.672 (1.000) <0-02:20:19> ({'r_t':  1000.4514, 'eps':     1.0000, 'critic_loss':   165.9844, 'actor_loss':     0.6832, 'eps_e':     1.0000})
Step:  365000, Reward:   143.951 [   6.445], Avg:    59.903 (1.000) <0-02:20:42> ({'r_t':  1133.8037, 'eps':     1.0000, 'critic_loss':   108.3552, 'actor_loss':    -0.8721, 'eps_e':     1.0000})
Step:  366000, Reward:   125.881 [  41.695], Avg:    60.083 (1.000) <0-02:21:06> ({'r_t':  1076.5000, 'eps':     1.0000, 'critic_loss':    57.2255, 'actor_loss':     0.3136, 'eps_e':     1.0000})
Step:  367000, Reward:   137.728 [  12.479], Avg:    60.294 (1.000) <0-02:21:30> ({'r_t':  1195.1008, 'eps':     1.0000, 'critic_loss':    14.9753, 'actor_loss':    -0.0889, 'eps_e':     1.0000})
Step:  368000, Reward:   148.369 [   6.070], Avg:    60.532 (1.000) <0-02:21:53> ({'r_t':  1206.6705, 'eps':     1.0000, 'critic_loss':    43.2367, 'actor_loss':    -0.0338, 'eps_e':     1.0000})
Step:  369000, Reward:   118.781 [  11.060], Avg:    60.690 (1.000) <0-02:22:17> ({'r_t':  1088.9012, 'eps':     1.0000, 'critic_loss':    38.6225, 'actor_loss':     0.9076, 'eps_e':     1.0000})
Step:  370000, Reward:   126.015 [  12.924], Avg:    60.866 (1.000) <0-02:22:41> ({'r_t':  1026.2083, 'eps':     1.0000, 'critic_loss':    61.5365, 'actor_loss':     0.1183, 'eps_e':     1.0000})
Step:  371000, Reward:   126.822 [  42.121], Avg:    61.043 (1.000) <0-02:23:04> ({'r_t':  1012.7291, 'eps':     1.0000, 'critic_loss':   130.2545, 'actor_loss':     0.2280, 'eps_e':     1.0000})
Step:  372000, Reward:   119.712 [  33.318], Avg:    61.200 (1.000) <0-02:23:28> ({'r_t':  1084.2394, 'eps':     1.0000, 'critic_loss':   102.3864, 'actor_loss':     0.1911, 'eps_e':     1.0000})
Step:  373000, Reward:   128.689 [  36.069], Avg:    61.381 (1.000) <0-02:23:51> ({'r_t':  1029.8304, 'eps':     1.0000, 'critic_loss':   104.6205, 'actor_loss':     0.2069, 'eps_e':     1.0000})
Step:  374000, Reward:   135.287 [   9.883], Avg:    61.578 (1.000) <0-02:24:14> ({'r_t':  1119.0363, 'eps':     1.0000, 'critic_loss':    52.8899, 'actor_loss':    -0.0877, 'eps_e':     1.0000})
Step:  375000, Reward:   118.607 [  40.054], Avg:    61.730 (1.000) <0-02:24:38> ({'r_t':  1124.4133, 'eps':     1.0000, 'critic_loss':    49.6428, 'actor_loss':     0.0059, 'eps_e':     1.0000})
Step:  376000, Reward:   121.243 [  50.076], Avg:    61.887 (1.000) <0-02:25:02> ({'r_t':  1103.3084, 'eps':     1.0000, 'critic_loss':    58.4352, 'actor_loss':     0.3355, 'eps_e':     1.0000})
Step:  377000, Reward:   130.434 [  43.331], Avg:    62.069 (1.000) <0-02:25:25> ({'r_t':  1089.4962, 'eps':     1.0000, 'critic_loss':    80.9465, 'actor_loss':    -0.2129, 'eps_e':     1.0000})
Step:  378000, Reward:   142.227 [   7.592], Avg:    62.280 (1.000) <0-02:25:49> ({'r_t':  1123.6553, 'eps':     1.0000, 'critic_loss':    89.1168, 'actor_loss':     0.3085, 'eps_e':     1.0000})
Step:  379000, Reward:    93.471 [  38.129], Avg:    62.362 (1.000) <0-02:26:12> ({'r_t':   947.1662, 'eps':     1.0000, 'critic_loss':    43.5331, 'actor_loss':     1.6795, 'eps_e':     1.0000})
Step:  380000, Reward:   -10.479 [ 354.663], Avg:    62.171 (1.000) <0-02:26:36> ({'r_t':    48.7003, 'eps':     1.0000, 'critic_loss':  7123.0981, 'actor_loss':     6.3854, 'eps_e':     1.0000})
Step:  381000, Reward:   119.699 [  62.071], Avg:    62.322 (1.000) <0-02:27:01> ({'r_t':   378.3361, 'eps':     1.0000, 'critic_loss':  6560.1084, 'actor_loss':    -2.0416, 'eps_e':     1.0000})
Step:  382000, Reward:   119.475 [  41.664], Avg:    62.471 (1.000) <0-02:27:24> ({'r_t':  1019.9261, 'eps':     1.0000, 'critic_loss':  1269.9583, 'actor_loss':    -2.8330, 'eps_e':     1.0000})
Step:  383000, Reward:   122.898 [  41.341], Avg:    62.628 (1.000) <0-02:27:48> ({'r_t':  1087.0219, 'eps':     1.0000, 'critic_loss':    52.7059, 'actor_loss':    -0.9888, 'eps_e':     1.0000})
Step:  384000, Reward:    83.471 [  62.623], Avg:    62.682 (1.000) <0-02:28:12> ({'r_t':  1003.9027, 'eps':     1.0000, 'critic_loss':  1267.9982, 'actor_loss':     2.1462, 'eps_e':     1.0000})
Step:  385000, Reward:   102.219 [  53.232], Avg:    62.785 (1.000) <0-02:28:35> ({'r_t':   977.3303, 'eps':     1.0000, 'critic_loss':   103.5857, 'actor_loss':    -1.0545, 'eps_e':     1.0000})
Step:  386000, Reward:   140.611 [  14.515], Avg:    62.986 (1.000) <0-02:28:58> ({'r_t':  1182.9542, 'eps':     1.0000, 'critic_loss':    54.9874, 'actor_loss':    -0.5268, 'eps_e':     1.0000})
Step:  387000, Reward:    72.203 [  23.951], Avg:    63.010 (1.000) <0-02:29:23> ({'r_t':   470.5679, 'eps':     1.0000, 'critic_loss':  3257.8025, 'actor_loss':     3.3799, 'eps_e':     1.0000})
Step:  388000, Reward:   127.660 [  36.143], Avg:    63.176 (1.000) <0-02:29:47> ({'r_t':   357.7054, 'eps':     1.0000, 'critic_loss':  2544.5928, 'actor_loss':     0.7503, 'eps_e':     1.0000})
Step:  389000, Reward:   128.043 [  14.369], Avg:    63.342 (1.000) <0-02:30:11> ({'r_t':  1040.8123, 'eps':     1.0000, 'critic_loss':    47.7758, 'actor_loss':    -2.0283, 'eps_e':     1.0000})
Step:  390000, Reward:   127.486 [  42.330], Avg:    63.506 (1.000) <0-02:30:35> ({'r_t':  1082.7182, 'eps':     1.0000, 'critic_loss':    32.3598, 'actor_loss':     0.2991, 'eps_e':     1.0000})
Step:  391000, Reward:   140.058 [  12.131], Avg:    63.702 (1.000) <0-02:30:59> ({'r_t':  1082.6036, 'eps':     1.0000, 'critic_loss':    29.1003, 'actor_loss':     0.1494, 'eps_e':     1.0000})
Step:  392000, Reward:   121.674 [  58.890], Avg:    63.849 (1.000) <0-02:31:22> ({'r_t':  1119.2797, 'eps':     1.0000, 'critic_loss':    96.2514, 'actor_loss':     0.3404, 'eps_e':     1.0000})
Step:  393000, Reward:   141.957 [  10.572], Avg:    64.047 (1.000) <0-02:31:46> ({'r_t':  1074.4765, 'eps':     1.0000, 'critic_loss':   137.3245, 'actor_loss':     0.0702, 'eps_e':     1.0000})
Step:  394000, Reward:   118.837 [  39.254], Avg:    64.186 (1.000) <0-02:32:10> ({'r_t':  1114.2487, 'eps':     1.0000, 'critic_loss':    41.6856, 'actor_loss':     0.3971, 'eps_e':     1.0000})
Step:  395000, Reward:   121.457 [  49.068], Avg:    64.331 (1.000) <0-02:32:33> ({'r_t':  1124.0424, 'eps':     1.0000, 'critic_loss':    28.3826, 'actor_loss':     0.2577, 'eps_e':     1.0000})
Step:  396000, Reward:   140.547 [  11.352], Avg:    64.523 (1.000) <0-02:32:57> ({'r_t':  1086.5106, 'eps':     1.0000, 'critic_loss':   150.4563, 'actor_loss':     0.4952, 'eps_e':     1.0000})
Step:  397000, Reward:   134.762 [  37.415], Avg:    64.699 (1.000) <0-02:33:20> ({'r_t':  1150.0474, 'eps':     1.0000, 'critic_loss':    55.9091, 'actor_loss':     0.1337, 'eps_e':     1.0000})
Step:  398000, Reward:   105.888 [  81.603], Avg:    64.802 (1.000) <0-02:33:44> ({'r_t':  1131.0816, 'eps':     1.0000, 'critic_loss':    86.7574, 'actor_loss':    -0.0565, 'eps_e':     1.0000})
Step:  399000, Reward:   133.456 [   7.676], Avg:    64.974 (1.000) <0-02:34:08> ({'r_t':   930.6214, 'eps':     1.0000, 'critic_loss':  1253.7505, 'actor_loss':     0.3435, 'eps_e':     1.0000})
Step:  400000, Reward:    99.474 [  29.080], Avg:    65.060 (1.000) <0-02:34:34> ({'r_t':   639.6125, 'eps':     1.0000, 'critic_loss':  1117.7263, 'actor_loss':     0.0809, 'eps_e':     1.0000})
Step:  401000, Reward:    96.156 [  41.438], Avg:    65.137 (1.000) <0-02:34:58> ({'r_t':   670.7560, 'eps':     1.0000, 'critic_loss':   657.0905, 'actor_loss':     0.6506, 'eps_e':     1.0000})
Step:  402000, Reward:   131.626 [  16.273], Avg:    65.302 (1.000) <0-02:35:23> ({'r_t':   856.5879, 'eps':     1.0000, 'critic_loss':    63.3617, 'actor_loss':     0.2087, 'eps_e':     1.0000})
Step:  403000, Reward:  -499.367 [ 652.217], Avg:    63.905 (1.000) <0-02:35:53> ({'r_t':   368.7382, 'eps':     1.0000, 'critic_loss':  1082.7560, 'actor_loss':     6.0909, 'eps_e':     1.0000})
Step:  404000, Reward:  -280.321 [ 413.913], Avg:    63.055 (1.000) <0-02:36:21> ({'r_t':  -736.1173, 'eps':     1.0000, 'critic_loss':    89.6320, 'actor_loss':     9.1851, 'eps_e':     1.0000})
Step:  405000, Reward:  -206.808 [  94.306], Avg:    62.390 (1.000) <0-02:36:50> ({'r_t':  -875.0312, 'eps':     1.0000, 'critic_loss':   896.0373, 'actor_loss':     3.4358, 'eps_e':     1.0000})
Step:  406000, Reward:  -189.094 [ 110.156], Avg:    61.772 (1.000) <0-02:37:18> ({'r_t':  -824.9668, 'eps':     1.0000, 'critic_loss':    36.1291, 'actor_loss':    -0.5767, 'eps_e':     1.0000})
Step:  407000, Reward:  -146.704 [  70.899], Avg:    61.261 (1.000) <0-02:37:45> ({'r_t':  -776.4427, 'eps':     1.0000, 'critic_loss':    29.4942, 'actor_loss':    -0.9497, 'eps_e':     1.0000})
Step:  408000, Reward:  -113.193 [  94.460], Avg:    60.835 (1.000) <0-02:38:13> ({'r_t':  -732.5500, 'eps':     1.0000, 'critic_loss':    27.7532, 'actor_loss':    -0.3070, 'eps_e':     1.0000})
Step:  409000, Reward:   -67.504 [  90.393], Avg:    60.522 (1.000) <0-02:38:40> ({'r_t':  -695.6231, 'eps':     1.0000, 'critic_loss':    34.0911, 'actor_loss':     0.0577, 'eps_e':     1.0000})
Step:  410000, Reward:   -80.573 [  99.257], Avg:    60.178 (1.000) <0-02:39:06> ({'r_t':  -516.4724, 'eps':     1.0000, 'critic_loss':    32.6111, 'actor_loss':    -0.6729, 'eps_e':     1.0000})
Step:  411000, Reward:   -27.214 [  56.455], Avg:    59.966 (1.000) <0-02:39:33> ({'r_t':  -360.1025, 'eps':     1.0000, 'critic_loss':    32.3652, 'actor_loss':    -0.3212, 'eps_e':     1.0000})
Step:  412000, Reward:    -4.946 [  16.063], Avg:    59.809 (1.000) <0-02:39:56> ({'r_t':  -238.0549, 'eps':     1.0000, 'critic_loss':    27.4264, 'actor_loss':     0.0247, 'eps_e':     1.0000})
Step:  413000, Reward:     2.122 [  10.445], Avg:    59.670 (1.000) <0-02:40:19> ({'r_t':  -108.8475, 'eps':     1.0000, 'critic_loss':    18.3484, 'actor_loss':    -0.3140, 'eps_e':     1.0000})
Step:  414000, Reward:    10.892 [  17.525], Avg:    59.552 (1.000) <0-02:40:41> ({'r_t':    41.5291, 'eps':     1.0000, 'critic_loss':    20.0920, 'actor_loss':    -0.2590, 'eps_e':     1.0000})
Step:  415000, Reward:    31.796 [  19.640], Avg:    59.485 (1.000) <0-02:41:04> ({'r_t':   251.5746, 'eps':     1.0000, 'critic_loss':    32.2386, 'actor_loss':    -0.5726, 'eps_e':     1.0000})
Step:  416000, Reward:    42.088 [  16.592], Avg:    59.444 (1.000) <0-02:41:27> ({'r_t':   347.7616, 'eps':     1.0000, 'critic_loss':    42.4512, 'actor_loss':     0.0378, 'eps_e':     1.0000})
Step:  417000, Reward:    51.612 [  25.395], Avg:    59.425 (1.000) <0-02:41:49> ({'r_t':   389.6459, 'eps':     1.0000, 'critic_loss':    54.5023, 'actor_loss':     0.2065, 'eps_e':     1.0000})
Step:  418000, Reward:    38.585 [  38.517], Avg:    59.375 (1.000) <0-02:42:12> ({'r_t':   508.1173, 'eps':     1.0000, 'critic_loss':    47.2932, 'actor_loss':    -0.2173, 'eps_e':     1.0000})
Step:  419000, Reward:    51.572 [  36.407], Avg:    59.357 (1.000) <0-02:42:34> ({'r_t':   478.8129, 'eps':     1.0000, 'critic_loss':    64.0573, 'actor_loss':     0.8230, 'eps_e':     1.0000})
Step:  420000, Reward:    27.427 [  46.659], Avg:    59.281 (1.000) <0-02:42:59> ({'r_t':   467.2159, 'eps':     1.0000, 'critic_loss':    89.9198, 'actor_loss':    -1.0763, 'eps_e':     1.0000})
Step:  421000, Reward:    38.704 [  28.327], Avg:    59.232 (1.000) <0-02:43:21> ({'r_t':   342.2817, 'eps':     1.0000, 'critic_loss':   831.6730, 'actor_loss':     2.4188, 'eps_e':     1.0000})
Step:  422000, Reward:    55.582 [  13.360], Avg:    59.223 (1.000) <0-02:43:44> ({'r_t':   480.1817, 'eps':     1.0000, 'critic_loss':    37.5540, 'actor_loss':     0.8770, 'eps_e':     1.0000})
Step:  423000, Reward:    43.701 [  31.511], Avg:    59.187 (1.000) <0-02:44:06> ({'r_t':   504.6385, 'eps':     1.0000, 'critic_loss':    38.1388, 'actor_loss':    -0.0100, 'eps_e':     1.0000})
Step:  424000, Reward:    57.627 [  23.800], Avg:    59.183 (1.000) <0-02:44:29> ({'r_t':   567.4562, 'eps':     1.0000, 'critic_loss':    37.1824, 'actor_loss':     0.0880, 'eps_e':     1.0000})
Step:  425000, Reward:    57.460 [  27.035], Avg:    59.179 (1.000) <0-02:44:51> ({'r_t':   529.0512, 'eps':     1.0000, 'critic_loss':    68.1775, 'actor_loss':     0.1716, 'eps_e':     1.0000})
Step:  426000, Reward:    68.362 [  14.882], Avg:    59.201 (1.000) <0-02:45:14> ({'r_t':   465.8142, 'eps':     1.0000, 'critic_loss':    67.2946, 'actor_loss':     0.0288, 'eps_e':     1.0000})
Step:  427000, Reward:    73.338 [  11.108], Avg:    59.234 (1.000) <0-02:45:37> ({'r_t':   481.4862, 'eps':     1.0000, 'critic_loss':    43.4441, 'actor_loss':    -0.6491, 'eps_e':     1.0000})
Step:  428000, Reward:    65.413 [  13.140], Avg:    59.248 (1.000) <0-02:46:00> ({'r_t':   485.1543, 'eps':     1.0000, 'critic_loss':    41.4132, 'actor_loss':     0.6176, 'eps_e':     1.0000})
Step:  429000, Reward:   -38.494 [ 377.367], Avg:    59.021 (1.000) <0-02:46:24> ({'r_t':   622.9581, 'eps':     1.0000, 'critic_loss':    43.0073, 'actor_loss':     0.1152, 'eps_e':     1.0000})
Step:  430000, Reward:  -330.764 [ 672.164], Avg:    58.116 (1.000) <0-02:46:49> ({'r_t': -1187.5606, 'eps':     1.0000, 'critic_loss': 20280.8945, 'actor_loss':    25.2322, 'eps_e':     1.0000})
Step:  431000, Reward:  -157.091 [ 535.395], Avg:    57.618 (1.000) <0-02:47:15> ({'r_t': -1530.9301, 'eps':     1.0000, 'critic_loss': 26484.3613, 'actor_loss':   -21.9320, 'eps_e':     1.0000})
Step:  432000, Reward:  -458.533 [ 714.655], Avg:    56.426 (1.000) <0-02:47:39> ({'r_t': -1096.3978, 'eps':     1.0000, 'critic_loss': 10069.6787, 'actor_loss':    -1.4206, 'eps_e':     1.0000})
Step:  433000, Reward:  -154.169 [ 513.803], Avg:    55.941 (1.000) <0-02:48:04> ({'r_t':  -911.2108, 'eps':     1.0000, 'critic_loss':  4916.6064, 'actor_loss':    -2.0001, 'eps_e':     1.0000})
Step:  434000, Reward:    54.534 [  27.903], Avg:    55.938 (1.000) <0-02:48:26> ({'r_t':   -99.2977, 'eps':     1.0000, 'critic_loss':  6151.4453, 'actor_loss':     0.4302, 'eps_e':     1.0000})
Step:  435000, Reward:    61.726 [  13.619], Avg:    55.951 (1.000) <0-02:48:49> ({'r_t':   341.3981, 'eps':     1.0000, 'critic_loss':   800.7720, 'actor_loss':     0.1507, 'eps_e':     1.0000})
Step:  436000, Reward:    60.675 [  18.906], Avg:    55.962 (1.000) <0-02:49:11> ({'r_t':   612.8858, 'eps':     1.0000, 'critic_loss':    63.2333, 'actor_loss':     0.3378, 'eps_e':     1.0000})
Step:  437000, Reward:   -26.680 [ 383.459], Avg:    55.773 (1.000) <0-02:49:36> ({'r_t':   371.1003, 'eps':     1.0000, 'critic_loss':   876.4569, 'actor_loss':     2.4101, 'eps_e':     1.0000})
Step:  438000, Reward:   -54.582 [ 350.836], Avg:    55.522 (1.000) <0-02:49:59> ({'r_t':   220.4559, 'eps':     1.0000, 'critic_loss':  3302.0105, 'actor_loss':    -1.2904, 'eps_e':     1.0000})
Step:  439000, Reward:   -23.999 [ 351.924], Avg:    55.341 (1.000) <0-02:50:23> ({'r_t':   121.1628, 'eps':     1.0000, 'critic_loss':  3719.8950, 'actor_loss':     0.7012, 'eps_e':     1.0000})
Step:  440000, Reward:   -22.412 [ 362.824], Avg:    55.165 (1.000) <0-02:50:46> ({'r_t':  -154.5716, 'eps':     1.0000, 'critic_loss':  7367.1699, 'actor_loss':     0.9296, 'eps_e':     1.0000})
Step:  441000, Reward:    58.234 [  22.425], Avg:    55.172 (1.000) <0-02:51:11> ({'r_t':  -621.4517, 'eps':     1.0000, 'critic_loss':  8862.3018, 'actor_loss':     3.9278, 'eps_e':     1.0000})
Step:  442000, Reward:    51.692 [  25.797], Avg:    55.164 (1.000) <0-02:51:34> ({'r_t':   104.1096, 'eps':     1.0000, 'critic_loss':  2803.4956, 'actor_loss':    -1.1049, 'eps_e':     1.0000})
Step:  443000, Reward:    48.829 [  26.681], Avg:    55.149 (1.000) <0-02:51:57> ({'r_t':   291.4906, 'eps':     1.0000, 'critic_loss':   855.3424, 'actor_loss':     2.5712, 'eps_e':     1.0000})
Step:  444000, Reward:    43.837 [  47.418], Avg:    55.124 (1.000) <0-02:52:23> ({'r_t':   532.1265, 'eps':     1.0000, 'critic_loss':   225.3846, 'actor_loss':    -1.6689, 'eps_e':     1.0000})
Step:  445000, Reward:    57.800 [  22.749], Avg:    55.130 (1.000) <0-02:52:45> ({'r_t':   403.0205, 'eps':     1.0000, 'critic_loss':   955.6101, 'actor_loss':     1.0851, 'eps_e':     1.0000})
Step:  446000, Reward:    58.495 [  42.860], Avg:    55.138 (1.000) <0-02:53:08> ({'r_t':   537.0710, 'eps':     1.0000, 'critic_loss':    87.8064, 'actor_loss':    -0.6745, 'eps_e':     1.0000})
Step:  447000, Reward:    52.973 [  47.373], Avg:    55.133 (1.000) <0-02:53:32> ({'r_t':   510.6633, 'eps':     1.0000, 'critic_loss':    76.6167, 'actor_loss':    -0.3086, 'eps_e':     1.0000})
Step:  448000, Reward:    59.197 [  49.649], Avg:    55.142 (1.000) <0-02:53:56> ({'r_t':   666.1059, 'eps':     1.0000, 'critic_loss':    53.5914, 'actor_loss':    -0.6108, 'eps_e':     1.0000})
Step:  449000, Reward:    72.792 [  25.659], Avg:    55.181 (1.000) <0-02:54:19> ({'r_t':   620.4949, 'eps':     1.0000, 'critic_loss':    75.1570, 'actor_loss':     0.4296, 'eps_e':     1.0000})
Step:  450000, Reward:    87.638 [  28.385], Avg:    55.253 (1.000) <0-02:54:44> ({'r_t':   770.1032, 'eps':     1.0000, 'critic_loss':    38.9843, 'actor_loss':    -0.6465, 'eps_e':     1.0000})
Step:  451000, Reward:    86.818 [  23.776], Avg:    55.323 (1.000) <0-02:55:07> ({'r_t':   784.0238, 'eps':     1.0000, 'critic_loss':    48.2133, 'actor_loss':     0.5011, 'eps_e':     1.0000})
Step:  452000, Reward:    -1.665 [  22.805], Avg:    55.197 (1.000) <0-02:55:31> ({'r_t':   427.3815, 'eps':     1.0000, 'critic_loss':  1113.5968, 'actor_loss':     2.8509, 'eps_e':     1.0000})
Step:  453000, Reward:    24.815 [  31.538], Avg:    55.130 (1.000) <0-02:55:54> ({'r_t':   137.6310, 'eps':     1.0000, 'critic_loss':    62.2388, 'actor_loss':    -0.1525, 'eps_e':     1.0000})
Step:  454000, Reward:    61.147 [  20.462], Avg:    55.143 (1.000) <0-02:56:16> ({'r_t':   405.0550, 'eps':     1.0000, 'critic_loss':    81.2820, 'actor_loss':    -0.4801, 'eps_e':     1.0000})
Step:  455000, Reward:    45.766 [  39.201], Avg:    55.123 (1.000) <0-02:56:38> ({'r_t':   530.2972, 'eps':     1.0000, 'critic_loss':    77.1721, 'actor_loss':    -0.3505, 'eps_e':     1.0000})
Step:  456000, Reward:    67.129 [  15.250], Avg:    55.149 (1.000) <0-02:57:01> ({'r_t':   623.1740, 'eps':     1.0000, 'critic_loss':    56.4386, 'actor_loss':     0.4112, 'eps_e':     1.0000})
Step:  457000, Reward:    71.608 [  19.775], Avg:    55.185 (1.000) <0-02:57:24> ({'r_t':   638.8602, 'eps':     1.0000, 'critic_loss':    37.2085, 'actor_loss':     0.3013, 'eps_e':     1.0000})
Step:  458000, Reward:    65.978 [  28.961], Avg:    55.208 (1.000) <0-02:57:46> ({'r_t':   552.0418, 'eps':     1.0000, 'critic_loss':   378.4796, 'actor_loss':     1.2548, 'eps_e':     1.0000})
Step:  459000, Reward:    61.067 [  51.331], Avg:    55.221 (1.000) <0-02:58:11> ({'r_t':   735.5911, 'eps':     1.0000, 'critic_loss':    31.1819, 'actor_loss':    -0.4330, 'eps_e':     1.0000})
Step:  460000, Reward:    75.101 [  27.417], Avg:    55.264 (1.000) <0-02:58:34> ({'r_t':   447.7712, 'eps':     1.0000, 'critic_loss':  2380.2229, 'actor_loss':     1.9830, 'eps_e':     1.0000})
Step:  461000, Reward:    77.139 [  13.944], Avg:    55.312 (1.000) <0-02:58:56> ({'r_t':   192.0784, 'eps':     1.0000, 'critic_loss':  7304.5557, 'actor_loss':     0.6325, 'eps_e':     1.0000})
Step:  462000, Reward:    71.580 [  35.961], Avg:    55.347 (1.000) <0-02:59:22> ({'r_t':   -17.2103, 'eps':     1.0000, 'critic_loss':  6861.8398, 'actor_loss':     7.9765, 'eps_e':     1.0000})
Step:  463000, Reward:    82.874 [   2.190], Avg:    55.406 (1.000) <0-02:59:44> ({'r_t':   730.9059, 'eps':     1.0000, 'critic_loss':    40.8537, 'actor_loss':    -0.4463, 'eps_e':     1.0000})
Step:  464000, Reward:    79.039 [  14.027], Avg:    55.457 (1.000) <0-03:00:07> ({'r_t':   785.2341, 'eps':     1.0000, 'critic_loss':   141.4959, 'actor_loss':    -0.3855, 'eps_e':     1.0000})
Step:  465000, Reward:    65.146 [  55.361], Avg:    55.478 (1.000) <0-03:00:32> ({'r_t':   814.3159, 'eps':     1.0000, 'critic_loss':     9.0423, 'actor_loss':     0.0521, 'eps_e':     1.0000})
Step:  466000, Reward:    86.909 [   3.248], Avg:    55.545 (1.000) <0-03:00:54> ({'r_t':   809.8809, 'eps':     1.0000, 'critic_loss':    50.1211, 'actor_loss':     0.1714, 'eps_e':     1.0000})
Step:  467000, Reward:    88.076 [   3.118], Avg:    55.615 (1.000) <0-03:01:17> ({'r_t':   816.5063, 'eps':     1.0000, 'critic_loss':    27.5257, 'actor_loss':     0.0116, 'eps_e':     1.0000})
Step:  468000, Reward:    78.209 [  14.924], Avg:    55.663 (1.000) <0-03:01:40> ({'r_t':   828.9924, 'eps':     1.0000, 'critic_loss':    18.5150, 'actor_loss':    -0.1157, 'eps_e':     1.0000})
Step:  469000, Reward:    88.077 [   2.818], Avg:    55.732 (1.000) <0-03:02:04> ({'r_t':   802.0239, 'eps':     1.0000, 'critic_loss':    22.1489, 'actor_loss':     0.4854, 'eps_e':     1.0000})
Step:  470000, Reward:    78.249 [  47.616], Avg:    55.780 (1.000) <0-03:02:28> ({'r_t':   900.9337, 'eps':     1.0000, 'critic_loss':    28.5075, 'actor_loss':     0.1517, 'eps_e':     1.0000})
Step:  471000, Reward:    80.354 [  32.219], Avg:    55.832 (1.000) <0-03:02:52> ({'r_t':   832.8762, 'eps':     1.0000, 'critic_loss':    17.3686, 'actor_loss':     0.1647, 'eps_e':     1.0000})
Step:  472000, Reward:    54.609 [  53.271], Avg:    55.829 (1.000) <0-03:03:15> ({'r_t':   557.9971, 'eps':     1.0000, 'critic_loss':   158.4966, 'actor_loss':     0.7942, 'eps_e':     1.0000})
Step:  473000, Reward:    91.019 [   4.526], Avg:    55.903 (1.000) <0-03:03:37> ({'r_t':   852.3905, 'eps':     1.0000, 'critic_loss':    95.1687, 'actor_loss':    -0.9134, 'eps_e':     1.0000})
Step:  474000, Reward:    82.303 [  38.085], Avg:    55.959 (1.000) <0-03:04:01> ({'r_t':   924.1894, 'eps':     1.0000, 'critic_loss':    22.4060, 'actor_loss':     0.0057, 'eps_e':     1.0000})
Step:  475000, Reward:    89.821 [  24.713], Avg:    56.030 (1.000) <0-03:04:25> ({'r_t':   935.5960, 'eps':     1.0000, 'critic_loss':     7.5324, 'actor_loss':     0.0247, 'eps_e':     1.0000})
Step:  476000, Reward:    95.218 [   5.192], Avg:    56.112 (1.000) <0-03:04:48> ({'r_t':   947.6213, 'eps':     1.0000, 'critic_loss':    10.5304, 'actor_loss':     0.3855, 'eps_e':     1.0000})
Step:  477000, Reward:    16.174 [  12.671], Avg:    56.029 (1.000) <0-03:05:09> ({'r_t':   216.4359, 'eps':     1.0000, 'critic_loss':  1576.1670, 'actor_loss':     3.6702, 'eps_e':     1.0000})
Step:  478000, Reward:    29.900 [   4.953], Avg:    55.974 (1.000) <0-03:05:31> ({'r_t':   223.5280, 'eps':     1.0000, 'critic_loss':    30.6450, 'actor_loss':    -0.0792, 'eps_e':     1.0000})
Step:  479000, Reward:    32.796 [   5.410], Avg:    55.926 (1.000) <0-03:05:53> ({'r_t':   309.0194, 'eps':     1.0000, 'critic_loss':    61.9558, 'actor_loss':    -0.8799, 'eps_e':     1.0000})
Step:  480000, Reward:    38.253 [  15.189], Avg:    55.889 (1.000) <0-03:06:15> ({'r_t':   447.1610, 'eps':     1.0000, 'critic_loss':    14.2600, 'actor_loss':    -0.1438, 'eps_e':     1.0000})
Step:  481000, Reward:    35.501 [  37.780], Avg:    55.847 (1.000) <0-03:06:37> ({'r_t':   558.3591, 'eps':     1.0000, 'critic_loss':     8.0162, 'actor_loss':    -0.4586, 'eps_e':     1.0000})
Step:  482000, Reward:    50.260 [  17.949], Avg:    55.835 (1.000) <0-03:06:59> ({'r_t':   527.1939, 'eps':     1.0000, 'critic_loss':    21.9674, 'actor_loss':     0.2216, 'eps_e':     1.0000})
Step:  483000, Reward:    52.777 [   6.073], Avg:    55.829 (1.000) <0-03:07:21> ({'r_t':   563.5357, 'eps':     1.0000, 'critic_loss':    16.0057, 'actor_loss':    -0.2395, 'eps_e':     1.0000})
Step:  484000, Reward:    62.266 [   5.362], Avg:    55.842 (1.000) <0-03:07:43> ({'r_t':   641.2602, 'eps':     1.0000, 'critic_loss':    10.4834, 'actor_loss':     0.1378, 'eps_e':     1.0000})
Step:  485000, Reward:    62.065 [   7.689], Avg:    55.855 (1.000) <0-03:08:05> ({'r_t':   685.2504, 'eps':     1.0000, 'critic_loss':     5.2232, 'actor_loss':    -0.1043, 'eps_e':     1.0000})
Step:  486000, Reward:    67.837 [   6.425], Avg:    55.879 (1.000) <0-03:08:28> ({'r_t':   729.6381, 'eps':     1.0000, 'critic_loss':    20.2740, 'actor_loss':     0.2456, 'eps_e':     1.0000})
Step:  487000, Reward:    75.278 [  11.003], Avg:    55.919 (1.000) <0-03:08:50> ({'r_t':   802.6964, 'eps':     1.0000, 'critic_loss':    14.6320, 'actor_loss':    -0.4736, 'eps_e':     1.0000})
Step:  488000, Reward:    61.028 [  23.556], Avg:    55.930 (1.000) <0-03:09:13> ({'r_t':   834.1321, 'eps':     1.0000, 'critic_loss':    11.1901, 'actor_loss':    -0.0574, 'eps_e':     1.0000})
Step:  489000, Reward:    76.294 [  17.449], Avg:    55.971 (1.000) <0-03:09:35> ({'r_t':   900.6789, 'eps':     1.0000, 'critic_loss':    11.9970, 'actor_loss':     0.0271, 'eps_e':     1.0000})
Step:  490000, Reward:    78.545 [   8.677], Avg:    56.017 (1.000) <0-03:09:58> ({'r_t':   818.9859, 'eps':     1.0000, 'critic_loss':    30.2235, 'actor_loss':     0.5552, 'eps_e':     1.0000})
Step:  491000, Reward:    71.986 [   9.023], Avg:    56.050 (1.000) <0-03:10:20> ({'r_t':   829.0503, 'eps':     1.0000, 'critic_loss':     9.7427, 'actor_loss':     0.0564, 'eps_e':     1.0000})
Step:  492000, Reward:    70.025 [  19.203], Avg:    56.078 (1.000) <0-03:10:42> ({'r_t':   884.4125, 'eps':     1.0000, 'critic_loss':    11.9267, 'actor_loss':     0.0348, 'eps_e':     1.0000})
Step:  493000, Reward:    72.523 [  17.513], Avg:    56.111 (1.000) <0-03:11:05> ({'r_t':   811.7932, 'eps':     1.0000, 'critic_loss':    18.7528, 'actor_loss':    -0.0322, 'eps_e':     1.0000})
Step:  494000, Reward:    82.422 [  10.608], Avg:    56.164 (1.000) <0-03:11:27> ({'r_t':   946.4130, 'eps':     1.0000, 'critic_loss':     9.4364, 'actor_loss':    -0.3160, 'eps_e':     1.0000})
Step:  495000, Reward:    75.208 [  11.589], Avg:    56.203 (1.000) <0-03:11:49> ({'r_t':   838.0309, 'eps':     1.0000, 'critic_loss':    20.6325, 'actor_loss':     0.0193, 'eps_e':     1.0000})
Step:  496000, Reward:    69.068 [  18.405], Avg:    56.229 (1.000) <0-03:12:12> ({'r_t':   919.1551, 'eps':     1.0000, 'critic_loss':    14.8340, 'actor_loss':    -0.1402, 'eps_e':     1.0000})
Step:  497000, Reward:    81.468 [   9.049], Avg:    56.279 (1.000) <0-03:12:34> ({'r_t':   873.0818, 'eps':     1.0000, 'critic_loss':    20.6224, 'actor_loss':     0.0481, 'eps_e':     1.0000})
Step:  498000, Reward:    83.473 [   7.765], Avg:    56.334 (1.000) <0-03:12:56> ({'r_t':   930.3288, 'eps':     1.0000, 'critic_loss':    10.6444, 'actor_loss':     0.1445, 'eps_e':     1.0000})
Step:  499000, Reward:    88.448 [   7.057], Avg:    56.398 (1.000) <0-03:13:18> ({'r_t':   910.2285, 'eps':     1.0000, 'critic_loss':    11.8403, 'actor_loss':     0.1193, 'eps_e':     1.0000})
Step:  500000, Reward:    84.705 [   8.928], Avg:    56.455 (1.000) <0-03:13:41> ({'r_t':   918.3477, 'eps':     1.0000, 'critic_loss':    15.9212, 'actor_loss':    -0.0690, 'eps_e':     1.0000})
Step:  501000, Reward:    85.540 [  13.393], Avg:    56.513 (1.000) <0-03:14:03> ({'r_t':   951.4691, 'eps':     1.0000, 'critic_loss':    14.9121, 'actor_loss':     0.2727, 'eps_e':     1.0000})
Step:  502000, Reward:    84.986 [  13.526], Avg:    56.569 (1.000) <0-03:14:25> ({'r_t':   976.1718, 'eps':     1.0000, 'critic_loss':    11.5496, 'actor_loss':    -0.0324, 'eps_e':     1.0000})
Step:  503000, Reward:    79.663 [  22.868], Avg:    56.615 (1.000) <0-03:14:48> ({'r_t':   956.6045, 'eps':     1.0000, 'critic_loss':    13.5302, 'actor_loss':     0.0285, 'eps_e':     1.0000})
Step:  504000, Reward:    80.104 [  14.309], Avg:    56.662 (1.000) <0-03:15:10> ({'r_t':   980.3428, 'eps':     1.0000, 'critic_loss':    14.9862, 'actor_loss':     0.2107, 'eps_e':     1.0000})
Step:  505000, Reward:    83.483 [  18.477], Avg:    56.715 (1.000) <0-03:15:33> ({'r_t':   972.7638, 'eps':     1.0000, 'critic_loss':     8.7391, 'actor_loss':     0.1741, 'eps_e':     1.0000})
Step:  506000, Reward:    87.232 [  16.802], Avg:    56.775 (1.000) <0-03:15:56> ({'r_t':   985.5415, 'eps':     1.0000, 'critic_loss':    10.9452, 'actor_loss':    -0.1110, 'eps_e':     1.0000})
Step:  507000, Reward:    91.082 [   9.369], Avg:    56.842 (1.000) <0-03:16:18> ({'r_t':  1008.5946, 'eps':     1.0000, 'critic_loss':     5.8882, 'actor_loss':    -0.0794, 'eps_e':     1.0000})
Step:  508000, Reward:    87.290 [  16.920], Avg:    56.902 (1.000) <0-03:16:40> ({'r_t':  1007.9239, 'eps':     1.0000, 'critic_loss':     8.6990, 'actor_loss':     0.2010, 'eps_e':     1.0000})
Step:  509000, Reward:    92.032 [   6.548], Avg:    56.971 (1.000) <0-03:17:02> ({'r_t':  1031.6439, 'eps':     1.0000, 'critic_loss':     6.6324, 'actor_loss':    -0.1646, 'eps_e':     1.0000})
Step:  510000, Reward:    92.092 [   3.816], Avg:    57.040 (1.000) <0-03:17:25> ({'r_t':  1010.9091, 'eps':     1.0000, 'critic_loss':     9.4764, 'actor_loss':     0.2682, 'eps_e':     1.0000})
Step:  511000, Reward:    92.289 [   6.173], Avg:    57.109 (1.000) <0-03:17:47> ({'r_t':  1027.5446, 'eps':     1.0000, 'critic_loss':     6.1598, 'actor_loss':    -0.0147, 'eps_e':     1.0000})
Step:  512000, Reward:    88.169 [   9.117], Avg:    57.169 (1.000) <0-03:18:09> ({'r_t':   983.9088, 'eps':     1.0000, 'critic_loss':    14.7275, 'actor_loss':     0.5092, 'eps_e':     1.0000})
Step:  513000, Reward:    91.847 [   9.852], Avg:    57.237 (1.000) <0-03:18:31> ({'r_t':  1054.1886, 'eps':     1.0000, 'critic_loss':     7.0470, 'actor_loss':     0.0472, 'eps_e':     1.0000})
Step:  514000, Reward:    92.799 [   5.394], Avg:    57.306 (1.000) <0-03:18:53> ({'r_t':  1068.7395, 'eps':     1.0000, 'critic_loss':     5.4864, 'actor_loss':     0.2924, 'eps_e':     1.0000})
Step:  515000, Reward:    62.483 [  28.674], Avg:    57.316 (1.000) <0-03:19:16> ({'r_t':   976.9631, 'eps':     1.0000, 'critic_loss':     7.7190, 'actor_loss':     0.7655, 'eps_e':     1.0000})
Step:  516000, Reward:    94.470 [   5.351], Avg:    57.388 (1.000) <0-03:19:38> ({'r_t':   889.7035, 'eps':     1.0000, 'critic_loss':    23.3159, 'actor_loss':    -0.2684, 'eps_e':     1.0000})
Step:  517000, Reward:    94.246 [   4.992], Avg:    57.459 (1.000) <0-03:20:01> ({'r_t':  1017.8182, 'eps':     1.0000, 'critic_loss':     9.7851, 'actor_loss':    -0.4731, 'eps_e':     1.0000})
Step:  518000, Reward:    96.104 [   3.709], Avg:    57.533 (1.000) <0-03:20:23> ({'r_t':  1045.9488, 'eps':     1.0000, 'critic_loss':     5.6852, 'actor_loss':    -0.1367, 'eps_e':     1.0000})
Step:  519000, Reward:    85.064 [  27.534], Avg:    57.586 (1.000) <0-03:20:46> ({'r_t':  1073.4746, 'eps':     1.0000, 'critic_loss':     5.6310, 'actor_loss':    -0.0579, 'eps_e':     1.0000})
Step:  520000, Reward:    95.087 [   8.138], Avg:    57.658 (1.000) <0-03:21:08> ({'r_t':  1062.7169, 'eps':     1.0000, 'critic_loss':     4.5617, 'actor_loss':    -0.0143, 'eps_e':     1.0000})
Step:  521000, Reward:    94.452 [   5.552], Avg:    57.729 (1.000) <0-03:21:30> ({'r_t':  1008.5956, 'eps':     1.0000, 'critic_loss':     9.0853, 'actor_loss':     0.1582, 'eps_e':     1.0000})
Step:  522000, Reward:    76.645 [  69.204], Avg:    57.765 (1.000) <0-03:21:57> ({'r_t':  1006.8557, 'eps':     1.0000, 'critic_loss':     8.1055, 'actor_loss':     0.1046, 'eps_e':     1.0000})
Step:  523000, Reward:    90.965 [  32.989], Avg:    57.828 (1.000) <0-03:22:21> ({'r_t':  1051.1260, 'eps':     1.0000, 'critic_loss':     4.4862, 'actor_loss':     0.1011, 'eps_e':     1.0000})
Step:  524000, Reward:    97.399 [   6.823], Avg:    57.903 (1.000) <0-03:22:43> ({'r_t':  1003.8255, 'eps':     1.0000, 'critic_loss':     9.8043, 'actor_loss':     0.1532, 'eps_e':     1.0000})
Step:  525000, Reward:    85.655 [  40.374], Avg:    57.956 (1.000) <0-03:23:08> ({'r_t':  1011.0095, 'eps':     1.0000, 'critic_loss':     9.8702, 'actor_loss':     0.1887, 'eps_e':     1.0000})
Step:  526000, Reward:    99.030 [   4.717], Avg:    58.034 (1.000) <0-03:23:31> ({'r_t':   872.0208, 'eps':     1.0000, 'critic_loss':    21.0531, 'actor_loss':     0.2408, 'eps_e':     1.0000})
Step:  527000, Reward:    91.602 [  30.409], Avg:    58.098 (1.000) <0-03:23:56> ({'r_t':   988.0610, 'eps':     1.0000, 'critic_loss':     9.6835, 'actor_loss':    -0.2854, 'eps_e':     1.0000})
Step:  528000, Reward:    98.979 [   4.207], Avg:    58.175 (1.000) <0-03:24:18> ({'r_t':  1054.6125, 'eps':     1.0000, 'critic_loss':     8.3487, 'actor_loss':    -0.0582, 'eps_e':     1.0000})
Step:  529000, Reward:   100.543 [   3.755], Avg:    58.255 (1.000) <0-03:24:41> ({'r_t':   990.3720, 'eps':     1.0000, 'critic_loss':     9.6880, 'actor_loss':     0.3057, 'eps_e':     1.0000})
Step:  530000, Reward:    99.052 [   7.463], Avg:    58.332 (1.000) <0-03:25:04> ({'r_t':   972.2792, 'eps':     1.0000, 'critic_loss':     8.8462, 'actor_loss':    -0.0876, 'eps_e':     1.0000})
Step:  531000, Reward:    93.697 [  26.974], Avg:    58.398 (1.000) <0-03:25:28> ({'r_t':  1047.9358, 'eps':     1.0000, 'critic_loss':     8.1840, 'actor_loss':     0.0865, 'eps_e':     1.0000})
Step:  532000, Reward:    95.537 [  22.586], Avg:    58.468 (1.000) <0-03:25:53> ({'r_t':   995.5134, 'eps':     1.0000, 'critic_loss':     8.0377, 'actor_loss':    -0.0826, 'eps_e':     1.0000})
Step:  533000, Reward:    99.789 [   5.069], Avg:    58.545 (1.000) <0-03:26:15> ({'r_t':   993.9986, 'eps':     1.0000, 'critic_loss':    10.1886, 'actor_loss':    -0.1813, 'eps_e':     1.0000})
Step:  534000, Reward:    96.919 [  26.278], Avg:    58.617 (1.000) <0-03:26:40> ({'r_t':   955.5477, 'eps':     1.0000, 'critic_loss':     9.4126, 'actor_loss':     0.1079, 'eps_e':     1.0000})
Step:  535000, Reward:    95.212 [  23.576], Avg:    58.685 (1.000) <0-03:27:03> ({'r_t':   899.1847, 'eps':     1.0000, 'critic_loss':    17.9865, 'actor_loss':    -0.0465, 'eps_e':     1.0000})
Step:  536000, Reward:    97.328 [  15.750], Avg:    58.757 (1.000) <0-03:27:27> ({'r_t':   783.2341, 'eps':     1.0000, 'critic_loss':    27.3919, 'actor_loss':     0.0932, 'eps_e':     1.0000})
Step:  537000, Reward:    96.813 [  30.579], Avg:    58.828 (1.000) <0-03:27:52> ({'r_t':   940.8731, 'eps':     1.0000, 'critic_loss':    18.6899, 'actor_loss':    -0.4038, 'eps_e':     1.0000})
Step:  538000, Reward:   110.794 [  11.493], Avg:    58.924 (1.000) <0-03:28:16> ({'r_t':   888.6557, 'eps':     1.0000, 'critic_loss':    20.6471, 'actor_loss':     0.0883, 'eps_e':     1.0000})
Step:  539000, Reward:    96.545 [  17.790], Avg:    58.994 (1.000) <0-03:28:40> ({'r_t':   917.7246, 'eps':     1.0000, 'critic_loss':    21.1779, 'actor_loss':     0.1659, 'eps_e':     1.0000})
Step:  540000, Reward:   114.585 [  10.101], Avg:    59.097 (1.000) <0-03:29:03> ({'r_t':   897.1168, 'eps':     1.0000, 'critic_loss':    19.4858, 'actor_loss':     0.2326, 'eps_e':     1.0000})
Step:  541000, Reward:   115.194 [  21.735], Avg:    59.200 (1.000) <0-03:29:28> ({'r_t':  1009.6314, 'eps':     1.0000, 'critic_loss':    22.7392, 'actor_loss':     0.0372, 'eps_e':     1.0000})
Step:  542000, Reward:   118.394 [  15.013], Avg:    59.309 (1.000) <0-03:29:51> ({'r_t':  1063.2421, 'eps':     1.0000, 'critic_loss':    19.8049, 'actor_loss':    -0.0709, 'eps_e':     1.0000})
Step:  543000, Reward:    53.153 [  21.232], Avg:    59.298 (1.000) <0-03:30:14> ({'r_t':   685.2625, 'eps':     1.0000, 'critic_loss':  1172.1476, 'actor_loss':     2.6067, 'eps_e':     1.0000})
Step:  544000, Reward:    87.884 [  24.013], Avg:    59.350 (1.000) <0-03:30:38> ({'r_t':   664.3868, 'eps':     1.0000, 'critic_loss':    45.0798, 'actor_loss':    -0.9301, 'eps_e':     1.0000})
Step:  545000, Reward:   102.075 [  13.773], Avg:    59.429 (1.000) <0-03:31:00> ({'r_t':   885.1044, 'eps':     1.0000, 'critic_loss':    22.9834, 'actor_loss':    -0.3741, 'eps_e':     1.0000})
Step:  546000, Reward:   105.659 [  12.196], Avg:    59.513 (1.000) <0-03:31:24> ({'r_t':   807.8492, 'eps':     1.0000, 'critic_loss':   631.3671, 'actor_loss':     0.6624, 'eps_e':     1.0000})
Step:  547000, Reward:    69.768 [  10.792], Avg:    59.532 (1.000) <0-03:31:48> ({'r_t':   900.2102, 'eps':     1.0000, 'critic_loss':  1064.3590, 'actor_loss':     1.1335, 'eps_e':     1.0000})
Step:  548000, Reward:    88.748 [  14.906], Avg:    59.585 (1.000) <0-03:32:11> ({'r_t':   793.4971, 'eps':     1.0000, 'critic_loss':    20.5575, 'actor_loss':     0.3406, 'eps_e':     1.0000})
Step:  549000, Reward:   100.407 [  12.328], Avg:    59.659 (1.000) <0-03:32:34> ({'r_t':   896.2937, 'eps':     1.0000, 'critic_loss':    22.2028, 'actor_loss':    -0.7130, 'eps_e':     1.0000})
Step:  550000, Reward:   112.629 [   9.663], Avg:    59.756 (1.000) <0-03:32:56> ({'r_t':  1052.6489, 'eps':     1.0000, 'critic_loss':    15.0605, 'actor_loss':    -0.4002, 'eps_e':     1.0000})
Step:  551000, Reward:   115.093 [   7.062], Avg:    59.856 (1.000) <0-03:33:19> ({'r_t':  1078.2285, 'eps':     1.0000, 'critic_loss':    12.8131, 'actor_loss':    -0.1516, 'eps_e':     1.0000})
Step:  552000, Reward:   123.309 [   3.700], Avg:    59.971 (1.000) <0-03:33:42> ({'r_t':  1126.9107, 'eps':     1.0000, 'critic_loss':     6.3631, 'actor_loss':    -0.1333, 'eps_e':     1.0000})
Step:  553000, Reward:   126.584 [   3.257], Avg:    60.091 (1.000) <0-03:34:05> ({'r_t':  1219.4164, 'eps':     1.0000, 'critic_loss':     8.3213, 'actor_loss':    -0.0040, 'eps_e':     1.0000})
Step:  554000, Reward:   126.098 [  12.906], Avg:    60.210 (1.000) <0-03:34:28> ({'r_t':  1246.9112, 'eps':     1.0000, 'critic_loss':     5.9152, 'actor_loss':    -0.0435, 'eps_e':     1.0000})
Step:  555000, Reward:   124.914 [   7.483], Avg:    60.326 (1.000) <0-03:34:51> ({'r_t':  1212.3466, 'eps':     1.0000, 'critic_loss':    11.5450, 'actor_loss':     0.0055, 'eps_e':     1.0000})
Step:  556000, Reward:   105.942 [  26.967], Avg:    60.408 (1.000) <0-03:35:15> ({'r_t':   949.3617, 'eps':     1.0000, 'critic_loss':    51.5924, 'actor_loss':     0.3680, 'eps_e':     1.0000})
Step:  557000, Reward:    10.311 [  35.986], Avg:    60.318 (1.000) <0-03:35:39> ({'r_t':   269.4697, 'eps':     1.0000, 'critic_loss':  1009.7927, 'actor_loss':     3.3499, 'eps_e':     1.0000})
Step:  558000, Reward:    36.479 [   3.559], Avg:    60.276 (1.000) <0-03:36:02> ({'r_t':   209.1534, 'eps':     1.0000, 'critic_loss':    26.8226, 'actor_loss':    -0.3623, 'eps_e':     1.0000})
Step:  559000, Reward:    85.698 [  17.932], Avg:    60.321 (1.000) <0-03:36:26> ({'r_t':  -501.3012, 'eps':     1.0000, 'critic_loss': 20951.0801, 'actor_loss':   -12.8509, 'eps_e':     1.0000})
Step:  560000, Reward:    81.339 [  31.213], Avg:    60.358 (1.000) <0-03:36:50> ({'r_t':   795.8897, 'eps':     1.0000, 'critic_loss':   112.7284, 'actor_loss':    -3.8416, 'eps_e':     1.0000})
Step:  561000, Reward:    94.346 [  37.660], Avg:    60.419 (1.000) <0-03:37:13> ({'r_t':   908.6811, 'eps':     1.0000, 'critic_loss':    63.9453, 'actor_loss':    -0.7188, 'eps_e':     1.0000})
Step:  562000, Reward:    95.465 [  62.821], Avg:    60.481 (1.000) <0-03:37:40> ({'r_t':   504.7462, 'eps':     1.0000, 'critic_loss':  2318.6257, 'actor_loss':     2.5154, 'eps_e':     1.0000})
Step:  563000, Reward:   115.480 [  16.848], Avg:    60.579 (1.000) <0-03:38:04> ({'r_t':   796.6958, 'eps':     1.0000, 'critic_loss':   918.8003, 'actor_loss':    -0.6242, 'eps_e':     1.0000})
Step:  564000, Reward:   121.341 [  19.185], Avg:    60.686 (1.000) <0-03:38:27> ({'r_t':   953.1178, 'eps':     1.0000, 'critic_loss':    42.7236, 'actor_loss':    -1.7464, 'eps_e':     1.0000})
Step:  565000, Reward:    32.871 [ 354.728], Avg:    60.637 (1.000) <0-03:38:51> ({'r_t':  1076.7371, 'eps':     1.0000, 'critic_loss':    22.4779, 'actor_loss':     0.0309, 'eps_e':     1.0000})
Step:  566000, Reward:   109.905 [  11.921], Avg:    60.724 (1.000) <0-03:39:14> ({'r_t':   939.9156, 'eps':     1.0000, 'critic_loss':  1137.3202, 'actor_loss':     1.7606, 'eps_e':     1.0000})
Step:  567000, Reward:    29.855 [ 357.137], Avg:    60.670 (1.000) <0-03:39:38> ({'r_t':  1141.3869, 'eps':     1.0000, 'critic_loss':    34.7235, 'actor_loss':    -0.5330, 'eps_e':     1.0000})
Step:  568000, Reward:    88.241 [  11.226], Avg:    60.718 (1.000) <0-03:40:01> ({'r_t':   939.4881, 'eps':     1.0000, 'critic_loss':  1690.1605, 'actor_loss':     3.5218, 'eps_e':     1.0000})
Step:  569000, Reward:    94.549 [  24.143], Avg:    60.777 (1.000) <0-03:40:24> ({'r_t':   637.7755, 'eps':     1.0000, 'critic_loss':  3218.5259, 'actor_loss':     2.7952, 'eps_e':     1.0000})
Step:  570000, Reward:    79.319 [  18.856], Avg:    60.810 (1.000) <0-03:40:47> ({'r_t':   754.3686, 'eps':     1.0000, 'critic_loss':  1213.6244, 'actor_loss':    -3.9981, 'eps_e':     1.0000})
Step:  571000, Reward:   -34.455 [ 471.750], Avg:    60.643 (1.000) <0-03:41:16> ({'r_t':   856.5258, 'eps':     1.0000, 'critic_loss':    41.7438, 'actor_loss':    -0.7348, 'eps_e':     1.0000})
Step:  572000, Reward:    97.783 [  57.805], Avg:    60.708 (1.000) <0-03:41:44> ({'r_t':   578.7929, 'eps':     1.0000, 'critic_loss':  1203.1040, 'actor_loss':     0.7771, 'eps_e':     1.0000})
Step:  573000, Reward:    89.809 [  57.865], Avg:    60.759 (1.000) <0-03:42:10> ({'r_t':   721.8459, 'eps':     1.0000, 'critic_loss':  1213.7480, 'actor_loss':    -0.0873, 'eps_e':     1.0000})
Step:  574000, Reward:    97.462 [  33.848], Avg:    60.823 (1.000) <0-03:42:33> ({'r_t':   631.1999, 'eps':     1.0000, 'critic_loss':  1112.9382, 'actor_loss':     0.9476, 'eps_e':     1.0000})
Step:  575000, Reward:    63.105 [ 117.455], Avg:    60.827 (1.000) <0-03:43:01> ({'r_t':   556.5554, 'eps':     1.0000, 'critic_loss':  1410.7833, 'actor_loss':    -0.1012, 'eps_e':     1.0000})
Step:  576000, Reward:    92.016 [  41.631], Avg:    60.881 (1.000) <0-03:43:28> ({'r_t':   777.2381, 'eps':     1.0000, 'critic_loss':   697.1431, 'actor_loss':    -1.1303, 'eps_e':     1.0000})
Step:  577000, Reward:   103.577 [  73.704], Avg:    60.955 (1.000) <0-03:43:54> ({'r_t':   648.2078, 'eps':     1.0000, 'critic_loss':  1832.5614, 'actor_loss':    -0.0031, 'eps_e':     1.0000})
Step:  578000, Reward:   118.734 [  83.189], Avg:    61.054 (1.000) <0-03:44:20> ({'r_t':   846.3952, 'eps':     1.0000, 'critic_loss':   226.3493, 'actor_loss':    -0.7952, 'eps_e':     1.0000})
Step:  579000, Reward:    92.252 [  74.440], Avg:    61.108 (1.000) <0-03:44:48> ({'r_t':   751.3294, 'eps':     1.0000, 'critic_loss':  1169.4828, 'actor_loss':     2.0337, 'eps_e':     1.0000})
Step:  580000, Reward:    82.956 [  34.678], Avg:    61.146 (1.000) <0-03:45:12> ({'r_t':   804.1766, 'eps':     1.0000, 'critic_loss':  1217.6069, 'actor_loss':     1.0610, 'eps_e':     1.0000})
Step:  581000, Reward:   123.458 [  46.547], Avg:    61.253 (1.000) <0-03:45:37> ({'r_t':   896.4306, 'eps':     1.0000, 'critic_loss':  1405.9700, 'actor_loss':     0.2353, 'eps_e':     1.0000})
Step:  582000, Reward:   114.065 [  52.977], Avg:    61.343 (1.000) <0-03:46:03> ({'r_t':  1186.1025, 'eps':     1.0000, 'critic_loss':   343.2094, 'actor_loss':     0.1838, 'eps_e':     1.0000})
Step:  583000, Reward:   190.056 [  68.015], Avg:    61.564 (1.000) <0-03:46:27> ({'r_t':  1157.6534, 'eps':     1.0000, 'critic_loss':   317.9264, 'actor_loss':     0.2744, 'eps_e':     1.0000})
Step:  584000, Reward:   240.883 [  45.606], Avg:    61.870 (1.000) <0-03:46:53> ({'r_t':  1268.8619, 'eps':     1.0000, 'critic_loss':  2222.2097, 'actor_loss':    -1.2301, 'eps_e':     1.0000})
Step:  585000, Reward:   183.417 [  76.228], Avg:    62.078 (1.000) <0-03:47:18> ({'r_t':  1219.9564, 'eps':     1.0000, 'critic_loss':  2291.0098, 'actor_loss':     2.0953, 'eps_e':     1.0000})
Step:  586000, Reward:    91.309 [ 360.677], Avg:    62.128 (1.000) <0-03:47:43> ({'r_t':  1482.9590, 'eps':     1.0000, 'critic_loss':  2478.2476, 'actor_loss':     2.2250, 'eps_e':     1.0000})
Step:  587000, Reward:   134.652 [ 362.885], Avg:    62.251 (1.000) <0-03:48:09> ({'r_t':  1452.9977, 'eps':     1.0000, 'critic_loss':  2069.5793, 'actor_loss':     1.6223, 'eps_e':     1.0000})
Step:  588000, Reward:    36.821 [ 491.400], Avg:    62.208 (1.000) <0-03:48:33> ({'r_t':  1110.0912, 'eps':     1.0000, 'critic_loss':  5269.9453, 'actor_loss':     0.7619, 'eps_e':     1.0000})
Step:  589000, Reward:   190.099 [  74.560], Avg:    62.424 (1.000) <0-03:49:00> ({'r_t':   935.6699, 'eps':     1.0000, 'critic_loss':  6088.2856, 'actor_loss':    -2.1212, 'eps_e':     1.0000})
Step:  590000, Reward:   194.632 [ 113.477], Avg:    62.648 (1.000) <0-03:49:28> ({'r_t':  1248.1327, 'eps':     1.0000, 'critic_loss':  3319.4673, 'actor_loss':    -1.1989, 'eps_e':     1.0000})
Step:  591000, Reward:   216.643 [  76.136], Avg:    62.908 (1.000) <0-03:49:53> ({'r_t':  1148.6874, 'eps':     1.0000, 'critic_loss':  4674.7207, 'actor_loss':     0.2286, 'eps_e':     1.0000})
Step:  592000, Reward:   233.415 [  82.154], Avg:    63.196 (1.000) <0-03:50:17> ({'r_t':  1535.7976, 'eps':     1.0000, 'critic_loss':  2401.3735, 'actor_loss':    -1.0222, 'eps_e':     1.0000})
Step:  593000, Reward:   242.010 [  84.701], Avg:    63.497 (1.000) <0-03:50:41> ({'r_t':  1586.5630, 'eps':     1.0000, 'critic_loss':  1274.9634, 'actor_loss':     0.9562, 'eps_e':     1.0000})
Step:  594000, Reward:   259.185 [  43.241], Avg:    63.826 (1.000) <0-03:51:07> ({'r_t':  1633.1194, 'eps':     1.0000, 'critic_loss':  1297.8577, 'actor_loss':    -1.4527, 'eps_e':     1.0000})
Step:  595000, Reward:   165.849 [ 370.639], Avg:    63.997 (1.000) <0-03:51:31> ({'r_t':  1652.5465, 'eps':     1.0000, 'critic_loss':  1345.1194, 'actor_loss':     2.4121, 'eps_e':     1.0000})
Step:  596000, Reward:   194.732 [ 101.493], Avg:    64.216 (1.000) <0-03:51:58> ({'r_t':  1426.0158, 'eps':     1.0000, 'critic_loss':  3033.5667, 'actor_loss':     2.8201, 'eps_e':     1.0000})
Step:  597000, Reward:   254.122 [  77.868], Avg:    64.533 (1.000) <0-03:52:24> ({'r_t':  1737.9597, 'eps':     1.0000, 'critic_loss':   346.3360, 'actor_loss':    -1.5494, 'eps_e':     1.0000})
Step:  598000, Reward:   226.295 [  82.324], Avg:    64.804 (1.000) <0-03:52:49> ({'r_t':  1472.1510, 'eps':     1.0000, 'critic_loss':  3019.1799, 'actor_loss':     2.8081, 'eps_e':     1.0000})
Step:  599000, Reward:   242.700 [  76.990], Avg:    65.100 (1.000) <0-03:53:13> ({'r_t':  1607.8756, 'eps':     1.0000, 'critic_loss':  2464.4622, 'actor_loss':    -1.5914, 'eps_e':     1.0000})
Step:  600000, Reward:   255.340 [  53.684], Avg:    65.417 (1.000) <0-03:53:38> ({'r_t':  1512.9219, 'eps':     1.0000, 'critic_loss':  3334.5088, 'actor_loss':     1.8741, 'eps_e':     1.0000})
Step:  601000, Reward:   187.635 [ 123.935], Avg:    65.620 (1.000) <0-03:54:05> ({'r_t':  1256.6634, 'eps':     1.0000, 'critic_loss':  2409.4558, 'actor_loss':     0.4686, 'eps_e':     1.0000})
Step:  602000, Reward:   120.664 [ 354.846], Avg:    65.711 (1.000) <0-03:54:31> ({'r_t':  1594.1163, 'eps':     1.0000, 'critic_loss':   278.6654, 'actor_loss':    -1.1379, 'eps_e':     1.0000})
Step:  603000, Reward:   238.098 [  90.516], Avg:    65.996 (1.000) <0-03:54:56> ({'r_t':  1393.1596, 'eps':     1.0000, 'critic_loss':  3475.7329, 'actor_loss':     2.4034, 'eps_e':     1.0000})
Step:  604000, Reward:   265.747 [  49.067], Avg:    66.326 (1.000) <0-03:55:21> ({'r_t':  1653.2205, 'eps':     1.0000, 'critic_loss':  1383.2388, 'actor_loss':    -0.1796, 'eps_e':     1.0000})
Step:  605000, Reward:   255.532 [  90.390], Avg:    66.639 (1.000) <0-03:55:46> ({'r_t':  1668.4128, 'eps':     1.0000, 'critic_loss':  1168.9127, 'actor_loss':     1.9686, 'eps_e':     1.0000})
Step:  606000, Reward:   160.367 [ 364.416], Avg:    66.793 (1.000) <0-03:56:10> ({'r_t':  1515.5890, 'eps':     1.0000, 'critic_loss':  4519.2515, 'actor_loss':     2.9745, 'eps_e':     1.0000})
Step:  607000, Reward:   214.006 [ 121.943], Avg:    67.035 (1.000) <0-03:56:35> ({'r_t':  1225.6000, 'eps':     1.0000, 'critic_loss':  7149.6216, 'actor_loss':     2.0694, 'eps_e':     1.0000})
Step:  608000, Reward:   292.248 [  15.669], Avg:    67.405 (1.000) <0-03:57:00> ({'r_t':  1513.4434, 'eps':     1.0000, 'critic_loss':  3497.2561, 'actor_loss':    -0.1968, 'eps_e':     1.0000})
Step:  609000, Reward:   267.824 [  66.611], Avg:    67.734 (1.000) <0-03:57:25> ({'r_t':  1614.9474, 'eps':     1.0000, 'critic_loss':  3532.7922, 'actor_loss':     1.8170, 'eps_e':     1.0000})
Step:  610000, Reward:   119.050 [ 355.251], Avg:    67.818 (1.000) <0-03:57:49> ({'r_t':  1519.3530, 'eps':     1.0000, 'critic_loss':  2499.1719, 'actor_loss':     0.2545, 'eps_e':     1.0000})
Step:  611000, Reward:   254.549 [  49.773], Avg:    68.123 (1.000) <0-03:58:15> ({'r_t':  1086.8735, 'eps':     1.0000, 'critic_loss':  3513.6692, 'actor_loss':     1.6016, 'eps_e':     1.0000})
Step:  612000, Reward:   235.918 [ 102.505], Avg:    68.396 (1.000) <0-03:58:41> ({'r_t':  1463.1001, 'eps':     1.0000, 'critic_loss':  3729.9895, 'actor_loss':     1.6347, 'eps_e':     1.0000})
Step:  613000, Reward:   275.614 [  33.011], Avg:    68.734 (1.000) <0-03:59:07> ({'r_t':  1881.1334, 'eps':     1.0000, 'critic_loss':   231.8836, 'actor_loss':    -3.5499, 'eps_e':     1.0000})
Step:  614000, Reward:   286.821 [  12.729], Avg:    69.089 (1.000) <0-03:59:31> ({'r_t':  1901.5665, 'eps':     1.0000, 'critic_loss':   155.6730, 'actor_loss':     0.4521, 'eps_e':     1.0000})
Step:  615000, Reward:   254.691 [  49.538], Avg:    69.390 (1.000) <0-03:59:56> ({'r_t':  1462.3345, 'eps':     1.0000, 'critic_loss':  3012.0989, 'actor_loss':     4.2801, 'eps_e':     1.0000})
Step:  616000, Reward:   242.186 [  80.446], Avg:    69.670 (1.000) <0-04:00:22> ({'r_t':  1598.0009, 'eps':     1.0000, 'critic_loss':  2760.1826, 'actor_loss':     0.0146, 'eps_e':     1.0000})
Step:  617000, Reward:   239.192 [ 118.074], Avg:    69.944 (1.000) <0-04:00:47> ({'r_t':  1739.5508, 'eps':     1.0000, 'critic_loss':  1340.7042, 'actor_loss':     0.2693, 'eps_e':     1.0000})
Step:  618000, Reward:   188.923 [ 139.819], Avg:    70.136 (1.000) <0-04:01:12> ({'r_t':  1722.3306, 'eps':     1.0000, 'critic_loss':   466.5594, 'actor_loss':     0.5350, 'eps_e':     1.0000})
Step:  619000, Reward:   269.081 [  74.669], Avg:    70.457 (1.000) <0-04:01:37> ({'r_t':  1907.3353, 'eps':     1.0000, 'critic_loss':   223.1831, 'actor_loss':    -0.4190, 'eps_e':     1.0000})
Step:  620000, Reward:   234.931 [  99.511], Avg:    70.722 (1.000) <0-04:02:02> ({'r_t':  1709.4388, 'eps':     1.0000, 'critic_loss':  2317.3792, 'actor_loss':     0.6829, 'eps_e':     1.0000})
Step:  621000, Reward:   277.986 [  36.545], Avg:    71.055 (1.000) <0-04:02:27> ({'r_t':  1907.0465, 'eps':     1.0000, 'critic_loss':   105.5441, 'actor_loss':     0.1694, 'eps_e':     1.0000})
Step:  622000, Reward:   193.247 [  98.019], Avg:    71.251 (1.000) <0-04:02:53> ({'r_t':  1795.2977, 'eps':     1.0000, 'critic_loss':  1080.0009, 'actor_loss':     1.0472, 'eps_e':     1.0000})
Step:  623000, Reward:   248.777 [  88.772], Avg:    71.536 (1.000) <0-04:03:18> ({'r_t':  1587.0565, 'eps':     1.0000, 'critic_loss':  1197.8971, 'actor_loss':     0.5330, 'eps_e':     1.0000})
Step:  624000, Reward:   197.697 [ 114.172], Avg:    71.738 (1.000) <0-04:03:43> ({'r_t':  1683.0357, 'eps':     1.0000, 'critic_loss':  2373.7581, 'actor_loss':     2.1168, 'eps_e':     1.0000})
Step:  625000, Reward:   256.772 [  98.819], Avg:    72.033 (1.000) <0-04:04:10> ({'r_t':  1703.0258, 'eps':     1.0000, 'critic_loss':   302.7898, 'actor_loss':    -0.1161, 'eps_e':     1.0000})
Step:  626000, Reward:   245.135 [  92.284], Avg:    72.310 (1.000) <0-04:04:35> ({'r_t':  1706.6771, 'eps':     1.0000, 'critic_loss':  1343.7770, 'actor_loss':     0.8314, 'eps_e':     1.0000})
Step:  627000, Reward:   211.578 [ 389.442], Avg:    72.531 (1.000) <0-04:05:00> ({'r_t':  1835.9911, 'eps':     1.0000, 'critic_loss':   342.2584, 'actor_loss':     0.9338, 'eps_e':     1.0000})
Step:  628000, Reward:   209.989 [  55.743], Avg:    72.750 (1.000) <0-04:05:25> ({'r_t':  1510.5271, 'eps':     1.0000, 'critic_loss':  2393.7217, 'actor_loss':     3.9075, 'eps_e':     1.0000})
Step:  629000, Reward:   236.565 [  94.589], Avg:    73.010 (1.000) <0-04:05:52> ({'r_t':  1480.9063, 'eps':     1.0000, 'critic_loss':   280.9618, 'actor_loss':    -0.5969, 'eps_e':     1.0000})
Step:  630000, Reward:   252.575 [  41.013], Avg:    73.294 (1.000) <0-04:06:18> ({'r_t':  1636.8627, 'eps':     1.0000, 'critic_loss':  1733.0164, 'actor_loss':     0.6883, 'eps_e':     1.0000})
Step:  631000, Reward:   277.191 [  62.066], Avg:    73.617 (1.000) <0-04:06:44> ({'r_t':  1559.4902, 'eps':     1.0000, 'critic_loss':  1412.4015, 'actor_loss':    -1.6889, 'eps_e':     1.0000})
Step:  632000, Reward:   192.868 [ 366.645], Avg:    73.805 (1.000) <0-04:07:09> ({'r_t':  1798.6834, 'eps':     1.0000, 'critic_loss':  1165.2953, 'actor_loss':    -0.0650, 'eps_e':     1.0000})
Step:  633000, Reward:   240.191 [ 111.793], Avg:    74.068 (1.000) <0-04:07:34> ({'r_t':  1569.6768, 'eps':     1.0000, 'critic_loss':  3151.5706, 'actor_loss':     4.8790, 'eps_e':     1.0000})
Step:  634000, Reward:   285.333 [  52.131], Avg:    74.401 (1.000) <0-04:07:59> ({'r_t':  1888.1866, 'eps':     1.0000, 'critic_loss':   490.9993, 'actor_loss':    -4.4720, 'eps_e':     1.0000})
Step:  635000, Reward:   303.153 [  18.700], Avg:    74.760 (1.000) <0-04:08:25> ({'r_t':  1694.8937, 'eps':     1.0000, 'critic_loss':  2314.7844, 'actor_loss':     1.8514, 'eps_e':     1.0000})
Step:  636000, Reward:   280.866 [  53.489], Avg:    75.084 (1.000) <0-04:08:50> ({'r_t':  1809.1752, 'eps':     1.0000, 'critic_loss':  1394.0071, 'actor_loss':    -2.1875, 'eps_e':     1.0000})
Step:  637000, Reward:   266.752 [ 116.361], Avg:    75.384 (1.000) <0-04:09:15> ({'r_t':  1851.0089, 'eps':     1.0000, 'critic_loss':   932.7338, 'actor_loss':     0.2217, 'eps_e':     1.0000})
Step:  638000, Reward:   307.179 [  27.005], Avg:    75.747 (1.000) <0-04:09:40> ({'r_t':  1709.9903, 'eps':     1.0000, 'critic_loss':  2259.4385, 'actor_loss':     0.8805, 'eps_e':     1.0000})
Step:  639000, Reward:   205.567 [ 107.389], Avg:    75.950 (1.000) <0-04:10:04> ({'r_t':  1843.1186, 'eps':     1.0000, 'critic_loss':  1459.9648, 'actor_loss':     1.3549, 'eps_e':     1.0000})
Step:  640000, Reward:   262.333 [ 106.418], Avg:    76.241 (1.000) <0-04:10:30> ({'r_t':  1725.8322, 'eps':     1.0000, 'critic_loss':  1054.9500, 'actor_loss':    -0.5353, 'eps_e':     1.0000})
Step:  641000, Reward:   300.558 [  51.282], Avg:    76.590 (1.000) <0-04:10:56> ({'r_t':  1322.7881, 'eps':     1.0000, 'critic_loss':  5130.8740, 'actor_loss':     1.5073, 'eps_e':     1.0000})
Step:  642000, Reward:   275.488 [  81.228], Avg:    76.899 (1.000) <0-04:11:22> ({'r_t':  2003.2179, 'eps':     1.0000, 'critic_loss':   426.2639, 'actor_loss':    -3.5338, 'eps_e':     1.0000})
Step:  643000, Reward:   322.181 [  45.707], Avg:    77.280 (1.000) <0-04:11:47> ({'r_t':  1881.7755, 'eps':     1.0000, 'critic_loss':  1406.2524, 'actor_loss':     0.9963, 'eps_e':     1.0000})
Step:  644000, Reward:   109.906 [ 497.690], Avg:    77.331 (1.000) <0-04:12:12> ({'r_t':  1942.8681, 'eps':     1.0000, 'critic_loss':  1312.2109, 'actor_loss':    -0.5364, 'eps_e':     1.0000})
Step:  645000, Reward:   200.758 [ 363.759], Avg:    77.522 (1.000) <0-04:12:38> ({'r_t':  1850.9128, 'eps':     1.0000, 'critic_loss':  2394.6707, 'actor_loss':     3.5143, 'eps_e':     1.0000})
Step:  646000, Reward:   -30.619 [ 581.280], Avg:    77.355 (1.000) <0-04:13:03> ({'r_t':   608.2887, 'eps':     1.0000, 'critic_loss':  7446.1860, 'actor_loss':     4.0174, 'eps_e':     1.0000})
Step:  647000, Reward:   235.436 [ 352.921], Avg:    77.599 (1.000) <0-04:13:29> ({'r_t':  1092.8807, 'eps':     1.0000, 'critic_loss':  8211.5352, 'actor_loss':    -1.7012, 'eps_e':     1.0000})
Step:  648000, Reward:   335.660 [  37.404], Avg:    77.996 (1.000) <0-04:13:54> ({'r_t':  1333.9089, 'eps':     1.0000, 'critic_loss':  6342.9502, 'actor_loss':    -0.9170, 'eps_e':     1.0000})
Step:  649000, Reward:   337.004 [  41.263], Avg:    78.395 (1.000) <0-04:14:20> ({'r_t':  1783.5666, 'eps':     1.0000, 'critic_loss':  3226.3372, 'actor_loss':     2.4284, 'eps_e':     1.0000})
Step:  650000, Reward:   216.266 [ 355.544], Avg:    78.607 (1.000) <0-04:14:46> ({'r_t':  1745.3656, 'eps':     1.0000, 'critic_loss':  1976.4716, 'actor_loss':    -2.3833, 'eps_e':     1.0000})
Step:  651000, Reward:   167.585 [ 461.286], Avg:    78.743 (1.000) <0-04:15:11> ({'r_t':  2016.9369, 'eps':     1.0000, 'critic_loss':   350.8736, 'actor_loss':    -0.8535, 'eps_e':     1.0000})
Step:  652000, Reward:   275.884 [ 129.005], Avg:    79.045 (1.000) <0-04:15:38> ({'r_t':  1884.4882, 'eps':     1.0000, 'critic_loss':  1747.7112, 'actor_loss':     1.1719, 'eps_e':     1.0000})
Step:  653000, Reward:   330.850 [  94.932], Avg:    79.430 (1.000) <0-04:16:03> ({'r_t':  2003.3664, 'eps':     1.0000, 'critic_loss':   265.4531, 'actor_loss':    -0.8714, 'eps_e':     1.0000})
Step:  654000, Reward:   348.741 [  31.208], Avg:    79.841 (1.000) <0-04:16:29> ({'r_t':  2079.7858, 'eps':     1.0000, 'critic_loss':   314.3792, 'actor_loss':     0.8820, 'eps_e':     1.0000})
Step:  655000, Reward:   358.561 [  31.451], Avg:    80.266 (1.000) <0-04:16:55> ({'r_t':  2090.8942, 'eps':     1.0000, 'critic_loss':   238.2437, 'actor_loss':    -1.3840, 'eps_e':     1.0000})
Step:  656000, Reward:   357.182 [  24.902], Avg:    80.687 (1.000) <0-04:17:21> ({'r_t':  2120.9607, 'eps':     1.0000, 'critic_loss':  1366.2828, 'actor_loss':     1.3109, 'eps_e':     1.0000})
Step:  657000, Reward:   321.917 [  99.529], Avg:    81.054 (1.000) <0-04:17:46> ({'r_t':  2166.0268, 'eps':     1.0000, 'critic_loss':   122.9565, 'actor_loss':     0.5877, 'eps_e':     1.0000})
Step:  658000, Reward:   353.945 [  30.274], Avg:    81.468 (1.000) <0-04:18:12> ({'r_t':  2147.2094, 'eps':     1.0000, 'critic_loss':  1221.7793, 'actor_loss':     1.3354, 'eps_e':     1.0000})
Step:  659000, Reward:   271.755 [ 360.370], Avg:    81.756 (1.000) <0-04:18:37> ({'r_t':  2241.3841, 'eps':     1.0000, 'critic_loss':   131.2503, 'actor_loss':    -0.0676, 'eps_e':     1.0000})
Step:  660000, Reward:   358.479 [  28.949], Avg:    82.175 (1.000) <0-04:19:03> ({'r_t':  1902.0163, 'eps':     1.0000, 'critic_loss':  2048.6658, 'actor_loss':     0.0717, 'eps_e':     1.0000})
Step:  661000, Reward:   369.015 [  16.454], Avg:    82.608 (1.000) <0-04:19:28> ({'r_t':  2137.4631, 'eps':     1.0000, 'critic_loss':   281.3962, 'actor_loss':     0.0283, 'eps_e':     1.0000})
Step:  662000, Reward:   144.237 [ 490.683], Avg:    82.701 (1.000) <0-04:19:54> ({'r_t':  1665.2862, 'eps':     1.0000, 'critic_loss':  4592.2734, 'actor_loss':     5.5575, 'eps_e':     1.0000})
Step:  663000, Reward:   254.084 [ 361.395], Avg:    82.959 (1.000) <0-04:20:22> ({'r_t':   951.5562, 'eps':     1.0000, 'critic_loss':  8214.3027, 'actor_loss':     5.1546, 'eps_e':     1.0000})
Step:  664000, Reward:   192.060 [ 389.135], Avg:    83.124 (1.000) <0-04:20:49> ({'r_t':  1080.5761, 'eps':     1.0000, 'critic_loss':  7531.5649, 'actor_loss':    -3.6273, 'eps_e':     1.0000})
Step:  665000, Reward:   200.461 [ 388.711], Avg:    83.300 (1.000) <0-04:21:16> ({'r_t':  1466.2863, 'eps':     1.0000, 'critic_loss':  2405.1338, 'actor_loss':     0.4212, 'eps_e':     1.0000})
Step:  666000, Reward:   225.280 [ 376.848], Avg:    83.513 (1.000) <0-04:21:41> ({'r_t':  1789.3285, 'eps':     1.0000, 'critic_loss':  2217.2146, 'actor_loss':    -1.9195, 'eps_e':     1.0000})
Step:  667000, Reward:   321.213 [  82.639], Avg:    83.868 (1.000) <0-04:22:07> ({'r_t':  1951.7944, 'eps':     1.0000, 'critic_loss':  1337.7795, 'actor_loss':     0.1523, 'eps_e':     1.0000})
Step:  668000, Reward:   273.829 [ 157.509], Avg:    84.152 (1.000) <0-04:22:35> ({'r_t':  1911.9943, 'eps':     1.0000, 'critic_loss':  1345.0437, 'actor_loss':     1.0180, 'eps_e':     1.0000})
Step:  669000, Reward:   351.079 [  34.291], Avg:    84.551 (1.000) <0-04:23:01> ({'r_t':  1908.9759, 'eps':     1.0000, 'critic_loss':   605.6743, 'actor_loss':     0.1545, 'eps_e':     1.0000})
Step:  670000, Reward:   326.587 [  37.646], Avg:    84.911 (1.000) <0-04:23:26> ({'r_t':  1593.5957, 'eps':     1.0000, 'critic_loss':  6310.6772, 'actor_loss':     2.6887, 'eps_e':     1.0000})
Step:  671000, Reward:   348.278 [  30.939], Avg:    85.303 (1.000) <0-04:23:52> ({'r_t':  1703.2422, 'eps':     1.0000, 'critic_loss':  3175.6804, 'actor_loss':    -2.7911, 'eps_e':     1.0000})
Step:  672000, Reward:   265.645 [ 359.204], Avg:    85.571 (1.000) <0-04:24:17> ({'r_t':  1892.9278, 'eps':     1.0000, 'critic_loss':  1900.7882, 'actor_loss':    -3.9444, 'eps_e':     1.0000})
Step:  673000, Reward:   320.749 [  43.997], Avg:    85.920 (1.000) <0-04:24:43> ({'r_t':  1658.8926, 'eps':     1.0000, 'critic_loss':  3940.6807, 'actor_loss':    -0.4087, 'eps_e':     1.0000})
Step:  674000, Reward:   345.283 [  48.811], Avg:    86.304 (1.000) <0-04:25:09> ({'r_t':  2045.4389, 'eps':     1.0000, 'critic_loss':   315.0668, 'actor_loss':    -3.4711, 'eps_e':     1.0000})
Step:  675000, Reward:   350.860 [  35.519], Avg:    86.696 (1.000) <0-04:25:35> ({'r_t':  1859.6769, 'eps':     1.0000, 'critic_loss':   895.0423, 'actor_loss':     0.0126, 'eps_e':     1.0000})
Step:  676000, Reward:   358.372 [  26.357], Avg:    87.097 (1.000) <0-04:26:01> ({'r_t':  2161.5966, 'eps':     1.0000, 'critic_loss':  1038.0117, 'actor_loss':    -0.5956, 'eps_e':     1.0000})
Step:  677000, Reward:   366.519 [  24.727], Avg:    87.509 (1.000) <0-04:26:25> ({'r_t':  2216.6731, 'eps':     1.0000, 'critic_loss':   146.3325, 'actor_loss':     0.3530, 'eps_e':     1.0000})
Step:  678000, Reward:   330.126 [  90.360], Avg:    87.867 (1.000) <0-04:26:52> ({'r_t':  2068.1158, 'eps':     1.0000, 'critic_loss':   184.7924, 'actor_loss':    -0.2309, 'eps_e':     1.0000})
Step:  679000, Reward:   259.520 [ 374.159], Avg:    88.119 (1.000) <0-04:27:18> ({'r_t':  2053.9919, 'eps':     1.0000, 'critic_loss':  1450.6865, 'actor_loss':     0.6160, 'eps_e':     1.0000})
Step:  680000, Reward:   222.919 [ 374.936], Avg:    88.317 (1.000) <0-04:27:45> ({'r_t':  1601.0730, 'eps':     1.0000, 'critic_loss':  4768.6733, 'actor_loss':     4.0601, 'eps_e':     1.0000})
Step:  681000, Reward:   342.571 [  99.081], Avg:    88.690 (1.000) <0-04:28:11> ({'r_t':  1477.7379, 'eps':     1.0000, 'critic_loss':  3219.2537, 'actor_loss':     1.0776, 'eps_e':     1.0000})
Step:  682000, Reward:   274.604 [ 360.454], Avg:    88.962 (1.000) <0-04:28:36> ({'r_t':  1872.2484, 'eps':     1.0000, 'critic_loss':  2229.8442, 'actor_loss':     0.3893, 'eps_e':     1.0000})
Step:  683000, Reward:   278.906 [ 110.780], Avg:    89.240 (1.000) <0-04:29:03> ({'r_t':  1984.4851, 'eps':     1.0000, 'critic_loss':  1956.0680, 'actor_loss':    -0.2001, 'eps_e':     1.0000})
Step:  684000, Reward:   268.798 [ 125.254], Avg:    89.502 (1.000) <0-04:29:28> ({'r_t':  1615.1175, 'eps':     1.0000, 'critic_loss':  1926.4110, 'actor_loss':     6.2751, 'eps_e':     1.0000})
Step:  685000, Reward:   328.519 [  39.938], Avg:    89.850 (1.000) <0-04:29:54> ({'r_t':  1811.0266, 'eps':     1.0000, 'critic_loss':  1420.9589, 'actor_loss':     0.6892, 'eps_e':     1.0000})
Step:  686000, Reward:   297.664 [  88.140], Avg:    90.153 (1.000) <0-04:30:22> ({'r_t':  1895.3825, 'eps':     1.0000, 'critic_loss':   289.6837, 'actor_loss':    -1.2886, 'eps_e':     1.0000})
Step:  687000, Reward:   313.128 [ 105.570], Avg:    90.477 (1.000) <0-04:30:47> ({'r_t':  1612.7541, 'eps':     1.0000, 'critic_loss':  3870.7356, 'actor_loss':     2.1897, 'eps_e':     1.0000})
Step:  688000, Reward:   372.515 [  17.325], Avg:    90.886 (1.000) <0-04:31:13> ({'r_t':  2085.1534, 'eps':     1.0000, 'critic_loss':   442.0419, 'actor_loss':    -3.3438, 'eps_e':     1.0000})
Step:  689000, Reward:   351.998 [  31.558], Avg:    91.265 (1.000) <0-04:31:38> ({'r_t':  1806.1443, 'eps':     1.0000, 'critic_loss':  3225.5322, 'actor_loss':     2.2099, 'eps_e':     1.0000})
Step:  690000, Reward:   314.864 [ 104.426], Avg:    91.588 (1.000) <0-04:32:05> ({'r_t':  1751.8231, 'eps':     1.0000, 'critic_loss':  1924.5632, 'actor_loss':     0.5157, 'eps_e':     1.0000})
Step:  691000, Reward:   350.879 [  41.417], Avg:    91.963 (1.000) <0-04:32:31> ({'r_t':  2083.2920, 'eps':     1.0000, 'critic_loss':   290.9303, 'actor_loss':    -1.3810, 'eps_e':     1.0000})
Step:  692000, Reward:   348.958 [  45.142], Avg:    92.334 (1.000) <0-04:32:56> ({'r_t':  2015.4414, 'eps':     1.0000, 'critic_loss':   923.5062, 'actor_loss':     0.2240, 'eps_e':     1.0000})
Step:  693000, Reward:   324.844 [  90.534], Avg:    92.669 (1.000) <0-04:33:23> ({'r_t':  2016.9433, 'eps':     1.0000, 'critic_loss':  1151.1753, 'actor_loss':     1.6219, 'eps_e':     1.0000})
Step:  694000, Reward:   285.079 [ 124.550], Avg:    92.946 (1.000) <0-04:33:49> ({'r_t':  1975.7762, 'eps':     1.0000, 'critic_loss':  1427.4738, 'actor_loss':    -0.6950, 'eps_e':     1.0000})
Step:  695000, Reward:   371.530 [  38.728], Avg:    93.346 (1.000) <0-04:34:15> ({'r_t':  1992.1200, 'eps':     1.0000, 'critic_loss':   371.1767, 'actor_loss':     0.5860, 'eps_e':     1.0000})
Step:  696000, Reward:   233.604 [ 360.791], Avg:    93.547 (1.000) <0-04:34:41> ({'r_t':  2017.7775, 'eps':     1.0000, 'critic_loss':   298.9754, 'actor_loss':     0.6184, 'eps_e':     1.0000})
Step:  697000, Reward:   368.835 [  34.544], Avg:    93.941 (1.000) <0-04:35:06> ({'r_t':  2126.9151, 'eps':     1.0000, 'critic_loss':   369.8110, 'actor_loss':    -0.5867, 'eps_e':     1.0000})
Step:  698000, Reward:   315.685 [ 117.325], Avg:    94.259 (1.000) <0-04:35:32> ({'r_t':  1925.2818, 'eps':     1.0000, 'critic_loss':  2354.7485, 'actor_loss':     2.0840, 'eps_e':     1.0000})
Step:  699000, Reward:   370.117 [  23.135], Avg:    94.653 (1.000) <0-04:35:58> ({'r_t':  1952.8874, 'eps':     1.0000, 'critic_loss':  1339.7148, 'actor_loss':    -2.4052, 'eps_e':     1.0000})
Step:  700000, Reward:   334.261 [  60.211], Avg:    94.995 (1.000) <0-04:36:25> ({'r_t':  1811.2690, 'eps':     1.0000, 'critic_loss':  2151.1687, 'actor_loss':     2.7562, 'eps_e':     1.0000})
Step:  701000, Reward:   319.189 [ 128.669], Avg:    95.314 (1.000) <0-04:36:50> ({'r_t':  1799.2954, 'eps':     1.0000, 'critic_loss':  2840.0457, 'actor_loss':    -0.6083, 'eps_e':     1.0000})
Step:  702000, Reward:   329.932 [ 121.426], Avg:    95.648 (1.000) <0-04:37:16> ({'r_t':  2173.1922, 'eps':     1.0000, 'critic_loss':   291.8078, 'actor_loss':    -0.1805, 'eps_e':     1.0000})
Step:  703000, Reward:   379.606 [  34.155], Avg:    96.051 (1.000) <0-04:37:41> ({'r_t':  2245.8656, 'eps':     1.0000, 'critic_loss':   149.8176, 'actor_loss':     0.5568, 'eps_e':     1.0000})
Step:  704000, Reward:   170.336 [ 361.693], Avg:    96.156 (1.000) <0-04:38:07> ({'r_t':  1332.0618, 'eps':     1.0000, 'critic_loss':  6800.3125, 'actor_loss':    12.2357, 'eps_e':     1.0000})
Step:  705000, Reward:   262.032 [ 357.460], Avg:    96.391 (1.000) <0-04:38:33> ({'r_t':  1799.7529, 'eps':     1.0000, 'critic_loss':  2116.4680, 'actor_loss':    -1.6369, 'eps_e':     1.0000})
Step:  706000, Reward:   283.141 [ 144.162], Avg:    96.656 (1.000) <0-04:38:59> ({'r_t':  1940.6598, 'eps':     1.0000, 'critic_loss':  1162.0354, 'actor_loss':    -0.8237, 'eps_e':     1.0000})
Step:  707000, Reward:   312.892 [ 136.211], Avg:    96.961 (1.000) <0-04:39:24> ({'r_t':  1868.3524, 'eps':     1.0000, 'critic_loss':  1567.1853, 'actor_loss':    -1.5740, 'eps_e':     1.0000})
Step:  708000, Reward:   367.238 [  27.406], Avg:    97.342 (1.000) <0-04:39:50> ({'r_t':  2096.3680, 'eps':     1.0000, 'critic_loss':   205.6287, 'actor_loss':    -1.7786, 'eps_e':     1.0000})
Step:  709000, Reward:   365.543 [  64.522], Avg:    97.720 (1.000) <0-04:40:15> ({'r_t':  2228.1812, 'eps':     1.0000, 'critic_loss':   219.9218, 'actor_loss':    -0.1308, 'eps_e':     1.0000})
Step:  710000, Reward:   284.284 [  81.010], Avg:    97.982 (1.000) <0-04:40:41> ({'r_t':  1886.0176, 'eps':     1.0000, 'critic_loss':  2395.2229, 'actor_loss':     3.4760, 'eps_e':     1.0000})
Step:  711000, Reward:   359.791 [  24.966], Avg:    98.350 (1.000) <0-04:41:07> ({'r_t':  1868.9634, 'eps':     1.0000, 'critic_loss':   974.9920, 'actor_loss':     1.5109, 'eps_e':     1.0000})
Step:  712000, Reward:   354.986 [  32.583], Avg:    98.710 (1.000) <0-04:41:33> ({'r_t':  2155.6444, 'eps':     1.0000, 'critic_loss':   147.7784, 'actor_loss':    -1.5997, 'eps_e':     1.0000})
Step:  713000, Reward:   364.257 [  29.007], Avg:    99.082 (1.000) <0-04:41:59> ({'r_t':  2142.5737, 'eps':     1.0000, 'critic_loss':   193.6395, 'actor_loss':     0.3482, 'eps_e':     1.0000})
Step:  714000, Reward:   384.400 [  24.651], Avg:    99.481 (1.000) <0-04:42:24> ({'r_t':  2197.3990, 'eps':     1.0000, 'critic_loss':   127.9259, 'actor_loss':    -0.2343, 'eps_e':     1.0000})
Step:  715000, Reward:   378.220 [  19.847], Avg:    99.870 (1.000) <0-04:42:49> ({'r_t':  2312.9048, 'eps':     1.0000, 'critic_loss':    63.0470, 'actor_loss':    -0.9233, 'eps_e':     1.0000})
Step:  716000, Reward:   264.120 [ 353.000], Avg:   100.099 (1.000) <0-04:43:15> ({'r_t':  2293.8701, 'eps':     1.0000, 'critic_loss':    61.3961, 'actor_loss':     0.1721, 'eps_e':     1.0000})
Step:  717000, Reward:   358.831 [  59.876], Avg:   100.460 (1.000) <0-04:43:41> ({'r_t':  2242.4946, 'eps':     1.0000, 'critic_loss':    69.9854, 'actor_loss':     0.5658, 'eps_e':     1.0000})
Step:  718000, Reward:   257.104 [ 133.390], Avg:   100.677 (1.000) <0-04:44:07> ({'r_t':  1566.9036, 'eps':     1.0000, 'critic_loss':  1512.8868, 'actor_loss':     5.2899, 'eps_e':     1.0000})
Step:  719000, Reward:   154.550 [ 120.518], Avg:   100.752 (1.000) <0-04:44:33> ({'r_t':  1604.2021, 'eps':     1.0000, 'critic_loss':  1253.4799, 'actor_loss':     5.6108, 'eps_e':     1.0000})
Step:  720000, Reward:   295.315 [  81.927], Avg:   101.022 (1.000) <0-04:44:58> ({'r_t':  1632.3925, 'eps':     1.0000, 'critic_loss':   424.1602, 'actor_loss':    -0.7607, 'eps_e':     1.0000})
Step:  721000, Reward:   294.454 [  85.331], Avg:   101.290 (1.000) <0-04:45:23> ({'r_t':  1720.4648, 'eps':     1.0000, 'critic_loss':  2462.3816, 'actor_loss':     1.7200, 'eps_e':     1.0000})
Step:  722000, Reward:    65.461 [ 597.711], Avg:   101.241 (1.000) <0-04:45:50> ({'r_t':   639.1263, 'eps':     1.0000, 'critic_loss': 10210.9023, 'actor_loss':    12.6913, 'eps_e':     1.0000})
Step:  723000, Reward:  -221.119 [ 725.660], Avg:   100.795 (1.000) <0-04:46:15> ({'r_t': -1202.1168, 'eps':     1.0000, 'critic_loss': 23748.5488, 'actor_loss':     7.8954, 'eps_e':     1.0000})
Step:  724000, Reward:   193.582 [ 373.336], Avg:   100.923 (1.000) <0-04:46:43> ({'r_t': -1166.6693, 'eps':     1.0000, 'critic_loss': 22974.7422, 'actor_loss':    -2.0733, 'eps_e':     1.0000})
Step:  725000, Reward:  -187.058 [ 655.163], Avg:   100.527 (1.000) <0-04:47:08> ({'r_t':   179.7121, 'eps':     1.0000, 'critic_loss': 12490.7588, 'actor_loss':    -6.9294, 'eps_e':     1.0000})
Step:  726000, Reward:   118.523 [ 372.345], Avg:   100.551 (1.000) <0-04:47:35> ({'r_t':   262.5077, 'eps':     1.0000, 'critic_loss': 13019.5625, 'actor_loss':     1.6814, 'eps_e':     1.0000})
Step:  727000, Reward:    16.200 [ 572.032], Avg:   100.435 (1.000) <0-04:48:01> ({'r_t':   201.2159, 'eps':     1.0000, 'critic_loss': 13643.9678, 'actor_loss':     3.1622, 'eps_e':     1.0000})
Step:  728000, Reward:   283.790 [ 124.451], Avg:   100.687 (1.000) <0-04:48:26> ({'r_t':  1460.1278, 'eps':     1.0000, 'critic_loss':  3652.6436, 'actor_loss':    -7.6363, 'eps_e':     1.0000})
Step:  729000, Reward:   212.447 [ 370.731], Avg:   100.840 (1.000) <0-04:48:52> ({'r_t':  1380.1606, 'eps':     1.0000, 'critic_loss':  5158.6729, 'actor_loss':     0.7223, 'eps_e':     1.0000})
Step:  730000, Reward:   330.921 [  88.953], Avg:   101.155 (1.000) <0-04:49:18> ({'r_t':  1676.4884, 'eps':     1.0000, 'critic_loss':  4025.5903, 'actor_loss':    -3.1044, 'eps_e':     1.0000})
Step:  731000, Reward:   352.267 [  41.800], Avg:   101.498 (1.000) <0-04:49:43> ({'r_t':  1736.8189, 'eps':     1.0000, 'critic_loss':  2613.9436, 'actor_loss':     0.1745, 'eps_e':     1.0000})
Step:  732000, Reward:   344.138 [  50.909], Avg:   101.829 (1.000) <0-04:50:09> ({'r_t':  1945.7864, 'eps':     1.0000, 'critic_loss':   270.4899, 'actor_loss':    -1.5981, 'eps_e':     1.0000})
Step:  733000, Reward:   314.263 [  96.529], Avg:   102.118 (1.000) <0-04:50:36> ({'r_t':  1827.5958, 'eps':     1.0000, 'critic_loss':  1895.3866, 'actor_loss':     1.1792, 'eps_e':     1.0000})
Step:  734000, Reward:   370.926 [  36.398], Avg:   102.484 (1.000) <0-04:51:01> ({'r_t':  1991.8081, 'eps':     1.0000, 'critic_loss':  1138.1946, 'actor_loss':     1.2992, 'eps_e':     1.0000})
Step:  735000, Reward:   371.080 [  31.538], Avg:   102.849 (1.000) <0-04:51:27> ({'r_t':  1956.4968, 'eps':     1.0000, 'critic_loss':  1539.2256, 'actor_loss':    -1.7178, 'eps_e':     1.0000})
Step:  736000, Reward:   260.115 [ 173.794], Avg:   103.062 (1.000) <0-04:51:53> ({'r_t':  2159.3678, 'eps':     1.0000, 'critic_loss':   174.1641, 'actor_loss':    -1.8224, 'eps_e':     1.0000})
Step:  737000, Reward:   377.818 [  21.824], Avg:   103.435 (1.000) <0-04:52:18> ({'r_t':  2304.1432, 'eps':     1.0000, 'critic_loss':   253.8661, 'actor_loss':     0.4430, 'eps_e':     1.0000})
Step:  738000, Reward:   310.085 [  84.522], Avg:   103.714 (1.000) <0-04:52:48> ({'r_t':  1971.4515, 'eps':     1.0000, 'critic_loss':  1384.1384, 'actor_loss':     2.0716, 'eps_e':     1.0000})
Step:  739000, Reward:   276.563 [ 150.447], Avg:   103.948 (1.000) <0-04:53:14> ({'r_t':  1775.2662, 'eps':     1.0000, 'critic_loss':   797.6365, 'actor_loss':    -0.4074, 'eps_e':     1.0000})
Step:  740000, Reward:   375.856 [  17.504], Avg:   104.315 (1.000) <0-04:53:40> ({'r_t':  1997.5401, 'eps':     1.0000, 'critic_loss':   996.2639, 'actor_loss':    -0.6143, 'eps_e':     1.0000})
Step:  741000, Reward:   359.233 [  32.018], Avg:   104.658 (1.000) <0-04:54:06> ({'r_t':  2016.0452, 'eps':     1.0000, 'critic_loss':   236.9297, 'actor_loss':     0.0466, 'eps_e':     1.0000})
Step:  742000, Reward:   212.264 [ 393.758], Avg:   104.803 (1.000) <0-04:54:32> ({'r_t':  1905.2810, 'eps':     1.0000, 'critic_loss':  1973.7963, 'actor_loss':     2.8051, 'eps_e':     1.0000})
Step:  743000, Reward:   164.972 [ 479.936], Avg:   104.884 (1.000) <0-04:54:58> ({'r_t':  2110.5991, 'eps':     1.0000, 'critic_loss':   310.2851, 'actor_loss':    -0.9156, 'eps_e':     1.0000})
Step:  744000, Reward:   373.993 [  27.410], Avg:   105.245 (1.000) <0-04:55:24> ({'r_t':  2046.6875, 'eps':     1.0000, 'critic_loss':  1446.3655, 'actor_loss':     0.7012, 'eps_e':     1.0000})
Step:  745000, Reward:   360.518 [  29.037], Avg:   105.587 (1.000) <0-04:55:50> ({'r_t':  2147.1911, 'eps':     1.0000, 'critic_loss':   294.1197, 'actor_loss':    -0.7615, 'eps_e':     1.0000})
Step:  746000, Reward:   373.997 [  25.452], Avg:   105.947 (1.000) <0-04:56:16> ({'r_t':  2196.5042, 'eps':     1.0000, 'critic_loss':   126.7278, 'actor_loss':    -0.2351, 'eps_e':     1.0000})
Step:  747000, Reward:   335.254 [  60.757], Avg:   106.253 (1.000) <0-04:56:41> ({'r_t':  1766.5285, 'eps':     1.0000, 'critic_loss':  1308.3872, 'actor_loss':     3.4664, 'eps_e':     1.0000})
Step:  748000, Reward:   274.744 [ 365.850], Avg:   106.478 (1.000) <0-04:57:06> ({'r_t':  2048.1051, 'eps':     1.0000, 'critic_loss':   257.2406, 'actor_loss':    -1.1980, 'eps_e':     1.0000})
Step:  749000, Reward:   346.417 [  88.220], Avg:   106.798 (1.000) <0-04:57:32> ({'r_t':  2011.5270, 'eps':     1.0000, 'critic_loss':  1344.7181, 'actor_loss':    -1.1127, 'eps_e':     1.0000})
Step:  750000, Reward:   359.790 [  27.830], Avg:   107.135 (1.000) <0-04:57:59> ({'r_t':  1944.7643, 'eps':     1.0000, 'critic_loss':  2814.0278, 'actor_loss':     1.3915, 'eps_e':     1.0000})
Step:  751000, Reward:   269.300 [ 366.315], Avg:   107.351 (1.000) <0-04:58:24> ({'r_t':  2112.5761, 'eps':     1.0000, 'critic_loss':   145.2764, 'actor_loss':    -1.0776, 'eps_e':     1.0000})
Step:  752000, Reward:   373.584 [  20.826], Avg:   107.704 (1.000) <0-04:58:49> ({'r_t':  2211.6582, 'eps':     1.0000, 'critic_loss':   178.2101, 'actor_loss':    -0.2014, 'eps_e':     1.0000})
Step:  753000, Reward:   318.969 [  87.177], Avg:   107.985 (1.000) <0-04:59:17> ({'r_t':  1852.0763, 'eps':     1.0000, 'critic_loss':  1125.2424, 'actor_loss':     2.4769, 'eps_e':     1.0000})
Step:  754000, Reward:   352.897 [  83.885], Avg:   108.309 (1.000) <0-04:59:42> ({'r_t':  2036.0868, 'eps':     1.0000, 'critic_loss':   211.6761, 'actor_loss':    -0.7552, 'eps_e':     1.0000})
Step:  755000, Reward:   223.939 [ 471.420], Avg:   108.462 (1.000) <0-05:00:07> ({'r_t':  2175.4248, 'eps':     1.0000, 'critic_loss':   136.1822, 'actor_loss':    -1.7478, 'eps_e':     1.0000})
Step:  756000, Reward:   270.174 [ 353.873], Avg:   108.675 (1.000) <0-05:00:33> ({'r_t':  2134.5712, 'eps':     1.0000, 'critic_loss':  1094.2747, 'actor_loss':    -0.2510, 'eps_e':     1.0000})
Step:  757000, Reward:   360.075 [  34.424], Avg:   109.007 (1.000) <0-05:00:58> ({'r_t':  2166.5906, 'eps':     1.0000, 'critic_loss':  1616.5372, 'actor_loss':     1.6386, 'eps_e':     1.0000})
Step:  758000, Reward:   368.830 [  32.822], Avg:   109.349 (1.000) <0-05:01:25> ({'r_t':  2140.5743, 'eps':     1.0000, 'critic_loss':   261.6241, 'actor_loss':    -1.3443, 'eps_e':     1.0000})
Step:  759000, Reward:   371.469 [  15.378], Avg:   109.694 (1.000) <0-05:01:50> ({'r_t':  2191.3898, 'eps':     1.0000, 'critic_loss':   699.5099, 'actor_loss':     1.0246, 'eps_e':     1.0000})
Step:  760000, Reward:   181.580 [ 500.898], Avg:   109.789 (1.000) <0-05:02:15> ({'r_t':  2059.3438, 'eps':     1.0000, 'critic_loss':  2204.4697, 'actor_loss':     2.2794, 'eps_e':     1.0000})
Step:  761000, Reward:   352.370 [  42.864], Avg:   110.107 (1.000) <0-05:02:41> ({'r_t':  1751.9726, 'eps':     1.0000, 'critic_loss':  3306.6831, 'actor_loss':    -0.7853, 'eps_e':     1.0000})
Step:  762000, Reward:   270.367 [ 143.059], Avg:   110.317 (1.000) <0-05:03:07> ({'r_t':  1593.6321, 'eps':     1.0000, 'critic_loss':  4339.8726, 'actor_loss':     1.1615, 'eps_e':     1.0000})
Step:  763000, Reward:   296.845 [ 130.596], Avg:   110.561 (1.000) <0-05:03:33> ({'r_t':  1855.3953, 'eps':     1.0000, 'critic_loss':  1231.1315, 'actor_loss':    -1.3253, 'eps_e':     1.0000})
Step:  764000, Reward:   316.383 [ 123.416], Avg:   110.830 (1.000) <0-05:03:58> ({'r_t':  1661.1802, 'eps':     1.0000, 'critic_loss':  4015.8508, 'actor_loss':     3.6351, 'eps_e':     1.0000})
Step:  765000, Reward:   338.768 [  44.597], Avg:   111.128 (1.000) <0-05:04:24> ({'r_t':  1865.5793, 'eps':     1.0000, 'critic_loss':  2358.3892, 'actor_loss':    -1.8566, 'eps_e':     1.0000})
Step:  766000, Reward:   352.103 [  39.874], Avg:   111.442 (1.000) <0-05:04:49> ({'r_t':  2055.5156, 'eps':     1.0000, 'critic_loss':   398.9396, 'actor_loss':    -0.2810, 'eps_e':     1.0000})
Step:  767000, Reward:   352.557 [  33.855], Avg:   111.756 (1.000) <0-05:05:15> ({'r_t':  1884.4072, 'eps':     1.0000, 'critic_loss':   516.9891, 'actor_loss':    -0.0424, 'eps_e':     1.0000})
Step:  768000, Reward:   342.036 [  37.907], Avg:   112.056 (1.000) <0-05:05:41> ({'r_t':  2015.1579, 'eps':     1.0000, 'critic_loss':  3120.7119, 'actor_loss':     2.5020, 'eps_e':     1.0000})
Step:  769000, Reward:   276.529 [ 146.081], Avg:   112.269 (1.000) <0-05:06:07> ({'r_t':  1971.5336, 'eps':     1.0000, 'critic_loss':   213.4465, 'actor_loss':    -1.7316, 'eps_e':     1.0000})
Step:  770000, Reward:   331.594 [  79.302], Avg:   112.554 (1.000) <0-05:06:33> ({'r_t':  2093.3280, 'eps':     1.0000, 'critic_loss':    94.7041, 'actor_loss':    -0.3052, 'eps_e':     1.0000})
Step:  771000, Reward:   331.018 [  46.870], Avg:   112.837 (1.000) <0-05:06:59> ({'r_t':  1333.8746, 'eps':     1.0000, 'critic_loss':  6057.4897, 'actor_loss':     4.8097, 'eps_e':     1.0000})
Step:  772000, Reward:   360.894 [  34.083], Avg:   113.158 (1.000) <0-05:07:25> ({'r_t':  1969.3938, 'eps':     1.0000, 'critic_loss':   400.3923, 'actor_loss':    -4.1438, 'eps_e':     1.0000})
Step:  773000, Reward:   361.803 [  35.242], Avg:   113.479 (1.000) <0-05:07:50> ({'r_t':  1942.6876, 'eps':     1.0000, 'critic_loss':  1691.1920, 'actor_loss':     2.9321, 'eps_e':     1.0000})
Step:  774000, Reward:   343.614 [  80.459], Avg:   113.776 (1.000) <0-05:08:17> ({'r_t':  2002.5672, 'eps':     1.0000, 'critic_loss':  1068.3695, 'actor_loss':     2.6498, 'eps_e':     1.0000})
Step:  775000, Reward:   347.990 [  65.735], Avg:   114.078 (1.000) <0-05:08:42> ({'r_t':  1880.9515, 'eps':     1.0000, 'critic_loss':  2456.1228, 'actor_loss':    -0.5954, 'eps_e':     1.0000})
Step:  776000, Reward:   312.872 [ 162.896], Avg:   114.333 (1.000) <0-05:09:08> ({'r_t':  2061.1367, 'eps':     1.0000, 'critic_loss':   171.7091, 'actor_loss':    -1.5680, 'eps_e':     1.0000})
Step:  777000, Reward:   181.199 [ 130.629], Avg:   114.419 (1.000) <0-05:09:33> ({'r_t':  1726.5497, 'eps':     1.0000, 'critic_loss':  1441.6698, 'actor_loss':     2.9709, 'eps_e':     1.0000})
Step:  778000, Reward:   352.403 [  77.151], Avg:   114.725 (1.000) <0-05:09:59> ({'r_t':  1665.2730, 'eps':     1.0000, 'critic_loss':   914.4997, 'actor_loss':    -1.6303, 'eps_e':     1.0000})
Step:  779000, Reward:   345.979 [  80.959], Avg:   115.021 (1.000) <0-05:10:25> ({'r_t':  1964.3360, 'eps':     1.0000, 'critic_loss':  1293.5122, 'actor_loss':     0.6003, 'eps_e':     1.0000})
Step:  780000, Reward:   358.151 [  25.839], Avg:   115.333 (1.000) <0-05:10:51> ({'r_t':  2056.8601, 'eps':     1.0000, 'critic_loss':  1149.8523, 'actor_loss':     0.7202, 'eps_e':     1.0000})
Step:  781000, Reward:   291.074 [ 362.813], Avg:   115.557 (1.000) <0-05:11:17> ({'r_t':  2139.0948, 'eps':     1.0000, 'critic_loss':  1403.8121, 'actor_loss':     0.1004, 'eps_e':     1.0000})
Step:  782000, Reward:   353.055 [  95.888], Avg:   115.861 (1.000) <0-05:11:42> ({'r_t':  2122.4637, 'eps':     1.0000, 'critic_loss':  1309.2460, 'actor_loss':    -0.4011, 'eps_e':     1.0000})
Step:  783000, Reward:   378.752 [  21.654], Avg:   116.196 (1.000) <0-05:12:08> ({'r_t':  2287.9877, 'eps':     1.0000, 'critic_loss':    47.2757, 'actor_loss':    -0.6607, 'eps_e':     1.0000})
Step:  784000, Reward:   368.526 [  27.591], Avg:   116.517 (1.000) <0-05:12:34> ({'r_t':  2222.1952, 'eps':     1.0000, 'critic_loss':   286.4022, 'actor_loss':     1.0751, 'eps_e':     1.0000})
Step:  785000, Reward:   328.584 [ 153.191], Avg:   116.787 (1.000) <0-05:12:59> ({'r_t':  2175.9885, 'eps':     1.0000, 'critic_loss':   309.1910, 'actor_loss':     0.3853, 'eps_e':     1.0000})
Step:  786000, Reward:   390.881 [  11.000], Avg:   117.135 (1.000) <0-05:13:24> ({'r_t':  2117.6246, 'eps':     1.0000, 'critic_loss':  1519.6729, 'actor_loss':     1.5528, 'eps_e':     1.0000})
Step:  787000, Reward:   371.512 [  32.883], Avg:   117.458 (1.000) <0-05:13:49> ({'r_t':  2393.5262, 'eps':     1.0000, 'critic_loss':    93.7841, 'actor_loss':    -2.3307, 'eps_e':     1.0000})
Step:  788000, Reward:   337.043 [ 119.461], Avg:   117.737 (1.000) <0-05:14:15> ({'r_t':  2007.8674, 'eps':     1.0000, 'critic_loss':  1838.4049, 'actor_loss':     3.1347, 'eps_e':     1.0000})
Step:  789000, Reward:   366.537 [  32.359], Avg:   118.052 (1.000) <0-05:14:42> ({'r_t':  1978.7664, 'eps':     1.0000, 'critic_loss':  2583.2568, 'actor_loss':     1.2548, 'eps_e':     1.0000})
Step:  790000, Reward:   369.807 [  59.857], Avg:   118.370 (1.000) <0-05:15:08> ({'r_t':  2263.7754, 'eps':     1.0000, 'critic_loss':   310.4314, 'actor_loss':     0.0375, 'eps_e':     1.0000})
Step:  791000, Reward:   367.509 [  33.566], Avg:   118.684 (1.000) <0-05:15:33> ({'r_t':  2269.3775, 'eps':     1.0000, 'critic_loss':   183.6136, 'actor_loss':     0.0771, 'eps_e':     1.0000})
Step:  792000, Reward:   331.765 [  50.170], Avg:   118.953 (1.000) <0-05:15:59> ({'r_t':  1597.1876, 'eps':     1.0000, 'critic_loss':  4953.6616, 'actor_loss':     3.9480, 'eps_e':     1.0000})
Step:  793000, Reward:   224.497 [ 373.216], Avg:   119.086 (1.000) <0-05:16:25> ({'r_t':  1635.2817, 'eps':     1.0000, 'critic_loss':  4444.6201, 'actor_loss':     1.8029, 'eps_e':     1.0000})
Step:  794000, Reward:   330.990 [ 114.877], Avg:   119.353 (1.000) <0-05:16:52> ({'r_t':  1772.9942, 'eps':     1.0000, 'critic_loss':  2247.4778, 'actor_loss':     2.9058, 'eps_e':     1.0000})
Step:  795000, Reward:   319.407 [ 113.662], Avg:   119.604 (1.000) <0-05:17:17> ({'r_t':  2091.8395, 'eps':     1.0000, 'critic_loss':  1113.2336, 'actor_loss':    -3.4212, 'eps_e':     1.0000})
Step:  796000, Reward:   278.763 [ 361.371], Avg:   119.804 (1.000) <0-05:17:43> ({'r_t':  2131.4397, 'eps':     1.0000, 'critic_loss':   479.7028, 'actor_loss':     0.5075, 'eps_e':     1.0000})
Step:  797000, Reward:   354.107 [  41.950], Avg:   120.097 (1.000) <0-05:18:08> ({'r_t':  1949.5666, 'eps':     1.0000, 'critic_loss':  2262.0244, 'actor_loss':     1.2698, 'eps_e':     1.0000})
Step:  798000, Reward:   359.891 [  27.437], Avg:   120.397 (1.000) <0-05:18:33> ({'r_t':  2020.0059, 'eps':     1.0000, 'critic_loss':  1363.3384, 'actor_loss':     0.6020, 'eps_e':     1.0000})
Step:  799000, Reward:   338.260 [ 109.223], Avg:   120.670 (1.000) <0-05:18:59> ({'r_t':  2038.0437, 'eps':     1.0000, 'critic_loss':  1928.4353, 'actor_loss':     0.4865, 'eps_e':     1.0000})
Step:  800000, Reward:   351.702 [  66.375], Avg:   120.958 (1.000) <0-05:19:25> ({'r_t':  1938.6595, 'eps':     1.0000, 'critic_loss':  1146.7833, 'actor_loss':     1.6842, 'eps_e':     1.0000})
Step:  801000, Reward:   353.436 [ 110.282], Avg:   121.248 (1.000) <0-05:19:51> ({'r_t':  2187.3068, 'eps':     1.0000, 'critic_loss':   172.7076, 'actor_loss':    -2.7704, 'eps_e':     1.0000})
Step:  802000, Reward:   346.221 [  65.019], Avg:   121.528 (1.000) <0-05:20:16> ({'r_t':  1894.7588, 'eps':     1.0000, 'critic_loss':  1851.6731, 'actor_loss':     3.3761, 'eps_e':     1.0000})
Step:  803000, Reward:   371.113 [  29.013], Avg:   121.839 (1.000) <0-05:20:42> ({'r_t':  2098.6709, 'eps':     1.0000, 'critic_loss':   179.3814, 'actor_loss':    -2.4833, 'eps_e':     1.0000})
Step:  804000, Reward:   357.810 [  61.745], Avg:   122.132 (1.000) <0-05:21:08> ({'r_t':  2089.7565, 'eps':     1.0000, 'critic_loss':  1649.0160, 'actor_loss':     1.7085, 'eps_e':     1.0000})
Step:  805000, Reward:   313.870 [ 142.862], Avg:   122.370 (1.000) <0-05:21:35> ({'r_t':  2101.1916, 'eps':     1.0000, 'critic_loss':   116.5697, 'actor_loss':    -1.0673, 'eps_e':     1.0000})
Step:  806000, Reward:   361.589 [  60.092], Avg:   122.666 (1.000) <0-05:22:01> ({'r_t':  2100.6411, 'eps':     1.0000, 'critic_loss':   138.9020, 'actor_loss':    -0.1378, 'eps_e':     1.0000})
Step:  807000, Reward:   171.630 [ 151.862], Avg:   122.727 (1.000) <0-05:22:27> ({'r_t':  2051.3602, 'eps':     1.0000, 'critic_loss':  1180.3516, 'actor_loss':     2.4613, 'eps_e':     1.0000})
Step:  808000, Reward:   249.567 [ 152.504], Avg:   122.883 (1.000) <0-05:22:53> ({'r_t':  1675.3339, 'eps':     1.0000, 'critic_loss':   684.7429, 'actor_loss':     0.6391, 'eps_e':     1.0000})
Step:  809000, Reward:   324.326 [ 140.434], Avg:   123.132 (1.000) <0-05:23:19> ({'r_t':  1794.0818, 'eps':     1.0000, 'critic_loss':  1738.8372, 'actor_loss':    -0.4881, 'eps_e':     1.0000})
Step:  810000, Reward:   358.549 [  78.832], Avg:   123.422 (1.000) <0-05:23:44> ({'r_t':  2158.3276, 'eps':     1.0000, 'critic_loss':   417.4006, 'actor_loss':    -1.5664, 'eps_e':     1.0000})
Step:  811000, Reward:   276.679 [ 366.802], Avg:   123.611 (1.000) <0-05:24:10> ({'r_t':  2109.8300, 'eps':     1.0000, 'critic_loss':   278.8517, 'actor_loss':     1.3600, 'eps_e':     1.0000})
Step:  812000, Reward:   369.952 [  32.407], Avg:   123.914 (1.000) <0-05:24:36> ({'r_t':  2288.7038, 'eps':     1.0000, 'critic_loss':   130.2581, 'actor_loss':    -0.3213, 'eps_e':     1.0000})
Step:  813000, Reward:   316.857 [  74.929], Avg:   124.151 (1.000) <0-05:25:02> ({'r_t':  1986.6979, 'eps':     1.0000, 'critic_loss':  1846.0693, 'actor_loss':     3.7267, 'eps_e':     1.0000})
Step:  814000, Reward:   304.359 [ 120.848], Avg:   124.372 (1.000) <0-05:25:29> ({'r_t':  1875.2136, 'eps':     1.0000, 'critic_loss':   937.1008, 'actor_loss':     0.2465, 'eps_e':     1.0000})
Step:  815000, Reward:   304.911 [ 146.601], Avg:   124.593 (1.000) <0-05:25:57> ({'r_t':  2117.3300, 'eps':     1.0000, 'critic_loss':   243.2931, 'actor_loss':    -0.6443, 'eps_e':     1.0000})
Step:  816000, Reward:   338.377 [  48.186], Avg:   124.855 (1.000) <0-05:26:22> ({'r_t':  2099.9994, 'eps':     1.0000, 'critic_loss':   169.1478, 'actor_loss':    -0.5881, 'eps_e':     1.0000})
Step:  817000, Reward:   335.913 [  39.853], Avg:   125.113 (1.000) <0-05:26:49> ({'r_t':  1832.7133, 'eps':     1.0000, 'critic_loss':  2029.3766, 'actor_loss':     3.5623, 'eps_e':     1.0000})
Step:  818000, Reward:   356.016 [  33.903], Avg:   125.395 (1.000) <0-05:27:14> ({'r_t':  1863.4817, 'eps':     1.0000, 'critic_loss':  1197.2780, 'actor_loss':    -0.8880, 'eps_e':     1.0000})
Step:  819000, Reward:    24.550 [   4.648], Avg:   125.272 (1.000) <0-05:27:37> ({'r_t':   857.6968, 'eps':     1.0000, 'critic_loss':  2082.4331, 'actor_loss':     8.5753, 'eps_e':     1.0000})
Step:  820000, Reward:    30.151 [   5.204], Avg:   125.156 (1.000) <0-05:27:59> ({'r_t':   313.8591, 'eps':     1.0000, 'critic_loss':    21.3995, 'actor_loss':    -0.0631, 'eps_e':     1.0000})
Step:  821000, Reward:    28.650 [  18.735], Avg:   125.039 (1.000) <0-05:28:22> ({'r_t':   389.3687, 'eps':     1.0000, 'critic_loss':    20.7980, 'actor_loss':    -0.0487, 'eps_e':     1.0000})
Step:  822000, Reward:    34.211 [  16.483], Avg:   124.929 (1.000) <0-05:28:44> ({'r_t':   402.1862, 'eps':     1.0000, 'critic_loss':    23.8405, 'actor_loss':    -0.3027, 'eps_e':     1.0000})
Step:  823000, Reward:    45.714 [   9.944], Avg:   124.832 (1.000) <0-05:29:06> ({'r_t':   412.4776, 'eps':     1.0000, 'critic_loss':    32.5630, 'actor_loss':    -0.0272, 'eps_e':     1.0000})
Step:  824000, Reward:    47.248 [  18.419], Avg:   124.738 (1.000) <0-05:29:29> ({'r_t':   540.0030, 'eps':     1.0000, 'critic_loss':    22.9561, 'actor_loss':    -0.3025, 'eps_e':     1.0000})
Step:  825000, Reward:    59.620 [  11.409], Avg:   124.659 (1.000) <0-05:29:51> ({'r_t':   573.4757, 'eps':     1.0000, 'critic_loss':    82.4833, 'actor_loss':    -0.0914, 'eps_e':     1.0000})
Step:  826000, Reward:    57.100 [  25.606], Avg:   124.578 (1.000) <0-05:30:14> ({'r_t':   610.0924, 'eps':     1.0000, 'critic_loss':    36.5378, 'actor_loss':    -0.2286, 'eps_e':     1.0000})
Step:  827000, Reward:   168.613 [ 122.678], Avg:   124.631 (1.000) <0-05:30:38> ({'r_t':   791.7211, 'eps':     1.0000, 'critic_loss':  1941.6643, 'actor_loss':    -1.3305, 'eps_e':     1.0000})
Step:  828000, Reward:    76.552 [ 367.564], Avg:   124.573 (1.000) <0-05:31:05> ({'r_t':  1076.1943, 'eps':     1.0000, 'critic_loss':  3531.8806, 'actor_loss':    -4.2614, 'eps_e':     1.0000})
Step:  829000, Reward:   -16.133 [ 537.966], Avg:   124.403 (1.000) <0-05:31:31> ({'r_t':   582.3907, 'eps':     1.0000, 'critic_loss': 10865.4297, 'actor_loss':     0.7607, 'eps_e':     1.0000})
Step:  830000, Reward:   222.760 [ 164.658], Avg:   124.522 (1.000) <0-05:31:57> ({'r_t':   542.7023, 'eps':     1.0000, 'critic_loss': 11129.4844, 'actor_loss':     0.8294, 'eps_e':     1.0000})
Step:  831000, Reward:   252.494 [ 162.440], Avg:   124.676 (1.000) <0-05:32:22> ({'r_t':  1576.2470, 'eps':     1.0000, 'critic_loss':  5336.8320, 'actor_loss':    -1.1998, 'eps_e':     1.0000})
Step:  832000, Reward:   335.923 [  43.872], Avg:   124.929 (1.000) <0-05:32:47> ({'r_t':  1575.4968, 'eps':     1.0000, 'critic_loss':  3604.9324, 'actor_loss':     1.4857, 'eps_e':     1.0000})
Step:  833000, Reward:   359.080 [  65.316], Avg:   125.210 (1.000) <0-05:33:13> ({'r_t':  1878.5756, 'eps':     1.0000, 'critic_loss':  1659.9819, 'actor_loss':     0.6843, 'eps_e':     1.0000})
Step:  834000, Reward:   323.177 [ 126.396], Avg:   125.447 (1.000) <0-05:33:39> ({'r_t':  2060.0929, 'eps':     1.0000, 'critic_loss':   516.3198, 'actor_loss':    -1.2127, 'eps_e':     1.0000})
Step:  835000, Reward:   355.714 [  90.796], Avg:   125.723 (1.000) <0-05:34:05> ({'r_t':  2128.7756, 'eps':     1.0000, 'critic_loss':   354.9938, 'actor_loss':    -0.3225, 'eps_e':     1.0000})
Step:  836000, Reward:   380.536 [  33.919], Avg:   126.027 (1.000) <0-05:34:31> ({'r_t':  2296.6471, 'eps':     1.0000, 'critic_loss':   148.7326, 'actor_loss':    -0.3216, 'eps_e':     1.0000})
Step:  837000, Reward:   259.420 [ 371.503], Avg:   126.186 (1.000) <0-05:34:56> ({'r_t':  1957.1269, 'eps':     1.0000, 'critic_loss':  1468.0024, 'actor_loss':     2.8139, 'eps_e':     1.0000})
Step:  838000, Reward:   373.051 [  24.645], Avg:   126.480 (1.000) <0-05:35:21> ({'r_t':  1357.3727, 'eps':     1.0000, 'critic_loss':  5462.0127, 'actor_loss':    -1.4255, 'eps_e':     1.0000})
Step:  839000, Reward:   318.299 [ 118.643], Avg:   126.709 (1.000) <0-05:35:47> ({'r_t':  2010.5385, 'eps':     1.0000, 'critic_loss':  3906.6270, 'actor_loss':    -2.4213, 'eps_e':     1.0000})
Step:  840000, Reward:   348.027 [  92.176], Avg:   126.972 (1.000) <0-05:36:13> ({'r_t':  2131.0917, 'eps':     1.0000, 'critic_loss':   292.2960, 'actor_loss':    -0.6332, 'eps_e':     1.0000})
Step:  841000, Reward:   368.525 [  38.335], Avg:   127.259 (1.000) <0-05:36:38> ({'r_t':  2210.8494, 'eps':     1.0000, 'critic_loss':   156.1790, 'actor_loss':    -0.1264, 'eps_e':     1.0000})
Step:  842000, Reward:   385.219 [  17.152], Avg:   127.565 (1.000) <0-05:37:04> ({'r_t':  2129.5498, 'eps':     1.0000, 'critic_loss':   990.6868, 'actor_loss':     1.4236, 'eps_e':     1.0000})
Step:  843000, Reward:   355.922 [  35.792], Avg:   127.835 (1.000) <0-05:37:29> ({'r_t':  2064.6024, 'eps':     1.0000, 'critic_loss':   673.7969, 'actor_loss':     0.8056, 'eps_e':     1.0000})
Step:  844000, Reward:   342.515 [  93.267], Avg:   128.089 (1.000) <0-05:37:54> ({'r_t':  2273.7346, 'eps':     1.0000, 'critic_loss':   131.3363, 'actor_loss':    -1.1221, 'eps_e':     1.0000})
Step:  845000, Reward:   361.792 [  39.300], Avg:   128.366 (1.000) <0-05:38:20> ({'r_t':  2176.3819, 'eps':     1.0000, 'critic_loss':   406.0352, 'actor_loss':    -0.5505, 'eps_e':     1.0000})
Step:  846000, Reward:   262.919 [ 362.881], Avg:   128.524 (1.000) <0-05:38:46> ({'r_t':  2082.1770, 'eps':     1.0000, 'critic_loss':   133.3555, 'actor_loss':     0.5453, 'eps_e':     1.0000})
Step:  847000, Reward:   313.436 [ 141.939], Avg:   128.743 (1.000) <0-05:39:12> ({'r_t':  2091.7600, 'eps':     1.0000, 'critic_loss':   355.2334, 'actor_loss':     1.9220, 'eps_e':     1.0000})
Step:  848000, Reward:   211.267 [ 391.233], Avg:   128.840 (1.000) <0-05:39:37> ({'r_t':  1738.2750, 'eps':     1.0000, 'critic_loss':  3125.6182, 'actor_loss':     3.3965, 'eps_e':     1.0000})
Step:  849000, Reward:   322.220 [ 111.149], Avg:   129.067 (1.000) <0-05:40:02> ({'r_t':  1905.1655, 'eps':     1.0000, 'critic_loss':  1705.3622, 'actor_loss':    -1.8994, 'eps_e':     1.0000})
Step:  850000, Reward:   349.647 [  28.414], Avg:   129.326 (1.000) <0-05:40:28> ({'r_t':  1947.7790, 'eps':     1.0000, 'critic_loss':  1517.9493, 'actor_loss':     0.0960, 'eps_e':     1.0000})
Step:  851000, Reward:   327.044 [  94.895], Avg:   129.559 (1.000) <0-05:40:54> ({'r_t':  1875.4849, 'eps':     1.0000, 'critic_loss':  1279.6862, 'actor_loss':    -2.9448, 'eps_e':     1.0000})
Step:  852000, Reward:   372.572 [  31.704], Avg:   129.843 (1.000) <0-05:41:19> ({'r_t':  2088.1250, 'eps':     1.0000, 'critic_loss':  1091.6897, 'actor_loss':    -0.1486, 'eps_e':     1.0000})
Step:  853000, Reward:   321.262 [  98.840], Avg:   130.068 (1.000) <0-05:41:44> ({'r_t':  2165.7374, 'eps':     1.0000, 'critic_loss':  1381.0287, 'actor_loss':    -0.3163, 'eps_e':     1.0000})
Step:  854000, Reward:   376.324 [  50.226], Avg:   130.356 (1.000) <0-05:42:10> ({'r_t':  2323.7407, 'eps':     1.0000, 'critic_loss':    88.5281, 'actor_loss':    -0.9275, 'eps_e':     1.0000})
Step:  855000, Reward:   359.360 [  37.638], Avg:   130.623 (1.000) <0-05:42:37> ({'r_t':  2187.8036, 'eps':     1.0000, 'critic_loss':  1228.0669, 'actor_loss':     1.6436, 'eps_e':     1.0000})
Step:  856000, Reward:   386.146 [  20.152], Avg:   130.921 (1.000) <0-05:43:02> ({'r_t':  2271.5926, 'eps':     1.0000, 'critic_loss':   303.2884, 'actor_loss':     0.3352, 'eps_e':     1.0000})
Step:  857000, Reward:   -94.649 [ 582.899], Avg:   130.658 (1.000) <0-05:43:27> ({'r_t':  2234.9019, 'eps':     1.0000, 'critic_loss':   358.5151, 'actor_loss':     1.1699, 'eps_e':     1.0000})
Step:  858000, Reward:   187.903 [  53.285], Avg:   130.725 (1.000) <0-05:43:51> ({'r_t':  1347.1735, 'eps':     1.0000, 'critic_loss':  2114.2537, 'actor_loss':     8.7104, 'eps_e':     1.0000})
Step:  859000, Reward:   243.112 [  56.109], Avg:   130.856 (1.000) <0-05:44:16> ({'r_t':  1697.5896, 'eps':     1.0000, 'critic_loss':  1026.5482, 'actor_loss':     0.0382, 'eps_e':     1.0000})
Step:  860000, Reward:   361.361 [  48.825], Avg:   131.123 (1.000) <0-05:44:41> ({'r_t':  2148.2166, 'eps':     1.0000, 'critic_loss':   379.9497, 'actor_loss':    -5.4304, 'eps_e':     1.0000})
Step:  861000, Reward:   346.773 [  98.518], Avg:   131.374 (1.000) <0-05:45:06> ({'r_t':  2021.8281, 'eps':     1.0000, 'critic_loss':  1601.1770, 'actor_loss':     0.4938, 'eps_e':     1.0000})
Step:  862000, Reward:   321.834 [ 148.777], Avg:   131.594 (1.000) <0-05:45:32> ({'r_t':  2012.5155, 'eps':     1.0000, 'critic_loss':   889.2759, 'actor_loss':    -3.2548, 'eps_e':     1.0000})
Step:  863000, Reward:   355.979 [  95.495], Avg:   131.854 (1.000) <0-05:45:56> ({'r_t':  2092.1823, 'eps':     1.0000, 'critic_loss':  1456.4767, 'actor_loss':     0.8613, 'eps_e':     1.0000})
Step:  864000, Reward:   370.862 [  31.697], Avg:   132.130 (1.000) <0-05:46:23> ({'r_t':  2385.7758, 'eps':     1.0000, 'critic_loss':   100.5239, 'actor_loss':    -1.6951, 'eps_e':     1.0000})
Step:  865000, Reward:   374.304 [  23.759], Avg:   132.410 (1.000) <0-05:46:48> ({'r_t':  1965.2397, 'eps':     1.0000, 'critic_loss':  1652.8634, 'actor_loss':     1.9028, 'eps_e':     1.0000})
Step:  866000, Reward:   375.991 [  25.950], Avg:   132.691 (1.000) <0-05:47:14> ({'r_t':  2073.3873, 'eps':     1.0000, 'critic_loss':   562.1925, 'actor_loss':    -1.3844, 'eps_e':     1.0000})
Step:  867000, Reward:   378.259 [  21.556], Avg:   132.974 (1.000) <0-05:47:39> ({'r_t':  2353.2459, 'eps':     1.0000, 'critic_loss':   132.1221, 'actor_loss':    -1.0174, 'eps_e':     1.0000})
Step:  868000, Reward:   376.834 [  29.573], Avg:   133.254 (1.000) <0-05:48:06> ({'r_t':  2173.9409, 'eps':     1.0000, 'critic_loss':   131.6828, 'actor_loss':     1.0331, 'eps_e':     1.0000})
Step:  869000, Reward:    91.128 [  58.886], Avg:   133.206 (1.000) <0-05:48:33> ({'r_t':  2145.0602, 'eps':     1.0000, 'critic_loss':  1201.2235, 'actor_loss':     1.1198, 'eps_e':     1.0000})
Step:  870000, Reward:    97.990 [ 100.813], Avg:   133.166 (1.000) <0-05:49:01> ({'r_t':    23.8666, 'eps':     1.0000, 'critic_loss':  5296.5469, 'actor_loss':    19.2363, 'eps_e':     1.0000})
Step:  871000, Reward:   100.535 [  97.461], Avg:   133.128 (1.000) <0-05:49:26> ({'r_t':   707.6802, 'eps':     1.0000, 'critic_loss':  1035.8263, 'actor_loss':    -1.9836, 'eps_e':     1.0000})
Step:  872000, Reward:   115.560 [  92.785], Avg:   133.108 (1.000) <0-05:49:52> ({'r_t':   968.1992, 'eps':     1.0000, 'critic_loss':   452.2781, 'actor_loss':    -2.5761, 'eps_e':     1.0000})
Step:  873000, Reward:    88.997 [  89.025], Avg:   133.058 (1.000) <0-05:50:18> ({'r_t':   986.9282, 'eps':     1.0000, 'critic_loss':  1082.9156, 'actor_loss':    -1.0480, 'eps_e':     1.0000})
Step:  874000, Reward:   227.342 [ 146.628], Avg:   133.165 (1.000) <0-05:50:43> ({'r_t':  1185.9747, 'eps':     1.0000, 'critic_loss':   614.0101, 'actor_loss':    -0.5357, 'eps_e':     1.0000})
Step:  875000, Reward:   217.375 [ 151.896], Avg:   133.261 (1.000) <0-05:51:08> ({'r_t':  1424.1889, 'eps':     1.0000, 'critic_loss':   776.5630, 'actor_loss':    -2.4037, 'eps_e':     1.0000})
Step:  876000, Reward:   202.370 [ 143.396], Avg:   133.340 (1.000) <0-05:51:33> ({'r_t':  1583.1040, 'eps':     1.0000, 'critic_loss':   795.2635, 'actor_loss':    -1.5667, 'eps_e':     1.0000})
Step:  877000, Reward:   281.407 [ 143.642], Avg:   133.509 (1.000) <0-05:51:59> ({'r_t':  1691.8348, 'eps':     1.0000, 'critic_loss':   717.2707, 'actor_loss':    -3.1944, 'eps_e':     1.0000})
Step:  878000, Reward:   286.810 [  88.012], Avg:   133.683 (1.000) <0-05:52:25> ({'r_t':  1847.7738, 'eps':     1.0000, 'critic_loss':  1235.1935, 'actor_loss':    -0.3015, 'eps_e':     1.0000})
Step:  879000, Reward:   353.011 [  81.660], Avg:   133.933 (1.000) <0-05:52:52> ({'r_t':  1980.9656, 'eps':     1.0000, 'critic_loss':   296.7929, 'actor_loss':    -1.1690, 'eps_e':     1.0000})
Step:  880000, Reward:   378.627 [  21.978], Avg:   134.210 (1.000) <0-05:53:17> ({'r_t':  2273.4598, 'eps':     1.0000, 'critic_loss':   115.8984, 'actor_loss':    -1.6489, 'eps_e':     1.0000})
Step:  881000, Reward:   338.958 [ 113.642], Avg:   134.442 (1.000) <0-05:53:43> ({'r_t':  2105.9006, 'eps':     1.0000, 'critic_loss':   970.6685, 'actor_loss':     1.4824, 'eps_e':     1.0000})
Step:  882000, Reward:   372.793 [  19.389], Avg:   134.712 (1.000) <0-05:54:09> ({'r_t':  2257.8485, 'eps':     1.0000, 'critic_loss':   132.3663, 'actor_loss':    -0.0038, 'eps_e':     1.0000})
Step:  883000, Reward:   374.582 [  48.135], Avg:   134.984 (1.000) <0-05:54:36> ({'r_t':  2290.1468, 'eps':     1.0000, 'critic_loss':    49.9264, 'actor_loss':    -0.3067, 'eps_e':     1.0000})
Step:  884000, Reward:   382.154 [  13.732], Avg:   135.263 (1.000) <0-05:55:01> ({'r_t':  2319.1676, 'eps':     1.0000, 'critic_loss':    48.5702, 'actor_loss':    -0.1831, 'eps_e':     1.0000})
Step:  885000, Reward:   335.344 [ 164.870], Avg:   135.489 (1.000) <0-05:55:33> ({'r_t':  2328.0464, 'eps':     1.0000, 'critic_loss':   115.2947, 'actor_loss':     0.7616, 'eps_e':     1.0000})
Step:  886000, Reward:   373.149 [  33.166], Avg:   135.757 (1.000) <0-05:55:59> ({'r_t':  2298.0493, 'eps':     1.0000, 'critic_loss':    47.4607, 'actor_loss':    -0.3707, 'eps_e':     1.0000})
Step:  887000, Reward:   385.025 [  13.972], Avg:   136.037 (1.000) <0-05:56:24> ({'r_t':  2415.2786, 'eps':     1.0000, 'critic_loss':    53.4605, 'actor_loss':    -0.1217, 'eps_e':     1.0000})
Step:  888000, Reward:   374.550 [  16.803], Avg:   136.306 (1.000) <0-05:56:49> ({'r_t':  2311.2302, 'eps':     1.0000, 'critic_loss':   114.3755, 'actor_loss':     0.6539, 'eps_e':     1.0000})
Step:  889000, Reward:   377.339 [  19.126], Avg:   136.577 (1.000) <0-05:57:15> ({'r_t':  2126.0648, 'eps':     1.0000, 'critic_loss':   963.3621, 'actor_loss':     1.0569, 'eps_e':     1.0000})
Step:  890000, Reward:    73.485 [  70.125], Avg:   136.506 (1.000) <0-05:57:41> ({'r_t':  1122.0502, 'eps':     1.0000, 'critic_loss':  1463.5208, 'actor_loss':    11.0281, 'eps_e':     1.0000})
Step:  891000, Reward:   157.968 [  95.621], Avg:   136.530 (1.000) <0-05:58:07> ({'r_t':   983.2045, 'eps':     1.0000, 'critic_loss':   187.2521, 'actor_loss':    -0.8543, 'eps_e':     1.0000})
Step:  892000, Reward:   215.540 [  92.445], Avg:   136.618 (1.000) <0-05:58:33> ({'r_t':  1344.9282, 'eps':     1.0000, 'critic_loss':   286.5247, 'actor_loss':    -2.9336, 'eps_e':     1.0000})
Step:  893000, Reward:   268.491 [  96.092], Avg:   136.766 (1.000) <0-05:58:58> ({'r_t':  1652.3851, 'eps':     1.0000, 'critic_loss':   297.5163, 'actor_loss':    -3.1087, 'eps_e':     1.0000})
Step:  894000, Reward:   323.973 [  58.154], Avg:   136.975 (1.000) <0-05:59:23> ({'r_t':  1885.5981, 'eps':     1.0000, 'critic_loss':  1295.6890, 'actor_loss':    -1.0717, 'eps_e':     1.0000})
Step:  895000, Reward:   349.556 [  38.554], Avg:   137.212 (1.000) <0-05:59:49> ({'r_t':  2073.6345, 'eps':     1.0000, 'critic_loss':   183.6945, 'actor_loss':    -1.6997, 'eps_e':     1.0000})
Step:  896000, Reward:   354.038 [  64.566], Avg:   137.454 (1.000) <0-06:00:14> ({'r_t':  2259.5620, 'eps':     1.0000, 'critic_loss':    84.2837, 'actor_loss':    -1.6582, 'eps_e':     1.0000})
Step:  897000, Reward:   335.985 [ 107.384], Avg:   137.675 (1.000) <0-06:00:40> ({'r_t':  2260.8548, 'eps':     1.0000, 'critic_loss':    85.7249, 'actor_loss':     0.6513, 'eps_e':     1.0000})
Step:  898000, Reward:   340.458 [  76.108], Avg:   137.901 (1.000) <0-06:01:06> ({'r_t':  2333.2528, 'eps':     1.0000, 'critic_loss':    81.8215, 'actor_loss':    -0.1044, 'eps_e':     1.0000})
Step:  899000, Reward:   338.854 [  96.046], Avg:   138.124 (1.000) <0-06:01:31> ({'r_t':  2216.8106, 'eps':     1.0000, 'critic_loss':   119.1505, 'actor_loss':     1.0170, 'eps_e':     1.0000})
Step:  900000, Reward:   371.212 [  29.405], Avg:   138.383 (1.000) <0-06:01:57> ({'r_t':  2318.5593, 'eps':     1.0000, 'critic_loss':   103.6380, 'actor_loss':     1.0239, 'eps_e':     1.0000})
Step:  901000, Reward:   370.677 [  46.512], Avg:   138.640 (1.000) <0-06:02:22> ({'r_t':  2335.4917, 'eps':     1.0000, 'critic_loss':   172.6591, 'actor_loss':     0.7041, 'eps_e':     1.0000})
Step:  902000, Reward:   325.164 [  91.604], Avg:   138.847 (1.000) <0-06:02:47> ({'r_t':  1791.0334, 'eps':     1.0000, 'critic_loss':  3508.7051, 'actor_loss':     4.6645, 'eps_e':     1.0000})
Step:  903000, Reward:   318.533 [ 135.054], Avg:   139.045 (1.000) <0-06:03:13> ({'r_t':  1683.3184, 'eps':     1.0000, 'critic_loss':  5401.2012, 'actor_loss':     2.9840, 'eps_e':     1.0000})
Step:  904000, Reward:   203.459 [ 470.522], Avg:   139.117 (1.000) <0-06:03:39> ({'r_t':  2260.5322, 'eps':     1.0000, 'critic_loss':   277.5058, 'actor_loss':    -1.4619, 'eps_e':     1.0000})
Step:  905000, Reward:   380.746 [  26.250], Avg:   139.383 (1.000) <0-06:04:05> ({'r_t':  1981.9266, 'eps':     1.0000, 'critic_loss':  1513.2157, 'actor_loss':     1.1229, 'eps_e':     1.0000})
Step:  906000, Reward:   335.304 [ 117.335], Avg:   139.599 (1.000) <0-06:04:30> ({'r_t':  1925.5881, 'eps':     1.0000, 'critic_loss':  4059.3772, 'actor_loss':     0.3373, 'eps_e':     1.0000})
Step:  907000, Reward:   366.959 [  28.495], Avg:   139.850 (1.000) <0-06:04:55> ({'r_t':  2378.0857, 'eps':     1.0000, 'critic_loss':    92.7098, 'actor_loss':    -3.4092, 'eps_e':     1.0000})
Step:  908000, Reward:   227.138 [ 378.459], Avg:   139.946 (1.000) <0-06:05:22> ({'r_t':  2275.7432, 'eps':     1.0000, 'critic_loss':  1000.2676, 'actor_loss':     1.1325, 'eps_e':     1.0000})
Step:  909000, Reward:   356.195 [  65.511], Avg:   140.183 (1.000) <0-06:05:47> ({'r_t':  2093.0075, 'eps':     1.0000, 'critic_loss':   746.8793, 'actor_loss':     1.0070, 'eps_e':     1.0000})
Step:  910000, Reward:   320.423 [ 135.099], Avg:   140.381 (1.000) <0-06:06:13> ({'r_t':  2282.8776, 'eps':     1.0000, 'critic_loss':   343.5224, 'actor_loss':    -0.7646, 'eps_e':     1.0000})
Step:  911000, Reward:   378.271 [  27.547], Avg:   140.642 (1.000) <0-06:06:38> ({'r_t':  2367.9719, 'eps':     1.0000, 'critic_loss':   110.6034, 'actor_loss':    -0.1927, 'eps_e':     1.0000})
Step:  912000, Reward:   256.197 [ 358.733], Avg:   140.769 (1.000) <0-06:07:04> ({'r_t':  2245.9523, 'eps':     1.0000, 'critic_loss':  1391.2106, 'actor_loss':     1.6027, 'eps_e':     1.0000})
Step:  913000, Reward:   365.124 [  33.673], Avg:   141.014 (1.000) <0-06:07:30> ({'r_t':  2150.2153, 'eps':     1.0000, 'critic_loss':  1307.8531, 'actor_loss':     1.6131, 'eps_e':     1.0000})
Step:  914000, Reward:   388.069 [  16.397], Avg:   141.284 (1.000) <0-06:07:55> ({'r_t':  2208.7839, 'eps':     1.0000, 'critic_loss':   730.4363, 'actor_loss':    -0.3083, 'eps_e':     1.0000})
Step:  915000, Reward:   382.388 [  21.839], Avg:   141.547 (1.000) <0-06:08:21> ({'r_t':  2228.1306, 'eps':     1.0000, 'critic_loss':  1262.7915, 'actor_loss':     0.7130, 'eps_e':     1.0000})
Step:  916000, Reward:   348.108 [ 101.145], Avg:   141.773 (1.000) <0-06:08:47> ({'r_t':  1742.0547, 'eps':     1.0000, 'critic_loss':  3790.0022, 'actor_loss':     5.0943, 'eps_e':     1.0000})
Step:  917000, Reward:   327.154 [ 120.933], Avg:   141.975 (1.000) <0-06:09:12> ({'r_t':  1902.6958, 'eps':     1.0000, 'critic_loss':  3482.4626, 'actor_loss':     2.1389, 'eps_e':     1.0000})
Step:  918000, Reward:   352.796 [  98.319], Avg:   142.204 (1.000) <0-06:09:37> ({'r_t':  2164.6151, 'eps':     1.0000, 'critic_loss':   413.9705, 'actor_loss':    -2.4899, 'eps_e':     1.0000})
Step:  919000, Reward:   392.007 [  26.904], Avg:   142.475 (1.000) <0-06:10:02> ({'r_t':  2324.9011, 'eps':     1.0000, 'critic_loss':   109.2468, 'actor_loss':    -1.2339, 'eps_e':     1.0000})
Step:  920000, Reward:   388.952 [  16.222], Avg:   142.743 (1.000) <0-06:10:28> ({'r_t':  2111.2834, 'eps':     1.0000, 'critic_loss':  2190.6467, 'actor_loss':     2.5510, 'eps_e':     1.0000})
Step:  921000, Reward:   387.754 [  12.882], Avg:   143.009 (1.000) <0-06:10:53> ({'r_t':  2382.0931, 'eps':     1.0000, 'critic_loss':   186.3484, 'actor_loss':    -2.4614, 'eps_e':     1.0000})
Step:  922000, Reward:   390.692 [  12.231], Avg:   143.277 (1.000) <0-06:11:18> ({'r_t':  2490.5896, 'eps':     1.0000, 'critic_loss':    38.6809, 'actor_loss':    -0.7791, 'eps_e':     1.0000})
Step:  923000, Reward:   381.683 [  23.353], Avg:   143.535 (1.000) <0-06:11:43> ({'r_t':  2362.2999, 'eps':     1.0000, 'critic_loss':    48.6152, 'actor_loss':    -0.2494, 'eps_e':     1.0000})
Step:  924000, Reward:   386.858 [  18.079], Avg:   143.798 (1.000) <0-06:12:09> ({'r_t':  2467.8390, 'eps':     1.0000, 'critic_loss':    64.8163, 'actor_loss':     0.4435, 'eps_e':     1.0000})
Step:  925000, Reward:   325.369 [ 153.108], Avg:   143.994 (1.000) <0-06:12:35> ({'r_t':  2327.4791, 'eps':     1.0000, 'critic_loss':   531.4547, 'actor_loss':     0.0994, 'eps_e':     1.0000})
Step:  926000, Reward:   396.816 [  12.504], Avg:   144.267 (1.000) <0-06:12:59> ({'r_t':  2325.8426, 'eps':     1.0000, 'critic_loss':   175.2446, 'actor_loss':     0.9567, 'eps_e':     1.0000})
Step:  927000, Reward:   296.294 [ 361.609], Avg:   144.431 (1.000) <0-06:13:24> ({'r_t':  2465.3511, 'eps':     1.0000, 'critic_loss':    63.8402, 'actor_loss':    -1.1517, 'eps_e':     1.0000})
Step:  928000, Reward:   190.871 [ 358.471], Avg:   144.481 (1.000) <0-06:13:52> ({'r_t':   535.5398, 'eps':     1.0000, 'critic_loss':  9001.5508, 'actor_loss':     7.9534, 'eps_e':     1.0000})
Step:  929000, Reward:   129.298 [ 361.613], Avg:   144.465 (1.000) <0-06:14:19> ({'r_t':   713.4102, 'eps':     1.0000, 'critic_loss':  9342.8047, 'actor_loss':    -0.8755, 'eps_e':     1.0000})
Step:  930000, Reward:   184.577 [ 359.800], Avg:   144.508 (1.000) <0-06:14:46> ({'r_t':   990.1865, 'eps':     1.0000, 'critic_loss':  5990.5586, 'actor_loss':    -5.0741, 'eps_e':     1.0000})
Step:  931000, Reward:   307.121 [ 116.279], Avg:   144.682 (1.000) <0-06:15:14> ({'r_t':   962.5948, 'eps':     1.0000, 'critic_loss':  6656.9312, 'actor_loss':     0.2566, 'eps_e':     1.0000})
Step:  932000, Reward:   330.128 [ 110.970], Avg:   144.881 (1.000) <0-06:15:40> ({'r_t':  1666.6847, 'eps':     1.0000, 'critic_loss':  2225.3955, 'actor_loss':    -6.9789, 'eps_e':     1.0000})
Step:  933000, Reward:   341.668 [  89.399], Avg:   145.092 (1.000) <0-06:16:07> ({'r_t':  1993.8071, 'eps':     1.0000, 'critic_loss':   701.8594, 'actor_loss':    -2.6363, 'eps_e':     1.0000})
Step:  934000, Reward:   380.348 [  27.737], Avg:   145.343 (1.000) <0-06:16:33> ({'r_t':  2120.1674, 'eps':     1.0000, 'critic_loss':   265.3350, 'actor_loss':    -0.6372, 'eps_e':     1.0000})
Step:  935000, Reward:   374.082 [  34.040], Avg:   145.588 (1.000) <0-06:17:00> ({'r_t':  2068.2952, 'eps':     1.0000, 'critic_loss':  1141.4017, 'actor_loss':     1.7266, 'eps_e':     1.0000})
Step:  936000, Reward:   358.960 [  84.025], Avg:   145.815 (1.000) <0-06:17:25> ({'r_t':  2195.0838, 'eps':     1.0000, 'critic_loss':   168.3073, 'actor_loss':    -1.3785, 'eps_e':     1.0000})
Step:  937000, Reward:   278.733 [ 184.858], Avg:   145.957 (1.000) <0-06:17:52> ({'r_t':  2097.3323, 'eps':     1.0000, 'critic_loss':  1021.9583, 'actor_loss':     1.6033, 'eps_e':     1.0000})
Step:  938000, Reward:     1.510 [ 698.750], Avg:   145.803 (1.000) <0-06:18:17> ({'r_t':  1557.9320, 'eps':     1.0000, 'critic_loss':  3715.5178, 'actor_loss':     8.2620, 'eps_e':     1.0000})
Step:  939000, Reward:   342.657 [ 110.726], Avg:   146.013 (1.000) <0-06:18:44> ({'r_t':  1812.3604, 'eps':     1.0000, 'critic_loss':  3150.8494, 'actor_loss':    -1.4753, 'eps_e':     1.0000})
Step:  940000, Reward:   344.106 [ 113.525], Avg:   146.223 (1.000) <0-06:19:09> ({'r_t':  2041.8405, 'eps':     1.0000, 'critic_loss':  1226.5675, 'actor_loss':    -1.4928, 'eps_e':     1.0000})
Step:  941000, Reward:   349.119 [  40.674], Avg:   146.438 (1.000) <0-06:19:35> ({'r_t':  2018.4259, 'eps':     1.0000, 'critic_loss':  1355.3752, 'actor_loss':     0.5163, 'eps_e':     1.0000})
Step:  942000, Reward:   288.185 [ 360.468], Avg:   146.589 (1.000) <0-06:20:00> ({'r_t':  2003.7170, 'eps':     1.0000, 'critic_loss':  1191.5524, 'actor_loss':    -0.5128, 'eps_e':     1.0000})
Step:  943000, Reward:   358.943 [  19.293], Avg:   146.814 (1.000) <0-06:20:27> ({'r_t':  2077.5847, 'eps':     1.0000, 'critic_loss':   235.5579, 'actor_loss':    -1.4189, 'eps_e':     1.0000})
Step:  944000, Reward:   252.640 [ 359.471], Avg:   146.926 (1.000) <0-06:20:54> ({'r_t':  2138.7399, 'eps':     1.0000, 'critic_loss':   153.2447, 'actor_loss':    -0.1134, 'eps_e':     1.0000})
Step:  945000, Reward:   373.139 [  26.333], Avg:   147.165 (1.000) <0-06:21:20> ({'r_t':  1977.5843, 'eps':     1.0000, 'critic_loss':  1357.2543, 'actor_loss':     1.1264, 'eps_e':     1.0000})
Step:  946000, Reward:   372.177 [  23.403], Avg:   147.402 (1.000) <0-06:21:45> ({'r_t':  2108.4274, 'eps':     1.0000, 'critic_loss':  1532.0192, 'actor_loss':     0.8694, 'eps_e':     1.0000})
Step:  947000, Reward:   372.483 [  32.024], Avg:   147.640 (1.000) <0-06:22:11> ({'r_t':  2341.0715, 'eps':     1.0000, 'critic_loss':   174.0485, 'actor_loss':    -0.7791, 'eps_e':     1.0000})
Step:  948000, Reward:   379.549 [  22.911], Avg:   147.884 (1.000) <0-06:22:37> ({'r_t':  2408.3964, 'eps':     1.0000, 'critic_loss':   118.4235, 'actor_loss':    -0.2628, 'eps_e':     1.0000})
Step:  949000, Reward:   335.348 [  70.473], Avg:   148.082 (1.000) <0-06:23:02> ({'r_t':  1892.1105, 'eps':     1.0000, 'critic_loss':  1376.7350, 'actor_loss':     2.6572, 'eps_e':     1.0000})
Step:  950000, Reward:   365.092 [  63.677], Avg:   148.310 (1.000) <0-06:23:28> ({'r_t':  2243.1689, 'eps':     1.0000, 'critic_loss':  1054.1573, 'actor_loss':    -1.0426, 'eps_e':     1.0000})
Step:  951000, Reward:   376.466 [  25.851], Avg:   148.549 (1.000) <0-06:23:53> ({'r_t':  2049.6159, 'eps':     1.0000, 'critic_loss':  1263.0559, 'actor_loss':     0.2606, 'eps_e':     1.0000})
Step:  952000, Reward:   360.521 [  35.544], Avg:   148.772 (1.000) <0-06:24:19> ({'r_t':  2088.5099, 'eps':     1.0000, 'critic_loss':  2775.2800, 'actor_loss':     3.9168, 'eps_e':     1.0000})
Step:  953000, Reward:   336.819 [  72.666], Avg:   148.969 (1.000) <0-06:24:44> ({'r_t':  1914.1077, 'eps':     1.0000, 'critic_loss':  2344.5608, 'actor_loss':    -0.4416, 'eps_e':     1.0000})
Step:  954000, Reward:   363.648 [  96.069], Avg:   149.194 (1.000) <0-06:25:09> ({'r_t':  2134.3883, 'eps':     1.0000, 'critic_loss':   953.6873, 'actor_loss':     0.2442, 'eps_e':     1.0000})
Step:  955000, Reward:   384.594 [  24.777], Avg:   149.440 (1.000) <0-06:25:35> ({'r_t':  2272.4579, 'eps':     1.0000, 'critic_loss':   182.7520, 'actor_loss':    -2.3073, 'eps_e':     1.0000})
Step:  956000, Reward:   396.269 [  21.106], Avg:   149.698 (1.000) <0-06:26:01> ({'r_t':  2321.3783, 'eps':     1.0000, 'critic_loss':   187.9774, 'actor_loss':     0.4418, 'eps_e':     1.0000})
Step:  957000, Reward:   359.313 [  85.958], Avg:   149.917 (1.000) <0-06:26:27> ({'r_t':  2301.6509, 'eps':     1.0000, 'critic_loss':   128.4202, 'actor_loss':    -0.4673, 'eps_e':     1.0000})
Step:  958000, Reward:   359.961 [  65.799], Avg:   150.136 (1.000) <0-06:26:53> ({'r_t':  2036.2811, 'eps':     1.0000, 'critic_loss':  1111.4980, 'actor_loss':     0.2343, 'eps_e':     1.0000})
Step:  959000, Reward:   394.005 [  20.680], Avg:   150.390 (1.000) <0-06:27:19> ({'r_t':  2348.0982, 'eps':     1.0000, 'critic_loss':    83.4288, 'actor_loss':    -0.6331, 'eps_e':     1.0000})
Step:  960000, Reward:   339.552 [  93.522], Avg:   150.587 (1.000) <0-06:27:44> ({'r_t':  2321.4481, 'eps':     1.0000, 'critic_loss':   872.9316, 'actor_loss':     1.2770, 'eps_e':     1.0000})
Step:  961000, Reward:   392.192 [  14.049], Avg:   150.838 (1.000) <0-06:28:10> ({'r_t':  2227.8278, 'eps':     1.0000, 'critic_loss':   140.8637, 'actor_loss':     1.6468, 'eps_e':     1.0000})
Step:  962000, Reward:   404.349 [  15.198], Avg:   151.101 (1.000) <0-06:28:35> ({'r_t':  2371.8422, 'eps':     1.0000, 'critic_loss':    32.0914, 'actor_loss':    -1.1803, 'eps_e':     1.0000})
Step:  963000, Reward:   395.777 [  20.821], Avg:   151.355 (1.000) <0-06:29:00> ({'r_t':  2292.7830, 'eps':     1.0000, 'critic_loss':   116.7462, 'actor_loss':     0.7457, 'eps_e':     1.0000})
Step:  964000, Reward:   392.814 [  17.359], Avg:   151.605 (1.000) <0-06:29:25> ({'r_t':  2328.9704, 'eps':     1.0000, 'critic_loss':  1039.0864, 'actor_loss':     1.7138, 'eps_e':     1.0000})
Step:  965000, Reward:   371.914 [ 111.995], Avg:   151.833 (1.000) <0-06:29:51> ({'r_t':  2391.2898, 'eps':     1.0000, 'critic_loss':    39.2644, 'actor_loss':     0.1659, 'eps_e':     1.0000})
Step:  966000, Reward:   397.500 [  17.999], Avg:   152.087 (1.000) <0-06:30:16> ({'r_t':  2469.6488, 'eps':     1.0000, 'critic_loss':    88.4057, 'actor_loss':    -0.1755, 'eps_e':     1.0000})
Step:  967000, Reward:   397.133 [  27.808], Avg:   152.340 (1.000) <0-06:30:42> ({'r_t':  2534.8089, 'eps':     1.0000, 'critic_loss':    22.3173, 'actor_loss':     0.0673, 'eps_e':     1.0000})
Step:  968000, Reward:   191.654 [ 499.296], Avg:   152.381 (1.000) <0-06:31:08> ({'r_t':  2240.0375, 'eps':     1.0000, 'critic_loss':  1737.3728, 'actor_loss':     2.9433, 'eps_e':     1.0000})
Step:  969000, Reward:   392.922 [  21.313], Avg:   152.629 (1.000) <0-06:31:33> ({'r_t':  2304.8263, 'eps':     1.0000, 'critic_loss':   539.7507, 'actor_loss':     1.0867, 'eps_e':     1.0000})
Step:  970000, Reward:   366.350 [ 119.501], Avg:   152.849 (1.000) <0-06:31:58> ({'r_t':  2485.7746, 'eps':     1.0000, 'critic_loss':    97.3214, 'actor_loss':    -0.8127, 'eps_e':     1.0000})
Step:  971000, Reward:   285.256 [ 123.456], Avg:   152.985 (1.000) <0-06:32:25> ({'r_t':  1985.8376, 'eps':     1.0000, 'critic_loss':  2246.1270, 'actor_loss':     4.7261, 'eps_e':     1.0000})
Step:  972000, Reward:   298.531 [ 134.431], Avg:   153.135 (1.000) <0-06:32:52> ({'r_t':  1543.1711, 'eps':     1.0000, 'critic_loss':  2391.4231, 'actor_loss':    -1.5081, 'eps_e':     1.0000})
Step:  973000, Reward:   282.021 [ 140.631], Avg:   153.267 (1.000) <0-06:33:18> ({'r_t':  1566.6931, 'eps':     1.0000, 'critic_loss':  1437.5009, 'actor_loss':    -1.1328, 'eps_e':     1.0000})
Step:  974000, Reward:   350.251 [  34.816], Avg:   153.469 (1.000) <0-06:33:45> ({'r_t':  1771.5378, 'eps':     1.0000, 'critic_loss':  2432.4055, 'actor_loss':     0.4015, 'eps_e':     1.0000})
Step:  975000, Reward:   350.197 [  80.353], Avg:   153.671 (1.000) <0-06:34:10> ({'r_t':  1922.2290, 'eps':     1.0000, 'critic_loss':   296.4065, 'actor_loss':    -1.6566, 'eps_e':     1.0000})
Step:  976000, Reward:   356.264 [  43.317], Avg:   153.878 (1.000) <0-06:34:36> ({'r_t':  2048.6188, 'eps':     1.0000, 'critic_loss':    62.2383, 'actor_loss':    -1.1369, 'eps_e':     1.0000})
Step:  977000, Reward:   351.351 [  34.513], Avg:   154.080 (1.000) <0-06:35:03> ({'r_t':  1973.1393, 'eps':     1.0000, 'critic_loss':  2354.8577, 'actor_loss':     1.3798, 'eps_e':     1.0000})
Step:  978000, Reward:   232.717 [ 364.219], Avg:   154.160 (1.000) <0-06:35:29> ({'r_t':  1724.9444, 'eps':     1.0000, 'critic_loss':  2361.2996, 'actor_loss':     0.0506, 'eps_e':     1.0000})
Step:  979000, Reward:   181.250 [ 372.384], Avg:   154.188 (1.000) <0-06:35:57> ({'r_t':  1307.0884, 'eps':     1.0000, 'critic_loss':  6101.7056, 'actor_loss':     2.7299, 'eps_e':     1.0000})
Step:  980000, Reward:   270.966 [ 128.250], Avg:   154.307 (1.000) <0-06:36:25> ({'r_t':  1284.4782, 'eps':     1.0000, 'critic_loss':  3572.2866, 'actor_loss':     0.7452, 'eps_e':     1.0000})
Step:  981000, Reward:   344.517 [  39.885], Avg:   154.501 (1.000) <0-06:36:51> ({'r_t':  1732.6309, 'eps':     1.0000, 'critic_loss':  1231.3342, 'actor_loss':    -1.9193, 'eps_e':     1.0000})
Step:  982000, Reward:   339.989 [  89.889], Avg:   154.689 (1.000) <0-06:37:18> ({'r_t':  1986.3826, 'eps':     1.0000, 'critic_loss':   131.3121, 'actor_loss':    -1.4740, 'eps_e':     1.0000})
Step:  983000, Reward:   359.625 [  33.026], Avg:   154.898 (1.000) <0-06:37:44> ({'r_t':  2108.1596, 'eps':     1.0000, 'critic_loss':   142.7681, 'actor_loss':     0.1423, 'eps_e':     1.0000})
Step:  984000, Reward:   372.173 [  32.180], Avg:   155.118 (1.000) <0-06:38:11> ({'r_t':  2131.6126, 'eps':     1.0000, 'critic_loss':   810.8018, 'actor_loss':     1.2107, 'eps_e':     1.0000})
Step:  985000, Reward:   381.953 [  15.740], Avg:   155.348 (1.000) <0-06:38:37> ({'r_t':  2242.9916, 'eps':     1.0000, 'critic_loss':    84.2084, 'actor_loss':    -0.8138, 'eps_e':     1.0000})
Step:  986000, Reward:   373.310 [  25.766], Avg:   155.569 (1.000) <0-06:39:03> ({'r_t':  2050.0292, 'eps':     1.0000, 'critic_loss':  1335.8396, 'actor_loss':     2.6402, 'eps_e':     1.0000})
Step:  987000, Reward:   347.458 [  65.902], Avg:   155.763 (1.000) <0-06:39:30> ({'r_t':  2126.9921, 'eps':     1.0000, 'critic_loss':   213.5257, 'actor_loss':     0.3366, 'eps_e':     1.0000})
Step:  988000, Reward:   378.796 [  25.595], Avg:   155.989 (1.000) <0-06:39:56> ({'r_t':  2180.2935, 'eps':     1.0000, 'critic_loss':   124.1585, 'actor_loss':    -0.9635, 'eps_e':     1.0000})
Step:  989000, Reward:   280.844 [ 362.782], Avg:   156.115 (1.000) <0-06:40:22> ({'r_t':  2245.6533, 'eps':     1.0000, 'critic_loss':    85.8096, 'actor_loss':    -0.4846, 'eps_e':     1.0000})
Step:  990000, Reward:   286.387 [ 186.884], Avg:   156.246 (1.000) <0-06:40:48> ({'r_t':  1941.8794, 'eps':     1.0000, 'critic_loss':  2191.4177, 'actor_loss':     2.9750, 'eps_e':     1.0000})
Step:  991000, Reward:   239.090 [ 375.687], Avg:   156.330 (1.000) <0-06:41:14> ({'r_t':  2118.3847, 'eps':     1.0000, 'critic_loss':   430.1401, 'actor_loss':     1.3371, 'eps_e':     1.0000})
Step:  992000, Reward:   339.705 [ 121.129], Avg:   156.515 (1.000) <0-06:41:40> ({'r_t':  2180.5290, 'eps':     1.0000, 'critic_loss':   484.6385, 'actor_loss':    -0.8439, 'eps_e':     1.0000})
Step:  993000, Reward:   387.502 [  30.767], Avg:   156.747 (1.000) <0-06:42:06> ({'r_t':  2180.6205, 'eps':     1.0000, 'critic_loss':   258.5604, 'actor_loss':    -0.2424, 'eps_e':     1.0000})
Step:  994000, Reward:   383.435 [  25.003], Avg:   156.975 (1.000) <0-06:42:32> ({'r_t':  2131.1366, 'eps':     1.0000, 'critic_loss':  1317.8813, 'actor_loss':    -0.1314, 'eps_e':     1.0000})
Step:  995000, Reward:   364.603 [ 112.110], Avg:   157.183 (1.000) <0-06:42:58> ({'r_t':  2336.1566, 'eps':     1.0000, 'critic_loss':  1857.4314, 'actor_loss':    -0.0518, 'eps_e':     1.0000})
Step:  996000, Reward:   381.870 [  37.391], Avg:   157.409 (1.000) <0-06:43:24> ({'r_t':  2193.8509, 'eps':     1.0000, 'critic_loss':   807.7968, 'actor_loss':     2.6074, 'eps_e':     1.0000})
Step:  997000, Reward:   384.447 [  66.241], Avg:   157.636 (1.000) <0-06:43:48> ({'r_t':  2342.5230, 'eps':     1.0000, 'critic_loss':   214.1218, 'actor_loss':    -2.8742, 'eps_e':     1.0000})
Step:  998000, Reward:   369.348 [ 112.173], Avg:   157.848 (1.000) <0-06:44:15> ({'r_t':  2391.1093, 'eps':     1.0000, 'critic_loss':   158.7210, 'actor_loss':     0.1596, 'eps_e':     1.0000})
Step:  999000, Reward:   397.124 [  14.684], Avg:   158.087 (1.000) <0-06:44:40> ({'r_t':  2253.8967, 'eps':     1.0000, 'critic_loss':  1269.1033, 'actor_loss':     2.3182, 'eps_e':     1.0000})
Step: 1000000, Reward:    60.616 [ 124.644], Avg:   157.990 (1.000) <0-06:45:06> ({'r_t':  1735.5800, 'eps':     1.0000, 'critic_loss':  2101.7703, 'actor_loss':     4.9138, 'eps_e':     1.0000})
