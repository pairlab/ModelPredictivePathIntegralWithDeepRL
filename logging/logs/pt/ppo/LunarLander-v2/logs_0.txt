Model: <class 'src.models.pytorch.agents.ppo.PPOAgent'>, Env: LunarLander-v2, Date: 07/06/2020 01:21:48
CPU: 20 Core, 0.0GHz, 377.59 GB, Linux-4.14.175-llgrid-10ms-x86_64-with-debian-buster-sid
GPU 0: Tesla V100-PCIE-32GB, 32.51 GB (Driver: 440.33.01)
Git URL: https://github.com/shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: df05964fa4262840095e5c93d6ca54a9f32dc498
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 1000000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   BATCH_SIZE = 32
   PPO_EPOCHS = 2
   ENTROPY_WEIGHT = 0.01
   CLIP_PARAM = 0.05
   dynamics_size = 8
   state_size = (8,)
   action_size = [4]
   env_name = LunarLander-v2
   rank = 0
   size = 17
   split = 17
   model = ppo
   framework = pt
   train_prop = 1.0
   tcp_ports = []
   tcp_rank = 0
   num_envs = 1
   nsteps = 500000
   render = False
   trial = False
   icm = False
   rs = False,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f775c5eeef0> 
	env = <GymEnv<TimeLimit<LunarLander<LunarLander-v2>>>> 
		env = <TimeLimit<LunarLander<LunarLander-v2>>> 
			env = <LunarLander<LunarLander-v2>> 
				np_random = RandomState(MT19937)
				viewer = None
				world = b2World(autoClearForces=True,
				        bodies=[b2Body(active=True,
				                      angle=0.0,
				                      angularDamping=0.0,
				                      angularVelocity=0.0,
				                      awake=True,
				                      bullet=False,
				                      contacts=[],
				                      fixedRotation=False,...  )],
				        bodyCount=4,
				        contactCount=0,
				        contactFilter=None,
				        contactListener=ContactDetector(),
				        contactManager=b2ContactManager(allocator=<Swig Object of type 'b2BlockAllocator *' at 0x7f775c6d4930>,
				                                        broadPhase=proxyCount=14,),
				                                        contactCount=0,
				                                        contactFilter=b2ContactFilter(),
				                                        contactList=None,
				                                        contactListener=b2ContactListener(),
				                                        ),
				        contacts=[],
				        continuousPhysics=True,
				        destructionListener=None,
				        gravity=b2Vec2(0,-10),
				        jointCount=2,
				        joints=[b2RevoluteJoint(active=True,
				                               anchorA=b2Vec2(9.96395,13.2933),
				                               anchorB=b2Vec2(9.96395,13.2933),
				                               angle=0.543124258518219,
				                               bodyA=b2Body(active=True,...  )],
				        locked=False,
				        proxyCount=14,
				        renderer=None,
				        subStepping=False,
				        warmStarting=True,
				        )
				moon = b2Body(active=True,
				       angle=0.0,
				       angularDamping=0.0,
				       angularVelocity=0.0,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.0,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.0,
				                                      awake=True,...  )],
				       inertia=0.0,
				       joints=[],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(0,0),
				       localCenter=b2Vec2(0,0),
				       mass=0.0,
				       massData=I=0.0,center=b2Vec2(0,0),mass=0.0,),
				       position=b2Vec2(0,0),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4e70> >,angle=0.0,position=b2Vec2(0,0),),
				       type=0,
				       userData=None,
				       worldCenter=b2Vec2(0,0),
				       )
				lander = b2Body(active=True,
				       angle=0.004183698445558548,
				       angularDamping=0.0,
				       angularVelocity=0.2067674845457077,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.004183698445558548,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.2067674845457077,...  )],
				       inertia=0.8333148956298828,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.96395,13.2933),
				                                                anchorB=b2Vec2(9.96395,13.2933),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.82565,-2.30901),
				       localCenter=b2Vec2(0,0.101307),
				       mass=4.816666603088379,
				       massData=I=0.8333148956298828,center=b2Vec2(0,0.101307),mass=4.816666603088379,),
				       position=b2Vec2(9.96395,13.2933),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4e40> >,angle=0.004183698911219835,position=b2Vec2(9.96395,13.2933),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.96353,13.3946),
				       )
				particles = []
				prev_reward = None
				observation_space = Box(8,) 
					dtype = float32
					shape = (8,)
					low = [-inf -inf -inf -inf -inf -inf -inf -inf]
					high = [ inf  inf  inf  inf  inf  inf  inf  inf]
					bounded_below = [False False False False False False False False]
					bounded_above = [False False False False False False False False]
					np_random = RandomState(MT19937)
				action_space = Discrete(4) 
					n = 4
					shape = ()
					dtype = int64
					np_random = RandomState(MT19937)
				game_over = False
				prev_shaping = -188.57818564718625
				helipad_x1 = 8.0
				helipad_x2 = 12.0
				helipad_y = 3.3333333333333335
				sky_polys = [[(0.0, 3.4757772968715956), (2.0, 2.1618221689673724), (2.0, 13.333333333333334), (0.0, 13.333333333333334)], [(2.0, 2.1618221689673724), (4.0, 1.73283145799239), (4.0, 13.333333333333334), (2.0, 13.333333333333334)], [(4.0, 1.73283145799239), (6.0, 2.8079998077724455), (6.0, 13.333333333333334), (4.0, 13.333333333333334)], [(6.0, 2.8079998077724455), (8.0, 3.3000000000000003), (8.0, 13.333333333333334), (6.0, 13.333333333333334)], [(8.0, 3.3000000000000003), (10.0, 3.3000000000000003), (10.0, 13.333333333333334), (8.0, 13.333333333333334)], [(10.0, 3.3000000000000003), (12.0, 3.3000000000000003), (12.0, 13.333333333333334), (10.0, 13.333333333333334)], [(12.0, 3.3000000000000003), (14.0, 3.6126090522894327), (14.0, 13.333333333333334), (12.0, 13.333333333333334)], [(14.0, 3.6126090522894327), (16.0, 3.49404249821974), (16.0, 13.333333333333334), (14.0, 13.333333333333334)], [(16.0, 3.49404249821974), (18.0, 2.610226304009373), (18.0, 13.333333333333334), (16.0, 13.333333333333334)], [(18.0, 2.610226304009373), (20.0, 3.119572187396609), (20.0, 13.333333333333334), (18.0, 13.333333333333334)]]
				legs = [b2Body(active=True,
				       angle=0.49730798602104187,
				       angularDamping=0.0,
				       angularVelocity=0.2067783772945404,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.49730798602104187,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.2067783772945404,
				                                      awake=True,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.96395,13.2933),
				                                                anchorB=b2Vec2(9.96395,13.2933),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.6739,-2.17754),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(10.8361,13.084),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4c30> >,angle=0.49730798602104187,position=b2Vec2(10.8361,13.084),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(10.8361,13.084),
				       ), b2Body(active=True,
				       angle=-0.4867312014102936,
				       angularDamping=0.0,
				       angularVelocity=0.20677247643470764,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=-0.4867312014102936,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.20677247643470764,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.96395,13.2933),
				                                                anchorB=b2Vec2(9.96395,13.2933),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.6739,-2.44049),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(9.09406,13.0748),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4ea0> >,angle=-0.48673123121261597,position=b2Vec2(9.09406,13.0748),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.09406,13.0748),
				       )]
				drawlist = [b2Body(active=True,
				       angle=0.004183698445558548,
				       angularDamping=0.0,
				       angularVelocity=0.2067674845457077,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.004183698445558548,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.2067674845457077,...  )],
				       inertia=0.8333148956298828,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.96395,13.2933),
				                                                anchorB=b2Vec2(9.96395,13.2933),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.82565,-2.30901),
				       localCenter=b2Vec2(0,0.101307),
				       mass=4.816666603088379,
				       massData=I=0.8333148956298828,center=b2Vec2(0,0.101307),mass=4.816666603088379,),
				       position=b2Vec2(9.96395,13.2933),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4e70> >,angle=0.004183698911219835,position=b2Vec2(9.96395,13.2933),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.96353,13.3946),
				       ), b2Body(active=True,
				       angle=0.49730798602104187,
				       angularDamping=0.0,
				       angularVelocity=0.2067783772945404,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=0.49730798602104187,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.2067783772945404,
				                                      awake=True,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.96395,13.2933),
				                                                anchorB=b2Vec2(9.96395,13.2933),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.6739,-2.17754),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(10.8361,13.084),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4a80> >,angle=0.49730798602104187,position=b2Vec2(10.8361,13.084),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(10.8361,13.084),
				       ), b2Body(active=True,
				       angle=-0.4867312014102936,
				       angularDamping=0.0,
				       angularVelocity=0.20677247643470764,
				       awake=True,
				       bullet=False,
				       contacts=[],
				       fixedRotation=False,
				       fixtures=[b2Fixture(body=b2Body(active=True,
				                                      angle=-0.4867312014102936,
				                                      angularDamping=0.0,
				                                      angularVelocity=0.20677247643470764,...  )],
				       inertia=0.0017909470479935408,
				       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,
				                                                anchorA=b2Vec2(9.96395,13.2933),
				                                                anchorB=b2Vec2(9.96395,13.2933),...  )],
				       linearDamping=0.0,
				       linearVelocity=b2Vec2(-1.6739,-2.44049),
				       localCenter=b2Vec2(0,0),
				       mass=0.07111112028360367,
				       massData=I=0.0017909470479935408,center=b2Vec2(0,0),mass=0.07111112028360367,),
				       position=b2Vec2(9.09406,13.0748),
				       sleepingAllowed=True,
				       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x7f775c6d4f90> >,angle=-0.48673123121261597,position=b2Vec2(9.09406,13.0748),),
				       type=2,
				       userData=None,
				       worldCenter=b2Vec2(9.09406,13.0748),
				       )]
				spec = EnvSpec(LunarLander-v2) 
					id = LunarLander-v2
					entry_point = gym.envs.box2d:LunarLander
					reward_threshold = 200
					nondeterministic = False
					max_episode_steps = 1000
				verbose = 0
			action_space = Discrete(4) 
				n = 4
				shape = ()
				dtype = int64
				np_random = RandomState(MT19937)
			observation_space = Box(8,) 
				dtype = float32
				shape = (8,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False]
				bounded_above = [False False False False False False False False]
				np_random = RandomState(MT19937)
			reward_range = (-inf, inf)
			metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}
		action_space = Discrete(4) 
			n = 4
			shape = ()
			dtype = int64
			np_random = RandomState(MT19937)
		observation_space = Box(8,) 
			dtype = float32
			shape = (8,)
			low = [-inf -inf -inf -inf -inf -inf -inf -inf]
			high = [ inf  inf  inf  inf  inf  inf  inf  inf]
			bounded_below = [False False False False False False False False]
			bounded_above = [False False False False False False False False]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f775c63be80> 
			observation_space = Box(8,) 
				dtype = float32
				shape = (8,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False]
				bounded_above = [False False False False False False False False]
				np_random = RandomState(MT19937)
	state_size = (8,)
	action_size = [4]
	action_space = Discrete(4) 
		n = 4
		shape = ()
		dtype = int64
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.MPIConnection object at 0x7f775c66cfd0> 
		root = 0
		rank = 0
		cluster = <list len=16>
	num_envs = 16
	max_steps = 1000,
agent: <src.models.wrappers.ParallelAgent object at 0x7f775c66cf28> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f775c66c908> 
		state_size = (8,)
	agent = <src.models.pytorch.agents.ppo.PPOAgent object at 0x7f775c66c940> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f775c66c860> 
			size = [4]
			dt = 0.2
			action = [-0.041  1.000 -0.438 -0.483]
			daction_dt = [-1.575  1.180 -0.719  1.036]
		discrete = True
		action_size = [4]
		state_size = (8,)
		config = <src.utils.config.Config object at 0x7f775ca16a58> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.998
			NUM_STEPS = 500
			MAX_BUFFER_SIZE = 1000000
			REPLAY_BATCH_SIZE = 32
			TARGET_UPDATE_RATE = 0.0004
			BATCH_SIZE = 32
			PPO_EPOCHS = 2
			ENTROPY_WEIGHT = 0.01
			CLIP_PARAM = 0.05
			dynamics_size = 8
			state_size = (8,)
			action_size = [4]
			env_name = LunarLander-v2
			rank = 0
			size = 17
			split = 17
			model = ppo
			framework = pt
			train_prop = 1.0
			tcp_ports = []
			tcp_rank = 0
			num_envs = 1
			nsteps = 500000
			render = False
			trial = False
			icm = False
			rs = False
		stats = <src.utils.logger.Stats object at 0x7f775c66c898> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = PPONetwork(
			  (actor_local): PPOActor(
			    (layer1): Linear(in_features=8, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=4, bias=True)
			  )
			  (actor_target): PPOActor(
			    (layer1): Linear(in_features=8, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=4, bias=True)
			  )
			  (critic_local): PPOCritic(
			    (layer1): Linear(in_features=8, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=1024, bias=True)
			    (layer3): Linear(in_features=1024, out_features=1024, bias=True)
			    (value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			  (critic_target): PPOCritic(
			    (layer1): Linear(in_features=8, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=1024, bias=True)
			    (layer3): Linear(in_features=1024, out_features=1024, bias=True)
			    (value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			) 
			training = True
			tau = 0.0004
			name = ppo
			stats = <src.utils.logger.Stats object at 0x7f775c66c7f0> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7f775ca16a58> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.998
				NUM_STEPS = 500
				MAX_BUFFER_SIZE = 1000000
				REPLAY_BATCH_SIZE = 32
				TARGET_UPDATE_RATE = 0.0004
				BATCH_SIZE = 32
				PPO_EPOCHS = 2
				ENTROPY_WEIGHT = 0.01
				CLIP_PARAM = 0.05
				dynamics_size = 8
				state_size = (8,)
				action_size = [4]
				env_name = LunarLander-v2
				rank = 0
				size = 17
				split = 17
				model = ppo
				framework = pt
				train_prop = 1.0
				tcp_ports = []
				tcp_rank = 0
				num_envs = 1
				nsteps = 500000
				render = False
				trial = False
				icm = False
				rs = False
			device = cuda
			src = ['class PPOActor(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config, use_discrete=False):\n\t\tsuper().__init__()\n\t\tinput_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN\n\t\tself.discrete = use_discrete and type(action_size) != tuple\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, actor_hidden)\n\t\tself.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)\n\t\tself.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\t\tself.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)\n\t\t\n\tdef forward(self, state, action_in=None, sample=True):\n\t\tstate = self.layer1(state).relu()\n\t\tstate = self.layer2(state).relu()\n\t\tstate = self.layer3(state).relu()\n\t\taction_mu = self.action_mu(state)\n\t\taction_sig = self.action_sig.exp().expand_as(action_mu)\n\t\tdist = self.dist(action_mu, action_sig)\n\t\taction = dist.sample() if action_in is None else action_in.argmax(-1) if self.discrete else action_in\n\t\taction_out = one_hot_from_indices(action, action_mu.size(-1)) if self.discrete else action\n\t\tlog_prob = dist.log_prob(action)\n\t\tentropy = dist.entropy()\n\t\treturn action_out, log_prob, entropy\n', 'class PPOCritic(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config):\n\t\tsuper().__init__()\n\t\tinput_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, critic_hidden)\n\t\tself.layer3 = torch.nn.Linear(critic_hidden, critic_hidden)\n\t\tself.value = torch.nn.Linear(critic_hidden, 1)\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\n\tdef forward(self, state):\n\t\tstate = self.layer1(state).relu()\n\t\tstate = self.layer2(state).relu()\n\t\tstate = self.layer3(state).relu()\n\t\tvalue = self.value(state)\n\t\treturn value\n']
			actor_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			critic_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f775c6de2b0> 
			buffer = deque([], maxlen=1000000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f775c6de240> 
		size = [4]
		dt = 0.2
		action = [-1.000  0.119  0.015  0.835]
		daction_dt = [-1.165  2.408  0.412 -1.980]
	discrete = True
	action_size = [4]
	state_size = (8,)
	config = <src.utils.config.Config object at 0x7f775ca16a58> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 1000000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		BATCH_SIZE = 32
		PPO_EPOCHS = 2
		ENTROPY_WEIGHT = 0.01
		CLIP_PARAM = 0.05
		dynamics_size = 8
		state_size = (8,)
		action_size = [4]
		env_name = LunarLander-v2
		rank = 0
		size = 17
		split = 17
		model = ppo
		framework = pt
		train_prop = 1.0
		tcp_ports = []
		tcp_rank = 0
		num_envs = 1
		nsteps = 500000
		render = False
		trial = False
		icm = False
		rs = False
	stats = <src.utils.logger.Stats object at 0x7f775c6de1d0> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import torch
import numpy as np
from .base import PTACNetwork, PTAgent, Conv, one_hot_from_indices
from src.utils.rand import ReplayBuffer, PrioritizedReplayBuffer

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size, config, use_discrete=False):
		super().__init__()
		input_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN
		self.discrete = use_discrete and type(action_size) != tuple
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, actor_hidden)
		self.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)
		self.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)
		
	def forward(self, state, action_in=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = self.dist(action_mu, action_sig)
		action = dist.sample() if action_in is None else action_in.argmax(-1) if self.discrete else action_in
		action_out = one_hot_from_indices(action, action_mu.size(-1)) if self.discrete else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_out, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		input_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, critic_hidden)
		self.layer3 = torch.nn.Linear(critic_hidden, critic_hidden)
		self.value = torch.nn.Linear(critic_hidden, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, config, actor=PPOActor, critic=PPOCritic, gpu=True, load=None, name="ppo"):
		super().__init__(state_size, action_size, config, actor=actor, critic=critic, gpu=gpu, load=load, name=name)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			action_or_entropy = action if action_in is None else entropy.mean()
			return (x.cpu().numpy() if numpy else x for x in [action_or_entropy, log_prob])

	def get_value(self, state, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device)).cpu().numpy() if numpy else self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, config):
		values = self.get_value(states, grad=True)
		critic_loss = (values - targets).pow(2).mean()
		self.step(self.critic_optimizer, critic_loss)

		entropy, new_log_probs = self.get_action_probs(states, actions, grad=True)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-config.CLIP_PARAM, 1.0+config.CLIP_PARAM)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + config.ENTROPY_WEIGHT*entropy).mean()
		self.step(self.actor_optimizer, actor_loss)
		self.stats.mean(critic_loss=critic_loss, actor_loss=actor_loss)

class PPOAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, PPONetwork, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		self.action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.config.NUM_STEPS:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			states = torch.cat([states, self.to_tensor(next_state).unsqueeze(0)], dim=0)
			values = self.network.get_value(states)
			targets, advantages = self.compute_gae(values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), values[:-1])
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states[:-1], actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*self.config.PPO_EPOCHS)//self.config.BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(self.config.BATCH_SIZE, torch.stack)[0]
				self.network.optimize(state, action, log_prob, target, advantage, config=self.config)
				

Step:       0, Reward:  -195.100 [ 141.065], Avg:  -195.100 (1.000) <0-00:00:00> ({'r_t':    -0.0020, 'eps':     1.0000, 'eps_e':     1.0000})
Step:    1000, Reward:  -177.403 [  83.185], Avg:  -186.252 (1.000) <0-00:00:08> ({'r_t': -1989.9315, 'eps':     1.0000, 'critic_loss':   523.1486, 'actor_loss':     9.3985, 'eps_e':     1.0000})
Step:    2000, Reward:  -124.359 [  65.727], Avg:  -165.621 (1.000) <0-00:00:14> ({'r_t': -1499.5580, 'eps':     1.0000, 'critic_loss':   330.3570, 'actor_loss':    -0.4218, 'eps_e':     1.0000})
Step:    3000, Reward:  -109.223 [  72.421], Avg:  -151.521 (1.000) <0-00:00:21> ({'r_t': -1057.5810, 'eps':     1.0000, 'critic_loss':   221.9985, 'actor_loss':    -1.9900, 'eps_e':     1.0000})
Step:    4000, Reward:   -70.760 [  59.555], Avg:  -135.369 (1.000) <0-00:00:31> ({'r_t':  -792.8984, 'eps':     1.0000, 'critic_loss':   194.1455, 'actor_loss':    -1.6620, 'eps_e':     1.0000})
Step:    5000, Reward:   -96.434 [  67.948], Avg:  -128.880 (1.000) <0-00:00:40> ({'r_t':  -547.9668, 'eps':     1.0000, 'critic_loss':   174.6747, 'actor_loss':    -1.1751, 'eps_e':     1.0000})
Step:    6000, Reward:   -87.120 [  64.219], Avg:  -122.914 (1.000) <0-00:00:54> ({'r_t':  -246.7808, 'eps':     1.0000, 'critic_loss':   160.9593, 'actor_loss':    -2.7127, 'eps_e':     1.0000})
Step:    7000, Reward:   -61.137 [  85.809], Avg:  -115.192 (1.000) <0-00:01:11> ({'r_t':  -175.2388, 'eps':     1.0000, 'critic_loss':   119.5908, 'actor_loss':    -3.3108, 'eps_e':     1.0000})
Step:    8000, Reward:   -25.838 [  56.781], Avg:  -105.264 (1.000) <0-00:01:29> ({'r_t':  -174.7762, 'eps':     1.0000, 'critic_loss':   147.9271, 'actor_loss':    -2.8847, 'eps_e':     1.0000})
Step:    9000, Reward:     6.041 [  29.903], Avg:   -94.133 (1.000) <0-00:01:42> ({'r_t':  -129.7181, 'eps':     1.0000, 'critic_loss':   137.9465, 'actor_loss':    -1.1882, 'eps_e':     1.0000})
Step:   10000, Reward:   -18.239 [  75.733], Avg:   -87.234 (1.000) <0-00:02:00> ({'r_t':   -43.1750, 'eps':     1.0000, 'critic_loss':   130.9380, 'actor_loss':    -1.7551, 'eps_e':     1.0000})
Step:   11000, Reward:    -0.112 [  60.772], Avg:   -79.974 (1.000) <0-00:02:17> ({'r_t':    -4.3868, 'eps':     1.0000, 'critic_loss':    88.3150, 'actor_loss':    -1.3193, 'eps_e':     1.0000})
Step:   12000, Reward:    -3.056 [  85.079], Avg:   -74.057 (1.000) <0-00:02:31> ({'r_t':    33.4724, 'eps':     1.0000, 'critic_loss':    53.4468, 'actor_loss':    -1.1517, 'eps_e':     1.0000})
Step:   13000, Reward:    14.910 [  47.642], Avg:   -67.702 (1.000) <0-00:02:46> ({'r_t':    19.9991, 'eps':     1.0000, 'critic_loss':    67.7610, 'actor_loss':    -1.0039, 'eps_e':     1.0000})
Step:   14000, Reward:    54.695 [  29.329], Avg:   -59.542 (1.000) <0-00:03:02> ({'r_t':     4.4725, 'eps':     1.0000, 'critic_loss':    78.8640, 'actor_loss':    -0.8099, 'eps_e':     1.0000})
Step:   15000, Reward:    29.756 [  63.838], Avg:   -53.961 (1.000) <0-00:03:18> ({'r_t':    79.4954, 'eps':     1.0000, 'critic_loss':    73.6407, 'actor_loss':    -1.3089, 'eps_e':     1.0000})
Step:   16000, Reward:    22.371 [  64.343], Avg:   -49.471 (1.000) <0-00:03:34> ({'r_t':    45.7546, 'eps':     1.0000, 'critic_loss':    43.1232, 'actor_loss':    -1.2064, 'eps_e':     1.0000})
Step:   17000, Reward:    52.292 [  48.990], Avg:   -43.818 (1.000) <0-00:03:50> ({'r_t':    62.4548, 'eps':     1.0000, 'critic_loss':    49.8119, 'actor_loss':    -0.5645, 'eps_e':     1.0000})
Step:   18000, Reward:    57.346 [  36.989], Avg:   -38.493 (1.000) <0-00:04:06> ({'r_t':    57.6424, 'eps':     1.0000, 'critic_loss':    42.4483, 'actor_loss':    -1.1946, 'eps_e':     1.0000})
Step:   19000, Reward:    89.005 [  33.956], Avg:   -32.118 (1.000) <0-00:04:23> ({'r_t':    34.7588, 'eps':     1.0000, 'critic_loss':    38.7855, 'actor_loss':    -0.4956, 'eps_e':     1.0000})
Step:   20000, Reward:    93.199 [  39.078], Avg:   -26.151 (1.000) <0-00:04:38> ({'r_t':   100.2513, 'eps':     1.0000, 'critic_loss':    34.0108, 'actor_loss':    -0.1087, 'eps_e':     1.0000})
Step:   21000, Reward:    46.139 [  76.349], Avg:   -22.865 (1.000) <0-00:04:54> ({'r_t':   104.2517, 'eps':     1.0000, 'critic_loss':    26.1231, 'actor_loss':    -0.3641, 'eps_e':     1.0000})
Step:   22000, Reward:    77.479 [  75.079], Avg:   -18.502 (1.000) <0-00:05:09> ({'r_t':    70.6166, 'eps':     1.0000, 'critic_loss':    31.1052, 'actor_loss':    -0.1031, 'eps_e':     1.0000})
Step:   23000, Reward:    68.981 [  63.547], Avg:   -14.857 (1.000) <0-00:05:25> ({'r_t':   101.0588, 'eps':     1.0000, 'critic_loss':    26.1903, 'actor_loss':    -0.3278, 'eps_e':     1.0000})
Step:   24000, Reward:    45.574 [ 106.461], Avg:   -12.440 (1.000) <0-00:05:40> ({'r_t':   101.7253, 'eps':     1.0000, 'critic_loss':    31.8929, 'actor_loss':    -0.0578, 'eps_e':     1.0000})
Step:   25000, Reward:    78.787 [  76.020], Avg:    -8.931 (1.000) <0-00:05:55> ({'r_t':    89.1566, 'eps':     1.0000, 'critic_loss':    28.0989, 'actor_loss':     0.3578, 'eps_e':     1.0000})
Step:   26000, Reward:    99.740 [  45.093], Avg:    -4.906 (1.000) <0-00:06:10> ({'r_t':   104.4932, 'eps':     1.0000, 'critic_loss':    12.1965, 'actor_loss':    -0.1101, 'eps_e':     1.0000})
Step:   27000, Reward:    82.905 [  90.983], Avg:    -1.770 (1.000) <0-00:06:25> ({'r_t':   101.7339, 'eps':     1.0000, 'critic_loss':    21.5663, 'actor_loss':     0.2951, 'eps_e':     1.0000})
Step:   28000, Reward:   117.835 [  24.050], Avg:     2.354 (1.000) <0-00:06:40> ({'r_t':   113.6660, 'eps':     1.0000, 'critic_loss':    11.6728, 'actor_loss':    -0.3730, 'eps_e':     1.0000})
Step:   29000, Reward:    90.004 [  69.709], Avg:     5.276 (1.000) <0-00:06:53> ({'r_t':    78.9692, 'eps':     1.0000, 'critic_loss':    22.3424, 'actor_loss':     0.6036, 'eps_e':     1.0000})
Step:   30000, Reward:   105.374 [  62.688], Avg:     8.505 (1.000) <0-00:07:08> ({'r_t':   117.5966, 'eps':     1.0000, 'critic_loss':    17.4044, 'actor_loss':     0.2445, 'eps_e':     1.0000})
Step:   31000, Reward:   109.841 [  69.666], Avg:    11.672 (1.000) <0-00:07:23> ({'r_t':   108.9930, 'eps':     1.0000, 'critic_loss':    16.5439, 'actor_loss':     0.3346, 'eps_e':     1.0000})
Step:   32000, Reward:   100.744 [  80.029], Avg:    14.371 (1.000) <0-00:07:38> ({'r_t':   128.0238, 'eps':     1.0000, 'critic_loss':     9.7420, 'actor_loss':    -0.5414, 'eps_e':     1.0000})
Step:   33000, Reward:   111.518 [  44.363], Avg:    17.228 (1.000) <0-00:07:54> ({'r_t':   117.8728, 'eps':     1.0000, 'critic_loss':     9.3193, 'actor_loss':    -0.2035, 'eps_e':     1.0000})
Step:   34000, Reward:   119.873 [  39.911], Avg:    20.161 (1.000) <0-00:08:09> ({'r_t':   107.7406, 'eps':     1.0000, 'critic_loss':     7.8779, 'actor_loss':    -0.0429, 'eps_e':     1.0000})
Step:   35000, Reward:   129.651 [  31.885], Avg:    23.202 (1.000) <0-00:08:24> ({'r_t':   129.1064, 'eps':     1.0000, 'critic_loss':     7.7535, 'actor_loss':     0.1211, 'eps_e':     1.0000})
Step:   36000, Reward:   123.947 [  66.009], Avg:    25.925 (1.000) <0-00:08:38> ({'r_t':   111.9975, 'eps':     1.0000, 'critic_loss':    12.4923, 'actor_loss':     0.1456, 'eps_e':     1.0000})
Step:   37000, Reward:   118.221 [  49.065], Avg:    28.354 (1.000) <0-00:08:53> ({'r_t':   125.8667, 'eps':     1.0000, 'critic_loss':     5.6113, 'actor_loss':    -0.1196, 'eps_e':     1.0000})
Step:   38000, Reward:   127.518 [  33.270], Avg:    30.897 (1.000) <0-00:09:08> ({'r_t':   132.6407, 'eps':     1.0000, 'critic_loss':    15.8880, 'actor_loss':     0.2655, 'eps_e':     1.0000})
Step:   39000, Reward:    92.720 [ 109.972], Avg:    32.442 (1.000) <0-00:09:23> ({'r_t':   141.3564, 'eps':     1.0000, 'critic_loss':     5.7670, 'actor_loss':    -0.2546, 'eps_e':     1.0000})
Step:   40000, Reward:   129.790 [  42.042], Avg:    34.816 (1.000) <0-00:09:38> ({'r_t':   132.0478, 'eps':     1.0000, 'critic_loss':     8.2901, 'actor_loss':    -0.1191, 'eps_e':     1.0000})
Step:   41000, Reward:   139.634 [  43.789], Avg:    37.312 (1.000) <0-00:09:52> ({'r_t':   113.0159, 'eps':     1.0000, 'critic_loss':    29.1448, 'actor_loss':     0.8682, 'eps_e':     1.0000})
Step:   42000, Reward:   135.540 [  41.547], Avg:    39.596 (1.000) <0-00:10:06> ({'r_t':   150.7482, 'eps':     1.0000, 'critic_loss':    25.6675, 'actor_loss':     0.1329, 'eps_e':     1.0000})
Step:   43000, Reward:   157.342 [  20.500], Avg:    42.272 (1.000) <0-00:10:20> ({'r_t':   143.8788, 'eps':     1.0000, 'critic_loss':     3.7020, 'actor_loss':    -0.2689, 'eps_e':     1.0000})
Step:   44000, Reward:   123.192 [  45.215], Avg:    44.071 (1.000) <0-00:10:34> ({'r_t':   151.0088, 'eps':     1.0000, 'critic_loss':    10.5124, 'actor_loss':     0.1437, 'eps_e':     1.0000})
Step:   45000, Reward:   138.521 [  46.144], Avg:    46.124 (1.000) <0-00:10:47> ({'r_t':   139.3153, 'eps':     1.0000, 'critic_loss':     5.1031, 'actor_loss':    -0.0184, 'eps_e':     1.0000})
Step:   46000, Reward:   131.121 [  38.874], Avg:    47.932 (1.000) <0-00:11:01> ({'r_t':   147.8802, 'eps':     1.0000, 'critic_loss':    23.8233, 'actor_loss':     0.3458, 'eps_e':     1.0000})
Step:   47000, Reward:   124.149 [  48.147], Avg:    49.520 (1.000) <0-00:11:16> ({'r_t':   152.8973, 'eps':     1.0000, 'critic_loss':     6.8673, 'actor_loss':    -0.3203, 'eps_e':     1.0000})
Step:   48000, Reward:   147.173 [  36.197], Avg:    51.513 (1.000) <0-00:11:30> ({'r_t':   149.0213, 'eps':     1.0000, 'critic_loss':     5.6163, 'actor_loss':    -0.2831, 'eps_e':     1.0000})
Step:   49000, Reward:   153.274 [  26.279], Avg:    53.548 (1.000) <0-00:11:44> ({'r_t':   153.9924, 'eps':     1.0000, 'critic_loss':     9.9900, 'actor_loss':     0.0683, 'eps_e':     1.0000})
Step:   50000, Reward:   145.142 [  48.941], Avg:    55.344 (1.000) <0-00:11:58> ({'r_t':   157.0346, 'eps':     1.0000, 'critic_loss':    15.7666, 'actor_loss':     0.3350, 'eps_e':     1.0000})
Step:   51000, Reward:   151.675 [  21.394], Avg:    57.197 (1.000) <0-00:12:13> ({'r_t':   162.0918, 'eps':     1.0000, 'critic_loss':     7.0761, 'actor_loss':    -0.1352, 'eps_e':     1.0000})
Step:   52000, Reward:   117.557 [  49.740], Avg:    58.336 (1.000) <0-00:12:27> ({'r_t':   146.0101, 'eps':     1.0000, 'critic_loss':    15.1437, 'actor_loss':     0.6297, 'eps_e':     1.0000})
Step:   53000, Reward:   126.376 [  59.098], Avg:    59.596 (1.000) <0-00:12:40> ({'r_t':   159.2183, 'eps':     1.0000, 'critic_loss':     8.2871, 'actor_loss':    -0.1370, 'eps_e':     1.0000})
Step:   54000, Reward:   133.168 [  58.357], Avg:    60.933 (1.000) <0-00:12:54> ({'r_t':   145.7414, 'eps':     1.0000, 'critic_loss':    12.1095, 'actor_loss':     0.4735, 'eps_e':     1.0000})
Step:   55000, Reward:   124.611 [  44.962], Avg:    62.071 (1.000) <0-00:13:08> ({'r_t':   182.8713, 'eps':     1.0000, 'critic_loss':    38.7943, 'actor_loss':     0.9693, 'eps_e':     1.0000})
Step:   56000, Reward:   141.688 [  48.802], Avg:    63.467 (1.000) <0-00:13:21> ({'r_t':   140.7461, 'eps':     1.0000, 'critic_loss':    26.2172, 'actor_loss':     0.5226, 'eps_e':     1.0000})
Step:   57000, Reward:   133.430 [  51.965], Avg:    64.674 (1.000) <0-00:13:35> ({'r_t':   144.7249, 'eps':     1.0000, 'critic_loss':     4.4858, 'actor_loss':     0.3986, 'eps_e':     1.0000})
Step:   58000, Reward:   139.229 [  46.651], Avg:    65.937 (1.000) <0-00:13:49> ({'r_t':   168.1865, 'eps':     1.0000, 'critic_loss':     7.1579, 'actor_loss':     0.1972, 'eps_e':     1.0000})
Step:   59000, Reward:   144.486 [  54.323], Avg:    67.246 (1.000) <0-00:14:02> ({'r_t':   173.5473, 'eps':     1.0000, 'critic_loss':    27.3806, 'actor_loss':     0.6349, 'eps_e':     1.0000})
Step:   60000, Reward:   122.940 [  79.490], Avg:    68.159 (1.000) <0-00:14:16> ({'r_t':   136.2545, 'eps':     1.0000, 'critic_loss':     5.9256, 'actor_loss':     0.1043, 'eps_e':     1.0000})
Step:   61000, Reward:   137.188 [  48.556], Avg:    69.273 (1.000) <0-00:14:29> ({'r_t':   154.5568, 'eps':     1.0000, 'critic_loss':    18.4142, 'actor_loss':     0.4322, 'eps_e':     1.0000})
Step:   62000, Reward:   142.759 [  40.422], Avg:    70.439 (1.000) <0-00:14:43> ({'r_t':   174.6877, 'eps':     1.0000, 'critic_loss':    32.1716, 'actor_loss':     0.8419, 'eps_e':     1.0000})
Step:   63000, Reward:   141.482 [  50.312], Avg:    71.549 (1.000) <0-00:14:56> ({'r_t':   152.7132, 'eps':     1.0000, 'critic_loss':    18.3740, 'actor_loss':    -0.1168, 'eps_e':     1.0000})
Step:   64000, Reward:   135.637 [  56.974], Avg:    72.535 (1.000) <0-00:15:09> ({'r_t':   160.3273, 'eps':     1.0000, 'critic_loss':     7.7012, 'actor_loss':    -0.4146, 'eps_e':     1.0000})
Step:   65000, Reward:   129.232 [  76.383], Avg:    73.394 (1.000) <0-00:15:23> ({'r_t':   173.6881, 'eps':     1.0000, 'critic_loss':    19.6568, 'actor_loss':     0.0767, 'eps_e':     1.0000})
Step:   66000, Reward:   156.503 [  33.603], Avg:    74.635 (1.000) <0-00:15:36> ({'r_t':   160.7889, 'eps':     1.0000, 'critic_loss':     8.6922, 'actor_loss':    -0.3678, 'eps_e':     1.0000})
Step:   67000, Reward:   140.049 [  43.246], Avg:    75.597 (1.000) <0-00:15:50> ({'r_t':   172.2018, 'eps':     1.0000, 'critic_loss':     9.9509, 'actor_loss':    -0.4318, 'eps_e':     1.0000})
Step:   68000, Reward:   136.721 [  49.652], Avg:    76.483 (1.000) <0-00:16:04> ({'r_t':   162.8743, 'eps':     1.0000, 'critic_loss':     8.9152, 'actor_loss':    -0.1563, 'eps_e':     1.0000})
Step:   69000, Reward:   158.250 [  24.270], Avg:    77.651 (1.000) <0-00:16:17> ({'r_t':   159.0921, 'eps':     1.0000, 'critic_loss':     6.4611, 'actor_loss':    -0.1572, 'eps_e':     1.0000})
Step:   70000, Reward:   131.927 [  52.018], Avg:    78.415 (1.000) <0-00:16:30> ({'r_t':   167.3331, 'eps':     1.0000, 'critic_loss':    17.5952, 'actor_loss':     0.2267, 'eps_e':     1.0000})
Step:   71000, Reward:   157.412 [  27.493], Avg:    79.512 (1.000) <0-00:16:44> ({'r_t':   153.9587, 'eps':     1.0000, 'critic_loss':    11.6108, 'actor_loss':    -0.1309, 'eps_e':     1.0000})
Step:   72000, Reward:   149.547 [  51.528], Avg:    80.472 (1.000) <0-00:16:57> ({'r_t':   159.4247, 'eps':     1.0000, 'critic_loss':     2.4318, 'actor_loss':    -0.1828, 'eps_e':     1.0000})
Step:   73000, Reward:   151.716 [  39.097], Avg:    81.434 (1.000) <0-00:17:10> ({'r_t':   163.9810, 'eps':     1.0000, 'critic_loss':    24.8360, 'actor_loss':     0.1200, 'eps_e':     1.0000})
Step:   74000, Reward:   159.781 [  24.638], Avg:    82.479 (1.000) <0-00:17:24> ({'r_t':   139.4380, 'eps':     1.0000, 'critic_loss':    24.5553, 'actor_loss':     0.6845, 'eps_e':     1.0000})
Step:   75000, Reward:   139.130 [  47.039], Avg:    83.224 (1.000) <0-00:17:37> ({'r_t':   150.5058, 'eps':     1.0000, 'critic_loss':    13.8701, 'actor_loss':     0.3518, 'eps_e':     1.0000})
Step:   76000, Reward:   108.854 [  58.005], Avg:    83.557 (1.000) <0-00:17:47> ({'r_t':   149.9509, 'eps':     1.0000, 'critic_loss':     6.6153, 'actor_loss':    -0.3947, 'eps_e':     1.0000})
Step:   77000, Reward:   139.992 [  43.175], Avg:    84.281 (1.000) <0-00:18:00> ({'r_t':   169.8652, 'eps':     1.0000, 'critic_loss':    13.5012, 'actor_loss':     0.0366, 'eps_e':     1.0000})
Step:   78000, Reward:   145.081 [  34.121], Avg:    85.050 (1.000) <0-00:18:13> ({'r_t':   140.9263, 'eps':     1.0000, 'critic_loss':     6.2803, 'actor_loss':    -0.2772, 'eps_e':     1.0000})
Step:   79000, Reward:    92.929 [ 105.342], Avg:    85.149 (1.000) <0-00:18:28> ({'r_t':   137.1385, 'eps':     1.0000, 'critic_loss':    21.6630, 'actor_loss':     0.1851, 'eps_e':     1.0000})
Step:   80000, Reward:   147.230 [  41.995], Avg:    85.915 (1.000) <0-00:18:41> ({'r_t':   151.6050, 'eps':     1.0000, 'critic_loss':    26.5005, 'actor_loss':    -0.1566, 'eps_e':     1.0000})
Step:   81000, Reward:   158.739 [  18.430], Avg:    86.803 (1.000) <0-00:18:54> ({'r_t':   196.7981, 'eps':     1.0000, 'critic_loss':    48.9069, 'actor_loss':     0.1487, 'eps_e':     1.0000})
Step:   82000, Reward:   140.286 [  56.231], Avg:    87.448 (1.000) <0-00:19:07> ({'r_t':   141.9595, 'eps':     1.0000, 'critic_loss':    12.6698, 'actor_loss':    -0.5257, 'eps_e':     1.0000})
Step:   83000, Reward:   141.310 [  61.721], Avg:    88.089 (1.000) <0-00:19:20> ({'r_t':   160.0479, 'eps':     1.0000, 'critic_loss':    15.0968, 'actor_loss':    -0.0743, 'eps_e':     1.0000})
Step:   84000, Reward:   149.846 [  65.875], Avg:    88.816 (1.000) <0-00:19:33> ({'r_t':   152.7735, 'eps':     1.0000, 'critic_loss':    15.1099, 'actor_loss':    -0.1706, 'eps_e':     1.0000})
Step:   85000, Reward:   140.640 [  64.321], Avg:    89.418 (1.000) <0-00:19:46> ({'r_t':   156.4124, 'eps':     1.0000, 'critic_loss':    13.9168, 'actor_loss':    -0.0888, 'eps_e':     1.0000})
Step:   86000, Reward:   137.361 [  62.973], Avg:    89.969 (1.000) <0-00:19:58> ({'r_t':   159.6169, 'eps':     1.0000, 'critic_loss':    20.2508, 'actor_loss':    -0.0511, 'eps_e':     1.0000})
Step:   87000, Reward:   143.232 [  90.258], Avg:    90.575 (1.000) <0-00:20:11> ({'r_t':   163.3093, 'eps':     1.0000, 'critic_loss':    19.3724, 'actor_loss':    -0.0968, 'eps_e':     1.0000})
Step:   88000, Reward:   123.038 [  76.385], Avg:    90.939 (1.000) <0-00:20:28> ({'r_t':   181.1501, 'eps':     1.0000, 'critic_loss':    28.7101, 'actor_loss':    -0.3020, 'eps_e':     1.0000})
Step:   89000, Reward:   152.986 [  59.830], Avg:    91.629 (1.000) <0-00:20:37> ({'r_t':   167.9126, 'eps':     1.0000, 'critic_loss':    10.3618, 'actor_loss':    -0.2697, 'eps_e':     1.0000})
Step:   90000, Reward:   130.325 [  69.762], Avg:    92.054 (1.000) <0-00:20:50> ({'r_t':   192.9301, 'eps':     1.0000, 'critic_loss':    13.9947, 'actor_loss':    -0.1007, 'eps_e':     1.0000})
Step:   91000, Reward:   144.546 [  88.739], Avg:    92.624 (1.000) <0-00:21:05> ({'r_t':   215.6698, 'eps':     1.0000, 'critic_loss':    35.4312, 'actor_loss':     0.0597, 'eps_e':     1.0000})
Step:   92000, Reward:   172.153 [ 109.535], Avg:    93.480 (1.000) <0-00:21:18> ({'r_t':   240.1834, 'eps':     1.0000, 'critic_loss':    66.7685, 'actor_loss':     0.0444, 'eps_e':     1.0000})
Step:   93000, Reward:   206.015 [  81.193], Avg:    94.677 (1.000) <0-00:21:31> ({'r_t':   257.5347, 'eps':     1.0000, 'critic_loss':    65.5677, 'actor_loss':     0.1071, 'eps_e':     1.0000})
Step:   94000, Reward:   200.632 [  89.861], Avg:    95.792 (1.000) <0-00:21:42> ({'r_t':   336.8492, 'eps':     1.0000, 'critic_loss':   109.8907, 'actor_loss':    -0.8602, 'eps_e':     1.0000})
Step:   95000, Reward:   231.914 [  83.782], Avg:    97.210 (1.000) <0-00:21:56> ({'r_t':   440.1590, 'eps':     1.0000, 'critic_loss':    96.2083, 'actor_loss':    -0.9062, 'eps_e':     1.0000})
Step:   96000, Reward:   159.953 [  94.317], Avg:    97.857 (1.000) <0-00:22:08> ({'r_t':   353.8633, 'eps':     1.0000, 'critic_loss':    86.1947, 'actor_loss':    -0.4988, 'eps_e':     1.0000})
Step:   97000, Reward:   194.453 [ 136.012], Avg:    98.843 (1.000) <0-00:22:20> ({'r_t':   344.0187, 'eps':     1.0000, 'critic_loss':    94.9277, 'actor_loss':     1.1305, 'eps_e':     1.0000})
Step:   98000, Reward:   244.072 [  78.816], Avg:   100.310 (1.000) <0-00:22:33> ({'r_t':   612.0822, 'eps':     1.0000, 'critic_loss':   108.5832, 'actor_loss':    -1.6055, 'eps_e':     1.0000})
Step:   99000, Reward:   213.146 [ 110.779], Avg:   101.438 (1.000) <0-00:22:48> ({'r_t':   600.0924, 'eps':     1.0000, 'critic_loss':    82.4662, 'actor_loss':    -1.0721, 'eps_e':     1.0000})
Step:  100000, Reward:   193.661 [ 121.822], Avg:   102.351 (1.000) <0-00:22:59> ({'r_t':   532.0708, 'eps':     1.0000, 'critic_loss':    90.8300, 'actor_loss':    -0.1169, 'eps_e':     1.0000})
Step:  101000, Reward:   265.327 [  37.343], Avg:   103.949 (1.000) <0-00:23:13> ({'r_t':   436.4345, 'eps':     1.0000, 'critic_loss':    51.3682, 'actor_loss':     1.3912, 'eps_e':     1.0000})
Step:  102000, Reward:   264.258 [  45.614], Avg:   105.505 (1.000) <0-00:23:23> ({'r_t':   758.2485, 'eps':     1.0000, 'critic_loss':    76.4657, 'actor_loss':    -0.4710, 'eps_e':     1.0000})
Step:  103000, Reward:   230.156 [ 105.384], Avg:   106.704 (1.000) <0-00:23:37> ({'r_t':   673.8587, 'eps':     1.0000, 'critic_loss':    89.7772, 'actor_loss':     0.3420, 'eps_e':     1.0000})
Step:  104000, Reward:   261.769 [  45.207], Avg:   108.181 (1.000) <0-00:23:49> ({'r_t':   910.0874, 'eps':     1.0000, 'critic_loss':    72.1545, 'actor_loss':    -0.2008, 'eps_e':     1.0000})
Step:  105000, Reward:   238.742 [  86.755], Avg:   109.412 (1.000) <0-00:24:02> ({'r_t':   767.3814, 'eps':     1.0000, 'critic_loss':    89.5954, 'actor_loss':     0.7822, 'eps_e':     1.0000})
Step:  106000, Reward:   272.711 [  16.851], Avg:   110.938 (1.000) <0-00:24:13> ({'r_t':   784.7660, 'eps':     1.0000, 'critic_loss':   104.7122, 'actor_loss':     0.4013, 'eps_e':     1.0000})
Step:  107000, Reward:   262.609 [  35.442], Avg:   112.343 (1.000) <0-00:24:26> ({'r_t':   956.6355, 'eps':     1.0000, 'critic_loss':    86.3996, 'actor_loss':     0.2237, 'eps_e':     1.0000})
Step:  108000, Reward:   271.074 [  21.046], Avg:   113.799 (1.000) <0-00:24:38> ({'r_t':   807.6579, 'eps':     1.0000, 'critic_loss':    59.5795, 'actor_loss':     0.4453, 'eps_e':     1.0000})
Step:  109000, Reward:   222.026 [  95.664], Avg:   114.783 (1.000) <0-00:24:49> ({'r_t':   857.1290, 'eps':     1.0000, 'critic_loss':    73.4942, 'actor_loss':     0.7983, 'eps_e':     1.0000})
Step:  110000, Reward:   263.580 [  58.599], Avg:   116.123 (1.000) <0-00:25:00> ({'r_t':  1006.5368, 'eps':     1.0000, 'critic_loss':    73.3543, 'actor_loss':     0.9611, 'eps_e':     1.0000})
Step:  111000, Reward:   259.527 [  44.841], Avg:   117.404 (1.000) <0-00:25:14> ({'r_t':   978.3760, 'eps':     1.0000, 'critic_loss':    45.6154, 'actor_loss':     0.5448, 'eps_e':     1.0000})
Step:  112000, Reward:   282.866 [  11.150], Avg:   118.868 (1.000) <0-00:25:25> ({'r_t':   927.5349, 'eps':     1.0000, 'critic_loss':    66.0346, 'actor_loss':     0.7090, 'eps_e':     1.0000})
Step:  113000, Reward:   267.615 [  58.297], Avg:   120.173 (1.000) <0-00:25:36> ({'r_t':   921.8191, 'eps':     1.0000, 'critic_loss':    86.2121, 'actor_loss':     1.3016, 'eps_e':     1.0000})
Step:  114000, Reward:   231.717 [  70.353], Avg:   121.143 (1.000) <0-00:25:47> ({'r_t':   833.1500, 'eps':     1.0000, 'critic_loss':    91.1762, 'actor_loss':    -0.1864, 'eps_e':     1.0000})
Step:  115000, Reward:   236.755 [  79.162], Avg:   122.140 (1.000) <0-00:26:01> ({'r_t':   901.8936, 'eps':     1.0000, 'critic_loss':   107.0186, 'actor_loss':     1.0455, 'eps_e':     1.0000})
Step:  116000, Reward:   250.581 [  86.173], Avg:   123.237 (1.000) <0-00:26:09> ({'r_t':   946.8101, 'eps':     1.0000, 'critic_loss':   104.6522, 'actor_loss':     0.6871, 'eps_e':     1.0000})
Step:  117000, Reward:   258.707 [  79.706], Avg:   124.385 (1.000) <0-00:26:23> ({'r_t':   961.9191, 'eps':     1.0000, 'critic_loss':    93.9137, 'actor_loss':     1.0749, 'eps_e':     1.0000})
Step:  118000, Reward:   243.215 [  83.649], Avg:   125.384 (1.000) <0-00:26:31> ({'r_t':   908.2617, 'eps':     1.0000, 'critic_loss':   119.3502, 'actor_loss':     1.5656, 'eps_e':     1.0000})
Step:  119000, Reward:   255.484 [  60.682], Avg:   126.468 (1.000) <0-00:26:44> ({'r_t':   998.5666, 'eps':     1.0000, 'critic_loss':   107.2559, 'actor_loss':     0.0934, 'eps_e':     1.0000})
Step:  120000, Reward:   229.376 [ 101.061], Avg:   127.319 (1.000) <0-00:26:54> ({'r_t':   999.2571, 'eps':     1.0000, 'critic_loss':    84.5911, 'actor_loss':     0.6547, 'eps_e':     1.0000})
Step:  121000, Reward:   252.553 [  74.650], Avg:   128.345 (1.000) <0-00:27:07> ({'r_t':   980.8380, 'eps':     1.0000, 'critic_loss':   115.8635, 'actor_loss':     0.5668, 'eps_e':     1.0000})
Step:  122000, Reward:   256.915 [  65.234], Avg:   129.390 (1.000) <0-00:27:19> ({'r_t':   922.1894, 'eps':     1.0000, 'critic_loss':    64.5753, 'actor_loss':     0.3704, 'eps_e':     1.0000})
Step:  123000, Reward:   219.333 [  88.957], Avg:   130.116 (1.000) <0-00:27:27> ({'r_t':   926.9615, 'eps':     1.0000, 'critic_loss':    50.1923, 'actor_loss':     0.1113, 'eps_e':     1.0000})
Step:  124000, Reward:   241.354 [  81.680], Avg:   131.006 (1.000) <0-00:27:41> ({'r_t':   853.5434, 'eps':     1.0000, 'critic_loss':    72.5801, 'actor_loss':    -0.0793, 'eps_e':     1.0000})
Step:  125000, Reward:   243.262 [  83.885], Avg:   131.897 (1.000) <0-00:27:51> ({'r_t':   782.2433, 'eps':     1.0000, 'critic_loss':    45.0394, 'actor_loss':     0.4991, 'eps_e':     1.0000})
Step:  126000, Reward:   264.782 [  66.055], Avg:   132.943 (1.000) <0-00:28:03> ({'r_t':   712.6910, 'eps':     1.0000, 'critic_loss':    72.4902, 'actor_loss':     0.8379, 'eps_e':     1.0000})
Step:  127000, Reward:   241.281 [  89.820], Avg:   133.789 (1.000) <0-00:28:16> ({'r_t':   913.8859, 'eps':     1.0000, 'critic_loss':    71.7829, 'actor_loss':     0.6047, 'eps_e':     1.0000})
Step:  128000, Reward:   258.868 [  44.697], Avg:   134.759 (1.000) <0-00:28:30> ({'r_t':  1009.4019, 'eps':     1.0000, 'critic_loss':   110.7106, 'actor_loss':     1.1937, 'eps_e':     1.0000})
Step:  129000, Reward:   257.072 [  63.636], Avg:   135.700 (1.000) <0-00:28:41> ({'r_t':   887.9037, 'eps':     1.0000, 'critic_loss':   103.6826, 'actor_loss':     0.7679, 'eps_e':     1.0000})
Step:  130000, Reward:   185.693 [ 143.357], Avg:   136.081 (1.000) <0-00:28:52> ({'r_t':   932.6546, 'eps':     1.0000, 'critic_loss':    60.3248, 'actor_loss':    -0.2862, 'eps_e':     1.0000})
Step:  131000, Reward:   265.052 [  65.142], Avg:   137.058 (1.000) <0-00:29:04> ({'r_t':   848.4029, 'eps':     1.0000, 'critic_loss':    69.7216, 'actor_loss':     0.4870, 'eps_e':     1.0000})
Step:  132000, Reward:   248.456 [  72.860], Avg:   137.896 (1.000) <0-00:29:15> ({'r_t':  1055.3229, 'eps':     1.0000, 'critic_loss':    40.8762, 'actor_loss':    -0.2005, 'eps_e':     1.0000})
Step:  133000, Reward:   218.385 [ 121.769], Avg:   138.497 (1.000) <0-00:29:27> ({'r_t':  1001.0768, 'eps':     1.0000, 'critic_loss':    92.4509, 'actor_loss':     0.9453, 'eps_e':     1.0000})
Step:  134000, Reward:   252.826 [  49.488], Avg:   139.344 (1.000) <0-00:29:42> ({'r_t':   888.0238, 'eps':     1.0000, 'critic_loss':    39.9537, 'actor_loss':     0.5171, 'eps_e':     1.0000})
Step:  135000, Reward:   241.239 [  72.572], Avg:   140.093 (1.000) <0-00:29:53> ({'r_t':   988.9192, 'eps':     1.0000, 'critic_loss':    39.1789, 'actor_loss':     0.2047, 'eps_e':     1.0000})
Step:  136000, Reward:   262.847 [  71.051], Avg:   140.989 (1.000) <0-00:30:05> ({'r_t':   972.0785, 'eps':     1.0000, 'critic_loss':    53.4608, 'actor_loss':     1.2219, 'eps_e':     1.0000})
Step:  137000, Reward:   251.352 [  56.644], Avg:   141.789 (1.000) <0-00:30:16> ({'r_t':  1163.6696, 'eps':     1.0000, 'critic_loss':    98.3541, 'actor_loss':     0.5719, 'eps_e':     1.0000})
Step:  138000, Reward:   274.123 [  22.123], Avg:   142.741 (1.000) <0-00:30:27> ({'r_t':  1099.3258, 'eps':     1.0000, 'critic_loss':    88.7939, 'actor_loss':    -0.0848, 'eps_e':     1.0000})
Step:  139000, Reward:   270.669 [  24.673], Avg:   143.654 (1.000) <0-00:30:38> ({'r_t':  1105.5335, 'eps':     1.0000, 'critic_loss':    73.3577, 'actor_loss':     1.5891, 'eps_e':     1.0000})
Step:  140000, Reward:   258.325 [  55.040], Avg:   144.468 (1.000) <0-00:30:50> ({'r_t':   968.3104, 'eps':     1.0000, 'critic_loss':    69.3441, 'actor_loss':     0.6320, 'eps_e':     1.0000})
Step:  141000, Reward:   242.984 [  79.610], Avg:   145.161 (1.000) <0-00:31:00> ({'r_t':   955.2734, 'eps':     1.0000, 'critic_loss':    86.5068, 'actor_loss':     0.5410, 'eps_e':     1.0000})
Step:  142000, Reward:   249.693 [  78.884], Avg:   145.892 (1.000) <0-00:31:10> ({'r_t':  1100.1590, 'eps':     1.0000, 'critic_loss':    84.2629, 'actor_loss':    -0.7740, 'eps_e':     1.0000})
Step:  143000, Reward:   241.590 [ 106.021], Avg:   146.557 (1.000) <0-00:31:22> ({'r_t':  1070.4595, 'eps':     1.0000, 'critic_loss':    40.7560, 'actor_loss':    -0.4834, 'eps_e':     1.0000})
Step:  144000, Reward:   248.495 [  80.573], Avg:   147.260 (1.000) <0-00:31:34> ({'r_t':  1066.0469, 'eps':     1.0000, 'critic_loss':    68.7240, 'actor_loss':     0.9442, 'eps_e':     1.0000})
Step:  145000, Reward:   280.935 [  18.871], Avg:   148.176 (1.000) <0-00:31:44> ({'r_t':  1073.2629, 'eps':     1.0000, 'critic_loss':    40.2546, 'actor_loss':     0.7460, 'eps_e':     1.0000})
Step:  146000, Reward:   265.175 [  61.927], Avg:   148.971 (1.000) <0-00:31:54> ({'r_t':  1084.1011, 'eps':     1.0000, 'critic_loss':    61.7631, 'actor_loss':     1.4347, 'eps_e':     1.0000})
Step:  147000, Reward:   237.959 [  70.521], Avg:   149.573 (1.000) <0-00:32:06> ({'r_t':  1061.9180, 'eps':     1.0000, 'critic_loss':    86.5212, 'actor_loss':     1.8105, 'eps_e':     1.0000})
Step:  148000, Reward:   267.614 [  64.888], Avg:   150.365 (1.000) <0-00:32:18> ({'r_t':  1060.5189, 'eps':     1.0000, 'critic_loss':    41.5563, 'actor_loss':    -0.4080, 'eps_e':     1.0000})
Step:  149000, Reward:   247.885 [  92.048], Avg:   151.015 (1.000) <0-00:32:29> ({'r_t':  1118.5759, 'eps':     1.0000, 'critic_loss':    48.8683, 'actor_loss':    -0.0498, 'eps_e':     1.0000})
Step:  150000, Reward:   272.786 [  21.075], Avg:   151.822 (1.000) <0-00:32:40> ({'r_t':  1019.4171, 'eps':     1.0000, 'critic_loss':    52.8870, 'actor_loss':     0.6784, 'eps_e':     1.0000})
Step:  151000, Reward:   260.079 [  69.373], Avg:   152.534 (1.000) <0-00:32:51> ({'r_t':  1056.5679, 'eps':     1.0000, 'critic_loss':    46.1410, 'actor_loss':     0.6055, 'eps_e':     1.0000})
Step:  152000, Reward:   269.377 [  41.802], Avg:   153.297 (1.000) <0-00:33:03> ({'r_t':  1176.6096, 'eps':     1.0000, 'critic_loss':    38.4365, 'actor_loss':     0.4331, 'eps_e':     1.0000})
Step:  153000, Reward:   266.504 [  45.692], Avg:   154.033 (1.000) <0-00:33:16> ({'r_t':  1079.3513, 'eps':     1.0000, 'critic_loss':    49.6662, 'actor_loss':     1.1547, 'eps_e':     1.0000})
Step:  154000, Reward:   259.537 [  47.518], Avg:   154.713 (1.000) <0-00:33:29> ({'r_t':  1038.9051, 'eps':     1.0000, 'critic_loss':    59.2176, 'actor_loss':     1.2744, 'eps_e':     1.0000})
Step:  155000, Reward:   272.120 [  58.530], Avg:   155.466 (1.000) <0-00:33:38> ({'r_t':  1151.9118, 'eps':     1.0000, 'critic_loss':    62.5421, 'actor_loss':     0.5663, 'eps_e':     1.0000})
Step:  156000, Reward:   257.319 [  55.254], Avg:   156.115 (1.000) <0-00:33:49> ({'r_t':  1212.9322, 'eps':     1.0000, 'critic_loss':    47.0012, 'actor_loss':     0.4258, 'eps_e':     1.0000})
Step:  157000, Reward:   180.746 [ 125.901], Avg:   156.270 (1.000) <0-00:34:01> ({'r_t':  1175.5951, 'eps':     1.0000, 'critic_loss':    40.3320, 'actor_loss':    -0.0020, 'eps_e':     1.0000})
Step:  158000, Reward:   254.334 [  72.660], Avg:   156.887 (1.000) <0-00:34:15> ({'r_t':  1130.4846, 'eps':     1.0000, 'critic_loss':    50.4566, 'actor_loss':     0.4627, 'eps_e':     1.0000})
Step:  159000, Reward:   249.661 [  67.333], Avg:   157.467 (1.000) <0-00:34:28> ({'r_t':  1024.5498, 'eps':     1.0000, 'critic_loss':    75.4693, 'actor_loss':     0.7547, 'eps_e':     1.0000})
Step:  160000, Reward:   262.130 [  42.655], Avg:   158.117 (1.000) <0-00:34:40> ({'r_t':  1138.1403, 'eps':     1.0000, 'critic_loss':    55.2949, 'actor_loss':     0.2007, 'eps_e':     1.0000})
Step:  161000, Reward:   217.143 [ 123.582], Avg:   158.481 (1.000) <0-00:34:50> ({'r_t':   926.6171, 'eps':     1.0000, 'critic_loss':    45.1721, 'actor_loss':     1.1955, 'eps_e':     1.0000})
Step:  162000, Reward:   271.963 [  33.881], Avg:   159.178 (1.000) <0-00:35:02> ({'r_t':  1151.8402, 'eps':     1.0000, 'critic_loss':    44.9422, 'actor_loss':     0.7828, 'eps_e':     1.0000})
Step:  163000, Reward:   260.617 [  52.735], Avg:   159.796 (1.000) <0-00:35:14> ({'r_t':  1124.2647, 'eps':     1.0000, 'critic_loss':    86.6892, 'actor_loss':     1.9087, 'eps_e':     1.0000})
Step:  164000, Reward:   226.407 [  98.806], Avg:   160.200 (1.000) <0-00:35:27> ({'r_t':  1078.0941, 'eps':     1.0000, 'critic_loss':    43.2215, 'actor_loss':     0.1934, 'eps_e':     1.0000})
Step:  165000, Reward:   211.921 [ 143.563], Avg:   160.512 (1.000) <0-00:35:39> ({'r_t':  1016.3530, 'eps':     1.0000, 'critic_loss':    88.1607, 'actor_loss':     1.2126, 'eps_e':     1.0000})
Step:  166000, Reward:   245.577 [  79.290], Avg:   161.021 (1.000) <0-00:35:49> ({'r_t':   966.2222, 'eps':     1.0000, 'critic_loss':    42.5690, 'actor_loss':     0.7527, 'eps_e':     1.0000})
Step:  167000, Reward:   240.180 [  71.264], Avg:   161.492 (1.000) <0-00:36:00> ({'r_t':  1116.4259, 'eps':     1.0000, 'critic_loss':    61.0295, 'actor_loss':     0.4169, 'eps_e':     1.0000})
Step:  168000, Reward:   278.306 [  24.107], Avg:   162.183 (1.000) <0-00:36:12> ({'r_t':   985.0347, 'eps':     1.0000, 'critic_loss':   101.9945, 'actor_loss':     0.6494, 'eps_e':     1.0000})
Step:  169000, Reward:   236.953 [  91.263], Avg:   162.623 (1.000) <0-00:36:25> ({'r_t':   974.1326, 'eps':     1.0000, 'critic_loss':    73.5455, 'actor_loss':     0.8745, 'eps_e':     1.0000})
Step:  170000, Reward:   259.384 [  59.280], Avg:   163.189 (1.000) <0-00:36:36> ({'r_t':  1180.5306, 'eps':     1.0000, 'critic_loss':   102.9596, 'actor_loss':     1.4224, 'eps_e':     1.0000})
Step:  171000, Reward:   263.214 [  57.937], Avg:   163.770 (1.000) <0-00:36:47> ({'r_t':  1075.6049, 'eps':     1.0000, 'critic_loss':    70.7564, 'actor_loss':     0.0626, 'eps_e':     1.0000})
Step:  172000, Reward:   255.936 [  71.903], Avg:   164.303 (1.000) <0-00:36:57> ({'r_t':  1183.5192, 'eps':     1.0000, 'critic_loss':    50.9341, 'actor_loss':     0.4744, 'eps_e':     1.0000})
Step:  173000, Reward:   274.022 [  42.197], Avg:   164.934 (1.000) <0-00:37:09> ({'r_t':  1095.2848, 'eps':     1.0000, 'critic_loss':    61.8830, 'actor_loss':     0.1068, 'eps_e':     1.0000})
Step:  174000, Reward:   259.812 [  68.455], Avg:   165.476 (1.000) <0-00:37:20> ({'r_t':  1203.1158, 'eps':     1.0000, 'critic_loss':    60.0946, 'actor_loss':     0.2504, 'eps_e':     1.0000})
Step:  175000, Reward:   247.882 [  61.093], Avg:   165.944 (1.000) <0-00:37:32> ({'r_t':  1138.2748, 'eps':     1.0000, 'critic_loss':    74.1670, 'actor_loss':     0.7278, 'eps_e':     1.0000})
Step:  176000, Reward:   280.611 [  17.808], Avg:   166.592 (1.000) <0-00:37:43> ({'r_t':  1094.1873, 'eps':     1.0000, 'critic_loss':    74.5336, 'actor_loss':     0.8028, 'eps_e':     1.0000})
Step:  177000, Reward:   271.162 [  27.387], Avg:   167.179 (1.000) <0-00:37:54> ({'r_t':   919.1206, 'eps':     1.0000, 'critic_loss':    87.0803, 'actor_loss':    -0.1031, 'eps_e':     1.0000})
Step:  178000, Reward:   268.412 [  53.640], Avg:   167.745 (1.000) <0-00:38:04> ({'r_t':  1015.0985, 'eps':     1.0000, 'critic_loss':    42.4621, 'actor_loss':     0.5773, 'eps_e':     1.0000})
Step:  179000, Reward:   251.803 [ 112.903], Avg:   168.212 (1.000) <0-00:38:17> ({'r_t':   933.1274, 'eps':     1.0000, 'critic_loss':    45.2906, 'actor_loss':     0.8930, 'eps_e':     1.0000})
Step:  180000, Reward:   243.087 [  83.040], Avg:   168.626 (1.000) <0-00:38:29> ({'r_t':  1063.2736, 'eps':     1.0000, 'critic_loss':    31.8417, 'actor_loss':    -0.3123, 'eps_e':     1.0000})
Step:  181000, Reward:   278.491 [  36.642], Avg:   169.229 (1.000) <0-00:38:42> ({'r_t':   997.9240, 'eps':     1.0000, 'critic_loss':    43.1503, 'actor_loss':     1.0382, 'eps_e':     1.0000})
Step:  182000, Reward:   271.093 [  41.357], Avg:   169.786 (1.000) <0-00:38:54> ({'r_t':  1116.6893, 'eps':     1.0000, 'critic_loss':    37.0399, 'actor_loss':    -0.4388, 'eps_e':     1.0000})
Step:  183000, Reward:   279.743 [  42.728], Avg:   170.384 (1.000) <0-00:39:06> ({'r_t':  1223.1098, 'eps':     1.0000, 'critic_loss':    25.8351, 'actor_loss':     0.2367, 'eps_e':     1.0000})
Step:  184000, Reward:   242.124 [  75.508], Avg:   170.771 (1.000) <0-00:39:18> ({'r_t':  1070.9112, 'eps':     1.0000, 'critic_loss':    55.3908, 'actor_loss':     1.9758, 'eps_e':     1.0000})
Step:  185000, Reward:   285.502 [  18.322], Avg:   171.388 (1.000) <0-00:39:29> ({'r_t':  1254.2026, 'eps':     1.0000, 'critic_loss':    29.1216, 'actor_loss':     0.5964, 'eps_e':     1.0000})
Step:  186000, Reward:   257.287 [  76.754], Avg:   171.848 (1.000) <0-00:39:40> ({'r_t':  1113.7127, 'eps':     1.0000, 'critic_loss':    30.4285, 'actor_loss':     0.7819, 'eps_e':     1.0000})
Step:  187000, Reward:   254.081 [  69.433], Avg:   172.285 (1.000) <0-00:39:53> ({'r_t':  1327.4301, 'eps':     1.0000, 'critic_loss':    37.9595, 'actor_loss':     0.2033, 'eps_e':     1.0000})
Step:  188000, Reward:   256.366 [  39.252], Avg:   172.730 (1.000) <0-00:40:05> ({'r_t':  1297.7699, 'eps':     1.0000, 'critic_loss':    28.3583, 'actor_loss':     0.2074, 'eps_e':     1.0000})
Step:  189000, Reward:   237.790 [  92.172], Avg:   173.072 (1.000) <0-00:40:18> ({'r_t':  1189.7171, 'eps':     1.0000, 'critic_loss':    38.3130, 'actor_loss':     1.0541, 'eps_e':     1.0000})
Step:  190000, Reward:   285.563 [  13.768], Avg:   173.661 (1.000) <0-00:40:29> ({'r_t':  1028.2973, 'eps':     1.0000, 'critic_loss':    39.1999, 'actor_loss':     0.6279, 'eps_e':     1.0000})
Step:  191000, Reward:   229.172 [  95.987], Avg:   173.950 (1.000) <0-00:40:41> ({'r_t':   900.1771, 'eps':     1.0000, 'critic_loss':    49.5871, 'actor_loss':     1.3341, 'eps_e':     1.0000})
Step:  192000, Reward:   280.241 [  16.633], Avg:   174.501 (1.000) <0-00:40:52> ({'r_t':   983.4783, 'eps':     1.0000, 'critic_loss':    32.3387, 'actor_loss':     0.3552, 'eps_e':     1.0000})
Step:  193000, Reward:   277.509 [  23.893], Avg:   175.032 (1.000) <0-00:41:02> ({'r_t':   990.6618, 'eps':     1.0000, 'critic_loss':    45.9904, 'actor_loss':     0.8892, 'eps_e':     1.0000})
Step:  194000, Reward:   280.008 [  33.101], Avg:   175.570 (1.000) <0-00:41:15> ({'r_t':  1158.0336, 'eps':     1.0000, 'critic_loss':    42.9818, 'actor_loss':    -0.0942, 'eps_e':     1.0000})
Step:  195000, Reward:   258.470 [  51.815], Avg:   175.993 (1.000) <0-00:41:24> ({'r_t':  1119.8095, 'eps':     1.0000, 'critic_loss':    45.2725, 'actor_loss':     0.5845, 'eps_e':     1.0000})
Step:  196000, Reward:   281.934 [  13.906], Avg:   176.531 (1.000) <0-00:41:36> ({'r_t':  1122.7684, 'eps':     1.0000, 'critic_loss':    46.7467, 'actor_loss':     0.7323, 'eps_e':     1.0000})
Step:  197000, Reward:   263.120 [  57.501], Avg:   176.968 (1.000) <0-00:41:47> ({'r_t':  1091.4237, 'eps':     1.0000, 'critic_loss':    41.2897, 'actor_loss':     0.2802, 'eps_e':     1.0000})
Step:  198000, Reward:   268.445 [  31.976], Avg:   177.428 (1.000) <0-00:42:00> ({'r_t':  1255.4041, 'eps':     1.0000, 'critic_loss':    45.8996, 'actor_loss':    -0.2897, 'eps_e':     1.0000})
Step:  199000, Reward:   285.732 [  19.852], Avg:   177.970 (1.000) <0-00:42:10> ({'r_t':  1218.3827, 'eps':     1.0000, 'critic_loss':    33.1918, 'actor_loss':    -0.3114, 'eps_e':     1.0000})
Step:  200000, Reward:   269.037 [  44.725], Avg:   178.423 (1.000) <0-00:42:23> ({'r_t':  1329.5864, 'eps':     1.0000, 'critic_loss':    26.3262, 'actor_loss':    -0.1424, 'eps_e':     1.0000})
Step:  201000, Reward:   255.073 [  53.918], Avg:   178.802 (1.000) <0-00:42:35> ({'r_t':  1225.9613, 'eps':     1.0000, 'critic_loss':    19.7749, 'actor_loss':     0.0111, 'eps_e':     1.0000})
Step:  202000, Reward:   254.779 [ 104.604], Avg:   179.176 (1.000) <0-00:42:48> ({'r_t':  1229.4209, 'eps':     1.0000, 'critic_loss':    27.0465, 'actor_loss':     0.8674, 'eps_e':     1.0000})
Step:  203000, Reward:   283.705 [  19.322], Avg:   179.689 (1.000) <0-00:42:57> ({'r_t':  1211.6785, 'eps':     1.0000, 'critic_loss':    26.8220, 'actor_loss':     0.8372, 'eps_e':     1.0000})
Step:  204000, Reward:   278.793 [  19.313], Avg:   180.172 (1.000) <0-00:43:08> ({'r_t':  1140.4914, 'eps':     1.0000, 'critic_loss':    33.2668, 'actor_loss':     0.6202, 'eps_e':     1.0000})
Step:  205000, Reward:   285.241 [  17.217], Avg:   180.682 (1.000) <0-00:43:18> ({'r_t':  1294.0115, 'eps':     1.0000, 'critic_loss':    34.9176, 'actor_loss':     0.5494, 'eps_e':     1.0000})
Step:  206000, Reward:   281.203 [  17.311], Avg:   181.168 (1.000) <0-00:43:29> ({'r_t':  1194.9646, 'eps':     1.0000, 'critic_loss':    33.2150, 'actor_loss':     0.4785, 'eps_e':     1.0000})
Step:  207000, Reward:   283.495 [  16.426], Avg:   181.660 (1.000) <0-00:43:40> ({'r_t':  1417.2612, 'eps':     1.0000, 'critic_loss':    25.0830, 'actor_loss':     0.6151, 'eps_e':     1.0000})
Step:  208000, Reward:   273.275 [  22.539], Avg:   182.098 (1.000) <0-00:43:51> ({'r_t':  1341.7807, 'eps':     1.0000, 'critic_loss':    28.1674, 'actor_loss':     0.6118, 'eps_e':     1.0000})
Step:  209000, Reward:   285.770 [  20.318], Avg:   182.592 (1.000) <0-00:44:02> ({'r_t':  1315.5271, 'eps':     1.0000, 'critic_loss':    28.0828, 'actor_loss':     1.2691, 'eps_e':     1.0000})
Step:  210000, Reward:   288.399 [  17.087], Avg:   183.093 (1.000) <0-00:44:12> ({'r_t':  1393.4196, 'eps':     1.0000, 'critic_loss':    20.9665, 'actor_loss':     0.1567, 'eps_e':     1.0000})
Step:  211000, Reward:   283.087 [  21.217], Avg:   183.565 (1.000) <0-00:44:23> ({'r_t':  1369.6042, 'eps':     1.0000, 'critic_loss':    21.9471, 'actor_loss':     0.4294, 'eps_e':     1.0000})
Step:  212000, Reward:   290.925 [  15.807], Avg:   184.069 (1.000) <0-00:44:33> ({'r_t':  1343.5665, 'eps':     1.0000, 'critic_loss':    33.0596, 'actor_loss':     1.4736, 'eps_e':     1.0000})
Step:  213000, Reward:   250.485 [  82.002], Avg:   184.379 (1.000) <0-00:44:41> ({'r_t':  1415.6546, 'eps':     1.0000, 'critic_loss':    16.4219, 'actor_loss':     0.0550, 'eps_e':     1.0000})
Step:  214000, Reward:   263.091 [  52.355], Avg:   184.745 (1.000) <0-00:44:57> ({'r_t':  1356.9369, 'eps':     1.0000, 'critic_loss':    14.4303, 'actor_loss':     0.4082, 'eps_e':     1.0000})
Step:  215000, Reward:   256.928 [  77.571], Avg:   185.080 (1.000) <0-00:45:08> ({'r_t':  1416.9776, 'eps':     1.0000, 'critic_loss':    19.4069, 'actor_loss':     0.3340, 'eps_e':     1.0000})
Step:  216000, Reward:   258.183 [  57.350], Avg:   185.417 (1.000) <0-00:45:20> ({'r_t':  1509.0015, 'eps':     1.0000, 'critic_loss':    12.8122, 'actor_loss':     0.1885, 'eps_e':     1.0000})
Step:  217000, Reward:   288.347 [  10.647], Avg:   185.889 (1.000) <0-00:45:31> ({'r_t':  1329.1451, 'eps':     1.0000, 'critic_loss':    15.7110, 'actor_loss':     1.0951, 'eps_e':     1.0000})
Step:  218000, Reward:   286.100 [  18.424], Avg:   186.346 (1.000) <0-00:45:41> ({'r_t':  1362.7027, 'eps':     1.0000, 'critic_loss':    36.4548, 'actor_loss':     1.1778, 'eps_e':     1.0000})
Step:  219000, Reward:   280.599 [  15.028], Avg:   186.775 (1.000) <0-00:45:52> ({'r_t':  1185.9585, 'eps':     1.0000, 'critic_loss':    23.1869, 'actor_loss':     0.5257, 'eps_e':     1.0000})
Step:  220000, Reward:   284.570 [  23.886], Avg:   187.217 (1.000) <0-00:46:03> ({'r_t':   949.7233, 'eps':     1.0000, 'critic_loss':    30.4574, 'actor_loss':     1.3570, 'eps_e':     1.0000})
Step:  221000, Reward:   271.815 [  32.158], Avg:   187.598 (1.000) <0-00:46:13> ({'r_t':  1187.2321, 'eps':     1.0000, 'critic_loss':    42.2479, 'actor_loss':     0.0813, 'eps_e':     1.0000})
Step:  222000, Reward:   269.940 [  75.877], Avg:   187.968 (1.000) <0-00:46:28> ({'r_t':  1362.1151, 'eps':     1.0000, 'critic_loss':    51.1405, 'actor_loss':     0.2018, 'eps_e':     1.0000})
Step:  223000, Reward:   286.947 [  32.672], Avg:   188.409 (1.000) <0-00:46:41> ({'r_t':  1246.5054, 'eps':     1.0000, 'critic_loss':    27.1084, 'actor_loss':     0.0663, 'eps_e':     1.0000})
Step:  224000, Reward:   285.333 [  18.889], Avg:   188.840 (1.000) <0-00:46:51> ({'r_t':  1245.1593, 'eps':     1.0000, 'critic_loss':    23.3580, 'actor_loss':     0.2901, 'eps_e':     1.0000})
Step:  225000, Reward:   288.693 [  15.898], Avg:   189.282 (1.000) <0-00:47:01> ({'r_t':  1290.9892, 'eps':     1.0000, 'critic_loss':    21.5735, 'actor_loss':     0.2245, 'eps_e':     1.0000})
Step:  226000, Reward:   285.162 [  22.035], Avg:   189.704 (1.000) <0-00:47:13> ({'r_t':  1360.6155, 'eps':     1.0000, 'critic_loss':    13.4254, 'actor_loss':     0.3305, 'eps_e':     1.0000})
Step:  227000, Reward:   252.549 [  86.001], Avg:   189.980 (1.000) <0-00:47:23> ({'r_t':  1385.6753, 'eps':     1.0000, 'critic_loss':    31.3635, 'actor_loss':     0.5824, 'eps_e':     1.0000})
Step:  228000, Reward:   283.272 [  16.217], Avg:   190.387 (1.000) <0-00:47:34> ({'r_t':  1196.6544, 'eps':     1.0000, 'critic_loss':    23.8146, 'actor_loss':     0.4992, 'eps_e':     1.0000})
Step:  229000, Reward:   245.695 [  82.826], Avg:   190.628 (1.000) <0-00:47:44> ({'r_t':  1253.7630, 'eps':     1.0000, 'critic_loss':    57.2233, 'actor_loss':     1.0898, 'eps_e':     1.0000})
Step:  230000, Reward:   292.202 [  17.537], Avg:   191.068 (1.000) <0-00:47:55> ({'r_t':  1333.8727, 'eps':     1.0000, 'critic_loss':    28.4009, 'actor_loss':    -0.4739, 'eps_e':     1.0000})
Step:  231000, Reward:   267.044 [  66.481], Avg:   191.395 (1.000) <0-00:48:06> ({'r_t':  1374.9590, 'eps':     1.0000, 'critic_loss':    20.3869, 'actor_loss':    -0.4829, 'eps_e':     1.0000})
Step:  232000, Reward:   284.979 [  17.548], Avg:   191.797 (1.000) <0-00:48:17> ({'r_t':  1453.6728, 'eps':     1.0000, 'critic_loss':    23.6467, 'actor_loss':     0.6839, 'eps_e':     1.0000})
Step:  233000, Reward:   286.400 [  20.230], Avg:   192.201 (1.000) <0-00:48:27> ({'r_t':  1364.5080, 'eps':     1.0000, 'critic_loss':    15.6098, 'actor_loss':     0.3444, 'eps_e':     1.0000})
Step:  234000, Reward:   291.121 [  22.953], Avg:   192.622 (1.000) <0-00:48:37> ({'r_t':  1413.7323, 'eps':     1.0000, 'critic_loss':    17.5756, 'actor_loss':     0.4814, 'eps_e':     1.0000})
Step:  235000, Reward:   291.695 [  22.776], Avg:   193.042 (1.000) <0-00:48:50> ({'r_t':  1322.3620, 'eps':     1.0000, 'critic_loss':    23.5534, 'actor_loss':     0.9047, 'eps_e':     1.0000})
Step:  236000, Reward:   281.663 [  24.337], Avg:   193.416 (1.000) <0-00:49:00> ({'r_t':  1364.9048, 'eps':     1.0000, 'critic_loss':    13.5125, 'actor_loss':     0.2268, 'eps_e':     1.0000})
Step:  237000, Reward:   284.613 [  13.475], Avg:   193.799 (1.000) <0-00:49:11> ({'r_t':  1469.4177, 'eps':     1.0000, 'critic_loss':    15.0975, 'actor_loss':     0.0220, 'eps_e':     1.0000})
Step:  238000, Reward:   284.275 [  18.049], Avg:   194.177 (1.000) <0-00:49:22> ({'r_t':  1326.1560, 'eps':     1.0000, 'critic_loss':    24.9480, 'actor_loss':     1.0036, 'eps_e':     1.0000})
Step:  239000, Reward:   277.335 [  20.870], Avg:   194.524 (1.000) <0-00:49:31> ({'r_t':  1213.9358, 'eps':     1.0000, 'critic_loss':    44.1096, 'actor_loss':     1.1206, 'eps_e':     1.0000})
Step:  240000, Reward:   290.335 [  13.826], Avg:   194.921 (1.000) <0-00:49:44> ({'r_t':  1310.7104, 'eps':     1.0000, 'critic_loss':    45.3766, 'actor_loss':     0.0959, 'eps_e':     1.0000})
Step:  241000, Reward:   284.876 [  20.542], Avg:   195.293 (1.000) <0-00:49:54> ({'r_t':  1364.1576, 'eps':     1.0000, 'critic_loss':    29.3598, 'actor_loss':    -0.3162, 'eps_e':     1.0000})
Step:  242000, Reward:   260.403 [  65.818], Avg:   195.561 (1.000) <0-00:50:05> ({'r_t':  1341.4076, 'eps':     1.0000, 'critic_loss':    37.2144, 'actor_loss':     0.9760, 'eps_e':     1.0000})
Step:  243000, Reward:   282.716 [  19.864], Avg:   195.918 (1.000) <0-00:50:17> ({'r_t':  1088.7653, 'eps':     1.0000, 'critic_loss':    26.7942, 'actor_loss':     0.6834, 'eps_e':     1.0000})
Step:  244000, Reward:   267.403 [  53.540], Avg:   196.210 (1.000) <0-00:50:27> ({'r_t':  1394.6342, 'eps':     1.0000, 'critic_loss':    31.0985, 'actor_loss':     0.2799, 'eps_e':     1.0000})
Step:  245000, Reward:   289.535 [  16.161], Avg:   196.589 (1.000) <0-00:50:38> ({'r_t':  1268.0957, 'eps':     1.0000, 'critic_loss':    22.0555, 'actor_loss':     0.4889, 'eps_e':     1.0000})
Step:  246000, Reward:   287.333 [  24.171], Avg:   196.957 (1.000) <0-00:50:48> ({'r_t':  1432.9533, 'eps':     1.0000, 'critic_loss':    15.2017, 'actor_loss':    -0.2558, 'eps_e':     1.0000})
Step:  247000, Reward:   277.718 [  19.541], Avg:   197.282 (1.000) <0-00:51:00> ({'r_t':  1425.8605, 'eps':     1.0000, 'critic_loss':    14.3516, 'actor_loss':    -0.1385, 'eps_e':     1.0000})
Step:  248000, Reward:   280.425 [  14.440], Avg:   197.616 (1.000) <0-00:51:09> ({'r_t':  1321.2646, 'eps':     1.0000, 'critic_loss':    62.2134, 'actor_loss':     1.1182, 'eps_e':     1.0000})
Step:  249000, Reward:   253.554 [  85.434], Avg:   197.840 (1.000) <0-00:51:20> ({'r_t':  1312.2835, 'eps':     1.0000, 'critic_loss':    43.0236, 'actor_loss':    -0.3326, 'eps_e':     1.0000})
Step:  250000, Reward:   278.779 [  17.139], Avg:   198.163 (1.000) <0-00:51:30> ({'r_t':  1539.4084, 'eps':     1.0000, 'critic_loss':    34.6267, 'actor_loss':     0.2995, 'eps_e':     1.0000})
Step:  251000, Reward:   284.723 [  22.082], Avg:   198.506 (1.000) <0-00:51:42> ({'r_t':  1229.7008, 'eps':     1.0000, 'critic_loss':    17.2288, 'actor_loss':     0.3842, 'eps_e':     1.0000})
Step:  252000, Reward:   265.868 [  62.824], Avg:   198.772 (1.000) <0-00:51:54> ({'r_t':  1553.3545, 'eps':     1.0000, 'critic_loss':    12.6240, 'actor_loss':    -0.3456, 'eps_e':     1.0000})
Step:  253000, Reward:   290.267 [  17.183], Avg:   199.133 (1.000) <0-00:52:04> ({'r_t':  1502.0844, 'eps':     1.0000, 'critic_loss':    22.1835, 'actor_loss':     0.3372, 'eps_e':     1.0000})
Step:  254000, Reward:   283.510 [  21.708], Avg:   199.463 (1.000) <0-00:52:15> ({'r_t':  1277.9409, 'eps':     1.0000, 'critic_loss':    23.6856, 'actor_loss':     0.8785, 'eps_e':     1.0000})
Step:  255000, Reward:   275.420 [  30.943], Avg:   199.760 (1.000) <0-00:52:29> ({'r_t':  1349.5585, 'eps':     1.0000, 'critic_loss':    44.9599, 'actor_loss':     0.8844, 'eps_e':     1.0000})
Step:  256000, Reward:   286.881 [  20.257], Avg:   200.099 (1.000) <0-00:52:39> ({'r_t':  1288.1193, 'eps':     1.0000, 'critic_loss':    66.6187, 'actor_loss':     1.1259, 'eps_e':     1.0000})
Step:  257000, Reward:   251.463 [  84.132], Avg:   200.298 (1.000) <0-00:52:50> ({'r_t':  1363.7766, 'eps':     1.0000, 'critic_loss':    59.4185, 'actor_loss':     0.4563, 'eps_e':     1.0000})
Step:  258000, Reward:   272.985 [  41.095], Avg:   200.579 (1.000) <0-00:53:01> ({'r_t':  1351.7939, 'eps':     1.0000, 'critic_loss':    30.4459, 'actor_loss':     0.2573, 'eps_e':     1.0000})
Step:  259000, Reward:   272.784 [  33.947], Avg:   200.857 (1.000) <0-00:53:13> ({'r_t':  1251.6490, 'eps':     1.0000, 'critic_loss':    30.9863, 'actor_loss':    -0.2149, 'eps_e':     1.0000})
Step:  260000, Reward:   283.961 [  37.903], Avg:   201.175 (1.000) <0-00:53:27> ({'r_t':   998.9504, 'eps':     1.0000, 'critic_loss':    16.6321, 'actor_loss':     0.5366, 'eps_e':     1.0000})
Step:  261000, Reward:   273.953 [  37.454], Avg:   201.453 (1.000) <0-00:53:38> ({'r_t':  1417.6512, 'eps':     1.0000, 'critic_loss':    19.1956, 'actor_loss':     0.3537, 'eps_e':     1.0000})
Step:  262000, Reward:   252.138 [  74.987], Avg:   201.646 (1.000) <0-00:53:47> ({'r_t':  1382.7442, 'eps':     1.0000, 'critic_loss':    24.7637, 'actor_loss':     0.2865, 'eps_e':     1.0000})
Step:  263000, Reward:   280.009 [  19.970], Avg:   201.942 (1.000) <0-00:54:00> ({'r_t':  1209.1412, 'eps':     1.0000, 'critic_loss':    22.8948, 'actor_loss':     0.4893, 'eps_e':     1.0000})
Step:  264000, Reward:   286.568 [  31.761], Avg:   202.262 (1.000) <0-00:54:13> ({'r_t':  1266.0163, 'eps':     1.0000, 'critic_loss':    53.1949, 'actor_loss':     0.2727, 'eps_e':     1.0000})
Step:  265000, Reward:   279.320 [  40.099], Avg:   202.551 (1.000) <0-00:54:26> ({'r_t':  1267.7107, 'eps':     1.0000, 'critic_loss':    46.3584, 'actor_loss':     0.0025, 'eps_e':     1.0000})
Step:  266000, Reward:   281.031 [  21.770], Avg:   202.845 (1.000) <0-00:54:36> ({'r_t':  1361.3281, 'eps':     1.0000, 'critic_loss':    25.0890, 'actor_loss':    -0.1361, 'eps_e':     1.0000})
Step:  267000, Reward:   270.599 [  33.901], Avg:   203.098 (1.000) <0-00:54:49> ({'r_t':  1251.5646, 'eps':     1.0000, 'critic_loss':    49.7446, 'actor_loss':     0.6501, 'eps_e':     1.0000})
Step:  268000, Reward:   281.728 [  58.802], Avg:   203.390 (1.000) <0-00:54:59> ({'r_t':  1212.9039, 'eps':     1.0000, 'critic_loss':    36.2308, 'actor_loss':     0.2787, 'eps_e':     1.0000})
Step:  269000, Reward:   250.330 [  64.432], Avg:   203.564 (1.000) <0-00:55:12> ({'r_t':  1376.5059, 'eps':     1.0000, 'critic_loss':    23.1126, 'actor_loss':     0.2458, 'eps_e':     1.0000})
Step:  270000, Reward:   282.002 [  34.134], Avg:   203.854 (1.000) <0-00:55:24> ({'r_t':  1510.3921, 'eps':     1.0000, 'critic_loss':    12.4995, 'actor_loss':     0.2704, 'eps_e':     1.0000})
Step:  271000, Reward:   273.014 [  64.704], Avg:   204.108 (1.000) <0-00:55:34> ({'r_t':  1394.7184, 'eps':     1.0000, 'critic_loss':    24.7985, 'actor_loss':     0.5633, 'eps_e':     1.0000})
Step:  272000, Reward:   290.614 [  17.865], Avg:   204.425 (1.000) <0-00:55:42> ({'r_t':  1358.7496, 'eps':     1.0000, 'critic_loss':    30.6524, 'actor_loss':     0.7242, 'eps_e':     1.0000})
Step:  273000, Reward:   268.110 [  64.439], Avg:   204.657 (1.000) <0-00:55:55> ({'r_t':  1496.2581, 'eps':     1.0000, 'critic_loss':    14.6183, 'actor_loss':     0.4455, 'eps_e':     1.0000})
Step:  274000, Reward:   291.008 [  17.009], Avg:   204.971 (1.000) <0-00:56:06> ({'r_t':  1339.6244, 'eps':     1.0000, 'critic_loss':    33.2497, 'actor_loss':     0.8969, 'eps_e':     1.0000})
Step:  275000, Reward:   281.862 [  16.520], Avg:   205.250 (1.000) <0-00:56:13> ({'r_t':  1477.4642, 'eps':     1.0000, 'critic_loss':    39.7381, 'actor_loss':    -0.4913, 'eps_e':     1.0000})
Step:  276000, Reward:   283.925 [  29.249], Avg:   205.534 (1.000) <0-00:56:30> ({'r_t':  1431.7063, 'eps':     1.0000, 'critic_loss':    22.6979, 'actor_loss':     0.0616, 'eps_e':     1.0000})
Step:  277000, Reward:   280.104 [  15.406], Avg:   205.802 (1.000) <0-00:56:40> ({'r_t':  1520.4277, 'eps':     1.0000, 'critic_loss':    17.0042, 'actor_loss':     0.2977, 'eps_e':     1.0000})
Step:  278000, Reward:   282.745 [  19.728], Avg:   206.078 (1.000) <0-00:56:50> ({'r_t':  1532.5662, 'eps':     1.0000, 'critic_loss':    21.8246, 'actor_loss':     0.2894, 'eps_e':     1.0000})
Step:  279000, Reward:   292.940 [  20.387], Avg:   206.388 (1.000) <0-00:57:01> ({'r_t':  1400.5621, 'eps':     1.0000, 'critic_loss':    26.6876, 'actor_loss':    -0.1219, 'eps_e':     1.0000})
Step:  280000, Reward:   278.177 [  38.821], Avg:   206.644 (1.000) <0-00:57:12> ({'r_t':  1518.2100, 'eps':     1.0000, 'critic_loss':    14.3804, 'actor_loss':     0.1176, 'eps_e':     1.0000})
Step:  281000, Reward:   286.974 [  13.174], Avg:   206.928 (1.000) <0-00:57:23> ({'r_t':  1449.7269, 'eps':     1.0000, 'critic_loss':    28.6298, 'actor_loss':     0.6519, 'eps_e':     1.0000})
Step:  282000, Reward:   280.758 [  37.124], Avg:   207.189 (1.000) <0-00:57:34> ({'r_t':  1570.4362, 'eps':     1.0000, 'critic_loss':    29.7253, 'actor_loss':     0.2607, 'eps_e':     1.0000})
Step:  283000, Reward:   266.770 [  61.326], Avg:   207.399 (1.000) <0-00:57:46> ({'r_t':  1419.2953, 'eps':     1.0000, 'critic_loss':    14.9664, 'actor_loss':     0.5048, 'eps_e':     1.0000})
Step:  284000, Reward:   263.245 [  82.795], Avg:   207.595 (1.000) <0-00:57:57> ({'r_t':  1331.3444, 'eps':     1.0000, 'critic_loss':    37.4159, 'actor_loss':     0.5780, 'eps_e':     1.0000})
Step:  285000, Reward:   273.607 [  37.750], Avg:   207.826 (1.000) <0-00:58:09> ({'r_t':  1376.1441, 'eps':     1.0000, 'critic_loss':    60.2985, 'actor_loss':     0.1764, 'eps_e':     1.0000})
Step:  286000, Reward:   277.938 [  61.953], Avg:   208.070 (1.000) <0-00:58:19> ({'r_t':  1214.9424, 'eps':     1.0000, 'critic_loss':    40.1711, 'actor_loss':     0.1609, 'eps_e':     1.0000})
Step:  287000, Reward:   263.991 [  67.238], Avg:   208.264 (1.000) <0-00:58:31> ({'r_t':  1475.6554, 'eps':     1.0000, 'critic_loss':    50.0497, 'actor_loss':    -0.0742, 'eps_e':     1.0000})
Step:  288000, Reward:   270.704 [  61.648], Avg:   208.480 (1.000) <0-00:58:43> ({'r_t':  1290.1948, 'eps':     1.0000, 'critic_loss':    28.7854, 'actor_loss':    -0.1762, 'eps_e':     1.0000})
Step:  289000, Reward:   270.272 [  62.467], Avg:   208.693 (1.000) <0-00:58:56> ({'r_t':  1482.3289, 'eps':     1.0000, 'critic_loss':    15.5714, 'actor_loss':     0.1466, 'eps_e':     1.0000})
Step:  290000, Reward:   263.748 [  50.406], Avg:   208.883 (1.000) <0-00:59:08> ({'r_t':  1530.9129, 'eps':     1.0000, 'critic_loss':    13.6800, 'actor_loss':     0.1191, 'eps_e':     1.0000})
Step:  291000, Reward:   284.733 [  16.704], Avg:   209.142 (1.000) <0-00:59:19> ({'r_t':  1451.5429, 'eps':     1.0000, 'critic_loss':    13.5476, 'actor_loss':     0.2839, 'eps_e':     1.0000})
Step:  292000, Reward:   287.706 [  14.732], Avg:   209.411 (1.000) <0-00:59:30> ({'r_t':  1386.3374, 'eps':     1.0000, 'critic_loss':    39.1732, 'actor_loss':     0.6076, 'eps_e':     1.0000})
Step:  293000, Reward:   285.418 [  17.926], Avg:   209.669 (1.000) <0-00:59:41> ({'r_t':  1434.8137, 'eps':     1.0000, 'critic_loss':    54.5166, 'actor_loss':     0.5894, 'eps_e':     1.0000})
Step:  294000, Reward:   266.123 [  64.126], Avg:   209.860 (1.000) <0-00:59:51> ({'r_t':  1214.2805, 'eps':     1.0000, 'critic_loss':    27.4401, 'actor_loss':     0.5795, 'eps_e':     1.0000})
Step:  295000, Reward:   272.213 [  42.766], Avg:   210.071 (1.000) <0-01:00:03> ({'r_t':  1467.7244, 'eps':     1.0000, 'critic_loss':    52.6981, 'actor_loss':     1.0097, 'eps_e':     1.0000})
Step:  296000, Reward:   273.057 [  27.877], Avg:   210.283 (1.000) <0-01:00:16> ({'r_t':  1416.4621, 'eps':     1.0000, 'critic_loss':    17.0385, 'actor_loss':     0.0114, 'eps_e':     1.0000})
Step:  297000, Reward:   250.311 [  77.327], Avg:   210.418 (1.000) <0-01:00:26> ({'r_t':  1292.7982, 'eps':     1.0000, 'critic_loss':    44.6038, 'actor_loss':     0.8694, 'eps_e':     1.0000})
Step:  298000, Reward:   296.209 [  11.973], Avg:   210.704 (1.000) <0-01:00:38> ({'r_t':  1500.1283, 'eps':     1.0000, 'critic_loss':    21.5444, 'actor_loss':     0.0999, 'eps_e':     1.0000})
Step:  299000, Reward:   277.404 [  59.539], Avg:   210.927 (1.000) <0-01:00:47> ({'r_t':  1614.4596, 'eps':     1.0000, 'critic_loss':    13.9218, 'actor_loss':    -0.1052, 'eps_e':     1.0000})
Step:  300000, Reward:   274.601 [  67.281], Avg:   211.138 (1.000) <0-01:00:58> ({'r_t':  1537.5796, 'eps':     1.0000, 'critic_loss':    14.5621, 'actor_loss':     0.4297, 'eps_e':     1.0000})
Step:  301000, Reward:   275.772 [  40.124], Avg:   211.352 (1.000) <0-01:01:08> ({'r_t':  1516.8396, 'eps':     1.0000, 'critic_loss':    20.8914, 'actor_loss':     0.0638, 'eps_e':     1.0000})
Step:  302000, Reward:   279.484 [  35.726], Avg:   211.577 (1.000) <0-01:01:23> ({'r_t':  1497.5761, 'eps':     1.0000, 'critic_loss':    24.0221, 'actor_loss':     0.3688, 'eps_e':     1.0000})
Step:  303000, Reward:   278.478 [  32.526], Avg:   211.797 (1.000) <0-01:01:35> ({'r_t':  1373.3107, 'eps':     1.0000, 'critic_loss':    12.2174, 'actor_loss':     0.2378, 'eps_e':     1.0000})
Step:  304000, Reward:   286.194 [  14.766], Avg:   212.041 (1.000) <0-01:01:46> ({'r_t':  1281.3299, 'eps':     1.0000, 'critic_loss':    18.5892, 'actor_loss':     0.5642, 'eps_e':     1.0000})
Step:  305000, Reward:   282.273 [  37.050], Avg:   212.271 (1.000) <0-01:01:59> ({'r_t':  1296.2210, 'eps':     1.0000, 'critic_loss':    17.2162, 'actor_loss':     0.1456, 'eps_e':     1.0000})
Step:  306000, Reward:   281.156 [  14.721], Avg:   212.495 (1.000) <0-01:02:09> ({'r_t':  1509.2280, 'eps':     1.0000, 'critic_loss':    13.5062, 'actor_loss':    -0.1936, 'eps_e':     1.0000})
Step:  307000, Reward:   283.140 [  19.072], Avg:   212.724 (1.000) <0-01:02:20> ({'r_t':  1502.6660, 'eps':     1.0000, 'critic_loss':    14.5046, 'actor_loss':    -0.0529, 'eps_e':     1.0000})
Step:  308000, Reward:   287.556 [  17.358], Avg:   212.967 (1.000) <0-01:02:30> ({'r_t':  1467.8777, 'eps':     1.0000, 'critic_loss':    19.6877, 'actor_loss':     0.0717, 'eps_e':     1.0000})
Step:  309000, Reward:   282.450 [  18.846], Avg:   213.191 (1.000) <0-01:02:41> ({'r_t':  1412.4830, 'eps':     1.0000, 'critic_loss':    16.1361, 'actor_loss':    -0.0400, 'eps_e':     1.0000})
Step:  310000, Reward:   282.556 [  19.428], Avg:   213.414 (1.000) <0-01:02:52> ({'r_t':  1530.5947, 'eps':     1.0000, 'critic_loss':    13.8680, 'actor_loss':     0.1630, 'eps_e':     1.0000})
Step:  311000, Reward:   293.541 [  17.461], Avg:   213.671 (1.000) <0-01:03:02> ({'r_t':  1395.5620, 'eps':     1.0000, 'critic_loss':    20.1708, 'actor_loss':     0.5216, 'eps_e':     1.0000})
Step:  312000, Reward:   282.567 [  17.697], Avg:   213.891 (1.000) <0-01:03:13> ({'r_t':  1511.9657, 'eps':     1.0000, 'critic_loss':    14.9572, 'actor_loss':     0.0350, 'eps_e':     1.0000})
Step:  313000, Reward:   265.316 [  65.039], Avg:   214.055 (1.000) <0-01:03:25> ({'r_t':  1415.3249, 'eps':     1.0000, 'critic_loss':    15.1907, 'actor_loss':     0.4839, 'eps_e':     1.0000})
Step:  314000, Reward:   287.056 [  20.041], Avg:   214.286 (1.000) <0-01:03:36> ({'r_t':  1290.0579, 'eps':     1.0000, 'critic_loss':    27.2916, 'actor_loss':     0.8101, 'eps_e':     1.0000})
Step:  315000, Reward:   269.169 [  51.902], Avg:   214.460 (1.000) <0-01:03:47> ({'r_t':  1406.5026, 'eps':     1.0000, 'critic_loss':    22.0523, 'actor_loss':     0.8399, 'eps_e':     1.0000})
Step:  316000, Reward:   279.291 [  29.757], Avg:   214.664 (1.000) <0-01:04:01> ({'r_t':  1420.3651, 'eps':     1.0000, 'critic_loss':    15.2427, 'actor_loss':     0.2041, 'eps_e':     1.0000})
Step:  317000, Reward:   283.426 [  14.459], Avg:   214.881 (1.000) <0-01:04:11> ({'r_t':  1350.0892, 'eps':     1.0000, 'critic_loss':    19.4076, 'actor_loss':     0.5389, 'eps_e':     1.0000})
Step:  318000, Reward:   280.909 [  19.453], Avg:   215.088 (1.000) <0-01:04:22> ({'r_t':  1396.2355, 'eps':     1.0000, 'critic_loss':    22.3741, 'actor_loss':     0.3977, 'eps_e':     1.0000})
Step:  319000, Reward:   280.100 [  20.967], Avg:   215.291 (1.000) <0-01:04:33> ({'r_t':  1405.2278, 'eps':     1.0000, 'critic_loss':    24.1959, 'actor_loss':     0.8295, 'eps_e':     1.0000})
Step:  320000, Reward:   271.493 [  61.494], Avg:   215.466 (1.000) <0-01:04:43> ({'r_t':  1514.0315, 'eps':     1.0000, 'critic_loss':    17.4607, 'actor_loss':     0.1848, 'eps_e':     1.0000})
Step:  321000, Reward:   293.363 [  16.561], Avg:   215.708 (1.000) <0-01:04:54> ({'r_t':  1407.0607, 'eps':     1.0000, 'critic_loss':    24.9324, 'actor_loss':     0.7233, 'eps_e':     1.0000})
Step:  322000, Reward:   284.182 [  14.940], Avg:   215.920 (1.000) <0-01:05:02> ({'r_t':  1430.2081, 'eps':     1.0000, 'critic_loss':    28.2984, 'actor_loss':     0.1413, 'eps_e':     1.0000})
Step:  323000, Reward:   276.717 [  37.680], Avg:   216.107 (1.000) <0-01:05:17> ({'r_t':  1294.5399, 'eps':     1.0000, 'critic_loss':    28.0087, 'actor_loss':     0.0003, 'eps_e':     1.0000})
Step:  324000, Reward:   284.574 [  16.431], Avg:   216.318 (1.000) <0-01:05:25> ({'r_t':  1269.1092, 'eps':     1.0000, 'critic_loss':    29.8419, 'actor_loss':    -0.1768, 'eps_e':     1.0000})
Step:  325000, Reward:   269.503 [  57.447], Avg:   216.481 (1.000) <0-01:05:38> ({'r_t':  1502.9702, 'eps':     1.0000, 'critic_loss':    15.2757, 'actor_loss':     0.1611, 'eps_e':     1.0000})
Step:  326000, Reward:   264.363 [  59.978], Avg:   216.628 (1.000) <0-01:05:51> ({'r_t':  1513.4651, 'eps':     1.0000, 'critic_loss':    30.3654, 'actor_loss':     0.2520, 'eps_e':     1.0000})
Step:  327000, Reward:   272.666 [  64.197], Avg:   216.799 (1.000) <0-01:06:01> ({'r_t':  1271.8148, 'eps':     1.0000, 'critic_loss':    47.7080, 'actor_loss':    -0.0407, 'eps_e':     1.0000})
Step:  328000, Reward:   272.164 [  33.944], Avg:   216.967 (1.000) <0-01:06:15> ({'r_t':  1368.2164, 'eps':     1.0000, 'critic_loss':    29.7966, 'actor_loss':    -0.3274, 'eps_e':     1.0000})
Step:  329000, Reward:   285.661 [  14.322], Avg:   217.175 (1.000) <0-01:06:27> ({'r_t':  1416.3548, 'eps':     1.0000, 'critic_loss':    28.5721, 'actor_loss':     0.4502, 'eps_e':     1.0000})
Step:  330000, Reward:   270.948 [  41.808], Avg:   217.337 (1.000) <0-01:06:40> ({'r_t':  1212.7808, 'eps':     1.0000, 'critic_loss':    23.1252, 'actor_loss':     0.1387, 'eps_e':     1.0000})
Step:  331000, Reward:   281.881 [  19.096], Avg:   217.532 (1.000) <0-01:06:50> ({'r_t':  1318.4189, 'eps':     1.0000, 'critic_loss':    28.5164, 'actor_loss':    -0.0447, 'eps_e':     1.0000})
Step:  332000, Reward:   292.108 [  15.143], Avg:   217.756 (1.000) <0-01:07:01> ({'r_t':  1160.6266, 'eps':     1.0000, 'critic_loss':    23.1741, 'actor_loss':     0.1884, 'eps_e':     1.0000})
Step:  333000, Reward:   264.984 [  62.439], Avg:   217.897 (1.000) <0-01:07:14> ({'r_t':  1470.3481, 'eps':     1.0000, 'critic_loss':    28.5405, 'actor_loss':    -0.2042, 'eps_e':     1.0000})
Step:  334000, Reward:   268.853 [  74.750], Avg:   218.049 (1.000) <0-01:07:25> ({'r_t':  1257.1290, 'eps':     1.0000, 'critic_loss':    17.2106, 'actor_loss':    -0.2499, 'eps_e':     1.0000})
Step:  335000, Reward:   271.822 [  41.290], Avg:   218.209 (1.000) <0-01:07:38> ({'r_t':  1376.3588, 'eps':     1.0000, 'critic_loss':    32.3225, 'actor_loss':     0.8130, 'eps_e':     1.0000})
Step:  336000, Reward:   282.803 [  13.446], Avg:   218.401 (1.000) <0-01:07:48> ({'r_t':  1485.5113, 'eps':     1.0000, 'critic_loss':    24.0200, 'actor_loss':     0.5350, 'eps_e':     1.0000})
Step:  337000, Reward:   289.324 [  15.286], Avg:   218.611 (1.000) <0-01:07:59> ({'r_t':  1276.0775, 'eps':     1.0000, 'critic_loss':    20.9137, 'actor_loss':     0.4904, 'eps_e':     1.0000})
Step:  338000, Reward:   264.717 [  84.151], Avg:   218.747 (1.000) <0-01:08:08> ({'r_t':  1325.8049, 'eps':     1.0000, 'critic_loss':    15.4194, 'actor_loss':     0.0914, 'eps_e':     1.0000})
Step:  339000, Reward:   289.329 [  14.691], Avg:   218.954 (1.000) <0-01:08:20> ({'r_t':  1500.5467, 'eps':     1.0000, 'critic_loss':    43.4149, 'actor_loss':    -0.2644, 'eps_e':     1.0000})
Step:  340000, Reward:   284.390 [  21.087], Avg:   219.146 (1.000) <0-01:08:30> ({'r_t':  1407.6526, 'eps':     1.0000, 'critic_loss':    28.4981, 'actor_loss':     0.1005, 'eps_e':     1.0000})
Step:  341000, Reward:   264.185 [  61.465], Avg:   219.278 (1.000) <0-01:08:42> ({'r_t':  1442.7946, 'eps':     1.0000, 'critic_loss':    26.8832, 'actor_loss':     0.0255, 'eps_e':     1.0000})
Step:  342000, Reward:   290.625 [  18.731], Avg:   219.486 (1.000) <0-01:08:53> ({'r_t':  1438.8091, 'eps':     1.0000, 'critic_loss':    29.9081, 'actor_loss':     0.4732, 'eps_e':     1.0000})
Step:  343000, Reward:   272.828 [  37.176], Avg:   219.641 (1.000) <0-01:09:06> ({'r_t':  1434.3768, 'eps':     1.0000, 'critic_loss':    27.7101, 'actor_loss':     0.3941, 'eps_e':     1.0000})
Step:  344000, Reward:   283.737 [  18.950], Avg:   219.827 (1.000) <0-01:09:17> ({'r_t':  1336.6020, 'eps':     1.0000, 'critic_loss':    40.6613, 'actor_loss':     0.5339, 'eps_e':     1.0000})
Step:  345000, Reward:   283.766 [  19.005], Avg:   220.012 (1.000) <0-01:09:27> ({'r_t':  1505.5996, 'eps':     1.0000, 'critic_loss':    17.5837, 'actor_loss':    -0.0737, 'eps_e':     1.0000})
Step:  346000, Reward:   263.887 [  66.469], Avg:   220.138 (1.000) <0-01:09:38> ({'r_t':  1511.0798, 'eps':     1.0000, 'critic_loss':    21.3569, 'actor_loss':     0.1230, 'eps_e':     1.0000})
Step:  347000, Reward:   284.447 [  24.896], Avg:   220.323 (1.000) <0-01:09:51> ({'r_t':  1291.9865, 'eps':     1.0000, 'critic_loss':    25.7567, 'actor_loss':     0.5289, 'eps_e':     1.0000})
Step:  348000, Reward:   269.802 [  55.960], Avg:   220.465 (1.000) <0-01:10:02> ({'r_t':  1531.2597, 'eps':     1.0000, 'critic_loss':    21.1881, 'actor_loss':     0.4421, 'eps_e':     1.0000})
Step:  349000, Reward:   277.008 [  32.628], Avg:   220.626 (1.000) <0-01:10:13> ({'r_t':  1465.8933, 'eps':     1.0000, 'critic_loss':    21.4198, 'actor_loss':     0.7238, 'eps_e':     1.0000})
Step:  350000, Reward:   266.754 [  68.040], Avg:   220.758 (1.000) <0-01:10:26> ({'r_t':  1610.9774, 'eps':     1.0000, 'critic_loss':    12.1890, 'actor_loss':     0.0821, 'eps_e':     1.0000})
Step:  351000, Reward:   287.124 [  14.483], Avg:   220.946 (1.000) <0-01:10:37> ({'r_t':  1381.8024, 'eps':     1.0000, 'critic_loss':    22.1202, 'actor_loss':     0.6890, 'eps_e':     1.0000})
Step:  352000, Reward:   279.852 [  59.619], Avg:   221.113 (1.000) <0-01:10:46> ({'r_t':  1368.7165, 'eps':     1.0000, 'critic_loss':    18.4226, 'actor_loss':     0.4380, 'eps_e':     1.0000})
Step:  353000, Reward:   279.218 [  16.167], Avg:   221.277 (1.000) <0-01:10:57> ({'r_t':  1478.3063, 'eps':     1.0000, 'critic_loss':    17.4102, 'actor_loss':     0.4339, 'eps_e':     1.0000})
Step:  354000, Reward:   282.248 [  20.669], Avg:   221.449 (1.000) <0-01:11:08> ({'r_t':  1482.2295, 'eps':     1.0000, 'critic_loss':    26.2836, 'actor_loss':     0.4214, 'eps_e':     1.0000})
Step:  355000, Reward:   276.550 [  60.747], Avg:   221.604 (1.000) <0-01:11:18> ({'r_t':  1571.3693, 'eps':     1.0000, 'critic_loss':    45.1623, 'actor_loss':     0.4418, 'eps_e':     1.0000})
Step:  356000, Reward:   280.114 [  15.909], Avg:   221.768 (1.000) <0-01:11:29> ({'r_t':  1498.4897, 'eps':     1.0000, 'critic_loss':    45.6286, 'actor_loss':     0.9310, 'eps_e':     1.0000})
Step:  357000, Reward:   281.684 [  28.190], Avg:   221.935 (1.000) <0-01:11:41> ({'r_t':  1518.5683, 'eps':     1.0000, 'critic_loss':    49.1040, 'actor_loss':     0.5306, 'eps_e':     1.0000})
Step:  358000, Reward:   281.449 [  21.860], Avg:   222.101 (1.000) <0-01:11:50> ({'r_t':  1305.5351, 'eps':     1.0000, 'critic_loss':    43.5556, 'actor_loss':     1.4547, 'eps_e':     1.0000})
Step:  359000, Reward:   276.791 [  17.617], Avg:   222.253 (1.000) <0-01:12:02> ({'r_t':  1276.9325, 'eps':     1.0000, 'critic_loss':    27.1989, 'actor_loss':     0.7932, 'eps_e':     1.0000})
Step:  360000, Reward:   281.561 [  39.266], Avg:   222.417 (1.000) <0-01:12:14> ({'r_t':  1440.9241, 'eps':     1.0000, 'critic_loss':    33.7673, 'actor_loss':     0.2745, 'eps_e':     1.0000})
Step:  361000, Reward:   259.798 [  84.078], Avg:   222.520 (1.000) <0-01:12:22> ({'r_t':  1426.2167, 'eps':     1.0000, 'critic_loss':    63.2701, 'actor_loss':     0.9109, 'eps_e':     1.0000})
Step:  362000, Reward:   258.362 [  73.617], Avg:   222.619 (1.000) <0-01:12:35> ({'r_t':  1454.3757, 'eps':     1.0000, 'critic_loss':    28.4649, 'actor_loss':     0.2223, 'eps_e':     1.0000})
Step:  363000, Reward:   260.710 [  61.438], Avg:   222.724 (1.000) <0-01:12:50> ({'r_t':  1502.4164, 'eps':     1.0000, 'critic_loss':    43.0177, 'actor_loss':     0.0753, 'eps_e':     1.0000})
Step:  364000, Reward:   289.910 [  16.472], Avg:   222.908 (1.000) <0-01:13:00> ({'r_t':  1507.4413, 'eps':     1.0000, 'critic_loss':    18.1889, 'actor_loss':     0.6649, 'eps_e':     1.0000})
Step:  365000, Reward:   268.083 [  60.045], Avg:   223.031 (1.000) <0-01:13:10> ({'r_t':  1580.7739, 'eps':     1.0000, 'critic_loss':    26.2217, 'actor_loss':     0.8241, 'eps_e':     1.0000})
Step:  366000, Reward:   273.876 [  29.940], Avg:   223.170 (1.000) <0-01:13:22> ({'r_t':  1447.5475, 'eps':     1.0000, 'critic_loss':    22.6393, 'actor_loss':     0.4710, 'eps_e':     1.0000})
Step:  367000, Reward:   252.408 [  58.084], Avg:   223.249 (1.000) <0-01:13:32> ({'r_t':  1443.4747, 'eps':     1.0000, 'critic_loss':    31.8895, 'actor_loss':     0.9867, 'eps_e':     1.0000})
Step:  368000, Reward:   291.614 [  17.034], Avg:   223.434 (1.000) <0-01:13:46> ({'r_t':  1445.5722, 'eps':     1.0000, 'critic_loss':    26.4874, 'actor_loss':     0.1014, 'eps_e':     1.0000})
Step:  369000, Reward:   282.883 [  12.973], Avg:   223.595 (1.000) <0-01:13:55> ({'r_t':  1554.2436, 'eps':     1.0000, 'critic_loss':    37.9720, 'actor_loss':    -0.1798, 'eps_e':     1.0000})
Step:  370000, Reward:   261.282 [  66.888], Avg:   223.697 (1.000) <0-01:14:09> ({'r_t':  1430.6760, 'eps':     1.0000, 'critic_loss':    15.9315, 'actor_loss':    -0.0438, 'eps_e':     1.0000})
Step:  371000, Reward:   231.780 [ 100.293], Avg:   223.718 (1.000) <0-01:14:21> ({'r_t':  1472.6569, 'eps':     1.0000, 'critic_loss':    18.9470, 'actor_loss':     0.2163, 'eps_e':     1.0000})
Step:  372000, Reward:   276.954 [  55.944], Avg:   223.861 (1.000) <0-01:14:32> ({'r_t':  1485.1847, 'eps':     1.0000, 'critic_loss':    47.7384, 'actor_loss':     0.2482, 'eps_e':     1.0000})
Step:  373000, Reward:   269.462 [  66.231], Avg:   223.983 (1.000) <0-01:14:42> ({'r_t':  1395.5853, 'eps':     1.0000, 'critic_loss':    48.5223, 'actor_loss':     0.6871, 'eps_e':     1.0000})
Step:  374000, Reward:   286.511 [  21.885], Avg:   224.150 (1.000) <0-01:14:52> ({'r_t':  1438.5535, 'eps':     1.0000, 'critic_loss':    29.3970, 'actor_loss':     0.6702, 'eps_e':     1.0000})
Step:  375000, Reward:   288.245 [  21.389], Avg:   224.320 (1.000) <0-01:15:04> ({'r_t':  1473.0626, 'eps':     1.0000, 'critic_loss':    31.1872, 'actor_loss':     0.1686, 'eps_e':     1.0000})
Step:  376000, Reward:   278.583 [  36.718], Avg:   224.464 (1.000) <0-01:15:16> ({'r_t':  1556.6473, 'eps':     1.0000, 'critic_loss':    22.5146, 'actor_loss':     0.3691, 'eps_e':     1.0000})
Step:  377000, Reward:   280.115 [  34.254], Avg:   224.611 (1.000) <0-01:15:29> ({'r_t':  1473.8409, 'eps':     1.0000, 'critic_loss':    32.9449, 'actor_loss':     0.0690, 'eps_e':     1.0000})
Step:  378000, Reward:   267.885 [  43.750], Avg:   224.726 (1.000) <0-01:15:41> ({'r_t':  1522.3411, 'eps':     1.0000, 'critic_loss':    19.2694, 'actor_loss':    -0.2470, 'eps_e':     1.0000})
Step:  379000, Reward:   292.329 [  21.478], Avg:   224.904 (1.000) <0-01:15:51> ({'r_t':  1397.4187, 'eps':     1.0000, 'critic_loss':    12.7461, 'actor_loss':     0.1343, 'eps_e':     1.0000})
Step:  380000, Reward:   283.823 [  19.537], Avg:   225.058 (1.000) <0-01:16:02> ({'r_t':  1404.8730, 'eps':     1.0000, 'critic_loss':    19.5717, 'actor_loss':     0.2603, 'eps_e':     1.0000})
Step:  381000, Reward:   264.662 [  76.315], Avg:   225.162 (1.000) <0-01:16:13> ({'r_t':  1392.0907, 'eps':     1.0000, 'critic_loss':    25.4248, 'actor_loss':     0.4544, 'eps_e':     1.0000})
Step:  382000, Reward:   268.420 [  64.697], Avg:   225.275 (1.000) <0-01:16:25> ({'r_t':  1250.2308, 'eps':     1.0000, 'critic_loss':    15.9617, 'actor_loss':     0.3760, 'eps_e':     1.0000})
Step:  383000, Reward:   294.025 [  13.923], Avg:   225.454 (1.000) <0-01:16:36> ({'r_t':  1303.7682, 'eps':     1.0000, 'critic_loss':    17.1332, 'actor_loss':     0.4197, 'eps_e':     1.0000})
Step:  384000, Reward:   291.389 [  14.299], Avg:   225.625 (1.000) <0-01:16:46> ({'r_t':  1478.3673, 'eps':     1.0000, 'critic_loss':    14.4208, 'actor_loss':     0.1409, 'eps_e':     1.0000})
Step:  385000, Reward:   290.116 [  18.285], Avg:   225.792 (1.000) <0-01:16:56> ({'r_t':  1636.2526, 'eps':     1.0000, 'critic_loss':    16.5107, 'actor_loss':     0.1125, 'eps_e':     1.0000})
Step:  386000, Reward:   267.621 [  67.549], Avg:   225.900 (1.000) <0-01:17:06> ({'r_t':  1654.9094, 'eps':     1.0000, 'critic_loss':    11.6793, 'actor_loss':    -0.1223, 'eps_e':     1.0000})
Step:  387000, Reward:   264.723 [  61.517], Avg:   226.000 (1.000) <0-01:17:19> ({'r_t':  1609.2589, 'eps':     1.0000, 'critic_loss':    16.5533, 'actor_loss':     0.4626, 'eps_e':     1.0000})
Step:  388000, Reward:   256.784 [  63.371], Avg:   226.079 (1.000) <0-01:17:31> ({'r_t':  1403.6048, 'eps':     1.0000, 'critic_loss':    17.7121, 'actor_loss':    -0.1774, 'eps_e':     1.0000})
Step:  389000, Reward:   271.904 [  69.271], Avg:   226.197 (1.000) <0-01:17:40> ({'r_t':  1625.3728, 'eps':     1.0000, 'critic_loss':    27.4912, 'actor_loss':     0.3964, 'eps_e':     1.0000})
Step:  390000, Reward:   289.012 [  13.514], Avg:   226.358 (1.000) <0-01:17:52> ({'r_t':  1515.5707, 'eps':     1.0000, 'critic_loss':    34.0857, 'actor_loss':     0.6507, 'eps_e':     1.0000})
Step:  391000, Reward:   290.476 [  17.610], Avg:   226.521 (1.000) <0-01:18:02> ({'r_t':  1405.3618, 'eps':     1.0000, 'critic_loss':    33.3827, 'actor_loss':     0.7485, 'eps_e':     1.0000})
Step:  392000, Reward:   285.362 [  18.664], Avg:   226.671 (1.000) <0-01:18:13> ({'r_t':  1534.0232, 'eps':     1.0000, 'critic_loss':    13.1470, 'actor_loss':    -0.1986, 'eps_e':     1.0000})
Step:  393000, Reward:   272.699 [  64.073], Avg:   226.788 (1.000) <0-01:18:23> ({'r_t':  1650.0712, 'eps':     1.0000, 'critic_loss':    30.3028, 'actor_loss':     0.4585, 'eps_e':     1.0000})
Step:  394000, Reward:   245.530 [  76.007], Avg:   226.835 (1.000) <0-01:18:33> ({'r_t':  1556.5976, 'eps':     1.0000, 'critic_loss':    25.0077, 'actor_loss':     0.5028, 'eps_e':     1.0000})
Step:  395000, Reward:   264.520 [  64.722], Avg:   226.930 (1.000) <0-01:18:44> ({'r_t':  1477.2412, 'eps':     1.0000, 'critic_loss':    24.6329, 'actor_loss':     0.3109, 'eps_e':     1.0000})
Step:  396000, Reward:   277.508 [  32.580], Avg:   227.058 (1.000) <0-01:18:57> ({'r_t':  1236.2518, 'eps':     1.0000, 'critic_loss':    13.8212, 'actor_loss':     0.4818, 'eps_e':     1.0000})
Step:  397000, Reward:   270.651 [  61.879], Avg:   227.167 (1.000) <0-01:19:09> ({'r_t':  1271.5761, 'eps':     1.0000, 'critic_loss':    34.5613, 'actor_loss':     0.2549, 'eps_e':     1.0000})
Step:  398000, Reward:   268.716 [  61.517], Avg:   227.271 (1.000) <0-01:19:22> ({'r_t':  1218.1292, 'eps':     1.0000, 'critic_loss':    24.5761, 'actor_loss':     0.4153, 'eps_e':     1.0000})
Step:  399000, Reward:   285.932 [  20.919], Avg:   227.418 (1.000) <0-01:19:33> ({'r_t':  1345.1185, 'eps':     1.0000, 'critic_loss':    29.7327, 'actor_loss':     0.6220, 'eps_e':     1.0000})
Step:  400000, Reward:   293.667 [  16.285], Avg:   227.583 (1.000) <0-01:19:43> ({'r_t':  1536.6676, 'eps':     1.0000, 'critic_loss':    19.4169, 'actor_loss':     0.4126, 'eps_e':     1.0000})
Step:  401000, Reward:   261.760 [  66.449], Avg:   227.668 (1.000) <0-01:19:55> ({'r_t':  1391.3529, 'eps':     1.0000, 'critic_loss':    24.0853, 'actor_loss':     0.8392, 'eps_e':     1.0000})
Step:  402000, Reward:   270.822 [  71.786], Avg:   227.775 (1.000) <0-01:20:07> ({'r_t':  1289.4253, 'eps':     1.0000, 'critic_loss':    22.0333, 'actor_loss':     0.4095, 'eps_e':     1.0000})
Step:  403000, Reward:   289.372 [  14.998], Avg:   227.928 (1.000) <0-01:20:17> ({'r_t':  1591.7299, 'eps':     1.0000, 'critic_loss':    40.0415, 'actor_loss':    -0.0068, 'eps_e':     1.0000})
Step:  404000, Reward:   282.234 [  17.880], Avg:   228.062 (1.000) <0-01:20:27> ({'r_t':  1583.2309, 'eps':     1.0000, 'critic_loss':    10.7434, 'actor_loss':    -0.2660, 'eps_e':     1.0000})
Step:  405000, Reward:   291.068 [  13.575], Avg:   228.217 (1.000) <0-01:20:36> ({'r_t':  1553.4600, 'eps':     1.0000, 'critic_loss':    35.0768, 'actor_loss':     0.4766, 'eps_e':     1.0000})
Step:  406000, Reward:   256.942 [  76.195], Avg:   228.288 (1.000) <0-01:20:47> ({'r_t':  1609.3803, 'eps':     1.0000, 'critic_loss':    30.8056, 'actor_loss':     0.5989, 'eps_e':     1.0000})
Step:  407000, Reward:   272.161 [  58.307], Avg:   228.395 (1.000) <0-01:20:58> ({'r_t':  1352.7223, 'eps':     1.0000, 'critic_loss':    31.2345, 'actor_loss':     1.3669, 'eps_e':     1.0000})
Step:  408000, Reward:   250.180 [  85.552], Avg:   228.448 (1.000) <0-01:21:11> ({'r_t':  1418.3939, 'eps':     1.0000, 'critic_loss':    35.0767, 'actor_loss':     0.7821, 'eps_e':     1.0000})
Step:  409000, Reward:   282.860 [  46.190], Avg:   228.581 (1.000) <0-01:21:22> ({'r_t':  1395.6949, 'eps':     1.0000, 'critic_loss':    28.9079, 'actor_loss':     0.0982, 'eps_e':     1.0000})
Step:  410000, Reward:   273.651 [  62.565], Avg:   228.691 (1.000) <0-01:21:33> ({'r_t':  1353.3362, 'eps':     1.0000, 'critic_loss':    32.5815, 'actor_loss':     0.1657, 'eps_e':     1.0000})
Step:  411000, Reward:   260.670 [  68.269], Avg:   228.768 (1.000) <0-01:21:47> ({'r_t':  1172.8362, 'eps':     1.0000, 'critic_loss':    21.7175, 'actor_loss':     0.3743, 'eps_e':     1.0000})
Step:  412000, Reward:   285.721 [  14.899], Avg:   228.906 (1.000) <0-01:21:57> ({'r_t':  1365.9952, 'eps':     1.0000, 'critic_loss':    27.3409, 'actor_loss':     0.2907, 'eps_e':     1.0000})
Step:  413000, Reward:   276.108 [  61.761], Avg:   229.020 (1.000) <0-01:22:08> ({'r_t':  1455.1241, 'eps':     1.0000, 'critic_loss':    18.5140, 'actor_loss':    -0.5592, 'eps_e':     1.0000})
Step:  414000, Reward:   285.537 [  15.728], Avg:   229.157 (1.000) <0-01:22:18> ({'r_t':  1590.0340, 'eps':     1.0000, 'critic_loss':    20.9909, 'actor_loss':     0.1555, 'eps_e':     1.0000})
Step:  415000, Reward:   268.558 [  67.604], Avg:   229.251 (1.000) <0-01:22:29> ({'r_t':  1596.4358, 'eps':     1.0000, 'critic_loss':    20.2071, 'actor_loss':     0.1888, 'eps_e':     1.0000})
Step:  416000, Reward:   271.954 [  62.245], Avg:   229.354 (1.000) <0-01:22:39> ({'r_t':  1580.3110, 'eps':     1.0000, 'critic_loss':    15.1297, 'actor_loss':     0.1311, 'eps_e':     1.0000})
Step:  417000, Reward:   272.764 [  59.277], Avg:   229.458 (1.000) <0-01:22:49> ({'r_t':  1629.0094, 'eps':     1.0000, 'critic_loss':    14.2126, 'actor_loss':     0.1077, 'eps_e':     1.0000})
Step:  418000, Reward:   279.358 [  19.226], Avg:   229.577 (1.000) <0-01:22:59> ({'r_t':  1640.5930, 'eps':     1.0000, 'critic_loss':    18.0785, 'actor_loss':    -0.1296, 'eps_e':     1.0000})
Step:  419000, Reward:   273.442 [  61.685], Avg:   229.681 (1.000) <0-01:23:09> ({'r_t':  1638.4735, 'eps':     1.0000, 'critic_loss':    36.2787, 'actor_loss':     0.3580, 'eps_e':     1.0000})
Step:  420000, Reward:   288.535 [  17.385], Avg:   229.821 (1.000) <0-01:23:19> ({'r_t':  1663.9306, 'eps':     1.0000, 'critic_loss':    10.7728, 'actor_loss':    -0.4846, 'eps_e':     1.0000})
Step:  421000, Reward:   282.354 [  17.852], Avg:   229.945 (1.000) <0-01:23:30> ({'r_t':  1564.7009, 'eps':     1.0000, 'critic_loss':    33.1526, 'actor_loss':     0.4733, 'eps_e':     1.0000})
Step:  422000, Reward:   271.395 [  63.457], Avg:   230.043 (1.000) <0-01:23:40> ({'r_t':  1593.0901, 'eps':     1.0000, 'critic_loss':    34.3614, 'actor_loss':     0.8222, 'eps_e':     1.0000})
Step:  423000, Reward:   274.192 [  57.012], Avg:   230.147 (1.000) <0-01:23:50> ({'r_t':  1573.3646, 'eps':     1.0000, 'critic_loss':    24.3881, 'actor_loss':     0.3954, 'eps_e':     1.0000})
Step:  424000, Reward:   245.864 [  89.612], Avg:   230.184 (1.000) <0-01:24:03> ({'r_t':  1655.7328, 'eps':     1.0000, 'critic_loss':    20.5459, 'actor_loss':     0.5438, 'eps_e':     1.0000})
Step:  425000, Reward:   291.412 [  15.522], Avg:   230.328 (1.000) <0-01:24:12> ({'r_t':  1712.5480, 'eps':     1.0000, 'critic_loss':    30.3063, 'actor_loss':     0.3398, 'eps_e':     1.0000})
Step:  426000, Reward:   288.630 [  16.754], Avg:   230.465 (1.000) <0-01:24:23> ({'r_t':  1500.3711, 'eps':     1.0000, 'critic_loss':    12.2206, 'actor_loss':     1.0855, 'eps_e':     1.0000})
Step:  427000, Reward:   270.431 [  55.044], Avg:   230.558 (1.000) <0-01:24:34> ({'r_t':  1498.3610, 'eps':     1.0000, 'critic_loss':    27.8964, 'actor_loss':     0.7272, 'eps_e':     1.0000})
Step:  428000, Reward:   273.604 [  58.129], Avg:   230.658 (1.000) <0-01:24:44> ({'r_t':  1659.3372, 'eps':     1.0000, 'critic_loss':    30.4401, 'actor_loss':     0.1622, 'eps_e':     1.0000})
Step:  429000, Reward:   285.660 [  12.751], Avg:   230.786 (1.000) <0-01:24:54> ({'r_t':  1586.4805, 'eps':     1.0000, 'critic_loss':    14.4160, 'actor_loss':     0.2442, 'eps_e':     1.0000})
Step:  430000, Reward:   289.047 [  14.254], Avg:   230.921 (1.000) <0-01:25:04> ({'r_t':  1628.1771, 'eps':     1.0000, 'critic_loss':    14.9746, 'actor_loss':     0.3902, 'eps_e':     1.0000})
Step:  431000, Reward:   279.807 [  17.100], Avg:   231.035 (1.000) <0-01:25:14> ({'r_t':  1698.3459, 'eps':     1.0000, 'critic_loss':    24.5457, 'actor_loss':     0.2215, 'eps_e':     1.0000})
Step:  432000, Reward:   282.022 [  20.294], Avg:   231.152 (1.000) <0-01:25:25> ({'r_t':  1649.6604, 'eps':     1.0000, 'critic_loss':    24.7866, 'actor_loss':     0.6733, 'eps_e':     1.0000})
Step:  433000, Reward:   281.336 [  14.796], Avg:   231.268 (1.000) <0-01:25:35> ({'r_t':  1657.7348, 'eps':     1.0000, 'critic_loss':    10.3532, 'actor_loss':    -0.0152, 'eps_e':     1.0000})
Step:  434000, Reward:   278.639 [  36.972], Avg:   231.377 (1.000) <0-01:25:47> ({'r_t':  1551.4319, 'eps':     1.0000, 'critic_loss':     9.2098, 'actor_loss':     0.1376, 'eps_e':     1.0000})
Step:  435000, Reward:   291.867 [  15.153], Avg:   231.516 (1.000) <0-01:25:57> ({'r_t':  1645.1953, 'eps':     1.0000, 'critic_loss':     8.7647, 'actor_loss':    -0.0703, 'eps_e':     1.0000})
Step:  436000, Reward:   264.050 [  62.228], Avg:   231.590 (1.000) <0-01:26:06> ({'r_t':  1420.2460, 'eps':     1.0000, 'critic_loss':    22.8066, 'actor_loss':     0.8215, 'eps_e':     1.0000})
Step:  437000, Reward:   273.774 [  61.123], Avg:   231.686 (1.000) <0-01:26:18> ({'r_t':  1572.3107, 'eps':     1.0000, 'critic_loss':    15.5726, 'actor_loss':     0.0490, 'eps_e':     1.0000})
Step:  438000, Reward:   293.583 [  15.905], Avg:   231.827 (1.000) <0-01:26:28> ({'r_t':  1649.1561, 'eps':     1.0000, 'critic_loss':    28.4037, 'actor_loss':     0.3266, 'eps_e':     1.0000})
Step:  439000, Reward:   283.543 [  18.020], Avg:   231.945 (1.000) <0-01:26:38> ({'r_t':  1537.3738, 'eps':     1.0000, 'critic_loss':    13.3795, 'actor_loss':    -0.0152, 'eps_e':     1.0000})
Step:  440000, Reward:   281.524 [  20.545], Avg:   232.057 (1.000) <0-01:26:50> ({'r_t':  1511.0592, 'eps':     1.0000, 'critic_loss':     9.1772, 'actor_loss':    -0.0139, 'eps_e':     1.0000})
Step:  441000, Reward:   254.524 [  85.865], Avg:   232.108 (1.000) <0-01:27:00> ({'r_t':  1603.0319, 'eps':     1.0000, 'critic_loss':    16.3449, 'actor_loss':     0.2251, 'eps_e':     1.0000})
Step:  442000, Reward:   290.917 [  18.119], Avg:   232.241 (1.000) <0-01:27:11> ({'r_t':  1534.1319, 'eps':     1.0000, 'critic_loss':    13.0800, 'actor_loss':     0.4439, 'eps_e':     1.0000})
Step:  443000, Reward:   287.772 [  17.626], Avg:   232.366 (1.000) <0-01:27:21> ({'r_t':  1476.9560, 'eps':     1.0000, 'critic_loss':    15.5820, 'actor_loss':     0.3919, 'eps_e':     1.0000})
Step:  444000, Reward:   266.089 [  70.488], Avg:   232.442 (1.000) <0-01:27:33> ({'r_t':  1565.0711, 'eps':     1.0000, 'critic_loss':    17.0998, 'actor_loss':     0.2351, 'eps_e':     1.0000})
Step:  445000, Reward:   285.372 [  16.468], Avg:   232.560 (1.000) <0-01:27:44> ({'r_t':  1557.6109, 'eps':     1.0000, 'critic_loss':    12.3214, 'actor_loss':     0.2716, 'eps_e':     1.0000})
Step:  446000, Reward:   275.062 [  60.546], Avg:   232.656 (1.000) <0-01:27:54> ({'r_t':  1664.9135, 'eps':     1.0000, 'critic_loss':    19.3555, 'actor_loss':     0.1962, 'eps_e':     1.0000})
Step:  447000, Reward:   282.052 [  33.938], Avg:   232.766 (1.000) <0-01:28:06> ({'r_t':  1563.4532, 'eps':     1.0000, 'critic_loss':    23.1981, 'actor_loss':     0.5009, 'eps_e':     1.0000})
Step:  448000, Reward:   289.712 [  17.438], Avg:   232.893 (1.000) <0-01:28:16> ({'r_t':  1682.8622, 'eps':     1.0000, 'critic_loss':    32.3687, 'actor_loss':    -0.0516, 'eps_e':     1.0000})
Step:  449000, Reward:   278.631 [  31.613], Avg:   232.994 (1.000) <0-01:28:29> ({'r_t':  1586.1323, 'eps':     1.0000, 'critic_loss':    15.3003, 'actor_loss':     0.3388, 'eps_e':     1.0000})
Step:  450000, Reward:   271.002 [  63.055], Avg:   233.079 (1.000) <0-01:28:41> ({'r_t':  1510.2240, 'eps':     1.0000, 'critic_loss':    24.9185, 'actor_loss':     0.3392, 'eps_e':     1.0000})
Step:  451000, Reward:   270.068 [  69.652], Avg:   233.160 (1.000) <0-01:28:51> ({'r_t':  1645.8130, 'eps':     1.0000, 'critic_loss':    25.5539, 'actor_loss':    -0.0762, 'eps_e':     1.0000})
Step:  452000, Reward:   252.451 [  88.936], Avg:   233.203 (1.000) <0-01:29:03> ({'r_t':  1668.5443, 'eps':     1.0000, 'critic_loss':    27.5743, 'actor_loss':    -0.0899, 'eps_e':     1.0000})
Step:  453000, Reward:   285.777 [  17.576], Avg:   233.319 (1.000) <0-01:29:13> ({'r_t':  1475.7176, 'eps':     1.0000, 'critic_loss':    44.9395, 'actor_loss':     0.4854, 'eps_e':     1.0000})
Step:  454000, Reward:   260.747 [  66.909], Avg:   233.379 (1.000) <0-01:29:26> ({'r_t':  1560.4987, 'eps':     1.0000, 'critic_loss':    21.0312, 'actor_loss':    -0.0835, 'eps_e':     1.0000})
Step:  455000, Reward:   276.026 [  32.237], Avg:   233.473 (1.000) <0-01:29:39> ({'r_t':  1446.2653, 'eps':     1.0000, 'critic_loss':    25.6430, 'actor_loss':     0.1501, 'eps_e':     1.0000})
Step:  456000, Reward:   290.398 [  18.158], Avg:   233.597 (1.000) <0-01:29:49> ({'r_t':  1431.7136, 'eps':     1.0000, 'critic_loss':    29.0438, 'actor_loss':     0.6189, 'eps_e':     1.0000})
Step:  457000, Reward:   290.119 [  17.953], Avg:   233.721 (1.000) <0-01:30:00> ({'r_t':  1485.1901, 'eps':     1.0000, 'critic_loss':    28.4887, 'actor_loss':    -0.1184, 'eps_e':     1.0000})
Step:  458000, Reward:   293.962 [  15.900], Avg:   233.852 (1.000) <0-01:30:10> ({'r_t':  1602.6522, 'eps':     1.0000, 'critic_loss':    15.1223, 'actor_loss':    -0.0564, 'eps_e':     1.0000})
Step:  459000, Reward:   291.130 [  18.143], Avg:   233.976 (1.000) <0-01:30:21> ({'r_t':  1537.3097, 'eps':     1.0000, 'critic_loss':    12.3898, 'actor_loss':     0.1024, 'eps_e':     1.0000})
Step:  460000, Reward:   287.205 [  21.758], Avg:   234.092 (1.000) <0-01:30:31> ({'r_t':  1540.3199, 'eps':     1.0000, 'critic_loss':    14.2049, 'actor_loss':     0.2695, 'eps_e':     1.0000})
Step:  461000, Reward:   290.617 [  20.243], Avg:   234.214 (1.000) <0-01:30:42> ({'r_t':  1647.8210, 'eps':     1.0000, 'critic_loss':    11.4741, 'actor_loss':     0.0069, 'eps_e':     1.0000})
Step:  462000, Reward:   263.186 [  83.887], Avg:   234.277 (1.000) <0-01:30:53> ({'r_t':  1491.1420, 'eps':     1.0000, 'critic_loss':    14.5271, 'actor_loss':     0.6927, 'eps_e':     1.0000})
Step:  463000, Reward:   290.631 [  14.570], Avg:   234.398 (1.000) <0-01:31:02> ({'r_t':  1456.3000, 'eps':     1.0000, 'critic_loss':    16.2040, 'actor_loss':     0.3686, 'eps_e':     1.0000})
Step:  464000, Reward:   272.702 [  57.975], Avg:   234.481 (1.000) <0-01:31:14> ({'r_t':  1525.5388, 'eps':     1.0000, 'critic_loss':    14.8566, 'actor_loss':     0.1199, 'eps_e':     1.0000})
Step:  465000, Reward:   251.340 [  64.593], Avg:   234.517 (1.000) <0-01:31:26> ({'r_t':  1586.5320, 'eps':     1.0000, 'critic_loss':    23.1029, 'actor_loss':     0.0529, 'eps_e':     1.0000})
Step:  466000, Reward:   279.995 [  30.030], Avg:   234.614 (1.000) <0-01:31:38> ({'r_t':  1493.0609, 'eps':     1.0000, 'critic_loss':    16.0874, 'actor_loss':     0.2902, 'eps_e':     1.0000})
Step:  467000, Reward:   289.474 [  17.133], Avg:   234.731 (1.000) <0-01:31:48> ({'r_t':  1449.9748, 'eps':     1.0000, 'critic_loss':    26.5604, 'actor_loss':     0.7568, 'eps_e':     1.0000})
Step:  468000, Reward:   292.539 [  16.467], Avg:   234.855 (1.000) <0-01:31:58> ({'r_t':  1401.7000, 'eps':     1.0000, 'critic_loss':    25.7467, 'actor_loss':     0.4525, 'eps_e':     1.0000})
Step:  469000, Reward:   253.139 [  84.957], Avg:   234.894 (1.000) <0-01:32:09> ({'r_t':  1468.6383, 'eps':     1.0000, 'critic_loss':    16.3871, 'actor_loss':     0.0305, 'eps_e':     1.0000})
Step:  470000, Reward:   269.556 [  62.601], Avg:   234.967 (1.000) <0-01:32:22> ({'r_t':  1422.7810, 'eps':     1.0000, 'critic_loss':    24.6999, 'actor_loss':     0.3780, 'eps_e':     1.0000})
Step:  471000, Reward:   285.367 [  23.317], Avg:   235.074 (1.000) <0-01:32:32> ({'r_t':  1514.1731, 'eps':     1.0000, 'critic_loss':    20.6640, 'actor_loss':    -0.3518, 'eps_e':     1.0000})
Step:  472000, Reward:   286.945 [  17.122], Avg:   235.184 (1.000) <0-01:32:43> ({'r_t':  1673.0207, 'eps':     1.0000, 'critic_loss':    29.4056, 'actor_loss':     0.5893, 'eps_e':     1.0000})
Step:  473000, Reward:   292.670 [  18.885], Avg:   235.305 (1.000) <0-01:32:53> ({'r_t':  1668.9365, 'eps':     1.0000, 'critic_loss':    12.9376, 'actor_loss':    -0.0267, 'eps_e':     1.0000})
Step:  474000, Reward:   290.128 [  18.207], Avg:   235.420 (1.000) <0-01:33:03> ({'r_t':  1666.5440, 'eps':     1.0000, 'critic_loss':    29.9744, 'actor_loss':     0.0273, 'eps_e':     1.0000})
Step:  475000, Reward:   270.881 [  61.057], Avg:   235.495 (1.000) <0-01:33:12> ({'r_t':  1439.1920, 'eps':     1.0000, 'critic_loss':    30.8043, 'actor_loss':     1.1093, 'eps_e':     1.0000})
Step:  476000, Reward:   286.932 [  18.699], Avg:   235.603 (1.000) <0-01:33:24> ({'r_t':  1550.0129, 'eps':     1.0000, 'critic_loss':    29.6923, 'actor_loss':     0.7103, 'eps_e':     1.0000})
Step:  477000, Reward:   269.589 [  65.180], Avg:   235.674 (1.000) <0-01:33:34> ({'r_t':  1598.3152, 'eps':     1.0000, 'critic_loss':    22.4519, 'actor_loss':     0.1403, 'eps_e':     1.0000})
Step:  478000, Reward:   274.211 [  35.583], Avg:   235.754 (1.000) <0-01:33:47> ({'r_t':  1620.8769, 'eps':     1.0000, 'critic_loss':    17.4368, 'actor_loss':     0.3305, 'eps_e':     1.0000})
Step:  479000, Reward:   270.844 [  51.920], Avg:   235.827 (1.000) <0-01:33:57> ({'r_t':  1580.1168, 'eps':     1.0000, 'critic_loss':    43.5072, 'actor_loss':     1.0337, 'eps_e':     1.0000})
Step:  480000, Reward:   293.779 [  16.774], Avg:   235.948 (1.000) <0-01:34:07> ({'r_t':  1590.2349, 'eps':     1.0000, 'critic_loss':    37.1039, 'actor_loss':    -0.1630, 'eps_e':     1.0000})
Step:  481000, Reward:   284.149 [  18.640], Avg:   236.048 (1.000) <0-01:34:18> ({'r_t':  1664.3969, 'eps':     1.0000, 'critic_loss':    31.7446, 'actor_loss':    -0.1556, 'eps_e':     1.0000})
Step:  482000, Reward:   282.971 [  37.209], Avg:   236.145 (1.000) <0-01:34:30> ({'r_t':  1588.0525, 'eps':     1.0000, 'critic_loss':    19.4098, 'actor_loss':     0.0190, 'eps_e':     1.0000})
Step:  483000, Reward:   289.162 [  23.374], Avg:   236.254 (1.000) <0-01:34:41> ({'r_t':  1585.8658, 'eps':     1.0000, 'critic_loss':    10.1276, 'actor_loss':     0.1233, 'eps_e':     1.0000})
Step:  484000, Reward:   267.539 [  59.054], Avg:   236.319 (1.000) <0-01:34:50> ({'r_t':  1419.5543, 'eps':     1.0000, 'critic_loss':    17.5840, 'actor_loss':     0.9906, 'eps_e':     1.0000})
Step:  485000, Reward:   269.548 [  50.198], Avg:   236.387 (1.000) <0-01:35:01> ({'r_t':  1467.8610, 'eps':     1.0000, 'critic_loss':    23.2738, 'actor_loss':     0.2078, 'eps_e':     1.0000})
Step:  486000, Reward:   290.959 [  21.726], Avg:   236.499 (1.000) <0-01:35:13> ({'r_t':  1461.7847, 'eps':     1.0000, 'critic_loss':    17.1282, 'actor_loss':    -0.0667, 'eps_e':     1.0000})
Step:  487000, Reward:   256.690 [  86.589], Avg:   236.541 (1.000) <0-01:35:24> ({'r_t':  1454.1439, 'eps':     1.0000, 'critic_loss':    33.5338, 'actor_loss':     0.4791, 'eps_e':     1.0000})
Step:  488000, Reward:   265.090 [  64.903], Avg:   236.599 (1.000) <0-01:35:36> ({'r_t':  1611.9909, 'eps':     1.0000, 'critic_loss':    18.8368, 'actor_loss':    -0.1720, 'eps_e':     1.0000})
Step:  489000, Reward:   285.792 [  21.425], Avg:   236.699 (1.000) <0-01:35:44> ({'r_t':  1423.9083, 'eps':     1.0000, 'critic_loss':    21.0693, 'actor_loss':     0.8247, 'eps_e':     1.0000})
Step:  490000, Reward:   246.624 [  92.302], Avg:   236.720 (1.000) <0-01:35:59> ({'r_t':  1309.1073, 'eps':     1.0000, 'critic_loss':    12.1346, 'actor_loss':     0.1531, 'eps_e':     1.0000})
Step:  491000, Reward:   286.259 [  17.422], Avg:   236.820 (1.000) <0-01:36:09> ({'r_t':  1501.5635, 'eps':     1.0000, 'critic_loss':    17.5583, 'actor_loss':     0.3156, 'eps_e':     1.0000})
Step:  492000, Reward:   265.635 [  56.502], Avg:   236.879 (1.000) <0-01:36:20> ({'r_t':  1642.2736, 'eps':     1.0000, 'critic_loss':    24.9602, 'actor_loss':     0.5335, 'eps_e':     1.0000})
Step:  493000, Reward:   287.320 [  17.372], Avg:   236.981 (1.000) <0-01:36:29> ({'r_t':  1508.2870, 'eps':     1.0000, 'critic_loss':    22.6298, 'actor_loss':     0.0282, 'eps_e':     1.0000})
Step:  494000, Reward:   278.213 [  40.152], Avg:   237.064 (1.000) <0-01:36:42> ({'r_t':  1476.6442, 'eps':     1.0000, 'critic_loss':    22.4447, 'actor_loss':    -0.0066, 'eps_e':     1.0000})
Step:  495000, Reward:   280.199 [  33.962], Avg:   237.151 (1.000) <0-01:36:54> ({'r_t':  1535.1591, 'eps':     1.0000, 'critic_loss':    28.2807, 'actor_loss':     0.1353, 'eps_e':     1.0000})
Step:  496000, Reward:   286.598 [  17.131], Avg:   237.251 (1.000) <0-01:37:05> ({'r_t':  1548.7606, 'eps':     1.0000, 'critic_loss':    37.5983, 'actor_loss':     0.6518, 'eps_e':     1.0000})
Step:  497000, Reward:   278.106 [  58.979], Avg:   237.333 (1.000) <0-01:37:15> ({'r_t':  1662.5845, 'eps':     1.0000, 'critic_loss':    21.5944, 'actor_loss':    -0.1278, 'eps_e':     1.0000})
Step:  498000, Reward:   287.197 [  19.156], Avg:   237.433 (1.000) <0-01:37:25> ({'r_t':  1676.7393, 'eps':     1.0000, 'critic_loss':    18.2487, 'actor_loss':     0.0949, 'eps_e':     1.0000})
Step:  499000, Reward:   273.721 [  61.465], Avg:   237.505 (1.000) <0-01:37:36> ({'r_t':  1653.8930, 'eps':     1.0000, 'critic_loss':    13.3251, 'actor_loss':     0.4295, 'eps_e':     1.0000})
Step:  500000, Reward:   285.660 [  16.622], Avg:   237.601 (1.000) <0-01:37:45> ({'r_t':  1624.7961, 'eps':     1.0000, 'critic_loss':    14.7469, 'actor_loss':     0.1631, 'eps_e':     1.0000})
