Model: <class 'src.models.pytorch.mpc.mppi.MPPIAgent'>, Env: CarRacing-v1, Date: 04/06/2020 18:46:09
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: a7bdb9e522d759a6aededa20f549a1b6f8af4a89
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.98
   NUM_STEPS = None
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 10000
   TARGET_UPDATE_RATE = 0.0004
   BATCH_SIZE = 250
   DYN_EPOCHS = 1
   TRAIN_EVERY = 10000
   ENV_MODEL = dfrntl
   MPC = 
      NSAMPLES = 100
      HORIZON = 40
      LAMBDA = 0.1
      COV = 0.5
   REWARD_MODEL = src.envs.CarRacing.objective.cost:CostModel
   DYNAMICS_SPEC = src.envs.CarRacing.car_racing:CarRacing
   dynamics_size = 13
   state_size = (80,)
   action_size = (3,)
   env_name = CarRacing-v1
   rank = 0
   size = 17
   split = 17
   model = mppi
   framework = pt
   train_prop = 1.0
   tcp_ports = [9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 1000000
   render = False
   trial = False
   icm = False
   rs = False
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.0001
      TRANSITION_HIDDEN = 512
      REWARD_HIDDEN = 256
      BETA_DYN = 1
      BETA_DOT = 0
      BETA_DDOT = 0,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f4cc815ca50> 
	env = <GymEnv<CarRacing<CarRacing-v1>>> 
		env = <CarRacing<CarRacing-v1>> 
			channel = <mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel object at 0x7f4cd0067b50>
			scale_sim = <function CarRacing.__init__.<locals>.<lambda> at 0x7f4cc37138c0>
			env = <UnityToGymWrapper instance> 
				visual_obs = None
				game_over = False
				name = CarBehavior?team=0
				group_spec = BehaviorSpec(observation_shapes=[(30,)], action_type=<ActionType.CONTINUOUS: 1>, action_shape=3)
				use_visual = False
				uint8_visual = False
			cost_model = <src.envs.CarRacing.objective.cost.CostModel object at 0x7f4cc8138e10> 
				track = <src.envs.CarRacing.objective.track.Track object at 0x7f4cc37149d0> 
					track = <list len=500>
					X = (1.540585208684206, 1.5814536064863205, 1.6016383588314056, 1.6350171357393264, 1.6559478223323822, 1.6717498254776002, 1.709812204837799, 1.7354034245014192, 1.7725858569145203, 1.8077154874801635, 1.958074402809143, 2.0178433418273927, 2.1851138830184937, 2.258661150932312, 2.3439700841903686, 2.452700424194336, 2.586679172515869, 2.782884216308594, 3.047244071960449, 3.4783129692077637, 3.9734771251678467, 4.596014499664307, 5.29957389831543, 6.05716609954834, 6.824328422546387, 7.646727561950684, 8.59219741821289, 9.675070762634277, 10.77119255065918, 11.868535041809082, 12.83842658996582, 13.727555274963379, 14.569844245910645, 15.391722679138184, 16.204023361206055, 17.02372169494629, 17.626384735107422, 18.072078704833984, 18.462026596069336, 18.803436279296875, 19.08125877380371, 19.200590133666992, 19.074377059936523, 18.833162307739258, 18.582487106323242, 18.339160919189453, 17.97744369506836, 17.59515380859375, 17.09140968322754, 16.50218391418457, 15.817791938781738, 14.983868598937988, 13.986822128295898, 12.817933082580566, 11.528505325317383, 10.241579055786133, 8.946599960327148, 7.588953971862793, 6.2032341957092285, 4.799948692321777, 3.3720505237579346, 1.9454675912857056, 0.4815756678581238, -0.9242660999298096, -2.3082480430603027, -3.7190709114074707, -5.090760231018066, -6.490819931030273, -7.933252811431885, -9.48039722442627, -11.141877174377441, -12.927711486816406, -14.796602249145508, -16.603300094604492, -18.390233993530273, -20.1385498046875, -21.805997848510742, -23.41408920288086, -25.02754783630371, -26.801597595214844, -28.776451110839844, -30.972705841064453, -33.385520935058594, -35.90762710571289, -38.527618408203125, -41.362369537353516, -44.435585021972656, -47.831398010253906, -51.587188720703125, -55.642662048339844, -59.980804443359375, -64.55036163330078, -69.1060562133789, -73.4732666015625, -77.65788269042969, -81.6474380493164, -85.45370483398438, -89.12055206298828, -92.67816925048828, -96.15220642089844, -99.54827117919922, -102.86875915527344, -106.01786804199219, -109.03597259521484, -111.96282958984375, -114.75870513916016, -117.48453521728516, -120.2335205078125, -123.01750946044922, -125.81232452392578, -128.56246948242188, -131.20936584472656, -133.767333984375, -136.21359252929688, -138.6573486328125, -141.0603485107422, -143.3613739013672, -145.4899444580078, -147.5723114013672, -149.41514587402344, -150.9908905029297, -152.32089233398438, -153.6006622314453, -154.83030700683594, -156.0063018798828, -157.14691162109375, -158.23680114746094, -159.30880737304688, -160.30152893066406, -161.2411651611328, -162.03582763671875, -162.72186279296875, -163.28753662109375, -163.81460571289062, -164.31549072265625, -164.78814697265625, -165.1201171875, -165.26596069335938, -165.24961853027344, -165.20376586914062, -165.07931518554688, -165.0469512939453, -165.03262329101562, -164.86660766601562, -164.62220764160156, -164.3842315673828, -164.145263671875, -163.90011596679688, -163.64981079101562, -163.3218231201172, -162.726318359375, -161.83493041992188, -160.71856689453125, -159.4139862060547, -157.9736328125, -156.54212951660156, -155.10464477539062, -153.63636779785156, -152.13641357421875, -150.6412811279297, -149.1659698486328, -147.64437866210938, -146.01336669921875, -144.21286010742188, -142.3518829345703, -140.49502563476562, -138.6591796875, -136.8135986328125, -134.9413604736328, -132.9547882080078, -130.7132110595703, -128.1597137451172, -125.3279037475586, -122.26266479492188, -118.97386932373047, -115.49871826171875, -111.90750122070312, -108.16539764404297, -104.34297180175781, -100.58757781982422, -96.96247863769531, -93.51396942138672, -90.1981201171875, -86.93607330322266, -83.70171356201172, -80.58210754394531, -77.49177551269531, -74.4620132446289, -71.53809356689453, -68.60317993164062, -65.52932739257812, -62.46957778930664, -59.48895263671875, -56.56187057495117, -53.813289642333984, -51.1711311340332, -48.648197174072266, -46.242332458496094, -43.94118118286133, -41.766075134277344, -39.70472717285156, -37.813140869140625, -36.01365280151367, -34.269657135009766, -32.50520706176758, -30.680166244506836, -28.837051391601562, -27.001256942749023, -25.25333023071289, -23.701873779296875, -22.668081283569336, -22.199195861816406, -22.169893264770508, -22.46630859375, -23.134033203125, -24.32797622680664, -26.001781463623047, -27.869766235351562, -29.80392074584961, -31.775949478149414, -33.793365478515625, -35.771907806396484, -37.70563888549805, -39.61886215209961, -41.516029357910156, -43.41127014160156, -45.27768325805664, -47.11109924316406, -48.94091796875, -50.77583694458008, -52.619163513183594, -54.48332977294922, -56.314815521240234, -58.103755950927734, -59.823333740234375, -61.56585693359375, -63.30061340332031, -64.97642517089844, -66.51130676269531, -67.94270324707031, -69.3357925415039, -70.66708374023438, -71.93402099609375, -73.18978118896484, -74.31753540039062, -75.23255920410156, -75.95966339111328, -76.61920166015625, -77.26768493652344, -77.9359130859375, -78.5946273803711, -79.26289367675781, -79.79534912109375, -80.2015380859375, -80.60335540771484, -81.02714538574219, -81.53772735595703, -82.04193878173828, -82.53047180175781, -83.04158020019531, -83.56088256835938, -84.14714813232422, -84.81393432617188, -85.55133056640625, -86.36656188964844, -87.24837493896484, -88.13751983642578, -88.99240112304688, -89.81124877929688, -90.60415649414062, -91.33631896972656, -92.02133178710938, -92.65229034423828, -93.23121643066406, -93.7853012084961, -94.3372573852539, -94.88070678710938, -95.41710662841797, -95.84803771972656, -96.24778747558594, -96.6568374633789, -97.0496826171875, -97.41992950439453, -97.77052307128906, -97.91485595703125, -97.96147155761719, -97.87026977539062, -97.53227233886719, -96.85386657714844, -95.81302642822266, -94.54135131835938, -93.15739440917969, -91.603271484375, -89.95466613769531, -88.35015106201172, -86.80291748046875, -85.39144134521484, -84.07344055175781, -82.86149597167969, -81.5972671508789, -80.11182403564453, -78.36345672607422, -76.40621948242188, -74.32894134521484, -72.0761489868164, -69.69659423828125, -67.17849731445312, -64.48152160644531, -61.61235046386719, -58.499427795410156, -55.10073471069336, -51.55522918701172, -47.74736785888672, -43.832923889160156, -39.801971435546875, -35.743858337402344, -31.80649757385254, -28.028738021850586, -24.38759994506836, -20.836519241333008, -17.374597549438477, -14.002902030944824, -10.617079734802246, -7.34421443939209, -4.187110424041748, -1.115414023399353, 2.037353277206421, 5.401520252227783, 8.870983123779297, 12.423381805419922, 16.180818557739258, 20.157392501831055, 24.33769989013672, 28.77823829650879, 33.3828010559082, 38.12346267700195, 42.767642974853516, 47.21396255493164, 51.497074127197266, 55.640106201171875, 59.61445999145508, 63.45794677734375, 67.16992950439453, 70.71627044677734, 74.12809753417969, 77.53622436523438, 80.97876739501953, 84.45626068115234, 87.9986572265625, 91.61026000976562, 95.1865234375, 98.68260192871094, 102.08172607421875, 105.37554168701172, 108.5978012084961, 111.72406005859375, 114.72969818115234, 117.6103515625, 120.28418731689453, 122.77039337158203, 125.10813903808594, 127.35991668701172, 129.5707550048828, 131.73577880859375, 133.8451385498047, 135.88076782226562, 137.81361389160156, 139.69195556640625, 141.56494140625, 143.51321411132812, 145.43582153320312, 147.37954711914062, 149.30592346191406, 151.1349334716797, 152.76832580566406, 154.18382263183594, 155.40008544921875, 156.48155212402344, 157.39840698242188, 158.19866943359375, 158.91281127929688, 159.4974822998047, 160.02337646484375, 160.31883239746094, 160.23129272460938, 159.7694854736328, 159.0675506591797, 158.11312866210938, 157.08311462402344, 155.8784942626953, 154.47816467285156, 152.8489990234375, 151.00660705566406, 149.11109924316406, 147.24368286132812, 145.35427856445312, 143.4554443359375, 141.39073181152344, 139.07090759277344, 136.57705688476562, 134.08177185058594, 131.63348388671875, 129.23263549804688, 126.91446685791016, 124.63007354736328, 122.27965545654297, 119.90943145751953, 117.51732635498047, 115.1493148803711, 112.83964538574219, 110.53994750976562, 108.22462463378906, 105.85285949707031, 103.4562759399414, 101.13794708251953, 98.82323455810547, 96.44384765625, 93.94629669189453, 91.3570556640625, 88.73168182373047, 86.05917358398438, 83.26211547851562, 80.25263214111328, 77.10718536376953, 73.97905731201172, 70.96484375, 68.1133804321289, 65.44701385498047, 62.890159606933594, 60.41355514526367, 57.95263671875, 55.59248352050781, 53.20044708251953, 50.7462272644043, 48.28958511352539, 45.88505935668945, 43.5562744140625, 41.31084442138672, 39.171634674072266, 37.183380126953125, 35.43268966674805, 33.800804138183594, 32.20466613769531, 30.66669273376465, 29.13826560974121, 27.552635192871094, 25.97852325439453, 24.294662475585938, 22.565439224243164, 20.874217987060547, 19.30082893371582, 17.831933975219727, 16.408084869384766, 15.044317245483398, 13.766607284545898, 12.577005386352539, 11.475253105163574, 10.496495246887207, 9.622332572937012, 8.769275665283203, 7.927954196929932, 7.112521648406982, 6.322704315185547, 5.563619136810303, 4.829586982727051, 4.113427639007568, 3.3697121143341064, 2.5567243099212646, 1.7977246046066284, 1.0246542692184448, 0.2572939395904541, -0.4480553865432739, -1.1242897510528564, -1.6556841135025024, -2.0525705814361572, -2.214649200439453, -2.169621467590332, -2.035892963409424, -1.9102517366409302, -1.7909443378448486, -1.7162281274795532, -1.651557445526123, -1.5775796175003052, -1.5097243785858154, -1.4451829195022583, -1.3808107376098633, -1.3076838254928589, -1.1195673942565918, -0.8252816200256348, -0.5349398255348206, -0.2580118477344513, 0.009828831069171429, 0.2716897428035736, 0.5349469780921936, 0.7902784943580627, 1.052398443222046, 1.31592857837677, 1.570581078529358, 1.6137370109558105, 1.6365979194641114)
					Z = (-0.8819639682769775, -0.8812801241874695, -0.8804802298545837, -0.8791921734809875, -0.8777425289154053, -0.8758563995361328, -0.873963475227356, -0.8539403676986694, -0.7802032232284546, -0.761174201965332, -0.7716957926750183, -0.8395041823387146, -0.8772552609443665, -0.8344407081604004, -0.788372814655304, -0.80742347240448, -0.8527643084526062, -0.8346409797668457, -0.824370265007019, -0.8134136199951172, -0.7967275381088257, -0.7752544283866882, -0.7417746782302856, -0.6927484273910522, -0.633834719657898, -0.5747796297073364, -0.5113369226455688, -0.4433113932609558, -0.3737497925758362, -0.3008161187171936, -0.2312106341123581, -0.16523221135139465, -0.09990986436605453, -0.033577218651771545, 0.03842548280954361, 0.11881522089242935, 0.1981208622455597, 0.28177762031555176, 0.38250869512557983, 0.5017393231391907, 0.625041127204895, 0.7394312620162964, 0.8367793560028076, 0.9279725551605225, 1.0242633819580078, 1.1258037090301514, 1.2272775173187256, 1.3421326875686646, 1.4506069421768188, 1.561546802520752, 1.6706804037094116, 1.7743912935256958, 1.8515067100524902, 1.9097793102264404, 1.948763370513916, 1.9814872741699219, 2.0233898162841797, 2.07637095451355, 2.132861375808716, 2.17509126663208, 2.2180161476135254, 2.274773597717285, 2.3546767234802246, 2.4420950412750244, 2.5328733921051025, 2.6344215869903564, 2.7358694076538086, 2.8366494178771973, 2.9418249130249023, 3.0620920658111572, 3.1827614307403564, 3.30625581741333, 3.427833080291748, 3.5489587783813477, 3.675954818725586, 3.79117488861084, 3.901960849761963, 4.005653381347656, 4.107993125915527, 4.2158284187316895, 4.328779220581055, 4.445080280303955, 4.569532871246338, 4.690032005310059, 4.799752712249756, 4.872299671173096, 4.92843770980835, 4.985036849975586, 5.057000637054443, 5.13352108001709, 5.213327884674072, 5.295718193054199, 5.3766703605651855, 5.451817512512207, 5.519579887390137, 5.582165718078613, 5.639312267303467, 5.692175388336182, 5.7414727210998535, 5.787367820739746, 5.830183506011963, 5.869744300842285, 5.905086994171143, 5.936120986938477, 5.963281154632568, 5.987318992614746, 6.008669376373291, 6.027542591094971, 6.044310569763184, 6.057828903198242, 6.067286968231201, 6.074985504150391, 6.081448554992676, 6.086737155914307, 6.091536998748779, 6.096595764160156, 6.1012773513793945, 6.104137420654297, 6.10720682144165, 6.105283260345459, 6.09289026260376, 6.069871425628662, 6.042582988739014, 6.011574745178223, 5.977062702178955, 5.945542812347412, 5.9195661544799805, 5.900696277618408, 5.875031471252441, 5.850343227386475, 5.822032451629639, 5.787215232849121, 5.749323844909668, 5.708043575286865, 5.672667503356934, 5.640613079071045, 5.58774995803833, 5.510519504547119, 5.4132280349731445, 5.318352222442627, 5.21757173538208, 5.129578113555908, 5.049224376678467, 4.955892086029053, 4.855170726776123, 4.759181022644043, 4.6699957847595215, 4.590251922607422, 4.507761478424072, 4.420248508453369, 4.298507213592529, 4.1367998123168945, 3.954977035522461, 3.7536673545837402, 3.5393548011779785, 3.336235761642456, 3.13871431350708, 2.941469192504883, 2.743802785873413, 2.5500059127807617, 2.362222671508789, 2.172161817550659, 1.9712504148483276, 1.7527763843536377, 1.5335578918457031, 1.3216581344604492, 1.11974036693573, 0.924856424331665, 0.7362942099571228, 0.548167884349823, 0.3510936498641968, 0.14911779761314392, -0.04503828287124634, -0.22794248163700104, -0.3905165493488312, -0.5209499597549438, -0.6174218654632568, -0.6916936039924622, -0.7458155751228333, -0.7768694162368774, -0.7899942994117737, -0.7893635630607605, -0.7789414525032043, -0.7635725736618042, -0.7461717128753662, -0.7283236980438232, -0.704211413860321, -0.6622856855392456, -0.5993924140930176, -0.5216199159622192, -0.426088809967041, -0.3150973916053772, -0.1974087506532669, -0.07835512608289719, 0.03133012354373932, 0.13556505739688873, 0.24022513628005981, 0.3493971824645996, 0.45991453528404236, 0.5715771317481995, 0.6827750205993652, 0.7940959930419922, 0.907843291759491, 1.025125503540039, 1.148614764213562, 1.2811535596847534, 1.417541265487671, 1.5532535314559937, 1.6824359893798828, 1.7986339330673218, 1.8819316625595093, 1.9304401874542236, 1.9543043375015259, 1.9636659622192383, 1.9588732719421387, 1.916387915611267, 1.8345577716827393, 1.7349056005477905, 1.6296110153198242, 1.5208213329315186, 1.405418872833252, 1.2866981029510498, 1.16438889503479, 1.0394600629806519, 0.9107307195663452, 0.7798608541488647, 0.6512886881828308, 0.5262399315834045, 0.4030036926269531, 0.2815271019935608, 0.16398224234580994, 0.05072043836116791, -0.05590145289897919, -0.15327762067317963, -0.24135041236877441, -0.3243723213672638, -0.3988741636276245, -0.4620799124240875, -0.542617678642273, -0.646656334400177, -0.7287228107452393, -0.7844877243041992, -0.806078314781189, -0.8148013949394226, -0.8116025924682617, -0.8039451837539673, -0.7978506088256836, -0.8006065487861633, -0.8066939115524292, -0.8129818439483643, -0.8215823173522949, -0.8290983438491821, -0.8362972736358643, -0.8428731560707092, -0.8489797711372375, -0.8558133840560913, -0.8626493811607361, -0.8682581186294556, -0.8741699457168579, -0.879978597164154, -0.8859436511993408, -0.8909560441970825, -0.8937748670578003, -0.8939367532730103, -0.8897822499275208, -0.8787690997123718, -0.8593403697013855, -0.8307321667671204, -0.8021003603935242, -0.7821503281593323, -0.7700151801109314, -0.7592963576316833, -0.7492351531982422, -0.7390634417533875, -0.7314242720603943, -0.7212424278259277, -0.7080341577529907, -0.6888165473937988, -0.66937655210495, -0.6463529467582703, -0.6128187775611877, -0.5654257535934448, -0.5037499666213989, -0.42715343832969666, -0.34471648931503296, -0.25006303191185, -0.14578062295913696, -0.03818090260028839, 0.0759134441614151, 0.21288788318634033, 0.35622480511665344, 0.515775203704834, 0.6532223224639893, 0.7738814949989319, 0.8932506442070007, 1.0421302318572998, 1.2146294116973877, 1.385721206665039, 1.5515326261520386, 1.7406084537506104, 1.9566478729248047, 2.214561700820923, 2.5135207176208496, 2.8274102210998535, 3.160696268081665, 3.501220941543579, 3.8431997299194336, 4.200472354888916, 4.574350357055664, 4.894090175628662, 5.0936360359191895, 5.216364860534668, 5.390469074249268, 5.586197853088379, 5.784314155578613, 5.985593795776367, 6.1828765869140625, 6.373883247375488, 6.556783199310303, 6.733740329742432, 6.906088829040527, 7.071183204650879, 7.233142852783203, 7.3868231773376465, 7.530625343322754, 7.665377616882324, 7.797634124755859, 7.930730819702148, 8.059279441833496, 8.180848121643066, 8.296680450439453, 8.406368255615234, 8.505520820617676, 8.589674949645996, 8.655287742614746, 8.70052719116211, 8.722027778625488, 8.70865249633789, 8.652679443359375, 8.560135841369629, 8.443024635314941, 8.307100296020508, 8.149582862854004, 7.971302032470703, 7.780361175537109, 7.575259685516357, 7.355491638183594, 7.124767303466797, 6.885737419128418, 6.638427257537842, 6.395895481109619, 6.166090488433838, 5.953654766082764, 5.738729953765869, 5.529703140258789, 5.342148303985596, 5.179572105407715, 5.024766445159912, 4.851255416870117, 4.646117210388184, 4.430662155151367, 4.217848777770996, 4.0131144523620605, 3.7878849506378174, 3.559556245803833, 3.3353841304779053, 3.1190574169158936, 2.9180359840393066, 2.7267343997955322, 2.5381720066070557, 2.3227102756500244, 2.0959630012512207, 1.8809078931808472, 1.6847819089889526, 1.495663046836853, 1.3055880069732666, 1.1171165704727173, 0.9520562887191772, 0.8042331337928772, 0.681337833404541, 0.5795820951461792, 0.5025584101676941, 0.46133852005004883, 0.4328932762145996, 0.3858243227005005, 0.3234015107154846, 0.2624247372150421, 0.19709435105323792, 0.15313704311847687, 0.11826862394809723, 0.08544927090406418, 0.04712279140949249, 0.0015682056546211243, -0.026410788297653198, -0.03486667573451996, -0.027389593422412872, -0.0065015703439712524, 0.0059362053871154785, 0.002570606768131256, -0.006264716386795044, -0.013282939791679382, -0.018584154546260834, -0.022372961044311523, -0.0232115238904953, -0.02133723348379135, -0.030498042702674866, -0.057736508548259735, -0.09805164486169815, -0.13833804428577423, -0.17615404725074768, -0.21290594339370728, -0.24737012386322021, -0.26589956879615784, -0.2773838937282562, -0.2822290062904358, -0.2861996591091156, -0.2940981388092041, -0.2990141808986664, -0.3035801351070404, -0.3050832152366638, -0.3049992024898529, -0.30373987555503845, -0.3003387153148651, -0.29614898562431335, -0.2985635995864868, -0.31389492750167847, -0.34401920437812805, -0.3844596743583679, -0.4300534129142761, -0.4741150140762329, -0.5105020999908447, -0.5354415774345398, -0.552415132522583, -0.5600359439849854, -0.5654557943344116, -0.5681073665618896, -0.5666967630386353, -0.5622239112854004, -0.5597591996192932, -0.5650179386138916, -0.579081654548645, -0.5969113707542419, -0.6101321578025818, -0.622231125831604, -0.6340838074684143, -0.6458472609519958, -0.657522976398468, -0.6685013771057129, -0.6801296472549438, -0.6912583708763123, -0.7032382488250732, -0.7155491709709167, -0.7265709042549133, -0.7348979115486145, -0.7445682287216187, -0.7536845207214355, -0.761847198009491, -0.7706142067909241, -0.7806366682052612, -0.7898868322372437, -0.7978246212005615, -0.8051745295524597, -0.8114349842071533, -0.8171375393867493, -0.821597158908844, -0.8264663219451904, -0.8312869071960449, -0.8363567590713501, -0.8399266004562378, -0.8434712290763855, -0.8482410907745361, -0.8517320156097412, -0.8557907342910767, -0.8605977296829224, -0.864855170249939, -0.8680832982063293, -0.869952917098999, -0.8720065951347351, -0.8741781711578369, -0.8759156465530396, -0.8775535821914673, -0.8793764710426331, -0.8817098140716553, -0.8832718729972839, -0.8847836852073669, -0.8870889544487, -0.8891378045082092, -0.8896875977516174, -0.8895387649536133, -0.8889559507369995, -0.8881706595420837, -0.8874912261962891, -0.8865614533424377, -0.8851791024208069, -0.8832001686096191, -0.8809881806373596, -0.8781297206878662, -0.8746054172515869, -0.8718098402023315, -0.8688086271286011)
					Y = (0.24426956474781036, 0.4990326166152954, 0.819128692150116, 1.153626799583435, 1.5026447772979736, 1.8859440088272095, 2.373248815536499, 2.968236207962036, 3.61586332321167, 4.355114459991455, 5.173743724822998, 6.038478374481201, 6.951005458831787, 7.899267673492432, 8.918261528015137, 10.051026344299316, 11.312947273254395, 12.90755558013916, 14.871548652648926, 17.198680877685547, 19.908754348754883, 22.898487091064453, 26.10063934326172, 29.397844314575195, 32.636375427246094, 35.74137878417969, 38.707183837890625, 41.484439849853516, 44.07951736450195, 46.60736846923828, 49.15201187133789, 51.65317916870117, 54.06341552734375, 56.4561882019043, 58.852813720703125, 61.29132080078125, 63.84211730957031, 66.49172973632812, 69.07376861572266, 71.62057495117188, 74.08918762207031, 76.49169158935547, 78.78299713134766, 80.95753479003906, 83.06936645507812, 85.1029281616211, 87.12429809570312, 89.12969970703125, 91.03314971923828, 92.87902069091797, 94.55635070800781, 96.09061431884766, 97.33863830566406, 98.26770782470703, 98.91900634765625, 99.34143829345703, 99.79500579833984, 100.22048950195312, 100.46652221679688, 100.50714111328125, 100.43055725097656, 100.3218765258789, 100.27439880371094, 100.24840545654297, 100.22171020507812, 100.19712829589844, 100.16851043701172, 100.09687042236328, 100.02641296386719, 99.95970153808594, 99.8285140991211, 99.58265686035156, 99.25724792480469, 98.94861602783203, 98.7610855102539, 98.6032943725586, 98.43841552734375, 98.27819061279297, 98.11662292480469, 97.93367004394531, 97.72758483886719, 97.4378662109375, 97.10028839111328, 96.74153900146484, 96.36189270019531, 95.95005798339844, 95.50723266601562, 95.01679229736328, 94.47090911865234, 93.8803482055664, 93.24833679199219, 92.5796127319336, 91.90768432617188, 91.14244079589844, 90.31917572021484, 89.48597717285156, 88.64861297607422, 87.82418823242188, 87.01628875732422, 86.22871398925781, 85.56230163574219, 84.96900177001953, 84.57625579833984, 84.36016082763672, 84.20700073242188, 84.08193969726562, 83.97764587402344, 83.87611389160156, 83.92423248291016, 84.14193725585938, 84.41809844970703, 84.70330810546875, 85.00025939941406, 85.29436492919922, 85.68895721435547, 86.27693176269531, 87.06804656982422, 88.0323715209961, 89.15747833251953, 90.61774444580078, 92.43035125732422, 94.46464538574219, 96.57106018066406, 98.82080078125, 101.0973129272461, 103.33666229248047, 105.50848388671875, 107.6570053100586, 109.891357421875, 112.15137481689453, 114.42011260986328, 116.68489074707031, 118.90473175048828, 121.11170959472656, 123.25049591064453, 125.32403564453125, 127.53121185302734, 129.89825439453125, 132.2855987548828, 134.6158905029297, 136.92697143554688, 139.15802001953125, 141.3134002685547, 143.4351806640625, 145.5569305419922, 147.65158081054688, 149.7096405029297, 151.71261596679688, 153.65261840820312, 155.51608276367188, 157.31924438476562, 159.11117553710938, 160.7533416748047, 162.2732696533203, 163.74002075195312, 165.19287109375, 166.6624298095703, 168.05679321289062, 169.36721801757812, 170.6645965576172, 171.94862365722656, 173.23680114746094, 174.46946716308594, 175.60227966308594, 176.68606567382812, 177.7667236328125, 178.8304901123047, 179.89537048339844, 180.9698944091797, 182.1023712158203, 183.38099670410156, 184.83396911621094, 186.4405059814453, 188.17733764648438, 190.03277587890625, 191.99041748046875, 193.9769287109375, 195.76626586914062, 197.2998809814453, 198.64427185058594, 199.84442138671875, 201.0236358642578, 202.19769287109375, 203.31591796875, 204.40118408203125, 205.4407196044922, 206.46392822265625, 207.45944213867188, 208.4150848388672, 209.36993408203125, 210.36520385742188, 211.35165405273438, 212.19497680664062, 212.80360412597656, 212.99081420898438, 212.8595428466797, 212.59893798828125, 212.30372619628906, 211.88113403320312, 211.2249298095703, 210.27505493164062, 209.16802978515625, 207.95042419433594, 206.6737060546875, 205.3536376953125, 203.98805236816406, 202.4827117919922, 200.79603576660156, 198.84075927734375, 196.52613830566406, 193.94662475585938, 191.1892852783203, 188.33187866210938, 185.4967803955078, 182.7758331298828, 180.3319091796875, 178.08534240722656, 175.87472534179688, 173.57350158691406, 171.1052703857422, 168.51658630371094, 165.9554443359375, 163.4188995361328, 160.97314453125, 158.5869903564453, 156.26071166992188, 154.0010223388672, 151.86273193359375, 149.84214782714844, 147.8561553955078, 145.87100219726562, 143.8812255859375, 141.9394073486328, 140.04071044921875, 138.22088623046875, 136.38259887695312, 134.54953002929688, 132.78271484375, 130.9574737548828, 129.08750915527344, 127.25975799560547, 125.4315185546875, 123.64933013916016, 121.882080078125, 120.05531311035156, 118.18463134765625, 116.25498962402344, 114.34269714355469, 112.4908447265625, 110.6985092163086, 108.94164276123047, 107.16153717041016, 105.32911682128906, 103.44462585449219, 101.6138916015625, 99.76459503173828, 97.91300964355469, 96.16510772705078, 94.41311645507812, 92.58258056640625, 90.4946517944336, 88.02781677246094, 85.19628143310547, 82.00907135009766, 78.48986053466797, 74.69635772705078, 70.86166381835938, 67.15168762207031, 63.572113037109375, 60.10674285888672, 56.803375244140625, 53.6189079284668, 50.549373626708984, 47.61164474487305, 44.77302932739258, 41.92876434326172, 39.06986999511719, 36.2219352722168, 33.32758331298828, 30.242610931396484, 26.973918914794922, 23.662368774414062, 20.41046714782715, 17.231449127197266, 14.126823425292969, 11.168815612792969, 8.347853660583496, 5.706920623779297, 3.3018741607666016, 1.2335699796676636, -0.5328974723815918, -2.043576717376709, -3.110535144805908, -3.740983486175537, -4.098943710327148, -4.4906511306762695, -4.8972249031066895, -5.2530198097229, -5.577995777130127, -5.934023857116699, -6.255759239196777, -6.630918025970459, -7.013139724731445, -7.412384033203125, -7.725191116333008, -8.017799377441406, -8.335323333740234, -8.662646293640137, -9.008383750915527, -9.383427619934082, -9.718378067016602, -10.013775825500488, -10.301630973815918, -10.562592506408691, -10.815587997436523, -11.065951347351074, -11.301687240600586, -11.448249816894531, -11.537090301513672, -11.524465560913086, -11.443005561828613, -11.383244514465332, -11.339241981506348, -11.295818328857422, -11.257658004760742, -11.223909378051758, -11.219079971313477, -11.304905891418457, -11.446738243103027, -11.616390228271484, -11.812542915344238, -12.02774429321289, -12.266841888427734, -12.534515380859375, -12.815123558044434, -13.006359100341797, -13.117430686950684, -13.182148933410645, -13.210461616516113, -13.223767280578613, -13.236565589904785, -13.257308006286621, -13.364906311035156, -13.60283374786377, -13.906349182128906, -14.247852325439453, -14.630463600158691, -15.034890174865723, -15.458684921264648, -15.909191131591797, -16.372478485107422, -16.83634376525879, -17.298728942871094, -17.954330444335938, -18.74985694885254, -19.579227447509766, -20.42566680908203, -21.43193817138672, -22.800357818603516, -24.44293212890625, -26.13048553466797, -27.82823944091797, -29.55722427368164, -31.477741241455078, -33.487709045410156, -35.511478424072266, -37.493263244628906, -39.456016540527344, -41.433685302734375, -43.504295349121094, -45.86669158935547, -48.45779037475586, -51.14822006225586, -53.83092498779297, -56.52829360961914, -59.291015625, -62.107452392578125, -64.86852264404297, -67.60960388183594, -70.36067199707031, -73.03939819335938, -75.66210174560547, -78.23661041259766, -80.80587005615234, -83.38500213623047, -85.95026397705078, -88.392578125, -90.68785095214844, -92.96864318847656, -95.2093505859375, -97.35236358642578, -99.36150360107422, -101.18042755126953, -102.92134857177734, -104.60369110107422, -106.27859497070312, -107.93692779541016, -109.50454711914062, -110.95790100097656, -112.26480102539062, -113.4476318359375, -114.55032348632812, -115.59841918945312, -116.59353637695312, -117.56787872314453, -118.43424987792969, -119.07018280029297, -119.529541015625, -119.9432144165039, -120.33118438720703, -120.70291137695312, -121.06876373291016, -121.57264709472656, -122.14915466308594, -122.72602844238281, -123.31329345703125, -123.84371948242188, -124.38484191894531, -124.94699096679688, -125.50639343261719, -126.06773376464844, -126.62725067138672, -127.21639251708984, -127.76771545410156, -128.14712524414062, -128.24986267089844, -128.0001220703125, -127.45743560791016, -126.70941925048828, -125.85266876220703, -124.98062133789062, -124.1561508178711, -123.36287689208984, -122.56819915771484, -121.65084838867188, -120.66740417480469, -119.70370483398438, -118.76301574707031, -117.76809692382812, -116.55887603759766, -115.09596252441406, -113.52935028076172, -111.99527740478516, -110.50000762939453, -108.9967041015625, -107.39553833007812, -105.7052001953125, -103.86796569824219, -101.89085388183594, -99.83897399902344, -97.75530242919922, -95.71993255615234, -93.73746490478516, -91.82310485839844, -89.95047760009766, -88.10604858398438, -86.26592254638672, -84.39051818847656, -82.42990112304688, -80.4601821899414, -78.54206085205078, -76.67953491210938, -74.87965393066406, -73.13782501220703, -71.447998046875, -69.79700469970703, -68.07174682617188, -66.20356750488281, -64.17756652832031, -62.02452850341797, -59.78955841064453, -57.599979400634766, -55.49079895019531, -53.38170623779297, -51.32799530029297, -49.24906539916992, -47.25999069213867, -45.2713508605957, -43.23389434814453, -41.17817687988281, -39.17205047607422, -37.22850799560547, -35.21967697143555, -33.25495910644531, -31.328039169311523, -29.30510902404785, -27.14748191833496, -24.93663215637207, -22.68917465209961, -20.511201858520508, -18.440406799316406, -16.442750930786133, -14.476696014404297, -12.49740982055664, -10.538829803466797, -8.549440383911133, -6.5612688064575195, -4.653802394866943, -2.830416679382324, -1.0931862592697144)
					Xmap = [-215.266 -214.266 -213.266 -212.266 -211.266 -210.266 -209.266 -208.266 -207.266 -206.266 -205.266 -204.266 -203.266 -202.266 -201.266 -200.266 -199.266 -198.266 -197.266 -196.266 -195.266 -194.266 -193.266 -192.266 -191.266 -190.266 -189.266 -188.266 -187.266 -186.266 -185.266 -184.266 -183.266 -182.266 -181.266 -180.266 -179.266 -178.266 -177.266 -176.266 -175.266 -174.266 -173.266 -172.266 -171.266 -170.266 -169.266 -168.266 -167.266 -166.266 -165.266 -164.266 -163.266 -162.266 -161.266 -160.266 -159.266 -158.266 -157.266 -156.266 -155.266 -154.266 -153.266 -152.266 -151.266 -150.266 -149.266 -148.266 -147.266 -146.266 -145.266 -144.266 -143.266 -142.266 -141.266 -140.266 -139.266 -138.266 -137.266 -136.266 -135.266 -134.266 -133.266 -132.266 -131.266 -130.266 -129.266 -128.266 -127.266 -126.266 -125.266 -124.266 -123.266 -122.266 -121.266 -120.266 -119.266 -118.266 -117.266 -116.266 -115.266 -114.266 -113.266 -112.266 -111.266 -110.266 -109.266 -108.266 -107.266 -106.266 -105.266 -104.266 -103.266 -102.266 -101.266 -100.266  -99.266  -98.266  -97.266  -96.266  -95.266  -94.266  -93.266  -92.266  -91.266  -90.266  -89.266  -88.266  -87.266  -86.266  -85.266  -84.266  -83.266  -82.266  -81.266  -80.266  -79.266  -78.266  -77.266  -76.266  -75.266  -74.266  -73.266  -72.266  -71.266  -70.266  -69.266  -68.266  -67.266  -66.266  -65.266  -64.266  -63.266  -62.266  -61.266  -60.266  -59.266  -58.266  -57.266  -56.266  -55.266  -54.266  -53.266  -52.266  -51.266  -50.266  -49.266  -48.266  -47.266  -46.266  -45.266  -44.266  -43.266  -42.266  -41.266  -40.266  -39.266  -38.266  -37.266  -36.266  -35.266  -34.266  -33.266  -32.266  -31.266  -30.266  -29.266  -28.266  -27.266  -26.266  -25.266  -24.266  -23.266  -22.266  -21.266  -20.266  -19.266  -18.266  -17.266  -16.266  -15.266  -14.266  -13.266  -12.266  -11.266  -10.266   -9.266   -8.266   -7.266   -6.266   -5.266   -4.266   -3.266   -2.266   -1.266   -0.266    0.734    1.734    2.734    3.734    4.734    5.734
					    6.734    7.734    8.734    9.734   10.734   11.734   12.734   13.734   14.734   15.734   16.734   17.734   18.734   19.734   20.734   21.734   22.734   23.734   24.734   25.734   26.734   27.734   28.734   29.734   30.734   31.734   32.734   33.734   34.734   35.734   36.734   37.734   38.734   39.734   40.734   41.734   42.734   43.734   44.734   45.734   46.734   47.734   48.734   49.734   50.734   51.734   52.734   53.734   54.734   55.734   56.734   57.734   58.734   59.734   60.734   61.734   62.734   63.734   64.734   65.734   66.734   67.734   68.734   69.734   70.734   71.734   72.734   73.734   74.734   75.734   76.734   77.734   78.734   79.734   80.734   81.734   82.734   83.734   84.734   85.734   86.734   87.734   88.734   89.734   90.734   91.734   92.734   93.734   94.734   95.734   96.734   97.734   98.734   99.734  100.734  101.734  102.734  103.734  104.734  105.734  106.734  107.734  108.734  109.734  110.734  111.734  112.734  113.734  114.734  115.734  116.734  117.734  118.734  119.734  120.734  121.734  122.734  123.734  124.734  125.734  126.734  127.734  128.734  129.734  130.734  131.734  132.734  133.734  134.734  135.734  136.734  137.734  138.734  139.734  140.734  141.734  142.734  143.734  144.734  145.734  146.734  147.734  148.734  149.734  150.734  151.734  152.734  153.734  154.734  155.734  156.734  157.734  158.734  159.734  160.734  161.734  162.734  163.734  164.734  165.734  166.734  167.734  168.734  169.734  170.734  171.734  172.734  173.734  174.734  175.734  176.734  177.734  178.734  179.734  180.734  181.734  182.734  183.734  184.734  185.734  186.734  187.734  188.734  189.734  190.734  191.734  192.734  193.734  194.734  195.734  196.734  197.734  198.734  199.734  200.734  201.734  202.734  203.734  204.734  205.734  206.734  207.734  208.734  209.734]
					Ymap = [-1.782e+02 -1.772e+02 -1.762e+02 -1.752e+02 -1.742e+02 -1.732e+02 -1.722e+02 -1.712e+02 -1.702e+02 -1.692e+02 -1.682e+02 -1.672e+02 -1.662e+02 -1.652e+02 -1.642e+02 -1.632e+02 -1.622e+02 -1.612e+02 -1.602e+02 -1.592e+02 -1.582e+02 -1.572e+02 -1.562e+02 -1.552e+02 -1.542e+02 -1.532e+02 -1.522e+02 -1.512e+02 -1.502e+02 -1.492e+02 -1.482e+02 -1.472e+02 -1.462e+02 -1.452e+02 -1.442e+02 -1.432e+02 -1.422e+02 -1.412e+02 -1.402e+02 -1.392e+02 -1.382e+02 -1.372e+02 -1.362e+02 -1.352e+02 -1.342e+02 -1.332e+02 -1.322e+02 -1.312e+02 -1.302e+02 -1.292e+02 -1.282e+02 -1.272e+02 -1.262e+02 -1.252e+02 -1.242e+02 -1.232e+02 -1.222e+02 -1.212e+02 -1.202e+02 -1.192e+02 -1.182e+02 -1.172e+02 -1.162e+02 -1.152e+02 -1.142e+02 -1.132e+02 -1.122e+02 -1.112e+02 -1.102e+02 -1.092e+02 -1.082e+02 -1.072e+02 -1.062e+02 -1.052e+02 -1.042e+02 -1.032e+02 -1.022e+02 -1.012e+02 -1.002e+02 -9.925e+01 -9.825e+01 -9.725e+01 -9.625e+01 -9.525e+01 -9.425e+01 -9.325e+01 -9.225e+01 -9.125e+01 -9.025e+01 -8.925e+01 -8.825e+01 -8.725e+01 -8.625e+01 -8.525e+01 -8.425e+01 -8.325e+01 -8.225e+01 -8.125e+01 -8.025e+01 -7.925e+01 -7.825e+01 -7.725e+01 -7.625e+01 -7.525e+01 -7.425e+01 -7.325e+01 -7.225e+01 -7.125e+01 -7.025e+01 -6.925e+01 -6.825e+01 -6.725e+01 -6.625e+01 -6.525e+01 -6.425e+01 -6.325e+01 -6.225e+01 -6.125e+01 -6.025e+01 -5.925e+01 -5.825e+01 -5.725e+01 -5.625e+01 -5.525e+01 -5.425e+01 -5.325e+01 -5.225e+01 -5.125e+01 -5.025e+01 -4.925e+01 -4.825e+01 -4.725e+01 -4.625e+01 -4.525e+01 -4.425e+01 -4.325e+01 -4.225e+01 -4.125e+01 -4.025e+01 -3.925e+01 -3.825e+01 -3.725e+01 -3.625e+01 -3.525e+01 -3.425e+01 -3.325e+01 -3.225e+01 -3.125e+01 -3.025e+01 -2.925e+01 -2.825e+01 -2.725e+01 -2.625e+01 -2.525e+01 -2.425e+01 -2.325e+01 -2.225e+01 -2.125e+01 -2.025e+01 -1.925e+01 -1.825e+01 -1.725e+01 -1.625e+01 -1.525e+01 -1.425e+01 -1.325e+01 -1.225e+01 -1.125e+01 -1.025e+01 -9.250e+00 -8.250e+00 -7.250e+00 -6.250e+00 -5.250e+00 -4.250e+00 -3.250e+00 -2.250e+00 -1.250e+00 -2.499e-01  7.501e-01  1.750e+00
					  2.750e+00  3.750e+00  4.750e+00  5.750e+00  6.750e+00  7.750e+00  8.750e+00  9.750e+00  1.075e+01  1.175e+01  1.275e+01  1.375e+01  1.475e+01  1.575e+01  1.675e+01  1.775e+01  1.875e+01  1.975e+01  2.075e+01  2.175e+01  2.275e+01  2.375e+01  2.475e+01  2.575e+01  2.675e+01  2.775e+01  2.875e+01  2.975e+01  3.075e+01  3.175e+01  3.275e+01  3.375e+01  3.475e+01  3.575e+01  3.675e+01  3.775e+01  3.875e+01  3.975e+01  4.075e+01  4.175e+01  4.275e+01  4.375e+01  4.475e+01  4.575e+01  4.675e+01  4.775e+01  4.875e+01  4.975e+01  5.075e+01  5.175e+01  5.275e+01  5.375e+01  5.475e+01  5.575e+01  5.675e+01  5.775e+01  5.875e+01  5.975e+01  6.075e+01  6.175e+01  6.275e+01  6.375e+01  6.475e+01  6.575e+01  6.675e+01  6.775e+01  6.875e+01  6.975e+01  7.075e+01  7.175e+01  7.275e+01  7.375e+01  7.475e+01  7.575e+01  7.675e+01  7.775e+01  7.875e+01  7.975e+01  8.075e+01  8.175e+01  8.275e+01  8.375e+01  8.475e+01  8.575e+01  8.675e+01  8.775e+01  8.875e+01  8.975e+01  9.075e+01  9.175e+01  9.275e+01  9.375e+01  9.475e+01  9.575e+01  9.675e+01  9.775e+01  9.875e+01  9.975e+01  1.008e+02  1.018e+02  1.028e+02  1.038e+02  1.048e+02  1.058e+02  1.068e+02  1.078e+02  1.088e+02  1.098e+02  1.108e+02  1.118e+02  1.128e+02  1.138e+02  1.148e+02  1.158e+02  1.168e+02  1.178e+02  1.188e+02  1.198e+02  1.208e+02  1.218e+02  1.228e+02  1.238e+02  1.248e+02  1.258e+02  1.268e+02  1.278e+02  1.288e+02  1.298e+02  1.308e+02  1.318e+02  1.328e+02  1.338e+02  1.348e+02  1.358e+02  1.368e+02  1.378e+02  1.388e+02  1.398e+02  1.408e+02  1.418e+02  1.428e+02  1.438e+02  1.448e+02  1.458e+02  1.468e+02  1.478e+02  1.488e+02  1.498e+02  1.508e+02  1.518e+02  1.528e+02  1.538e+02  1.548e+02  1.558e+02  1.568e+02  1.578e+02  1.588e+02  1.598e+02  1.608e+02  1.618e+02  1.628e+02  1.638e+02  1.648e+02  1.658e+02  1.668e+02  1.678e+02  1.688e+02  1.698e+02  1.708e+02  1.718e+02  1.728e+02  1.738e+02  1.748e+02  1.758e+02  1.768e+02  1.778e+02  1.788e+02  1.798e+02  1.808e+02  1.818e+02  1.828e+02
					  1.838e+02  1.848e+02  1.858e+02  1.868e+02  1.878e+02  1.888e+02  1.898e+02  1.908e+02  1.918e+02  1.928e+02  1.938e+02  1.948e+02  1.958e+02  1.968e+02  1.978e+02  1.988e+02  1.998e+02  2.008e+02  2.018e+02  2.028e+02  2.038e+02  2.048e+02  2.058e+02  2.068e+02  2.078e+02  2.088e+02  2.098e+02  2.108e+02  2.118e+02  2.128e+02  2.138e+02  2.148e+02  2.158e+02  2.168e+02  2.178e+02  2.188e+02  2.198e+02  2.208e+02  2.218e+02  2.228e+02  2.238e+02  2.248e+02  2.258e+02  2.268e+02  2.278e+02  2.288e+02  2.298e+02  2.308e+02  2.318e+02  2.328e+02  2.338e+02  2.348e+02  2.358e+02  2.368e+02  2.378e+02  2.388e+02  2.398e+02  2.408e+02  2.418e+02  2.428e+02  2.438e+02  2.448e+02  2.458e+02  2.468e+02  2.478e+02  2.488e+02  2.498e+02  2.508e+02  2.518e+02  2.528e+02  2.538e+02  2.548e+02  2.558e+02  2.568e+02  2.578e+02  2.588e+02  2.598e+02  2.608e+02  2.618e+02  2.628e+02]
					Zmap = [-5.894 -4.894 -3.894 -2.894 -1.894 -0.894  0.106  1.106  2.106  3.106  4.106  5.106  6.106  7.106  8.106  9.106 10.106 11.106 12.106 13.106]
					point_map = [[[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]]
					
					 [[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 [[291 291 291 ... 292 292 292]
					  [291 291 291 ... 291 292 292]
					  [291 291 291 ... 291 291 291]
					  ...
					  [162 162 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 ...
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [394 394 394 ... 394 394 394]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]]
					res = 1
					min_point = [-215.266 -178.250   -5.894]
					max_point = [ 209.734  262.750   13.106]
				X = [-215.266 -215.166 -215.066 ...  210.034  210.134  210.234]
				Y = [-178.250 -178.150 -178.050 ...  262.750  262.850  262.950]
				Z = [-0.894  8.722]
				cost_map = [[[ 214.381  214.381]
				  [ 214.299  214.299]
				  [ 214.217  214.217]
				  ...
				  [ 112.184  112.184]
				  [ 112.264  112.264]
				  [ 112.344  112.344]]
				
				 [[ 214.324  214.324]
				  [ 214.242  214.242]
				  [ 214.160  214.160]
				  ...
				  [ 112.124  112.124]
				  [ 112.204  112.204]
				  [ 112.284  112.284]]
				
				 [[ 214.267  214.267]
				  [ 214.185  214.185]
				  [ 214.103  214.103]
				  ...
				  [ 112.064  112.064]
				  [ 112.144  112.144]
				  [ 112.224  112.224]]
				
				 ...
				
				 [[  96.764   96.764]
				  [  96.690   96.690]
				  [  96.616   96.616]
				  ...
				  [ 242.661  242.661]
				  [ 242.689  242.689]
				  [ 242.717  242.717]]
				
				 [[  96.831   96.831]
				  [  96.757   96.757]
				  [  96.683   96.683]
				  ...
				  [ 242.757  242.757]
				  [ 242.785  242.785]
				  [ 242.813  242.813]]
				
				 [[  96.898   96.898]
				  [  96.824   96.824]
				  [  96.750   96.750]
				  ...
				  [ 242.852  242.852]
				  [ 242.881  242.881]
				  [ 242.909  242.909]]]
				res = 0.1
				min_point = [-215.266 -178.250   -0.894]
				max_point = [ 210.234  262.950    8.722]
				src = 
						def get_cost(self, state, prevstate=None):
							prevstate = state if prevstate is None else prevstate
							prevpos = prevstate["pos"][...,[0,2,1]]
							pos = state["pos"][...,[0,2,1]]
							vy = state["vel"][...,-1]
							cost = self.get_point_cost(pos, transform=True)
							progress = self.track.get_progress(prevpos, pos)
							reward = np.minimum(progress,0) + 2*progress + np.tanh(vy/self.vtarget)-np.power(self.vtarget-vy,2)/self.vtarget**2 - cost
							# reward = progress + np.tanh(vy/self.vtarget) - cost
				
				vtarget = 20
			action_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -1.000]
				high = [ 1.000  1.000  1.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			cost_queries = <list len=25>
			dynamics_size = 13
			obs = [ 1.617e-09 -3.908e-03 -7.273e-09  1.777e-12 -1.954e-01  3.555e-13  0.000e+00  0.000e+00  0.000e+00  1.000e+00  9.095e-13 -1.164e-10 -4.547e-12  0.000e+00  2.000e-02  3.657e-01  4.017e-01  4.572e-01  5.260e-01  6.036e-01  2.700e-01  3.171e-01  3.850e-01  4.646e-01  5.509e-01  1.792e-01  2.444e-01  3.277e-01  4.184e-01  5.125e-01  1.063e-01  1.973e-01  2.942e-01  3.927e-01  4.918e-01  1.024e-01  1.953e-01  2.929e-01  3.917e-01  4.910e-01]
			observation_space = Box(80,) 
				dtype = float32
				shape = (80,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
			src = 		return state
				
					def step(self, action):
						self.time += 1
						next_state, reward, done, info = self.env.step(action)
						idle = next_state[29]
						done = done or idle>self.idle_timeout or self.time > self.max_time
						next_state, next_spec = self.observation(next_state)
						terminal = -(1-self.time/self.max_time)*int(done)
						reward = -self.cost_model.get_cost(next_spec, self.spec) + terminal
						self.spec = next_spec
			
			max_time = 500
			time = 0
			idle_timeout = 10
			spec = EnvSpec(CarRacing-v1) 
				id = CarRacing-v1
				entry_point = <class 'src.envs.CarRacing.car_racing.CarRacing'> 
					reset = <function CarRacing.reset at 0x7f4d47498710>
					step = <function CarRacing.step at 0x7f4d47498680>
					render = <function CarRacing.render at 0x7f4d71902c20>
					dynamics_spec = <staticmethod object at 0x7f4d47497d10>
					track_spec = <function CarRacing.track_spec at 0x7f4d71902d40>
					observation = <function CarRacing.observation at 0x7f4d71902dd0>
					dynamics_keys = <staticmethod object at 0x7f4d47497c10>
					observation_spec = <staticmethod object at 0x7f4d47497c50>
					close = <function CarRacing.close at 0x7f4d71902f80>
					id = 2
				reward_threshold = None
				nondeterministic = False
				max_episode_steps = None
			verbose = 0
		action_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -1.000]
			high = [ 1.000  1.000  1.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		observation_space = Box(80,) 
			dtype = float32
			shape = (80,)
			low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
			high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
			bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': []}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f4cc3714710> 
			observation_space = Box(80,) 
				dtype = float32
				shape = (80,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
	state_size = (80,)
	action_size = (3,)
	action_space = Box(3,) 
		dtype = float32
		shape = (3,)
		low = [-1.000 -1.000 -1.000]
		high = [ 1.000  1.000  1.000]
		bounded_below = [ True  True  True]
		bounded_above = [ True  True  True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7f4cc3714990> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {9001: <socket.socket fd=201, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48696), raddr=('127.0.0.1', 9001)>, 9002: <socket.socket fd=202, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 36026), raddr=('127.0.0.1', 9002)>, 9003: <socket.socket fd=203, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49458), raddr=('127.0.0.1', 9003)>, 9004: <socket.socket fd=204, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 38198), raddr=('127.0.0.1', 9004)>, 9005: <socket.socket fd=205, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48774), raddr=('127.0.0.1', 9005)>, 9006: <socket.socket fd=206, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 53388), raddr=('127.0.0.1', 9006)>, 9007: <socket.socket fd=207, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 53590), raddr=('127.0.0.1', 9007)>, 9008: <socket.socket fd=208, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 43910), raddr=('127.0.0.1', 9008)>, 9009: <socket.socket fd=209, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 36624), raddr=('127.0.0.1', 9009)>, 9010: <socket.socket fd=210, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 43792), raddr=('127.0.0.1', 9010)>, 9011: <socket.socket fd=211, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48206), raddr=('127.0.0.1', 9011)>, 9012: <socket.socket fd=212, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 51162), raddr=('127.0.0.1', 9012)>, 9013: <socket.socket fd=213, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46428), raddr=('127.0.0.1', 9013)>, 9014: <socket.socket fd=214, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 60900), raddr=('127.0.0.1', 9014)>, 9015: <socket.socket fd=215, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59112), raddr=('127.0.0.1', 9015)>, 9016: <socket.socket fd=216, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 45750), raddr=('127.0.0.1', 9016)>}
	num_envs = 16
	max_steps = 1000,
agent: <src.models.wrappers.ParallelAgent object at 0x7f4cc3714050> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f4cc378be10> 
		state_size = (80,)
	agent = <src.models.pytorch.mpc.mppi.MPPIAgent object at 0x7f4cc378bb90> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f4cc378b250> 
			size = (3,)
			dt = 0.2
			action = [-0.764  0.477  0.057]
			daction_dt = [-1.329  1.563 -1.041]
		discrete = False
		action_size = (3,)
		state_size = (80,)
		config = <src.utils.config.Config object at 0x7f4cd00c1d50> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.98
			NUM_STEPS = None
			MAX_BUFFER_SIZE = 100000
			REPLAY_BATCH_SIZE = 10000
			TARGET_UPDATE_RATE = 0.0004
			BATCH_SIZE = 250
			DYN_EPOCHS = 1
			TRAIN_EVERY = 10000
			ENV_MODEL = dfrntl
			MPC = <src.utils.config.Config object at 0x7f4d7188fa50> 
				NSAMPLES = 100
				HORIZON = 40
				LAMBDA = 0.1
				COV = 0.5
			REWARD_MODEL = src.envs.CarRacing.objective.cost:CostModel
			DYNAMICS_SPEC = src.envs.CarRacing.car_racing:CarRacing
			dynamics_size = 13
			state_size = (80,)
			action_size = (3,)
			env_name = CarRacing-v1
			rank = 0
			size = 17
			split = 17
			model = mppi
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 1000000
			render = False
			trial = False
			icm = False
			rs = False
			DYN = <src.utils.config.Config object at 0x7f4cd00b2d10> 
				REG_LAMBDA = 1e-06
				FACTOR = 0.5
				PATIENCE = 5
				LEARN_RATE = 0.0001
				TRANSITION_HIDDEN = 512
				REWARD_HIDDEN = 256
				BETA_DYN = 1
				BETA_DOT = 0
				BETA_DDOT = 0
		stats = <src.utils.logger.Stats object at 0x7f4cc378bd50> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = MPPIController() 
			training = True
			tau = 0.0004
			name = mppi
			stats = <src.utils.logger.Stats object at 0x7f4cc378b610> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7f4cd00c1d50> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.98
				NUM_STEPS = None
				MAX_BUFFER_SIZE = 100000
				REPLAY_BATCH_SIZE = 10000
				TARGET_UPDATE_RATE = 0.0004
				BATCH_SIZE = 250
				DYN_EPOCHS = 1
				TRAIN_EVERY = 10000
				ENV_MODEL = dfrntl
				MPC = <src.utils.config.Config object at 0x7f4d7188fa50> 
					NSAMPLES = 100
					HORIZON = 40
					LAMBDA = 0.1
					COV = 0.5
				REWARD_MODEL = src.envs.CarRacing.objective.cost:CostModel
				DYNAMICS_SPEC = src.envs.CarRacing.car_racing:CarRacing
				dynamics_size = 13
				state_size = (80,)
				action_size = (3,)
				env_name = CarRacing-v1
				rank = 0
				size = 17
				split = 17
				model = mppi
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 1000000
				render = False
				trial = False
				icm = False
				rs = False
				DYN = <src.utils.config.Config object at 0x7f4cd00b2d10> 
					REG_LAMBDA = 1e-06
					FACTOR = 0.5
					PATIENCE = 5
					LEARN_RATE = 0.0001
					TRANSITION_HIDDEN = 512
					REWARD_HIDDEN = 256
					BETA_DYN = 1
					BETA_DOT = 0
					BETA_DDOT = 0
			device = cuda
			envmodel = <src.models.pytorch.mpc.EnvModel object at 0x7f4cc378be90> 
				network = DifferentialEnv(
					  (reward): RewardModel(
					    (linear1): Linear(in_features=29, out_features=256, bias=True)
					    (drop1): Dropout(p=0.5, inplace=False)
					    (linear2): Linear(in_features=256, out_features=256, bias=True)
					    (drop2): Dropout(p=0.5, inplace=False)
					    (linear3): Linear(in_features=256, out_features=256, bias=True)
					    (linear4): Linear(in_features=256, out_features=1, bias=True)
					  )
					  (dynamics): TransitionModel(
					    (gru): GRUCell(29, 512)
					    (linear1): Linear(in_features=512, out_features=512, bias=True)
					    (drop1): Dropout(p=0.5, inplace=False)
					    (linear2): Linear(in_features=512, out_features=512, bias=True)
					    (drop2): Dropout(p=0.5, inplace=False)
					    (state_ddot): Linear(in_features=512, out_features=13, bias=True)
					  )
					) 
					training = True
					tau = 0.0004
					name = dfrntl
					stats = <src.utils.logger.Stats object at 0x7f4cc80eebd0> 
						mean_dict = {}
						sum_dict = {}
					config = <src.utils.config.Config object at 0x7f4cd00c1d50> 
						TRIAL_AT = 1000
						SAVE_AT = 1
						SEED = 0
						REG_LAMBDA = 1e-06
						LEARN_RATE = 0.0001
						DISCOUNT_RATE = 0.99
						ADVANTAGE_DECAY = 0.95
						INPUT_LAYER = 512
						ACTOR_HIDDEN = 256
						CRITIC_HIDDEN = 1024
						EPS_MAX = 1.0
						EPS_MIN = 0.1
						EPS_DECAY = 0.98
						NUM_STEPS = None
						MAX_BUFFER_SIZE = 100000
						REPLAY_BATCH_SIZE = 10000
						TARGET_UPDATE_RATE = 0.0004
						BATCH_SIZE = 250
						DYN_EPOCHS = 1
						TRAIN_EVERY = 10000
						ENV_MODEL = dfrntl
						MPC = <src.utils.config.Config object at 0x7f4d7188fa50> 
							NSAMPLES = 100
							HORIZON = 40
							LAMBDA = 0.1
							COV = 0.5
						REWARD_MODEL = src.envs.CarRacing.objective.cost:CostModel
						DYNAMICS_SPEC = src.envs.CarRacing.car_racing:CarRacing
						dynamics_size = 13
						state_size = (80,)
						action_size = (3,)
						env_name = CarRacing-v1
						rank = 0
						size = 17
						split = 17
						model = mppi
						framework = pt
						train_prop = 1.0
						tcp_ports = <list len=17>
						tcp_rank = 0
						num_envs = 1
						nsteps = 1000000
						render = False
						trial = False
						icm = False
						rs = False
						DYN = <src.utils.config.Config object at 0x7f4cd00b2d10> 
							REG_LAMBDA = 1e-06
							FACTOR = 0.5
							PATIENCE = 5
							LEARN_RATE = 0.0001
							TRANSITION_HIDDEN = 512
							REWARD_HIDDEN = 256
							BETA_DYN = 1
							BETA_DOT = 0
							BETA_DDOT = 0
					device = cuda
					state_size = (80,)
					action_size = (3,)
					discrete = False
					dyn_index = 13
					optimizer = Adam (
					Parameter Group 0
					    amsgrad: False
					    betas: (0.9, 0.999)
					    eps: 1e-08
					    lr: 0.0001
					    weight_decay: 1e-06
					)
					scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cc3743b10>
				state_size = (80,)
				action_size = (3,)
			mu = [ 0.000  0.000  0.000]
			cov = [[ 0.500  0.000  0.000]
			 [ 0.000  0.500  0.000]
			 [ 0.000  0.000  0.500]]
			icov = [[ 2.000  0.000  0.000]
			 [ 0.000  2.000  0.000]
			 [ 0.000  0.000  2.000]]
			lamda = 0.1
			horizon = 40
			nsamples = 100
			action_size = (3,)
			control = [[[ 0.635  0.703  0.858]
			  [-0.814 -0.899 -0.584]
			  [-0.720  0.138  0.461]
			  [ 0.369  0.670  0.176]
			  [ 0.333 -0.634  0.930]
			  [-0.606 -0.451 -0.424]
			  [-0.042 -0.802  0.214]
			  [ 0.949  0.542 -0.289]
			  [-0.330  0.004  0.930]
			  [ 0.211 -0.843  0.425]
			  [ 0.737  0.194 -0.362]
			  [ 0.781  0.006  0.874]
			  [-0.292  0.736 -0.709]
			  [ 0.178  0.752  0.073]
			  [-0.102  0.722 -0.944]
			  [ 0.103  0.774  0.555]
			  [ 0.968 -0.993 -0.368]
			  [ 0.624  0.818 -0.286]
			  [ 0.636  0.838  0.389]
			  [-0.187  0.804 -0.764]
			  [-0.159 -0.054  0.557]
			  [-0.131  0.999  0.773]
			  [ 0.234 -0.278 -0.462]
			  [-0.724  0.777  0.576]
			  [-0.141 -0.286 -0.283]
			  [-0.833 -0.691 -0.597]
			  [-0.497 -0.799  0.445]
			  [-0.553 -0.896  0.156]
			  [-0.043  0.825 -0.255]
			  [ 0.594 -0.214  0.240]
			  [ 0.396 -0.427 -0.046]
			  [-0.331 -0.726  0.394]
			  [-0.377 -0.977 -0.358]
			  [-0.803  0.400 -0.824]
			  [ 0.472 -0.910 -0.049]
			  [-0.839  0.506  0.536]
			  [ 0.815  0.818  0.161]
			  [-0.868 -0.482  0.391]
			  [-0.724  0.481 -0.748]
			  [-0.239  0.889 -0.492]]]
			noise = [[[[-0.432  0.880  0.014]
			   [ 0.322  0.600 -0.061]
			   [ 0.420 -0.754  0.692]
			   ...
			   [-0.570  1.229  0.173]
			   [-0.385  0.579 -0.035]
			   [ 0.871 -0.344  0.261]]
			
			  [[ 1.132  1.102  0.049]
			   [ 0.543  0.431  0.542]
			   [-0.400  0.434 -0.026]
			   ...
			   [ 0.294  0.758 -0.841]
			   [ 0.628 -0.251  1.257]
			   [ 0.116  0.344 -0.555]]
			
			  [[ 0.138  0.622 -1.161]
			   [ 0.537 -0.894  1.049]
			   [-0.067 -0.050  0.905]
			   ...
			   [-0.341 -0.296 -0.851]
			   [ 0.450 -2.339 -0.529]
			   [-0.029  0.025  0.532]]
			
			  ...
			
			  [[ 0.668 -0.318  0.276]
			   [-0.706 -0.262  1.138]
			   [ 1.096 -0.278 -0.301]
			   ...
			   [-0.797 -0.420  1.377]
			   [-0.646  0.735 -0.510]
			   [-0.601 -0.154 -0.296]]
			
			  [[-0.425  0.246 -0.257]
			   [-0.218  0.766 -0.108]
			   [-0.024  0.876  1.197]
			   ...
			   [-0.477  0.422  0.854]
			   [ 0.909 -0.216 -0.862]
			   [ 1.002 -1.031  0.318]]
			
			  [[ 0.282 -2.057 -0.574]
			   [-0.531  0.396 -0.174]
			   [-0.322 -0.094  0.522]
			   ...
			   [-0.022 -0.476 -0.682]
			   [ 0.844  0.231 -0.578]
			   [ 0.708 -0.123 -0.213]]]]
			init_cost = [[ -3.781  12.216   2.008  10.082 -14.982  -7.652   8.520   5.809  11.891   0.976   8.361 -11.796  -1.101  -5.646   8.350  -3.584  -3.788 -13.170   9.146 -12.977  -4.839  -5.793  -3.439   2.478  14.877  12.790  -5.532  -2.187 -13.139  -4.575   7.550 -10.672 -10.054  -1.659  12.454  -9.500   4.633 -17.733   9.567 -11.836  -2.317  20.974 -25.495   9.581   0.548  19.092  -3.142   0.725  -9.913  -1.753   8.729   7.040   3.817  -4.712  -0.989  -0.930   6.509   1.787   2.445   4.471  14.881   6.058 -16.942  -5.418  -1.295  -0.833  -0.699  14.470  11.822   0.057 -14.894   6.092 -14.879  -0.676   7.314   2.264  -4.315  -2.667   2.151   2.592  15.445   1.623   0.589 -12.938   1.757  -3.744  -0.899 -16.901   1.740  -1.191   2.337 -10.952   0.422   8.264  13.109  -0.146  20.082   5.732  -1.012 -10.170]]
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f4cc3743a10> 
			buffer = deque([], maxlen=100000)
		buffer = []
		dataset = <class 'src.data.loaders.OnlineDataset'>
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f4cc3743bd0> 
		size = (3,)
		dt = 0.2
		action = [-0.533 -0.063 -0.777]
		daction_dt = [-1.283  0.147 -0.167]
	discrete = False
	action_size = (3,)
	state_size = (80,)
	config = <src.utils.config.Config object at 0x7f4cd00c1d50> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.98
		NUM_STEPS = None
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 10000
		TARGET_UPDATE_RATE = 0.0004
		BATCH_SIZE = 250
		DYN_EPOCHS = 1
		TRAIN_EVERY = 10000
		ENV_MODEL = dfrntl
		MPC = <src.utils.config.Config object at 0x7f4d7188fa50> 
			NSAMPLES = 100
			HORIZON = 40
			LAMBDA = 0.1
			COV = 0.5
		REWARD_MODEL = src.envs.CarRacing.objective.cost:CostModel
		DYNAMICS_SPEC = src.envs.CarRacing.car_racing:CarRacing
		dynamics_size = 13
		state_size = (80,)
		action_size = (3,)
		env_name = CarRacing-v1
		rank = 0
		size = 17
		split = 17
		model = mppi
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 1000000
		render = False
		trial = False
		icm = False
		rs = False
		DYN = <src.utils.config.Config object at 0x7f4cd00b2d10> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.0001
			TRANSITION_HIDDEN = 512
			REWARD_HIDDEN = 256
			BETA_DYN = 1
			BETA_DOT = 0
			BETA_DDOT = 0
	stats = <src.utils.logger.Stats object at 0x7f4cc20cd6d0> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import tqdm
import torch
import random
import numpy as np
import scipy as sp
from scipy.stats import multivariate_normal
from src.utils.rand import RandomAgent, ReplayBuffer
from src.utils.misc import load_module
from ..agents.base import PTNetwork, PTAgent, Conv, one_hot_from_indices
from . import EnvModel

class MPPIController(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="mppi"):
		super().__init__(config, gpu=gpu, name=name)
		self.envmodel = EnvModel(state_size, action_size, config, load=load, gpu=gpu)
		self.mu = np.zeros(action_size)
		self.cov = np.diag(np.ones(action_size))*config.MPC.COV
		self.icov = np.linalg.inv(self.cov)
		self.lamda = config.MPC.LAMBDA
		self.horizon = config.MPC.HORIZON
		self.nsamples = config.MPC.NSAMPLES
		self.config = config
		self.action_size = action_size
		self.init_control()

	def get_action(self, state, eps=None, sample=True):
		batch = state.shape[:-1]
		horizon = max(int((1-eps)*self.horizon),1) if eps else self.horizon
		if len(batch) and self.control.shape[0] != batch[0]: self.init_control(batch[0])
		x = torch.Tensor(state).view(*batch, 1,-1).repeat_interleave(self.nsamples, -2)
		controls = np.clip(self.control[:,None,:,:] + self.noise, -1, 1)
		self.states, rewards = self.envmodel.rollout(controls[...,:horizon,:], x, numpy=True)
		costs = -np.sum(rewards, -1) #+ self.lamda * np.copy(self.init_cost)
		beta = np.min(costs, -1, keepdims=True)
		costs_norm = -(costs - beta)/self.lamda
		weights = sp.special.softmax(costs_norm, axis=-1)
		self.control += np.sum(weights[:,:,None,None]*self.noise, len(batch))
		action = self.control[...,0,:]
		self.control = np.roll(self.control, -1, axis=-2)
		self.control[...,-1,:] = 0
		return action

	def init_control(self, batch_size=1):
		self.control = np.random.uniform(-1, 1, size=[1, self.horizon, *self.action_size]).repeat(batch_size, 0)
		self.noise = np.random.multivariate_normal(self.mu, self.cov, size=[1, self.nsamples, self.horizon]).repeat(batch_size, 0)
		self.init_cost = np.sum(self.control[:,None,:,None,:] @ self.icov[None,None,None,:,:] @ self.noise[:,:,:,:,None], axis=(2,3,4))

	def optimize(self, states, actions, next_states, rewards, dones):
		return self.envmodel.optimize(states, actions, next_states, rewards, dones)

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.save_model(dirname, name, net)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.load_model(dirname, name, net)

	def get_stats(self):
		return {**super().get_stats(), **self.envmodel.get_stats()}

class MPPIAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, MPPIController, gpu=gpu, load=load)
		self.dataset = load_module("src.data.loaders:OnlineDataset")

	def get_action(self, state, eps=None, sample=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		action_greedy = self.network.get_action(np.array(state), eps)
		action = np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action

	def partition(self, x):
		if self.config.NUM_STEPS is None:
			return x[None,...]
		num_splits = x.shape[0]//self.config.NUM_STEPS
		if num_splits == 0:
			arr = np.zeros([self.config.NUM_STEPS, *x.shape[1:]])
			arr[-x.shape[0]:] = x
			num_splits = 1
			x = arr
		arr = x[:num_splits*self.config.NUM_STEPS].reshape(num_splits, self.config.NUM_STEPS, *x.shape[1:])
		return arr

	def train(self, state, action, next_state, reward, done):
		self.time = getattr(self, "time", 0) + 1
		if not hasattr(self, "buffers"): self.buffers = [[] for _ in done]
		for buffer, s, a, ns, r, d in zip(self.buffers, state, action, next_state, reward, done):
			buffer.append((s, a, s if d else ns, r, d))
			if not d: continue
			states, actions, next_states, rewards, dones = map(np.array, zip(*buffer))
			states, actions, next_states, rewards, dones = [self.partition(x) for x in (states, actions, next_states, rewards, dones)]
			buffer.clear()
			self.replay_buffer.extend(list(zip(states, actions, next_states, rewards, dones)), shuffle=False)
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE and self.time % self.config.TRAIN_EVERY == 0:
			losses = []
			samples = list(self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=None)[0])
			dataset = self.dataset(self.config, samples, seq_len=self.config.MPC.HORIZON)
			loader = torch.utils.data.DataLoader(dataset, batch_size=self.config.BATCH_SIZE, shuffle=True)
			pbar = tqdm.tqdm(loader)
			for states, actions, next_states, rewards, dones in pbar:
				losses.append(self.network.optimize(states, actions, next_states, rewards, dones))
				pbar.set_postfix_str(f"Loss: {losses[-1]:.4f}")
			self.network.envmodel.network.schedule(np.mean(losses))
			self.eps = max(self.eps * self.config.EPS_DECAY, self.config.EPS_MIN)
		self.stats.mean(len=len(self.replay_buffer))


Step:       0, Reward:  -774.300 [  89.238], Avg:  -774.300 (1.000) <0-00:00:00> ({'r_t':    -1.1339, 'eps':     1.0000, 'len':   0.00e+00, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    1000, Reward:  -768.418 [  75.757], Avg:  -771.359 (1.000) <0-00:00:58> ({'r_t': -1901.9735, 'eps':     1.0000, 'len':    41.6380, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    2000, Reward:  -785.184 [  93.931], Avg:  -775.968 (1.000) <0-00:01:56> ({'r_t': -2107.9540, 'eps':     1.0000, 'len':   137.8950, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    3000, Reward:  -732.038 [ 124.002], Avg:  -764.985 (1.000) <0-00:02:55> ({'r_t': -2004.6622, 'eps':     1.0000, 'len':   236.1710, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    4000, Reward:  -736.409 [ 108.888], Avg:  -759.270 (1.000) <0-00:03:53> ({'r_t': -2111.9567, 'eps':     1.0000, 'len':   329.4050, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    5000, Reward:  -769.355 [  98.431], Avg:  -760.951 (1.000) <0-00:04:51> ({'r_t': -1891.3421, 'eps':     1.0000, 'len':   431.9310, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    6000, Reward:  -747.283 [ 124.082], Avg:  -758.998 (1.000) <0-00:05:49> ({'r_t': -2199.2282, 'eps':     1.0000, 'len':   522.8680, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    7000, Reward:  -744.312 [ 153.500], Avg:  -757.163 (1.000) <0-00:06:48> ({'r_t': -2054.1842, 'eps':     1.0000, 'len':   616.8680, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    8000, Reward:  -717.071 [ 155.878], Avg:  -752.708 (1.000) <0-00:07:46> ({'r_t': -2726.5919, 'eps':     1.0000, 'len':   713.7220, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    9000, Reward:  -756.755 [ 143.848], Avg:  -753.113 (1.000) <0-00:08:45> ({'r_t': -2006.7474, 'eps':     1.0000, 'len':   821.1370, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   10000, Reward:  -765.442 [ 108.988], Avg:  -754.234 (1.000) <0-00:09:43> ({'r_t': -1561.2785, 'eps':     1.0000, 'len':   929.4730, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   11000, Reward:  -808.642 [  68.864], Avg:  -758.768 (1.000) <0-00:10:41> ({'r_t': -1587.9774, 'eps':     1.0000, 'len':  1024.4300, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   12000, Reward:  -745.653 [ 115.659], Avg:  -757.759 (1.000) <0-00:11:40> ({'r_t': -1814.5331, 'eps':     1.0000, 'len':  1130.9050, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   13000, Reward:  -616.112 [ 196.757], Avg:  -747.641 (1.000) <0-00:12:39> ({'r_t': -1995.0273, 'eps':     1.0000, 'len':  1227.9380, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   14000, Reward:  -774.296 [  54.046], Avg:  -749.418 (1.000) <0-00:13:38> ({'r_t': -1988.4488, 'eps':     1.0000, 'len':  1329.4910, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   15000, Reward:  -758.920 [ 104.719], Avg:  -750.012 (1.000) <0-00:14:36> ({'r_t': -2840.5701, 'eps':     1.0000, 'len':  1419.3480, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   16000, Reward:  -755.051 [ 112.113], Avg:  -750.308 (1.000) <0-00:15:36> ({'r_t': -1823.0649, 'eps':     1.0000, 'len':  1515.3360, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   17000, Reward:  -748.925 [ 133.855], Avg:  -750.232 (1.000) <0-00:16:35> ({'r_t': -1943.0172, 'eps':     1.0000, 'len':  1613.6620, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   18000, Reward:  -762.032 [ 133.170], Avg:  -750.853 (1.000) <0-00:17:35> ({'r_t': -2130.2992, 'eps':     1.0000, 'len':  1717.3260, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   19000, Reward:  -725.329 [ 152.516], Avg:  -749.576 (1.000) <0-00:18:35> ({'r_t': -2134.6865, 'eps':     1.0000, 'len':  1821.6870, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   20000, Reward:  -769.147 [  69.036], Avg:  -750.508 (1.000) <0-00:19:35> ({'r_t': -1772.0432, 'eps':     1.0000, 'len':  1922.8480, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   21000, Reward:  -732.616 [ 124.704], Avg:  -749.695 (1.000) <0-00:20:35> ({'r_t': -1899.0659, 'eps':     1.0000, 'len':  2014.5500, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   22000, Reward:  -715.082 [ 145.526], Avg:  -748.190 (1.000) <0-00:21:34> ({'r_t': -1847.4868, 'eps':     1.0000, 'len':  2111.3880, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   23000, Reward:  -784.629 [  80.149], Avg:  -749.708 (1.000) <0-00:22:32> ({'r_t': -1601.0750, 'eps':     1.0000, 'len':  2201.9060, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   24000, Reward:  -686.669 [ 143.207], Avg:  -747.187 (1.000) <0-00:23:31> ({'r_t': -2083.4970, 'eps':     1.0000, 'len':  2297.0490, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   25000, Reward:  -741.272 [ 129.885], Avg:  -746.959 (1.000) <0-00:24:29> ({'r_t': -1910.2565, 'eps':     1.0000, 'len':  2395.4570, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   26000, Reward:  -736.737 [ 145.166], Avg:  -746.581 (1.000) <0-00:25:27> ({'r_t': -1366.1884, 'eps':     1.0000, 'len':  2498.7610, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   27000, Reward:  -764.790 [  88.313], Avg:  -747.231 (1.000) <0-00:26:26> ({'r_t': -1565.0887, 'eps':     1.0000, 'len':  2601.9150, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   28000, Reward:  -779.666 [  67.660], Avg:  -748.350 (1.000) <0-00:27:25> ({'r_t': -2371.3917, 'eps':     1.0000, 'len':  2699.1610, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   29000, Reward:  -725.548 [ 138.500], Avg:  -747.589 (1.000) <0-00:28:23> ({'r_t': -2417.7890, 'eps':     1.0000, 'len':  2794.6530, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   30000, Reward:  -731.563 [ 143.410], Avg:  -747.073 (1.000) <0-00:29:21> ({'r_t': -2369.7026, 'eps':     1.0000, 'len':  2893.4390, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   31000, Reward:  -770.797 [  73.176], Avg:  -747.814 (1.000) <0-00:30:20> ({'r_t': -1691.4439, 'eps':     1.0000, 'len':  2997.7350, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   32000, Reward:  -719.921 [ 164.440], Avg:  -746.969 (1.000) <0-00:31:19> ({'r_t': -2033.6240, 'eps':     1.0000, 'len':  3105.9940, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   33000, Reward:  -714.215 [ 123.898], Avg:  -746.005 (1.000) <0-00:32:17> ({'r_t': -1662.7412, 'eps':     1.0000, 'len':  3206.7110, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   34000, Reward:  -773.330 [  97.015], Avg:  -746.786 (1.000) <0-00:33:16> ({'r_t': -1855.6349, 'eps':     1.0000, 'len':  3313.6370, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   35000, Reward:  -741.640 [ 116.333], Avg:  -746.643 (1.000) <0-00:34:14> ({'r_t': -1914.8242, 'eps':     1.0000, 'len':  3411.9290, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   36000, Reward:  -785.515 [  88.755], Avg:  -747.694 (1.000) <0-00:35:13> ({'r_t': -1896.2335, 'eps':     1.0000, 'len':  3506.9500, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   37000, Reward:  -805.182 [  65.095], Avg:  -749.207 (1.000) <0-00:36:11> ({'r_t': -2145.6829, 'eps':     1.0000, 'len':  3611.7940, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   38000, Reward:  -695.582 [ 211.817], Avg:  -747.832 (1.000) <0-00:37:10> ({'r_t': -2028.4520, 'eps':     1.0000, 'len':  3713.7830, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   39000, Reward:  -759.325 [  88.504], Avg:  -748.119 (1.000) <0-00:38:08> ({'r_t': -1933.7091, 'eps':     1.0000, 'len':  3813.2180, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   40000, Reward:  -677.569 [ 209.824], Avg:  -746.398 (1.000) <0-00:39:07> ({'r_t': -1834.5177, 'eps':     1.0000, 'len':  3916.3770, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   41000, Reward:  -761.927 [ 109.002], Avg:  -746.768 (1.000) <0-00:40:06> ({'r_t': -2554.6312, 'eps':     1.0000, 'len':  4015.0160, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   42000, Reward:  -743.176 [ 112.282], Avg:  -746.684 (1.000) <0-00:41:04> ({'r_t': -1800.0252, 'eps':     1.0000, 'len':  4109.8070, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   43000, Reward:  -767.643 [  98.387], Avg:  -747.161 (1.000) <0-00:42:03> ({'r_t': -2639.3599, 'eps':     1.0000, 'len':  4211.3420, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   44000, Reward:  -713.731 [ 112.142], Avg:  -746.418 (1.000) <0-00:43:02> ({'r_t': -2535.4244, 'eps':     1.0000, 'len':  4312.6220, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   45000, Reward:  -677.204 [ 183.005], Avg:  -744.913 (1.000) <0-00:44:01> ({'r_t': -1908.4221, 'eps':     1.0000, 'len':  4420.2690, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   46000, Reward:  -765.487 [ 137.097], Avg:  -745.351 (1.000) <0-00:44:59> ({'r_t': -2373.8145, 'eps':     1.0000, 'len':  4514.0890, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   47000, Reward:  -774.785 [ 131.143], Avg:  -745.964 (1.000) <0-00:45:58> ({'r_t': -1908.4645, 'eps':     1.0000, 'len':  4611.4790, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   48000, Reward:  -756.691 [ 104.627], Avg:  -746.183 (1.000) <0-00:46:57> ({'r_t': -2969.1861, 'eps':     1.0000, 'len':  4707.5890, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   49000, Reward:  -749.579 [ 103.362], Avg:  -746.251 (1.000) <0-00:47:56> ({'r_t': -1700.8432, 'eps':     1.0000, 'len':  4796.1430, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   50000, Reward:  -662.736 [ 214.447], Avg:  -744.613 (1.000) <0-00:48:54> ({'r_t': -1979.8197, 'eps':     1.0000, 'len':  4897.2480, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   51000, Reward:  -742.446 [ 168.830], Avg:  -744.572 (1.000) <0-00:49:53> ({'r_t': -1992.5685, 'eps':     1.0000, 'len':  4997.0120, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   52000, Reward:  -711.240 [ 134.502], Avg:  -743.943 (1.000) <0-00:50:52> ({'r_t': -1802.5887, 'eps':     1.0000, 'len':  5094.7070, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   53000, Reward:  -722.752 [ 156.583], Avg:  -743.550 (1.000) <0-00:51:50> ({'r_t': -2617.9779, 'eps':     1.0000, 'len':  5185.7970, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   54000, Reward:  -724.910 [ 176.433], Avg:  -743.211 (1.000) <0-00:52:49> ({'r_t': -2352.0696, 'eps':     1.0000, 'len':  5279.5660, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   55000, Reward:  -718.355 [ 150.560], Avg:  -742.768 (1.000) <0-00:53:48> ({'r_t': -1560.8138, 'eps':     1.0000, 'len':  5381.3600, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   56000, Reward:  -748.556 [ 111.892], Avg:  -742.869 (1.000) <0-00:54:47> ({'r_t': -1813.3528, 'eps':     1.0000, 'len':  5474.6460, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   57000, Reward:  -699.569 [ 131.506], Avg:  -742.123 (1.000) <0-00:55:46> ({'r_t': -2492.5245, 'eps':     1.0000, 'len':  5569.6060, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   58000, Reward:  -793.909 [  71.506], Avg:  -743.000 (1.000) <0-00:56:44> ({'r_t': -1983.3628, 'eps':     1.0000, 'len':  5669.3640, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   59000, Reward:  -723.471 [ 156.802], Avg:  -742.675 (1.000) <0-00:57:43> ({'r_t': -1810.0390, 'eps':     1.0000, 'len':  5767.6230, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   60000, Reward:  -770.802 [  96.234], Avg:  -743.136 (1.000) <0-00:58:41> ({'r_t': -1814.1963, 'eps':     1.0000, 'len':  5876.1910, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   61000, Reward:  -788.231 [  78.475], Avg:  -743.863 (1.000) <0-00:59:40> ({'r_t': -2720.1181, 'eps':     1.0000, 'len':  5973.5240, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   62000, Reward:  -721.066 [ 133.553], Avg:  -743.501 (1.000) <0-01:00:39> ({'r_t': -1636.0308, 'eps':     1.0000, 'len':  6070.8260, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   63000, Reward:  -783.638 [  65.926], Avg:  -744.129 (1.000) <0-01:01:38> ({'r_t': -1621.4739, 'eps':     1.0000, 'len':  6168.1110, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   64000, Reward:  -732.442 [ 111.778], Avg:  -743.949 (1.000) <0-01:02:36> ({'r_t': -1936.9327, 'eps':     1.0000, 'len':  6270.1190, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   65000, Reward:  -768.493 [  68.373], Avg:  -744.321 (1.000) <0-01:03:35> ({'r_t': -2098.1600, 'eps':     1.0000, 'len':  6380.9250, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   66000, Reward:  -767.595 [  98.512], Avg:  -744.668 (1.000) <0-01:04:34> ({'r_t': -1757.8291, 'eps':     1.0000, 'len':  6484.1140, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   67000, Reward:  -746.888 [ 124.909], Avg:  -744.701 (1.000) <0-01:05:32> ({'r_t': -1966.2197, 'eps':     1.0000, 'len':  6584.8880, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   68000, Reward:  -796.134 [  80.161], Avg:  -745.446 (1.000) <0-01:06:31> ({'r_t': -2623.2321, 'eps':     1.0000, 'len':  6680.9880, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   69000, Reward:  -761.452 [  87.723], Avg:  -745.675 (1.000) <0-01:07:30> ({'r_t': -2178.8155, 'eps':     1.0000, 'len':  6779.0700, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   70000, Reward:  -732.868 [ 152.698], Avg:  -745.494 (1.000) <0-01:08:29> ({'r_t': -1713.5316, 'eps':     1.0000, 'len':  6880.9960, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   71000, Reward:  -725.631 [ 123.166], Avg:  -745.218 (1.000) <0-01:09:27> ({'r_t': -2102.6176, 'eps':     1.0000, 'len':  6979.3540, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   72000, Reward:  -762.242 [  94.674], Avg:  -745.452 (1.000) <0-01:10:27> ({'r_t': -1650.5433, 'eps':     1.0000, 'len':  7085.3120, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   73000, Reward:  -720.450 [ 157.692], Avg:  -745.114 (1.000) <0-01:11:26> ({'r_t': -1752.2001, 'eps':     1.0000, 'len':  7192.3570, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   74000, Reward:  -800.564 [  80.655], Avg:  -745.853 (1.000) <0-01:12:25> ({'r_t': -1958.6159, 'eps':     1.0000, 'len':  7286.4270, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   75000, Reward:  -794.138 [  75.058], Avg:  -746.488 (1.000) <0-01:13:24> ({'r_t': -1498.7967, 'eps':     1.0000, 'len':  7375.3650, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   76000, Reward:  -729.669 [ 155.420], Avg:  -746.270 (1.000) <0-01:14:23> ({'r_t': -1725.7744, 'eps':     1.0000, 'len':  7476.7310, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   77000, Reward:  -796.677 [  63.225], Avg:  -746.916 (1.000) <0-01:15:21> ({'r_t': -2033.2745, 'eps':     1.0000, 'len':  7572.1370, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   78000, Reward:  -703.471 [ 188.458], Avg:  -746.366 (1.000) <0-01:16:20> ({'r_t': -2026.3148, 'eps':     1.0000, 'len':  7670.2280, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   79000, Reward:  -738.637 [ 148.447], Avg:  -746.270 (1.000) <0-01:17:19> ({'r_t': -2355.0802, 'eps':     1.0000, 'len':  7766.0950, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   80000, Reward:  -765.989 [  96.669], Avg:  -746.513 (1.000) <0-01:18:18> ({'r_t': -1978.8176, 'eps':     1.0000, 'len':  7852.5010, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   81000, Reward:  -712.242 [ 116.001], Avg:  -746.095 (1.000) <0-01:19:16> ({'r_t': -2680.5481, 'eps':     1.0000, 'len':  7944.2820, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   82000, Reward:  -754.614 [  94.284], Avg:  -746.198 (1.000) <0-01:20:15> ({'r_t': -2325.4518, 'eps':     1.0000, 'len':  8036.9420, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   83000, Reward:  -769.574 [  97.028], Avg:  -746.476 (1.000) <0-01:21:14> ({'r_t': -2152.9224, 'eps':     1.0000, 'len':  8138.2030, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   84000, Reward:  -740.696 [ 122.235], Avg:  -746.408 (1.000) <0-01:22:12> ({'r_t': -1896.7318, 'eps':     1.0000, 'len':  8239.9520, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   85000, Reward:  -718.999 [ 121.386], Avg:  -746.089 (1.000) <0-01:23:11> ({'r_t': -1651.9403, 'eps':     1.0000, 'len':  8337.8960, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   86000, Reward:  -719.270 [ 188.692], Avg:  -745.781 (1.000) <0-01:24:09> ({'r_t': -1956.0864, 'eps':     1.0000, 'len':  8440.5950, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   87000, Reward:  -773.581 [  78.741], Avg:  -746.097 (1.000) <0-01:25:08> ({'r_t': -2011.8034, 'eps':     1.0000, 'len':  8538.0080, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   88000, Reward:  -762.134 [ 112.996], Avg:  -746.277 (1.000) <0-01:26:06> ({'r_t': -2239.4840, 'eps':     1.0000, 'len':  8632.0770, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   89000, Reward:  -809.102 [  61.335], Avg:  -746.975 (1.000) <0-01:27:05> ({'r_t': -2592.9694, 'eps':     1.0000, 'len':  8730.3410, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   90000, Reward:  -692.351 [ 196.941], Avg:  -746.375 (1.000) <0-01:28:04> ({'r_t': -2432.3662, 'eps':     1.0000, 'len':  8823.4280, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   91000, Reward:  -784.697 [  82.241], Avg:  -746.792 (1.000) <0-01:29:02> ({'r_t': -2079.3430, 'eps':     1.0000, 'len':  8921.9450, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   92000, Reward:  -779.915 [  67.713], Avg:  -747.148 (1.000) <0-01:30:01> ({'r_t': -1883.8841, 'eps':     1.0000, 'len':  9030.9710, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   93000, Reward:  -676.679 [ 158.117], Avg:  -746.398 (1.000) <0-01:31:00> ({'r_t': -2368.4120, 'eps':     1.0000, 'len':  9134.2810, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   94000, Reward:  -763.227 [ 146.749], Avg:  -746.575 (1.000) <0-01:31:58> ({'r_t': -2226.0697, 'eps':     1.0000, 'len':  9235.4960, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   95000, Reward:  -755.917 [ 100.255], Avg:  -746.673 (1.000) <0-01:32:57> ({'r_t': -2391.7143, 'eps':     1.0000, 'len':  9333.7150, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   96000, Reward:  -763.663 [ 108.595], Avg:  -746.848 (1.000) <0-01:33:55> ({'r_t': -2103.3787, 'eps':     1.0000, 'len':  9427.7270, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   97000, Reward:  -741.419 [ 175.963], Avg:  -746.792 (1.000) <0-01:34:54> ({'r_t': -1763.3170, 'eps':     1.0000, 'len':  9524.6350, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   98000, Reward:  -736.970 [ 142.757], Avg:  -746.693 (1.000) <0-01:35:52> ({'r_t': -1964.4475, 'eps':     1.0000, 'len':  9624.9130, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   99000, Reward:  -711.677 [ 132.830], Avg:  -746.343 (1.000) <0-01:36:51> ({'r_t': -2297.2207, 'eps':     1.0000, 'len':  9723.7020, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  100000, Reward:  -811.830 [  55.789], Avg:  -746.991 (1.000) <0-01:37:50> ({'r_t': -2756.0794, 'eps':     1.0000, 'len':  9824.4230, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  101000, Reward:  -714.411 [ 164.758], Avg:  -746.672 (1.000) <0-01:38:49> ({'r_t': -1458.4506, 'eps':     1.0000, 'len':  9926.8170, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  102000, Reward:  -721.114 [ 159.231], Avg:  -746.424 (1.000) <0-01:39:47> ({'r_t': -2010.7784, 'eps':     1.0000, 'len': 10028.2740, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  103000, Reward:  -768.190 [ 134.734], Avg:  -746.633 (1.000) <0-01:40:46> ({'r_t': -1955.1007, 'eps':     1.0000, 'len': 10120.5370, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  104000, Reward:  -721.617 [ 154.347], Avg:  -746.395 (1.000) <0-01:41:45> ({'r_t': -2169.3065, 'eps':     1.0000, 'len': 10228.4600, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  105000, Reward:  -701.971 [ 104.558], Avg:  -745.976 (1.000) <0-01:42:43> ({'r_t': -1863.3887, 'eps':     1.0000, 'len': 10330.2360, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  106000, Reward:  -780.291 [ 123.607], Avg:  -746.296 (1.000) <0-01:43:42> ({'r_t': -2124.8539, 'eps':     1.0000, 'len': 10424.8940, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  107000, Reward:  -761.941 [  99.555], Avg:  -746.441 (1.000) <0-01:44:41> ({'r_t': -1946.1208, 'eps':     1.0000, 'len': 10525.4570, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  108000, Reward:  -705.047 [ 151.311], Avg:  -746.062 (1.000) <0-01:45:39> ({'r_t': -1929.7277, 'eps':     1.0000, 'len': 10628.4740, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  109000, Reward:  -757.756 [ 134.671], Avg:  -746.168 (1.000) <0-01:46:37> ({'r_t': -1895.3726, 'eps':     1.0000, 'len': 10716.5470, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:  110000, Reward:  -179.249 [ 405.311], Avg:  -741.060 (0.980) <0-01:51:35> ({'r_t': -1887.0106, 'eps':     0.9800, 'len': 10820.2080, 'dyn_loss':  8819.3213, 'dot_loss':   126.5045, 'ddot_loss':    33.0342, 'rew_loss':   599.8549, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  111000, Reward:   -91.949 [  38.121], Avg:  -735.265 (0.980) <0-01:52:11> ({'r_t': -1711.8291, 'eps':     0.9800, 'len': 10921.4550, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  112000, Reward:  -198.495 [ 413.864], Avg:  -730.515 (0.980) <0-01:52:48> ({'r_t': -1966.1787, 'eps':     0.9800, 'len': 11014.3240, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  113000, Reward:  -213.328 [ 519.739], Avg:  -725.978 (0.980) <0-01:53:17> ({'r_t': -1620.3942, 'eps':     0.9800, 'len': 11112.2090, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  114000, Reward:   -71.942 [   8.897], Avg:  -720.291 (0.980) <0-01:53:46> ({'r_t': -1897.0766, 'eps':     0.9800, 'len': 11220.9230, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  115000, Reward:  -265.013 [ 578.684], Avg:  -716.366 (0.980) <0-01:54:41> ({'r_t': -2143.3535, 'eps':     0.9800, 'len': 11321.6720, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  116000, Reward:  -267.300 [ 586.874], Avg:  -712.528 (0.980) <0-01:55:40> ({'r_t': -1655.5249, 'eps':     0.9800, 'len': 11419.0060, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  117000, Reward:   -78.402 [  23.954], Avg:  -707.154 (0.980) <0-01:56:11> ({'r_t': -1445.8018, 'eps':     0.9800, 'len': 11520.8540, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  118000, Reward:   -75.906 [  45.762], Avg:  -701.849 (0.980) <0-01:56:46> ({'r_t': -1784.4355, 'eps':     0.9800, 'len': 11625.1440, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  119000, Reward:   -87.798 [  48.484], Avg:  -696.732 (0.980) <0-01:57:21> ({'r_t': -2046.7519, 'eps':     0.9800, 'len': 11723.4640, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:  120000, Reward:   -68.632 [  23.909], Avg:  -691.541 (0.960) <0-02:02:18> ({'r_t': -2118.0669, 'eps':     0.9604, 'len': 11823.1930, 'dyn_loss':   148.3640, 'dot_loss':    14.4621, 'ddot_loss':    14.7294, 'rew_loss':   566.7806, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  121000, Reward:  -211.536 [ 565.092], Avg:  -687.607 (0.960) <0-02:03:17> ({'r_t': -1949.8344, 'eps':     0.9604, 'len': 11920.7180, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  122000, Reward:  -246.877 [ 664.551], Avg:  -684.023 (0.960) <0-02:04:16> ({'r_t': -2066.9028, 'eps':     0.9604, 'len': 12029.6760, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  123000, Reward:  -147.918 [ 327.462], Avg:  -679.700 (0.960) <0-02:05:15> ({'r_t': -1389.0606, 'eps':     0.9604, 'len': 12132.7270, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  124000, Reward:   -63.919 [  35.534], Avg:  -674.774 (0.960) <0-02:05:51> ({'r_t': -1624.0105, 'eps':     0.9604, 'len': 12238.5740, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  125000, Reward:   -58.558 [  49.516], Avg:  -669.883 (0.960) <0-02:06:24> ({'r_t': -1861.4660, 'eps':     0.9604, 'len': 12349.3530, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  126000, Reward:   -61.620 [  25.184], Avg:  -665.094 (0.960) <0-02:06:57> ({'r_t': -1694.6693, 'eps':     0.9604, 'len': 12453.7940, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  127000, Reward:  -128.050 [ 252.688], Avg:  -660.898 (0.960) <0-02:07:44> ({'r_t': -1934.9921, 'eps':     0.9604, 'len': 12554.2180, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  128000, Reward:   -91.082 [  69.963], Avg:  -656.481 (0.960) <0-02:08:25> ({'r_t': -1652.0673, 'eps':     0.9604, 'len': 12655.2060, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  129000, Reward:   -59.316 [  27.118], Avg:  -651.887 (0.960) <0-02:08:59> ({'r_t': -1575.9734, 'eps':     0.9604, 'len': 12754.7860, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:  130000, Reward:  -176.733 [ 137.921], Avg:  -648.260 (0.941) <0-02:14:24> ({'r_t': -2060.6156, 'eps':     0.9412, 'len': 12844.2000, 'dyn_loss':    63.1977, 'dot_loss':     5.3341, 'ddot_loss':     7.2625, 'rew_loss':   557.2806, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  131000, Reward:   -98.137 [  39.627], Avg:  -644.093 (0.941) <0-02:15:04> ({'r_t': -1771.7436, 'eps':     0.9412, 'len': 12943.7610, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  132000, Reward:  -120.568 [  83.431], Avg:  -640.156 (0.941) <0-02:16:03> ({'r_t': -1831.7085, 'eps':     0.9412, 'len': 13049.1350, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  133000, Reward:  -104.750 [ 109.530], Avg:  -636.161 (0.941) <0-02:17:02> ({'r_t': -1557.0645, 'eps':     0.9412, 'len': 13152.2870, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  134000, Reward:  -160.197 [ 124.173], Avg:  -632.635 (0.941) <0-02:18:02> ({'r_t': -1959.8038, 'eps':     0.9412, 'len': 13255.6000, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  135000, Reward:  -209.970 [ 377.204], Avg:  -629.527 (0.941) <0-02:18:56> ({'r_t': -1998.3382, 'eps':     0.9412, 'len': 13356.2020, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  136000, Reward:  -350.238 [ 625.630], Avg:  -627.489 (0.941) <0-02:19:56> ({'r_t': -1676.9922, 'eps':     0.9412, 'len': 13455.2470, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  137000, Reward:  -225.474 [ 459.936], Avg:  -624.575 (0.941) <0-02:20:56> ({'r_t': -1507.9999, 'eps':     0.9412, 'len': 13571.2040, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  138000, Reward:   -76.119 [  42.219], Avg:  -620.630 (0.941) <0-02:21:41> ({'r_t': -1585.9675, 'eps':     0.9412, 'len': 13684.0970, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  139000, Reward:  -198.404 [ 350.412], Avg:  -617.614 (0.941) <0-02:22:40> ({'r_t': -1799.9119, 'eps':     0.9412, 'len': 13790.6430, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:  140000, Reward:   -64.998 [  46.086], Avg:  -613.695 (0.922) <0-02:27:50> ({'r_t': -1445.9734, 'eps':     0.9224, 'len': 13895.9340, 'dyn_loss':    35.6057, 'dot_loss':     3.2055, 'ddot_loss':     5.7105, 'rew_loss':   564.0730, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  141000, Reward:  -125.584 [  88.539], Avg:  -610.257 (0.922) <0-02:28:42> ({'r_t': -1619.0493, 'eps':     0.9224, 'len': 14001.3970, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  142000, Reward:  -120.146 [  75.474], Avg:  -606.830 (0.922) <0-02:29:28> ({'r_t': -1379.9776, 'eps':     0.9224, 'len': 14111.2930, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  143000, Reward:   -93.362 [  82.772], Avg:  -603.264 (0.922) <0-02:30:18> ({'r_t': -1456.7862, 'eps':     0.9224, 'len': 14210.8010, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  144000, Reward:   -68.388 [  38.684], Avg:  -599.575 (0.922) <0-02:31:04> ({'r_t': -1474.7888, 'eps':     0.9224, 'len': 14317.2330, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  145000, Reward:   -86.270 [  79.317], Avg:  -596.060 (0.922) <0-02:31:55> ({'r_t': -1582.4636, 'eps':     0.9224, 'len': 14419.6610, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  146000, Reward:   -90.254 [  58.445], Avg:  -592.619 (0.922) <0-02:32:46> ({'r_t': -1656.4135, 'eps':     0.9224, 'len': 14531.4630, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  147000, Reward:   -99.221 [ 114.658], Avg:  -589.285 (0.922) <0-02:33:46> ({'r_t': -1659.6528, 'eps':     0.9224, 'len': 14638.8270, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  148000, Reward:  -134.945 [ 124.117], Avg:  -586.236 (0.922) <0-02:34:38> ({'r_t': -1378.7470, 'eps':     0.9224, 'len': 14743.0800, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  149000, Reward:   -86.521 [  81.695], Avg:  -582.904 (0.922) <0-02:35:29> ({'r_t': -1550.3162, 'eps':     0.9224, 'len': 14843.6440, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:  150000, Reward:  -296.552 [ 107.442], Avg:  -581.008 (0.904) <0-02:40:48> ({'r_t': -1763.5779, 'eps':     0.9039, 'len': 14943.8750, 'dyn_loss':    22.1838, 'dot_loss':     2.6869, 'ddot_loss':     5.3423, 'rew_loss':   497.1274, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  151000, Reward:  -235.863 [ 144.918], Avg:  -578.737 (0.904) <0-02:41:48> ({'r_t': -1456.2041, 'eps':     0.9039, 'len': 15053.3260, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  152000, Reward:  -254.749 [ 145.728], Avg:  -576.620 (0.904) <0-02:42:47> ({'r_t': -1340.9764, 'eps':     0.9039, 'len': 15169.2340, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  153000, Reward:  -276.143 [ 132.878], Avg:  -574.668 (0.904) <0-02:43:46> ({'r_t': -1458.1508, 'eps':     0.9039, 'len': 15271.3680, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  154000, Reward:  -232.634 [ 156.217], Avg:  -572.462 (0.904) <0-02:44:44> ({'r_t': -1640.0276, 'eps':     0.9039, 'len': 15368.7150, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  155000, Reward:  -220.403 [ 156.716], Avg:  -570.205 (0.904) <0-02:45:44> ({'r_t': -1692.5385, 'eps':     0.9039, 'len': 15469.6920, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  156000, Reward:  -201.690 [ 139.866], Avg:  -567.858 (0.904) <0-02:46:45> ({'r_t': -1574.9925, 'eps':     0.9039, 'len': 15573.0770, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  157000, Reward:  -286.066 [ 152.991], Avg:  -566.074 (0.904) <0-02:47:44> ({'r_t': -1520.4055, 'eps':     0.9039, 'len': 15680.0530, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  158000, Reward:  -284.777 [ 136.808], Avg:  -564.305 (0.904) <0-02:48:44> ({'r_t': -1515.3368, 'eps':     0.9039, 'len': 15787.2590, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  159000, Reward:  -194.184 [ 117.509], Avg:  -561.992 (0.904) <0-02:49:43> ({'r_t': -1246.0174, 'eps':     0.9039, 'len': 15887.1930, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:  160000, Reward:  -152.406 [  78.990], Avg:  -559.448 (0.886) <0-02:55:06> ({'r_t': -1352.1823, 'eps':     0.8858, 'len': 15993.8580, 'dyn_loss':    17.4152, 'dot_loss':     2.3882, 'ddot_loss':     4.8904, 'rew_loss':   557.9996, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  161000, Reward:  -166.617 [  87.005], Avg:  -557.023 (0.886) <0-02:56:09> ({'r_t': -1268.2388, 'eps':     0.8858, 'len': 16101.9740, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  162000, Reward:  -148.825 [  72.386], Avg:  -554.519 (0.886) <0-02:57:10> ({'r_t': -1216.6815, 'eps':     0.8858, 'len': 16210.9530, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  163000, Reward:  -145.760 [  80.224], Avg:  -552.026 (0.886) <0-02:58:13> ({'r_t': -1409.1151, 'eps':     0.8858, 'len': 16321.1010, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  164000, Reward:  -143.122 [  74.443], Avg:  -549.548 (0.886) <0-02:59:16> ({'r_t': -1840.0637, 'eps':     0.8858, 'len': 16433.3680, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  165000, Reward:  -156.333 [  85.361], Avg:  -547.179 (0.886) <0-03:00:23> ({'r_t': -1397.0904, 'eps':     0.8858, 'len': 16541.4220, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  166000, Reward:  -223.277 [  97.525], Avg:  -545.240 (0.886) <0-03:01:29> ({'r_t': -1323.8758, 'eps':     0.8858, 'len': 16656.7390, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  167000, Reward:  -163.261 [  86.683], Avg:  -542.966 (0.886) <0-03:02:35> ({'r_t': -1286.0099, 'eps':     0.8858, 'len': 16758.8160, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  168000, Reward:  -124.810 [  72.115], Avg:  -540.492 (0.886) <0-03:03:40> ({'r_t': -1660.6386, 'eps':     0.8858, 'len': 16860.0800, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  169000, Reward:  -124.786 [  59.259], Avg:  -538.046 (0.886) <0-03:04:40> ({'r_t': -1449.6736, 'eps':     0.8858, 'len': 16954.2710, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:  170000, Reward:  -113.772 [  36.858], Avg:  -535.565 (0.868) <0-03:10:04> ({'r_t': -1364.8008, 'eps':     0.8681, 'len': 17054.7430, 'dyn_loss':    14.3954, 'dot_loss':     2.1393, 'ddot_loss':     4.4506, 'rew_loss':   490.0394, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  171000, Reward:  -124.417 [  43.131], Avg:  -533.175 (0.868) <0-03:10:57> ({'r_t': -1441.8518, 'eps':     0.8681, 'len': 17154.4280, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  172000, Reward:  -123.743 [  42.033], Avg:  -530.808 (0.868) <0-03:11:48> ({'r_t': -1228.0533, 'eps':     0.8681, 'len': 17245.6740, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  173000, Reward:  -135.952 [  52.581], Avg:  -528.539 (0.868) <0-03:12:42> ({'r_t': -1328.8420, 'eps':     0.8681, 'len': 17346.2730, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  174000, Reward:  -129.518 [  53.982], Avg:  -526.259 (0.868) <0-03:13:39> ({'r_t': -1189.8483, 'eps':     0.8681, 'len': 17451.9230, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  175000, Reward:  -140.722 [  52.255], Avg:  -524.068 (0.868) <0-03:14:33> ({'r_t': -1251.5376, 'eps':     0.8681, 'len': 17549.1740, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  176000, Reward:  -137.916 [  52.865], Avg:  -521.887 (0.868) <0-03:15:30> ({'r_t': -1225.8898, 'eps':     0.8681, 'len': 17647.6370, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  177000, Reward:  -116.933 [  58.243], Avg:  -519.612 (0.868) <0-03:16:29> ({'r_t': -1327.7123, 'eps':     0.8681, 'len': 17744.2450, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  178000, Reward:  -123.623 [  41.098], Avg:  -517.399 (0.868) <0-03:17:26> ({'r_t': -1370.7614, 'eps':     0.8681, 'len': 17842.6680, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  179000, Reward:  -121.949 [  44.770], Avg:  -515.202 (0.868) <0-03:18:23> ({'r_t': -1261.9881, 'eps':     0.8681, 'len': 17939.1430, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:  180000, Reward:  -136.154 [  60.098], Avg:  -513.108 (0.851) <0-03:23:49> ({'r_t': -1338.6997, 'eps':     0.8508, 'len': 18043.0490, 'dyn_loss':    12.3919, 'dot_loss':     1.9235, 'ddot_loss':     4.0599, 'rew_loss':   467.2898, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  181000, Reward:   -98.002 [  48.309], Avg:  -510.827 (0.851) <0-03:24:55> ({'r_t': -1242.0625, 'eps':     0.8508, 'len': 18140.3900, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  182000, Reward:   -92.906 [  43.279], Avg:  -508.544 (0.851) <0-03:25:53> ({'r_t': -1300.3827, 'eps':     0.8508, 'len': 18238.3540, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  183000, Reward:  -123.211 [  61.608], Avg:  -506.449 (0.851) <0-03:26:53> ({'r_t': -1209.9555, 'eps':     0.8508, 'len': 18338.1970, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  184000, Reward:  -113.367 [  59.755], Avg:  -504.325 (0.851) <0-03:27:52> ({'r_t': -1260.4459, 'eps':     0.8508, 'len': 18440.7490, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  185000, Reward:  -122.522 [  50.606], Avg:  -502.272 (0.851) <0-03:28:52> ({'r_t': -1225.2351, 'eps':     0.8508, 'len': 18538.2990, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  186000, Reward:  -115.034 [  55.746], Avg:  -500.201 (0.851) <0-03:29:52> ({'r_t': -1380.1098, 'eps':     0.8508, 'len': 18628.0330, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  187000, Reward:  -115.796 [  45.840], Avg:  -498.156 (0.851) <0-03:30:50> ({'r_t': -1226.7627, 'eps':     0.8508, 'len': 18730.1840, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  188000, Reward:  -119.526 [  58.127], Avg:  -496.153 (0.851) <0-03:31:53> ({'r_t': -1285.5269, 'eps':     0.8508, 'len': 18829.5680, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  189000, Reward:  -117.760 [  48.606], Avg:  -494.162 (0.851) <0-03:32:51> ({'r_t': -1265.5250, 'eps':     0.8508, 'len': 18926.2180, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:  190000, Reward:  -249.679 [ 113.646], Avg:  -492.882 (0.834) <0-03:38:12> ({'r_t': -1365.3935, 'eps':     0.8337, 'len': 19022.5230, 'dyn_loss':    11.8180, 'dot_loss':     1.8109, 'ddot_loss':     3.8485, 'rew_loss':   436.0174, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  191000, Reward:  -276.613 [ 103.829], Avg:  -491.755 (0.834) <0-03:39:16> ({'r_t': -1194.9759, 'eps':     0.8337, 'len': 19115.4830, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  192000, Reward:  -242.930 [ 112.235], Avg:  -490.466 (0.834) <0-03:40:20> ({'r_t': -1263.5873, 'eps':     0.8337, 'len': 19206.1520, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  193000, Reward:  -245.847 [ 110.292], Avg:  -489.205 (0.834) <0-03:41:24> ({'r_t': -1248.0053, 'eps':     0.8337, 'len': 19295.8720, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  194000, Reward:  -274.244 [ 113.723], Avg:  -488.103 (0.834) <0-03:42:28> ({'r_t': -1186.4736, 'eps':     0.8337, 'len': 19387.1980, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  195000, Reward:  -299.615 [ 103.412], Avg:  -487.141 (0.834) <0-03:43:32> ({'r_t': -1187.4656, 'eps':     0.8337, 'len': 19481.9560, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  196000, Reward:  -278.619 [ 105.383], Avg:  -486.083 (0.834) <0-03:44:37> ({'r_t': -1277.5576, 'eps':     0.8337, 'len': 19569.1940, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  197000, Reward:  -282.416 [  82.160], Avg:  -485.054 (0.834) <0-03:45:41> ({'r_t': -1224.6898, 'eps':     0.8337, 'len': 19656.5100, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  198000, Reward:  -313.336 [ 120.906], Avg:  -484.191 (0.834) <0-03:46:45> ({'r_t': -1250.1296, 'eps':     0.8337, 'len': 19748.1460, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  199000, Reward:  -240.091 [ 124.395], Avg:  -482.970 (0.834) <0-03:47:49> ({'r_t': -1212.9403, 'eps':     0.8337, 'len': 19847.0750, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:  200000, Reward:  -265.212 [ 158.058], Avg:  -481.887 (0.817) <0-03:53:11> ({'r_t': -1223.6713, 'eps':     0.8171, 'len': 19936.5250, 'dyn_loss':    11.2309, 'dot_loss':     1.7230, 'ddot_loss':     3.7138, 'rew_loss':   435.1364, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  201000, Reward:  -173.182 [ 148.537], Avg:  -480.359 (0.817) <0-03:54:17> ({'r_t': -1174.1506, 'eps':     0.8171, 'len': 20021.4100, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  202000, Reward:  -234.672 [ 166.654], Avg:  -479.149 (0.817) <0-03:55:23> ({'r_t': -1170.3058, 'eps':     0.8171, 'len': 20103.2220, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  203000, Reward:  -238.652 [ 169.276], Avg:  -477.970 (0.817) <0-03:56:29> ({'r_t': -1156.4519, 'eps':     0.8171, 'len': 20187.4060, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  204000, Reward:  -153.326 [ 173.214], Avg:  -476.386 (0.817) <0-03:57:36> ({'r_t': -1220.2553, 'eps':     0.8171, 'len': 20278.2380, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  205000, Reward:  -271.424 [ 177.258], Avg:  -475.391 (0.817) <0-03:58:43> ({'r_t': -1190.2747, 'eps':     0.8171, 'len': 20368.3430, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  206000, Reward:  -205.189 [ 179.513], Avg:  -474.086 (0.817) <0-03:59:49> ({'r_t': -1192.6181, 'eps':     0.8171, 'len': 20458.3110, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  207000, Reward:  -187.787 [ 190.173], Avg:  -472.709 (0.817) <0-04:00:56> ({'r_t': -1191.7095, 'eps':     0.8171, 'len': 20545.0850, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  208000, Reward:  -253.841 [ 156.546], Avg:  -471.662 (0.817) <0-04:02:03> ({'r_t': -1161.7940, 'eps':     0.8171, 'len': 20627.3700, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  209000, Reward:  -161.409 [ 181.786], Avg:  -470.185 (0.817) <0-04:03:09> ({'r_t': -1355.6546, 'eps':     0.8171, 'len': 20704.7940, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:  210000, Reward:  -686.865 [ 736.523], Avg:  -471.212 (0.801) <0-04:08:48> ({'r_t': -1219.1627, 'eps':     0.8007, 'len': 20787.5530, 'dyn_loss':    11.0468, 'dot_loss':     1.7013, 'ddot_loss':     3.7146, 'rew_loss':   424.6206, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  211000, Reward:  -958.228 [ 939.412], Avg:  -473.509 (0.801) <0-04:09:55> ({'r_t': -1152.7435, 'eps':     0.8007, 'len': 20864.2090, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  212000, Reward:  -934.924 [ 908.471], Avg:  -475.675 (0.801) <0-04:10:58> ({'r_t': -1162.0285, 'eps':     0.8007, 'len': 20944.4040, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  213000, Reward: -1342.293 [1227.689], Avg:  -479.725 (0.801) <0-04:12:06> ({'r_t': -1234.9982, 'eps':     0.8007, 'len': 21026.1560, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  214000, Reward:  -788.287 [ 704.082], Avg:  -481.160 (0.801) <0-04:13:12> ({'r_t': -1162.4287, 'eps':     0.8007, 'len': 21107.5320, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  215000, Reward:  -775.335 [ 745.780], Avg:  -482.522 (0.801) <0-04:14:19> ({'r_t': -1192.2963, 'eps':     0.8007, 'len': 21187.0380, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  216000, Reward:  -561.987 [ 674.801], Avg:  -482.888 (0.801) <0-04:15:25> ({'r_t': -1210.2167, 'eps':     0.8007, 'len': 21268.4020, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  217000, Reward: -1066.855 [1109.089], Avg:  -485.567 (0.801) <0-04:16:32> ({'r_t': -1160.2975, 'eps':     0.8007, 'len': 21347.6630, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  218000, Reward:  -992.650 [ 902.497], Avg:  -487.882 (0.801) <0-04:17:38> ({'r_t': -1134.7634, 'eps':     0.8007, 'len': 21434.1990, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  219000, Reward:  -889.620 [1206.503], Avg:  -489.708 (0.801) <0-04:18:44> ({'r_t': -1176.3748, 'eps':     0.8007, 'len': 21518.1860, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:  220000, Reward:  -119.608 [  79.315], Avg:  -488.034 (0.785) <0-04:24:19> ({'r_t': -1192.6187, 'eps':     0.7847, 'len': 21604.0240, 'dyn_loss':    10.8295, 'dot_loss':     1.6774, 'ddot_loss':     3.6921, 'rew_loss':   423.7717, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  221000, Reward:  -127.688 [  98.475], Avg:  -486.410 (0.785) <0-04:25:28> ({'r_t': -1151.3719, 'eps':     0.7847, 'len': 21686.3940, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  222000, Reward:  -115.238 [  53.928], Avg:  -484.746 (0.785) <0-04:26:31> ({'r_t': -1137.4733, 'eps':     0.7847, 'len': 21767.8170, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  223000, Reward:  -149.665 [  93.219], Avg:  -483.250 (0.785) <0-04:27:39> ({'r_t': -1109.4548, 'eps':     0.7847, 'len': 21841.4930, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  224000, Reward:  -113.558 [  81.429], Avg:  -481.607 (0.785) <0-04:28:47> ({'r_t': -1087.8516, 'eps':     0.7847, 'len': 21913.7330, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  225000, Reward:  -123.530 [  65.691], Avg:  -480.023 (0.785) <0-04:29:54> ({'r_t': -1166.4270, 'eps':     0.7847, 'len': 21985.7830, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  226000, Reward:   -92.367 [  50.237], Avg:  -478.315 (0.785) <0-04:30:57> ({'r_t': -1195.9999, 'eps':     0.7847, 'len': 22056.0040, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  227000, Reward:  -107.102 [  68.809], Avg:  -476.687 (0.785) <0-04:31:59> ({'r_t': -1099.4928, 'eps':     0.7847, 'len': 22130.8930, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  228000, Reward:   -84.668 [  68.684], Avg:  -474.975 (0.785) <0-04:33:00> ({'r_t': -1132.3860, 'eps':     0.7847, 'len': 22204.9660, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  229000, Reward:  -117.632 [  91.655], Avg:  -473.421 (0.785) <0-04:34:08> ({'r_t': -1116.9333, 'eps':     0.7847, 'len': 22277.5220, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:  230000, Reward:    34.307 [  63.893], Avg:  -471.223 (0.769) <0-04:39:37> ({'r_t': -1074.7032, 'eps':     0.7690, 'len': 22348.2230, 'dyn_loss':    10.0834, 'dot_loss':     1.6138, 'ddot_loss':     3.5947, 'rew_loss':   379.8095, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  231000, Reward:    31.615 [  95.040], Avg:  -469.056 (0.769) <0-04:40:46> ({'r_t': -1157.1680, 'eps':     0.7690, 'len': 22422.9770, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  232000, Reward:    53.485 [  51.329], Avg:  -466.813 (0.769) <0-04:41:43> ({'r_t': -1057.0692, 'eps':     0.7690, 'len': 22503.4000, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  233000, Reward:    46.219 [  46.235], Avg:  -464.621 (0.769) <0-04:42:43> ({'r_t': -1035.7016, 'eps':     0.7690, 'len': 22578.6250, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  234000, Reward:    17.638 [  89.942], Avg:  -462.569 (0.769) <0-04:43:49> ({'r_t': -1003.3835, 'eps':     0.7690, 'len': 22651.9950, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  235000, Reward:     1.044 [ 102.487], Avg:  -460.604 (0.769) <0-04:44:58> ({'r_t': -1003.8276, 'eps':     0.7690, 'len': 22730.1340, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  236000, Reward:    40.531 [ 108.359], Avg:  -458.490 (0.769) <0-04:46:07> ({'r_t': -1044.9937, 'eps':     0.7690, 'len': 22809.3350, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  237000, Reward:    45.338 [  60.137], Avg:  -456.373 (0.769) <0-04:47:02> ({'r_t': -1125.4545, 'eps':     0.7690, 'len': 22884.7580, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  238000, Reward:    24.180 [  75.635], Avg:  -454.362 (0.769) <0-04:48:09> ({'r_t': -1104.7836, 'eps':     0.7690, 'len': 22960.2920, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  239000, Reward:    27.957 [  66.063], Avg:  -452.352 (0.769) <0-04:49:10> ({'r_t': -1030.1736, 'eps':     0.7690, 'len': 23034.9900, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:  240000, Reward:   -96.725 [  70.652], Avg:  -450.877 (0.754) <0-04:54:54> ({'r_t': -1049.0547, 'eps':     0.7536, 'len': 23113.5040, 'dyn_loss':    10.6272, 'dot_loss':     1.6108, 'ddot_loss':     3.5871, 'rew_loss':   371.2095, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  241000, Reward:   -68.886 [  97.181], Avg:  -449.298 (0.754) <0-04:56:05> ({'r_t':  -992.9846, 'eps':     0.7536, 'len': 23192.4580, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  242000, Reward:  -101.963 [ 143.874], Avg:  -447.869 (0.754) <0-04:57:15> ({'r_t': -1022.5942, 'eps':     0.7536, 'len': 23265.2370, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  243000, Reward:  -131.819 [ 109.400], Avg:  -446.574 (0.754) <0-04:58:25> ({'r_t':  -997.7327, 'eps':     0.7536, 'len': 23340.3460, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  244000, Reward:  -109.939 [ 101.684], Avg:  -445.200 (0.754) <0-04:59:36> ({'r_t': -1056.9807, 'eps':     0.7536, 'len': 23409.6640, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  245000, Reward:   -85.268 [  91.157], Avg:  -443.736 (0.754) <0-05:00:46> ({'r_t': -1097.3136, 'eps':     0.7536, 'len': 23481.5720, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  246000, Reward:  -147.768 [ 126.866], Avg:  -442.538 (0.754) <0-05:01:56> ({'r_t':  -966.6158, 'eps':     0.7536, 'len': 23550.7640, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  247000, Reward:   -71.362 [  98.743], Avg:  -441.042 (0.754) <0-05:03:07> ({'r_t':  -974.9027, 'eps':     0.7536, 'len': 23622.7200, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  248000, Reward:  -105.470 [ 137.825], Avg:  -439.694 (0.754) <0-05:04:21> ({'r_t':  -982.2384, 'eps':     0.7536, 'len': 23691.0680, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  249000, Reward:  -106.497 [ 102.217], Avg:  -438.361 (0.754) <0-05:05:33> ({'r_t':  -992.7468, 'eps':     0.7536, 'len': 23764.3780, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:  250000, Reward:    26.920 [ 136.573], Avg:  -436.507 (0.739) <0-05:11:39> ({'r_t':  -982.4837, 'eps':     0.7386, 'len': 23837.5520, 'dyn_loss':    11.0614, 'dot_loss':     1.6376, 'ddot_loss':     3.6608, 'rew_loss':   356.1809, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  251000, Reward:    55.395 [ 100.657], Avg:  -434.555 (0.739) <0-05:12:53> ({'r_t':  -994.1617, 'eps':     0.7386, 'len': 23914.0120, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  252000, Reward:    89.666 [  67.601], Avg:  -432.483 (0.739) <0-05:14:04> ({'r_t':  -915.0187, 'eps':     0.7386, 'len': 23985.6120, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  253000, Reward:    14.261 [ 107.205], Avg:  -430.725 (0.739) <0-05:15:14> ({'r_t':  -944.1473, 'eps':     0.7386, 'len': 24059.0260, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  254000, Reward:    12.599 [ 111.986], Avg:  -428.986 (0.739) <0-05:16:25> ({'r_t':  -967.7835, 'eps':     0.7386, 'len': 24128.9890, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  255000, Reward:     2.316 [  91.811], Avg:  -427.301 (0.739) <0-05:17:36> ({'r_t':  -916.4372, 'eps':     0.7386, 'len': 24195.1500, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  256000, Reward:    19.563 [  93.746], Avg:  -425.562 (0.739) <0-05:18:47> ({'r_t':  -940.1440, 'eps':     0.7386, 'len': 24260.1480, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  257000, Reward:    12.963 [ 125.363], Avg:  -423.863 (0.739) <0-05:19:58> ({'r_t':  -878.5449, 'eps':     0.7386, 'len': 24326.6810, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  258000, Reward:    65.986 [  94.901], Avg:  -421.971 (0.739) <0-05:21:09> ({'r_t':  -981.2313, 'eps':     0.7386, 'len': 24390.9120, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  259000, Reward:    64.938 [ 103.340], Avg:  -420.099 (0.739) <0-05:22:20> ({'r_t': -1074.3332, 'eps':     0.7386, 'len': 24460.4880, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:  260000, Reward:    72.596 [  50.584], Avg:  -418.211 (0.724) <0-05:27:59> ({'r_t':  -992.8238, 'eps':     0.7238, 'len': 24528.3320, 'dyn_loss':    10.8135, 'dot_loss':     1.6195, 'ddot_loss':     3.6381, 'rew_loss':   313.6003, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  261000, Reward:    64.362 [  78.231], Avg:  -416.369 (0.724) <0-05:29:03> ({'r_t':  -844.8556, 'eps':     0.7238, 'len': 24605.1450, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  262000, Reward:    89.357 [  32.821], Avg:  -414.446 (0.724) <0-05:30:03> ({'r_t':  -792.1474, 'eps':     0.7238, 'len': 24675.7280, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  263000, Reward:    43.127 [  97.893], Avg:  -412.713 (0.724) <0-05:31:08> ({'r_t':  -883.2215, 'eps':     0.7238, 'len': 24735.6460, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  264000, Reward:    34.120 [ 103.322], Avg:  -411.027 (0.724) <0-05:32:21> ({'r_t':  -803.3137, 'eps':     0.7238, 'len': 24794.6080, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  265000, Reward:    71.306 [  58.077], Avg:  -409.214 (0.724) <0-05:33:23> ({'r_t':  -970.2534, 'eps':     0.7238, 'len': 24857.8700, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  266000, Reward:    51.566 [  94.784], Avg:  -407.488 (0.724) <0-05:34:29> ({'r_t':  -960.5190, 'eps':     0.7238, 'len': 24922.5480, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  267000, Reward:    43.770 [ 132.952], Avg:  -405.804 (0.724) <0-05:35:42> ({'r_t':  -827.3355, 'eps':     0.7238, 'len': 24985.7370, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  268000, Reward:    46.875 [  93.656], Avg:  -404.121 (0.724) <0-05:36:49> ({'r_t':  -880.2532, 'eps':     0.7238, 'len': 25042.7150, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  269000, Reward:    79.799 [  54.776], Avg:  -402.329 (0.724) <0-05:37:52> ({'r_t':  -921.4275, 'eps':     0.7238, 'len': 25099.4220, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:  270000, Reward:    57.485 [ 114.393], Avg:  -400.632 (0.709) <0-05:43:51> ({'r_t':  -881.8798, 'eps':     0.7093, 'len': 25158.1770, 'dyn_loss':    11.4377, 'dot_loss':     1.6559, 'ddot_loss':     3.7387, 'rew_loss':   322.4753, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  271000, Reward:    70.201 [ 100.155], Avg:  -398.901 (0.709) <0-05:45:03> ({'r_t':  -880.2453, 'eps':     0.7093, 'len': 25217.0810, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  272000, Reward:    28.339 [ 119.021], Avg:  -397.336 (0.709) <0-05:46:16> ({'r_t':  -874.4125, 'eps':     0.7093, 'len': 25279.5920, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  273000, Reward:    32.878 [ 104.778], Avg:  -395.766 (0.709) <0-05:47:29> ({'r_t':  -849.7000, 'eps':     0.7093, 'len': 25335.7730, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  274000, Reward:    58.515 [  86.585], Avg:  -394.114 (0.709) <0-05:48:41> ({'r_t':  -814.8120, 'eps':     0.7093, 'len': 25390.7800, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  275000, Reward:    58.432 [ 113.863], Avg:  -392.474 (0.709) <0-05:49:53> ({'r_t':  -757.1738, 'eps':     0.7093, 'len': 25448.1160, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  276000, Reward:    53.485 [  83.310], Avg:  -390.864 (0.709) <0-05:51:06> ({'r_t':  -784.9538, 'eps':     0.7093, 'len': 25505.0050, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  277000, Reward:    66.665 [ 107.518], Avg:  -389.219 (0.709) <0-05:52:18> ({'r_t': -1016.3977, 'eps':     0.7093, 'len': 25569.7870, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  278000, Reward:    84.560 [  82.673], Avg:  -387.521 (0.709) <0-05:53:31> ({'r_t':  -927.3845, 'eps':     0.7093, 'len': 25634.8180, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  279000, Reward:    75.762 [  92.947], Avg:  -385.866 (0.709) <0-05:54:43> ({'r_t':  -805.1326, 'eps':     0.7093, 'len': 25696.4140, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:  280000, Reward:     4.917 [ 104.659], Avg:  -384.475 (0.695) <0-06:00:47> ({'r_t':  -781.1612, 'eps':     0.6951, 'len': 25756.0830, 'dyn_loss':    11.5961, 'dot_loss':     1.6679, 'ddot_loss':     3.7857, 'rew_loss':   300.9766, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  281000, Reward:    -5.014 [ 102.943], Avg:  -383.130 (0.695) <0-06:02:01> ({'r_t':  -797.6566, 'eps':     0.6951, 'len': 25811.3680, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  282000, Reward:    49.516 [ 114.416], Avg:  -381.601 (0.695) <0-06:03:16> ({'r_t':  -868.3671, 'eps':     0.6951, 'len': 25864.5880, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  283000, Reward:    37.460 [ 122.479], Avg:  -380.125 (0.695) <0-06:04:30> ({'r_t':  -880.4332, 'eps':     0.6951, 'len': 25922.0090, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  284000, Reward:   -12.065 [ 118.814], Avg:  -378.834 (0.695) <0-06:05:44> ({'r_t':  -786.3594, 'eps':     0.6951, 'len': 25975.9790, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  285000, Reward:   -21.584 [  87.708], Avg:  -377.585 (0.695) <0-06:06:59> ({'r_t':  -914.3345, 'eps':     0.6951, 'len': 26031.7410, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  286000, Reward:    10.340 [ 116.995], Avg:  -376.233 (0.695) <0-06:08:13> ({'r_t':  -776.7771, 'eps':     0.6951, 'len': 26088.0580, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  287000, Reward:    46.786 [ 108.635], Avg:  -374.764 (0.695) <0-06:09:27> ({'r_t':  -772.1205, 'eps':     0.6951, 'len': 26141.4650, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  288000, Reward:    14.172 [  91.495], Avg:  -373.418 (0.695) <0-06:10:42> ({'r_t': -1022.0226, 'eps':     0.6951, 'len': 26192.7650, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  289000, Reward:    62.957 [ 126.043], Avg:  -371.914 (0.695) <0-06:11:56> ({'r_t':  -879.6647, 'eps':     0.6951, 'len': 26249.2560, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:  290000, Reward:   124.760 [  64.411], Avg:  -370.207 (0.681) <0-06:18:04> ({'r_t':  -898.2936, 'eps':     0.6812, 'len': 26301.1550, 'dyn_loss':    11.2529, 'dot_loss':     1.6659, 'ddot_loss':     3.8027, 'rew_loss':   341.7001, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  291000, Reward:    69.984 [  95.277], Avg:  -368.699 (0.681) <0-06:19:19> ({'r_t':  -850.1373, 'eps':     0.6812, 'len': 26355.5760, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  292000, Reward:   135.403 [  60.094], Avg:  -366.979 (0.681) <0-06:20:30> ({'r_t':  -773.5292, 'eps':     0.6812, 'len': 26405.5040, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  293000, Reward:   126.655 [  59.580], Avg:  -365.300 (0.681) <0-06:21:34> ({'r_t':  -762.1487, 'eps':     0.6812, 'len': 26457.5410, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  294000, Reward:    97.596 [ 111.959], Avg:  -363.731 (0.681) <0-06:22:48> ({'r_t':  -774.6956, 'eps':     0.6812, 'len': 26511.4580, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  295000, Reward:   131.164 [  77.006], Avg:  -362.059 (0.681) <0-06:24:03> ({'r_t':  -862.6284, 'eps':     0.6812, 'len': 26562.8130, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  296000, Reward:    96.655 [  96.251], Avg:  -360.514 (0.681) <0-06:25:19> ({'r_t':  -871.1862, 'eps':     0.6812, 'len': 26614.0300, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  297000, Reward:    79.209 [  88.142], Avg:  -359.039 (0.681) <0-06:26:35> ({'r_t':  -812.1159, 'eps':     0.6812, 'len': 26672.7790, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  298000, Reward:    92.930 [  80.381], Avg:  -357.527 (0.681) <0-06:27:53> ({'r_t':  -692.0553, 'eps':     0.6812, 'len': 26728.5770, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  299000, Reward:    82.635 [  78.886], Avg:  -356.060 (0.681) <0-06:29:10> ({'r_t':  -652.9161, 'eps':     0.6812, 'len': 26785.1610, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:  300000, Reward:   -10.595 [ 368.259], Avg:  -354.912 (0.668) <0-06:35:32> ({'r_t':  -763.7781, 'eps':     0.6676, 'len': 26840.1810, 'dyn_loss':    11.5344, 'dot_loss':     1.6948, 'ddot_loss':     3.8918, 'rew_loss':   284.7811, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  301000, Reward:    84.359 [  49.812], Avg:  -353.458 (0.668) <0-06:36:42> ({'r_t':  -752.7842, 'eps':     0.6676, 'len': 26890.5610, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  302000, Reward:     7.443 [ 110.865], Avg:  -352.267 (0.668) <0-06:38:00> ({'r_t':  -642.0375, 'eps':     0.6676, 'len': 26941.7400, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  303000, Reward:    68.105 [  98.525], Avg:  -350.884 (0.668) <0-06:39:20> ({'r_t':  -837.5953, 'eps':     0.6676, 'len': 26993.8400, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  304000, Reward:   -65.661 [ 346.824], Avg:  -349.949 (0.668) <0-06:40:36> ({'r_t':  -611.2357, 'eps':     0.6676, 'len': 27041.3070, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  305000, Reward:   -59.152 [ 346.869], Avg:  -348.998 (0.668) <0-06:41:54> ({'r_t':  -773.4972, 'eps':     0.6676, 'len': 27089.1810, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  306000, Reward:    56.396 [ 111.900], Avg:  -347.678 (0.668) <0-06:43:12> ({'r_t':  -723.8725, 'eps':     0.6676, 'len': 27140.9770, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  307000, Reward:    93.477 [  51.090], Avg:  -346.246 (0.668) <0-06:44:23> ({'r_t':  -736.7268, 'eps':     0.6676, 'len': 27186.6600, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  308000, Reward:    64.574 [  44.518], Avg:  -344.916 (0.668) <0-06:45:31> ({'r_t':  -871.2478, 'eps':     0.6676, 'len': 27233.3630, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  309000, Reward:    51.716 [  92.927], Avg:  -343.637 (0.668) <0-06:46:51> ({'r_t':  -663.0350, 'eps':     0.6676, 'len': 27286.2850, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:  310000, Reward:   159.365 [  52.214], Avg:  -342.019 (0.654) <0-06:53:22> ({'r_t':  -813.0778, 'eps':     0.6543, 'len': 27337.1830, 'dyn_loss':    11.5980, 'dot_loss':     1.6900, 'ddot_loss':     3.8990, 'rew_loss':   275.9445, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  311000, Reward:   142.200 [  92.050], Avg:  -340.467 (0.654) <0-06:54:40> ({'r_t':  -626.9908, 'eps':     0.6543, 'len': 27387.0750, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  312000, Reward:   120.971 [  87.680], Avg:  -338.993 (0.654) <0-06:55:57> ({'r_t':  -773.5435, 'eps':     0.6543, 'len': 27439.5150, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  313000, Reward:   139.293 [  85.851], Avg:  -337.470 (0.654) <0-06:57:16> ({'r_t':  -668.4686, 'eps':     0.6543, 'len': 27490.7440, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  314000, Reward:   156.212 [  57.459], Avg:  -335.903 (0.654) <0-06:58:32> ({'r_t':  -653.3779, 'eps':     0.6543, 'len': 27536.8940, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  315000, Reward:   100.678 [ 110.927], Avg:  -334.521 (0.654) <0-06:59:49> ({'r_t':  -776.4861, 'eps':     0.6543, 'len': 27592.4520, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  316000, Reward:    94.800 [ 108.243], Avg:  -333.167 (0.654) <0-07:01:06> ({'r_t':  -678.6029, 'eps':     0.6543, 'len': 27643.2890, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  317000, Reward:   159.954 [  73.870], Avg:  -331.616 (0.654) <0-07:02:16> ({'r_t':  -593.1243, 'eps':     0.6543, 'len': 27691.5940, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  318000, Reward:   142.620 [  76.851], Avg:  -330.129 (0.654) <0-07:03:33> ({'r_t':  -610.7251, 'eps':     0.6543, 'len': 27739.8880, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  319000, Reward:   139.010 [  95.838], Avg:  -328.663 (0.654) <0-07:04:49> ({'r_t':  -625.7452, 'eps':     0.6543, 'len': 27787.5270, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:  320000, Reward:    82.318 [ 106.806], Avg:  -327.383 (0.641) <0-07:11:24> ({'r_t':  -637.2702, 'eps':     0.6412, 'len': 27835.3370, 'dyn_loss':    11.6967, 'dot_loss':     1.6702, 'ddot_loss':     3.8474, 'rew_loss':   318.0286, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  321000, Reward:   122.943 [  56.520], Avg:  -325.984 (0.641) <0-07:12:42> ({'r_t':  -574.3671, 'eps':     0.6412, 'len': 27885.0410, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  322000, Reward:    70.316 [  70.058], Avg:  -324.757 (0.641) <0-07:14:00> ({'r_t':  -589.7720, 'eps':     0.6412, 'len': 27929.5020, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  323000, Reward:   111.059 [  72.357], Avg:  -323.412 (0.641) <0-07:15:19> ({'r_t':  -481.2210, 'eps':     0.6412, 'len': 27972.9720, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  324000, Reward:   113.460 [  61.199], Avg:  -322.068 (0.641) <0-07:16:37> ({'r_t':  -519.8788, 'eps':     0.6412, 'len': 28019.0530, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  325000, Reward:    92.117 [  73.037], Avg:  -320.798 (0.641) <0-07:17:56> ({'r_t':  -518.1824, 'eps':     0.6412, 'len': 28070.6790, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  326000, Reward:    85.479 [  79.421], Avg:  -319.555 (0.641) <0-07:19:14> ({'r_t':  -428.8640, 'eps':     0.6412, 'len': 28116.2910, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  327000, Reward:   103.824 [  75.824], Avg:  -318.264 (0.641) <0-07:20:32> ({'r_t':  -481.6475, 'eps':     0.6412, 'len': 28163.8250, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  328000, Reward:    75.932 [  93.548], Avg:  -317.066 (0.641) <0-07:21:51> ({'r_t':  -541.7503, 'eps':     0.6412, 'len': 28210.0010, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  329000, Reward:    74.408 [  71.438], Avg:  -315.880 (0.641) <0-07:23:09> ({'r_t':  -523.9810, 'eps':     0.6412, 'len': 28257.5330, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:  330000, Reward:   121.747 [  89.101], Avg:  -314.558 (0.628) <0-07:29:43> ({'r_t':  -438.0284, 'eps':     0.6283, 'len': 28300.1020, 'dyn_loss':    12.0094, 'dot_loss':     1.6864, 'ddot_loss':     3.8902, 'rew_loss':   266.2076, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  331000, Reward:   110.501 [  63.938], Avg:  -313.277 (0.628) <0-07:31:01> ({'r_t':  -525.7766, 'eps':     0.6283, 'len': 28343.3230, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  332000, Reward:    92.096 [  66.206], Avg:  -312.060 (0.628) <0-07:32:20> ({'r_t':  -511.9206, 'eps':     0.6283, 'len': 28386.0290, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  333000, Reward:   127.160 [  82.215], Avg:  -310.745 (0.628) <0-07:33:38> ({'r_t':  -625.1043, 'eps':     0.6283, 'len': 28432.7350, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  334000, Reward:   135.748 [  51.714], Avg:  -309.412 (0.628) <0-07:34:56> ({'r_t':  -575.9180, 'eps':     0.6283, 'len': 28477.7900, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  335000, Reward:   105.169 [  79.757], Avg:  -308.178 (0.628) <0-07:36:15> ({'r_t':  -558.3892, 'eps':     0.6283, 'len': 28522.7080, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  336000, Reward:   140.066 [  62.715], Avg:  -306.848 (0.628) <0-07:37:33> ({'r_t':  -489.7944, 'eps':     0.6283, 'len': 28568.0520, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  337000, Reward:   139.106 [  69.512], Avg:  -305.529 (0.628) <0-07:38:52> ({'r_t':  -524.7988, 'eps':     0.6283, 'len': 28614.8440, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  338000, Reward:   142.884 [  68.415], Avg:  -304.206 (0.628) <0-07:40:10> ({'r_t':  -579.9273, 'eps':     0.6283, 'len': 28661.2230, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  339000, Reward:   106.365 [  79.918], Avg:  -302.999 (0.628) <0-07:41:29> ({'r_t':  -582.3097, 'eps':     0.6283, 'len': 28708.0480, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:  340000, Reward:   172.291 [  47.191], Avg:  -301.605 (0.616) <0-07:48:12> ({'r_t':  -466.2407, 'eps':     0.6158, 'len': 28750.4000, 'dyn_loss':    12.4491, 'dot_loss':     1.7075, 'ddot_loss':     3.9442, 'rew_loss':   253.9600, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  341000, Reward:   149.200 [  73.193], Avg:  -300.287 (0.616) <0-07:49:30> ({'r_t':  -502.9019, 'eps':     0.6158, 'len': 28797.5600, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  342000, Reward:   130.057 [  83.280], Avg:  -299.032 (0.616) <0-07:50:47> ({'r_t':  -610.8855, 'eps':     0.6158, 'len': 28847.1950, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  343000, Reward:   146.506 [  70.557], Avg:  -297.737 (0.616) <0-07:52:07> ({'r_t':  -449.3154, 'eps':     0.6158, 'len': 28890.4110, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  344000, Reward:   151.115 [  50.543], Avg:  -296.436 (0.616) <0-07:53:27> ({'r_t':  -458.9772, 'eps':     0.6158, 'len': 28930.8550, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  345000, Reward:   144.555 [  63.919], Avg:  -295.161 (0.616) <0-07:54:46> ({'r_t':  -492.6486, 'eps':     0.6158, 'len': 28971.6340, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  346000, Reward:   149.372 [  35.213], Avg:  -293.880 (0.616) <0-07:56:06> ({'r_t':  -421.1531, 'eps':     0.6158, 'len': 29011.7910, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  347000, Reward:   149.946 [  63.161], Avg:  -292.605 (0.616) <0-07:57:26> ({'r_t':  -436.7880, 'eps':     0.6158, 'len': 29055.5760, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  348000, Reward:   182.389 [  54.518], Avg:  -291.244 (0.616) <0-07:58:40> ({'r_t':  -442.4562, 'eps':     0.6158, 'len': 29098.8930, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  349000, Reward:   129.250 [  72.701], Avg:  -290.042 (0.616) <0-08:00:00> ({'r_t':  -384.0614, 'eps':     0.6158, 'len': 29138.2200, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:  350000, Reward:   157.719 [  75.918], Avg:  -288.767 (0.603) <0-08:06:48> ({'r_t':  -404.8629, 'eps':     0.6035, 'len': 29182.7370, 'dyn_loss':    12.3168, 'dot_loss':     1.7191, 'ddot_loss':     3.9867, 'rew_loss':   240.6996, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  351000, Reward:   156.904 [  87.273], Avg:  -287.501 (0.603) <0-08:08:08> ({'r_t':  -410.8994, 'eps':     0.6035, 'len': 29226.0400, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  352000, Reward:   164.892 [  47.568], Avg:  -286.219 (0.603) <0-08:09:26> ({'r_t':  -267.2543, 'eps':     0.6035, 'len': 29266.0730, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  353000, Reward:   141.283 [  71.703], Avg:  -285.011 (0.603) <0-08:10:45> ({'r_t':  -388.6281, 'eps':     0.6035, 'len': 29305.3550, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  354000, Reward:   190.066 [  56.116], Avg:  -283.673 (0.603) <0-08:12:02> ({'r_t':  -390.7279, 'eps':     0.6035, 'len': 29344.5110, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  355000, Reward:   117.823 [  86.353], Avg:  -282.545 (0.603) <0-08:13:22> ({'r_t':  -336.1221, 'eps':     0.6035, 'len': 29382.7090, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  356000, Reward:   155.755 [  74.084], Avg:  -281.318 (0.603) <0-08:14:41> ({'r_t':  -354.9523, 'eps':     0.6035, 'len': 29426.9680, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  357000, Reward:   146.449 [  77.989], Avg:  -280.123 (0.603) <0-08:16:01> ({'r_t':  -407.5585, 'eps':     0.6035, 'len': 29469.1540, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  358000, Reward:   110.437 [  68.208], Avg:  -279.035 (0.603) <0-08:17:21> ({'r_t':  -366.2459, 'eps':     0.6035, 'len': 29509.5890, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  359000, Reward:   145.819 [  54.001], Avg:  -277.855 (0.603) <0-08:18:41> ({'r_t':  -452.0834, 'eps':     0.6035, 'len': 29553.6330, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:  360000, Reward:    99.453 [ 148.006], Avg:  -276.810 (0.591) <0-08:25:33> ({'r_t':  -401.7853, 'eps':     0.5914, 'len': 29601.2210, 'dyn_loss':    12.5846, 'dot_loss':     1.7138, 'ddot_loss':     3.9715, 'rew_loss':   236.9847, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  361000, Reward:   136.009 [  91.062], Avg:  -275.669 (0.591) <0-08:26:55> ({'r_t':  -404.4331, 'eps':     0.5914, 'len': 29642.4550, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  362000, Reward:   158.463 [  85.753], Avg:  -274.473 (0.591) <0-08:28:17> ({'r_t':  -256.2785, 'eps':     0.5914, 'len': 29681.3040, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  363000, Reward:   135.407 [ 102.018], Avg:  -273.347 (0.591) <0-08:29:38> ({'r_t':  -302.4550, 'eps':     0.5914, 'len': 29722.5570, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  364000, Reward:    92.616 [ 104.909], Avg:  -272.345 (0.591) <0-08:31:00> ({'r_t':  -458.9067, 'eps':     0.5914, 'len': 29762.6500, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  365000, Reward:   125.071 [  69.241], Avg:  -271.259 (0.591) <0-08:32:22> ({'r_t':  -401.2181, 'eps':     0.5914, 'len': 29801.4020, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  366000, Reward:   162.912 [  51.803], Avg:  -270.076 (0.591) <0-08:33:43> ({'r_t':  -239.4958, 'eps':     0.5914, 'len': 29840.3730, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  367000, Reward:    44.292 [ 366.512], Avg:  -269.221 (0.591) <0-08:35:05> ({'r_t':  -407.7218, 'eps':     0.5914, 'len': 29880.6310, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  368000, Reward:   155.207 [  69.421], Avg:  -268.071 (0.591) <0-08:36:26> ({'r_t':  -259.8804, 'eps':     0.5914, 'len': 29919.0260, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  369000, Reward:   166.530 [  72.639], Avg:  -266.897 (0.591) <0-08:37:48> ({'r_t':  -300.7251, 'eps':     0.5914, 'len': 29955.3610, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:  370000, Reward:   227.084 [  30.250], Avg:  -265.565 (0.580) <0-08:45:00> ({'r_t':  -360.0306, 'eps':     0.5796, 'len': 29992.1240, 'dyn_loss':    12.8853, 'dot_loss':     1.7351, 'ddot_loss':     4.0234, 'rew_loss':   208.4213, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  371000, Reward:   155.454 [  94.127], Avg:  -264.433 (0.580) <0-08:46:22> ({'r_t':  -200.7691, 'eps':     0.5796, 'len': 30030.1330, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  372000, Reward:   136.821 [  81.577], Avg:  -263.358 (0.580) <0-08:47:44> ({'r_t':  -243.4413, 'eps':     0.5796, 'len': 30068.0970, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  373000, Reward:   189.934 [  58.672], Avg:  -262.146 (0.580) <0-08:49:06> ({'r_t':  -272.8616, 'eps':     0.5796, 'len': 30104.6300, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  374000, Reward:   151.283 [  97.021], Avg:  -261.043 (0.580) <0-08:50:28> ({'r_t':  -183.8380, 'eps':     0.5796, 'len': 30142.6560, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  375000, Reward:   151.355 [  79.341], Avg:  -259.946 (0.580) <0-08:51:50> ({'r_t':  -250.8814, 'eps':     0.5796, 'len': 30184.8470, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  376000, Reward:   177.287 [  60.235], Avg:  -258.787 (0.580) <0-08:53:09> ({'r_t':  -236.4538, 'eps':     0.5796, 'len': 30226.8290, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  377000, Reward:   177.025 [  78.096], Avg:  -257.634 (0.580) <0-08:54:31> ({'r_t':  -245.2088, 'eps':     0.5796, 'len': 30266.4450, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  378000, Reward:   174.611 [ 101.523], Avg:  -256.493 (0.580) <0-08:55:54> ({'r_t':  -198.9975, 'eps':     0.5796, 'len': 30301.0820, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  379000, Reward:   149.517 [  91.096], Avg:  -255.425 (0.580) <0-08:57:16> ({'r_t':  -324.7016, 'eps':     0.5796, 'len': 30336.8510, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:  380000, Reward:   206.436 [ 133.956], Avg:  -254.212 (0.568) <0-09:04:27> ({'r_t':  -262.5655, 'eps':     0.5680, 'len': 30376.4150, 'dyn_loss':    12.9662, 'dot_loss':     1.7509, 'ddot_loss':     4.0699, 'rew_loss':   210.1418, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  381000, Reward:   115.174 [ 185.241], Avg:  -253.245 (0.568) <0-09:05:49> ({'r_t':  -135.1970, 'eps':     0.5680, 'len': 30414.5430, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  382000, Reward:   152.881 [ 138.352], Avg:  -252.185 (0.568) <0-09:07:13> ({'r_t':  -140.8122, 'eps':     0.5680, 'len': 30449.9370, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  383000, Reward:   163.411 [ 126.610], Avg:  -251.103 (0.568) <0-09:08:37> ({'r_t':  -154.0703, 'eps':     0.5680, 'len': 30488.0000, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  384000, Reward:   207.542 [  75.929], Avg:  -249.911 (0.568) <0-09:10:01> ({'r_t':  -189.1806, 'eps':     0.5680, 'len': 30525.5250, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  385000, Reward:   194.924 [  74.521], Avg:  -248.759 (0.568) <0-09:11:24> ({'r_t':  -268.3812, 'eps':     0.5680, 'len': 30562.3440, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  386000, Reward:   224.266 [  44.694], Avg:  -247.537 (0.568) <0-09:12:48> ({'r_t':  -140.2671, 'eps':     0.5680, 'len': 30598.4910, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  387000, Reward:   181.377 [ 127.563], Avg:  -246.431 (0.568) <0-09:14:13> ({'r_t':  -151.5200, 'eps':     0.5680, 'len': 30633.7400, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  388000, Reward:   182.844 [ 128.732], Avg:  -245.328 (0.568) <0-09:15:38> ({'r_t':  -257.9057, 'eps':     0.5680, 'len': 30669.6460, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  389000, Reward:   235.373 [  44.345], Avg:  -244.095 (0.568) <0-09:17:03> ({'r_t':  -162.7727, 'eps':     0.5680, 'len': 30705.9490, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:  390000, Reward:   125.606 [ 177.604], Avg:  -243.150 (0.557) <0-09:24:17> ({'r_t':  -129.1429, 'eps':     0.5566, 'len': 30741.0530, 'dyn_loss':    13.1833, 'dot_loss':     1.7537, 'ddot_loss':     4.0747, 'rew_loss':   213.3633, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  391000, Reward:   122.372 [ 187.059], Avg:  -242.217 (0.557) <0-09:25:41> ({'r_t':  -129.5911, 'eps':     0.5566, 'len': 30777.8170, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  392000, Reward:   211.436 [ 119.303], Avg:  -241.063 (0.557) <0-09:27:05> ({'r_t':  -176.4872, 'eps':     0.5566, 'len': 30814.3070, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  393000, Reward:   166.419 [ 179.493], Avg:  -240.029 (0.557) <0-09:28:29> ({'r_t':  -110.8987, 'eps':     0.5566, 'len': 30854.1610, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  394000, Reward:   152.541 [ 186.617], Avg:  -239.035 (0.557) <0-09:29:52> ({'r_t':  -116.7116, 'eps':     0.5566, 'len': 30892.3630, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  395000, Reward:   122.163 [ 200.592], Avg:  -238.123 (0.557) <0-09:31:16> ({'r_t':  -194.0629, 'eps':     0.5566, 'len': 30926.9240, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  396000, Reward:   -79.775 [ 742.448], Avg:  -237.724 (0.557) <0-09:32:40> ({'r_t':  -146.6197, 'eps':     0.5566, 'len': 30966.1300, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  397000, Reward:    -2.422 [ 533.038], Avg:  -237.133 (0.557) <0-09:34:05> ({'r_t':  -253.2166, 'eps':     0.5566, 'len': 31006.0270, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  398000, Reward:   -14.528 [ 726.390], Avg:  -236.575 (0.557) <0-09:35:29> ({'r_t':  -151.4905, 'eps':     0.5566, 'len': 31044.8920, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  399000, Reward:   150.547 [ 187.314], Avg:  -235.607 (0.557) <0-09:36:54> ({'r_t':  -184.8452, 'eps':     0.5566, 'len': 31084.0230, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:  400000, Reward:    87.371 [ 374.437], Avg:  -234.802 (0.545) <0-09:44:17> ({'r_t':  -148.4767, 'eps':     0.5455, 'len': 31123.6370, 'dyn_loss':    13.5557, 'dot_loss':     1.7684, 'ddot_loss':     4.1130, 'rew_loss':   228.6195, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  401000, Reward:   175.685 [ 174.133], Avg:  -233.780 (0.545) <0-09:45:38> ({'r_t':   -38.5706, 'eps':     0.5455, 'len': 31160.1070, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  402000, Reward:   184.440 [ 122.210], Avg:  -232.743 (0.545) <0-09:47:03> ({'r_t':   -14.5538, 'eps':     0.5455, 'len': 31196.7880, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  403000, Reward:   185.130 [ 167.425], Avg:  -231.708 (0.545) <0-09:48:29> ({'r_t':   -32.4886, 'eps':     0.5455, 'len': 31233.8140, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  404000, Reward:   160.906 [ 133.878], Avg:  -230.739 (0.545) <0-09:49:55> ({'r_t':  -110.5426, 'eps':     0.5455, 'len': 31268.8310, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  405000, Reward:   165.657 [ 139.433], Avg:  -229.763 (0.545) <0-09:51:20> ({'r_t':   -30.1716, 'eps':     0.5455, 'len': 31303.0220, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  406000, Reward:   149.027 [ 182.246], Avg:  -228.832 (0.545) <0-09:52:42> ({'r_t':  -137.8850, 'eps':     0.5455, 'len': 31340.3110, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  407000, Reward:   199.816 [ 122.617], Avg:  -227.781 (0.545) <0-09:54:08> ({'r_t':   -20.5608, 'eps':     0.5455, 'len': 31374.9970, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  408000, Reward:   129.798 [ 170.362], Avg:  -226.907 (0.545) <0-09:55:35> ({'r_t':   -81.2913, 'eps':     0.5455, 'len': 31410.0710, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  409000, Reward:   102.378 [ 225.106], Avg:  -226.104 (0.545) <0-09:57:01> ({'r_t':  -164.2645, 'eps':     0.5455, 'len': 31445.9480, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:  410000, Reward:  -154.999 [ 582.647], Avg:  -225.931 (0.535) <0-10:04:33> ({'r_t':     6.2149, 'eps':     0.5346, 'len': 31480.1760, 'dyn_loss':    13.6865, 'dot_loss':     1.7764, 'ddot_loss':     4.1250, 'rew_loss':   211.6855, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  411000, Reward:    31.837 [ 241.065], Avg:  -225.305 (0.535) <0-10:05:59> ({'r_t':     4.9887, 'eps':     0.5346, 'len': 31514.8820, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  412000, Reward:  -139.341 [ 841.990], Avg:  -225.097 (0.535) <0-10:07:25> ({'r_t':  -146.3390, 'eps':     0.5346, 'len': 31549.8790, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  413000, Reward:   193.669 [ 181.776], Avg:  -224.086 (0.535) <0-10:08:50> ({'r_t':   -34.5015, 'eps':     0.5346, 'len': 31584.9470, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  414000, Reward:    42.222 [ 230.134], Avg:  -223.444 (0.535) <0-10:10:16> ({'r_t':   -32.7921, 'eps':     0.5346, 'len': 31620.3840, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  415000, Reward:   -19.205 [ 394.026], Avg:  -222.953 (0.535) <0-10:11:42> ({'r_t':    -8.1555, 'eps':     0.5346, 'len': 31659.1080, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  416000, Reward:    40.437 [ 202.114], Avg:  -222.321 (0.535) <0-10:13:07> ({'r_t':  -111.2854, 'eps':     0.5346, 'len': 31695.2330, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  417000, Reward:  -316.372 [ 922.360], Avg:  -222.546 (0.535) <0-10:14:34> ({'r_t':   -15.1200, 'eps':     0.5346, 'len': 31732.4790, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  418000, Reward:  -352.558 [ 635.169], Avg:  -222.857 (0.535) <0-10:16:00> ({'r_t':  -166.5194, 'eps':     0.5346, 'len': 31768.3340, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  419000, Reward:   -24.207 [ 394.826], Avg:  -222.384 (0.535) <0-10:17:25> ({'r_t':   -25.5201, 'eps':     0.5346, 'len': 31802.5170, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:  420000, Reward:   -37.608 [ 510.952], Avg:  -221.945 (0.524) <0-10:25:02> ({'r_t':   -61.4996, 'eps':     0.5239, 'len': 31836.9950, 'dyn_loss':    13.3284, 'dot_loss':     1.7605, 'ddot_loss':     4.1195, 'rew_loss':   199.6566, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  421000, Reward:   118.111 [ 384.970], Avg:  -221.139 (0.524) <0-10:26:30> ({'r_t':    67.8496, 'eps':     0.5239, 'len': 31872.7530, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  422000, Reward:    59.328 [ 406.405], Avg:  -220.476 (0.524) <0-10:27:57> ({'r_t':   119.9063, 'eps':     0.5239, 'len': 31908.0070, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  423000, Reward:   177.801 [ 180.928], Avg:  -219.536 (0.524) <0-10:29:25> ({'r_t':    34.5285, 'eps':     0.5239, 'len': 31943.8660, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  424000, Reward:  -113.884 [ 484.915], Avg:  -219.288 (0.524) <0-10:30:52> ({'r_t':   -12.2963, 'eps':     0.5239, 'len': 31978.5380, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  425000, Reward:  -206.277 [1486.697], Avg:  -219.257 (0.524) <0-10:32:20> ({'r_t':    82.2641, 'eps':     0.5239, 'len': 32014.0600, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  426000, Reward:   138.072 [ 219.764], Avg:  -218.420 (0.524) <0-10:33:48> ({'r_t':   -79.5371, 'eps':     0.5239, 'len': 32048.3560, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  427000, Reward:    79.595 [ 240.356], Avg:  -217.724 (0.524) <0-10:35:16> ({'r_t':    62.8896, 'eps':     0.5239, 'len': 32082.4550, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  428000, Reward:    89.579 [ 253.768], Avg:  -217.008 (0.524) <0-10:36:44> ({'r_t':   -66.4971, 'eps':     0.5239, 'len': 32116.8640, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  429000, Reward:  -277.938 [ 915.768], Avg:  -217.150 (0.524) <0-10:38:12> ({'r_t':   107.7865, 'eps':     0.5239, 'len': 32151.0230, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:  430000, Reward:  -193.010 [ 824.585], Avg:  -217.094 (0.513) <0-10:45:56> ({'r_t':    52.6675, 'eps':     0.5134, 'len': 32185.6350, 'dyn_loss':    13.6886, 'dot_loss':     1.7803, 'ddot_loss':     4.1633, 'rew_loss':   189.1812, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  431000, Reward:   168.914 [ 166.590], Avg:  -216.200 (0.513) <0-10:47:23> ({'r_t':   -60.7918, 'eps':     0.5134, 'len': 32221.9000, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  432000, Reward:   -79.018 [ 501.517], Avg:  -215.883 (0.513) <0-10:48:51> ({'r_t':    34.2728, 'eps':     0.5134, 'len': 32256.2800, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  433000, Reward:   -91.585 [ 513.771], Avg:  -215.597 (0.513) <0-10:50:18> ({'r_t':    63.1943, 'eps':     0.5134, 'len': 32292.7010, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  434000, Reward:  -138.968 [ 494.252], Avg:  -215.421 (0.513) <0-10:51:46> ({'r_t':    43.8018, 'eps':     0.5134, 'len': 32327.6890, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  435000, Reward:   162.362 [ 178.994], Avg:  -214.554 (0.513) <0-10:53:13> ({'r_t':  -123.0456, 'eps':     0.5134, 'len': 32365.0770, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  436000, Reward:   202.005 [ 134.780], Avg:  -213.601 (0.513) <0-10:54:41> ({'r_t':    81.0494, 'eps':     0.5134, 'len': 32401.4820, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  437000, Reward:   188.968 [ 123.500], Avg:  -212.682 (0.513) <0-10:56:09> ({'r_t':   -73.8458, 'eps':     0.5134, 'len': 32434.5010, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  438000, Reward:    40.612 [ 416.554], Avg:  -212.105 (0.513) <0-10:57:36> ({'r_t':     6.9095, 'eps':     0.5134, 'len': 32469.1740, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  439000, Reward:   130.552 [ 380.398], Avg:  -211.326 (0.513) <0-10:59:04> ({'r_t':    15.5907, 'eps':     0.5134, 'len': 32505.1780, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:  440000, Reward:   136.451 [ 362.558], Avg:  -210.537 (0.503) <0-11:06:56> ({'r_t':    45.0981, 'eps':     0.5031, 'len': 32541.3500, 'dyn_loss':    14.3199, 'dot_loss':     1.8180, 'ddot_loss':     4.2538, 'rew_loss':   201.3185, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  441000, Reward:   183.457 [ 358.744], Avg:  -209.646 (0.503) <0-11:08:23> ({'r_t':   106.9269, 'eps':     0.5031, 'len': 32575.6390, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  442000, Reward:   -42.141 [ 566.273], Avg:  -209.268 (0.503) <0-11:09:51> ({'r_t':    30.4515, 'eps':     0.5031, 'len': 32610.3120, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  443000, Reward:   284.579 [  30.662], Avg:  -208.156 (0.503) <0-11:11:18> ({'r_t':   165.1000, 'eps':     0.5031, 'len': 32645.4230, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  444000, Reward:   -21.271 [ 578.365], Avg:  -207.736 (0.503) <0-11:12:46> ({'r_t':   120.2372, 'eps':     0.5031, 'len': 32681.0920, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  445000, Reward:    72.046 [ 484.008], Avg:  -207.108 (0.503) <0-11:14:13> ({'r_t':    40.9891, 'eps':     0.5031, 'len': 32720.2470, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  446000, Reward:  -170.290 [ 637.317], Avg:  -207.026 (0.503) <0-11:15:41> ({'r_t':   112.7472, 'eps':     0.5031, 'len': 32754.3160, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  447000, Reward:   104.696 [ 387.085], Avg:  -206.330 (0.503) <0-11:17:08> ({'r_t':   123.4433, 'eps':     0.5031, 'len': 32787.3880, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  448000, Reward:  -277.663 [1042.948], Avg:  -206.489 (0.503) <0-11:18:36> ({'r_t':   134.9504, 'eps':     0.5031, 'len': 32824.5620, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  449000, Reward:  -171.641 [ 618.844], Avg:  -206.412 (0.503) <0-11:20:03> ({'r_t':    49.5449, 'eps':     0.5031, 'len': 32864.3520, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:  450000, Reward:   308.514 [  34.062], Avg:  -205.270 (0.493) <0-11:28:05> ({'r_t':   105.2008, 'eps':     0.4931, 'len': 32899.4610, 'dyn_loss':    14.3254, 'dot_loss':     1.8082, 'ddot_loss':     4.2460, 'rew_loss':   180.0382, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  451000, Reward:   256.588 [  70.706], Avg:  -204.248 (0.493) <0-11:29:34> ({'r_t':    25.9776, 'eps':     0.4931, 'len': 32933.2840, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  452000, Reward:   193.870 [ 373.181], Avg:  -203.369 (0.493) <0-11:31:04> ({'r_t':   176.4193, 'eps':     0.4931, 'len': 32966.9760, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  453000, Reward:   292.582 [  40.253], Avg:  -202.277 (0.493) <0-11:32:33> ({'r_t':   217.8808, 'eps':     0.4931, 'len': 33000.6030, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  454000, Reward:   275.871 [  52.000], Avg:  -201.226 (0.493) <0-11:34:02> ({'r_t':   188.5626, 'eps':     0.4931, 'len': 33034.9670, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  455000, Reward:   216.033 [ 185.138], Avg:  -200.311 (0.493) <0-11:35:31> ({'r_t':   128.8116, 'eps':     0.4931, 'len': 33069.7370, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  456000, Reward:   131.370 [ 401.033], Avg:  -199.585 (0.493) <0-11:37:00> ({'r_t':   200.2528, 'eps':     0.4931, 'len': 33105.0040, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  457000, Reward:   243.163 [ 111.811], Avg:  -198.618 (0.493) <0-11:38:29> ({'r_t':    97.7777, 'eps':     0.4931, 'len': 33139.8220, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  458000, Reward:   -24.790 [1102.219], Avg:  -198.240 (0.493) <0-11:39:58> ({'r_t':    43.7641, 'eps':     0.4931, 'len': 33174.0140, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  459000, Reward:   252.007 [ 143.989], Avg:  -197.261 (0.493) <0-11:41:27> ({'r_t':   141.5338, 'eps':     0.4931, 'len': 33207.8750, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:  460000, Reward:    85.590 [ 513.152], Avg:  -196.647 (0.483) <0-11:49:36> ({'r_t':   143.7093, 'eps':     0.4832, 'len': 33241.8670, 'dyn_loss':    14.3687, 'dot_loss':     1.8301, 'ddot_loss':     4.3065, 'rew_loss':   182.1696, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  461000, Reward:   -82.423 [ 838.799], Avg:  -196.400 (0.483) <0-11:51:05> ({'r_t':   183.4116, 'eps':     0.4832, 'len': 33275.7270, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  462000, Reward:    94.982 [ 726.418], Avg:  -195.771 (0.483) <0-11:52:35> ({'r_t':   200.9156, 'eps':     0.4832, 'len': 33310.0830, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  463000, Reward:   -50.046 [ 657.826], Avg:  -195.457 (0.483) <0-11:54:04> ({'r_t':   221.8860, 'eps':     0.4832, 'len': 33343.2270, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  464000, Reward:   146.045 [ 486.559], Avg:  -194.722 (0.483) <0-11:55:34> ({'r_t':   140.1455, 'eps':     0.4832, 'len': 33377.0860, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  465000, Reward:    54.772 [ 497.616], Avg:  -194.187 (0.483) <0-11:57:03> ({'r_t':    11.3595, 'eps':     0.4832, 'len': 33411.2840, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  466000, Reward:  -198.059 [ 736.080], Avg:  -194.195 (0.483) <0-11:58:33> ({'r_t':    39.2427, 'eps':     0.4832, 'len': 33445.8610, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  467000, Reward:    -3.521 [ 602.239], Avg:  -193.788 (0.483) <0-12:00:02> ({'r_t':   195.9722, 'eps':     0.4832, 'len': 33479.3160, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  468000, Reward:  -249.060 [ 738.294], Avg:  -193.906 (0.483) <0-12:01:32> ({'r_t':   163.0087, 'eps':     0.4832, 'len': 33512.8650, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  469000, Reward:  -326.897 [ 753.696], Avg:  -194.189 (0.483) <0-12:03:02> ({'r_t':    41.6157, 'eps':     0.4832, 'len': 33545.2600, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:  470000, Reward:   317.919 [  24.362], Avg:  -193.101 (0.474) <0-12:11:10> ({'r_t':   216.3196, 'eps':     0.4735, 'len': 33578.4250, 'dyn_loss':    14.3783, 'dot_loss':     1.8379, 'ddot_loss':     4.3220, 'rew_loss':   165.5376, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  471000, Reward:   206.045 [ 356.381], Avg:  -192.256 (0.474) <0-12:12:41> ({'r_t':   304.2147, 'eps':     0.4735, 'len': 33612.1730, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  472000, Reward:   305.766 [  29.518], Avg:  -191.203 (0.474) <0-12:14:11> ({'r_t':   105.9540, 'eps':     0.4735, 'len': 33646.4350, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  473000, Reward:   302.803 [  34.266], Avg:  -190.161 (0.474) <0-12:15:41> ({'r_t':   246.8654, 'eps':     0.4735, 'len': 33680.1550, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  474000, Reward:   305.268 [  37.637], Avg:  -189.118 (0.474) <0-12:17:12> ({'r_t':    75.9663, 'eps':     0.4735, 'len': 33713.4000, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  475000, Reward:   230.960 [ 364.574], Avg:  -188.235 (0.474) <0-12:18:42> ({'r_t':   158.4560, 'eps':     0.4735, 'len': 33746.2130, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  476000, Reward:   295.843 [  28.142], Avg:  -187.220 (0.474) <0-12:20:13> ({'r_t':   271.3502, 'eps':     0.4735, 'len': 33780.3150, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  477000, Reward:   105.164 [ 492.724], Avg:  -186.609 (0.474) <0-12:21:44> ({'r_t':   282.8120, 'eps':     0.4735, 'len': 33813.2630, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  478000, Reward:   302.917 [  32.287], Avg:  -185.587 (0.474) <0-12:23:15> ({'r_t':   105.3730, 'eps':     0.4735, 'len': 33846.3830, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  479000, Reward:   305.318 [  29.896], Avg:  -184.564 (0.474) <0-12:24:45> ({'r_t':   300.6496, 'eps':     0.4735, 'len': 33880.4060, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:  480000, Reward:   181.622 [ 371.728], Avg:  -183.803 (0.464) <0-12:33:04> ({'r_t':   244.8619, 'eps':     0.4641, 'len': 33913.1220, 'dyn_loss':    14.4996, 'dot_loss':     1.8385, 'ddot_loss':     4.3167, 'rew_loss':   167.9550, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  481000, Reward:   277.036 [  50.532], Avg:  -182.846 (0.464) <0-12:34:34> ({'r_t':   179.6768, 'eps':     0.4641, 'len': 33945.3540, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  482000, Reward:   181.289 [ 362.312], Avg:  -182.093 (0.464) <0-12:36:05> ({'r_t':   292.3720, 'eps':     0.4641, 'len': 33978.0530, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  483000, Reward:   190.092 [ 365.295], Avg:  -181.324 (0.464) <0-12:37:35> ({'r_t':   209.6720, 'eps':     0.4641, 'len': 34011.5690, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  484000, Reward:   180.345 [ 390.526], Avg:  -180.578 (0.464) <0-12:39:05> ({'r_t':   227.7243, 'eps':     0.4641, 'len': 34044.4570, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  485000, Reward:   301.101 [  21.094], Avg:  -179.587 (0.464) <0-12:40:36> ({'r_t':    32.9102, 'eps':     0.4641, 'len': 34076.6200, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  486000, Reward:   277.040 [ 119.813], Avg:  -178.649 (0.464) <0-12:42:07> ({'r_t':   234.4856, 'eps':     0.4641, 'len': 34110.2670, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  487000, Reward:   279.884 [ 103.571], Avg:  -177.710 (0.464) <0-12:43:37> ({'r_t':   120.9260, 'eps':     0.4641, 'len': 34144.9790, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  488000, Reward:   283.527 [  34.298], Avg:  -176.766 (0.464) <0-12:45:08> ({'r_t':   121.1839, 'eps':     0.4641, 'len': 34178.5820, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  489000, Reward:   211.661 [ 369.566], Avg:  -175.974 (0.464) <0-12:46:38> ({'r_t':   168.4564, 'eps':     0.4641, 'len': 34211.5250, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:  490000, Reward:   316.114 [  30.988], Avg:  -174.971 (0.455) <0-12:55:03> ({'r_t':   274.8088, 'eps':     0.4548, 'len': 34244.2970, 'dyn_loss':    14.5727, 'dot_loss':     1.8439, 'ddot_loss':     4.3506, 'rew_loss':   186.1057, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  491000, Reward:   331.092 [  25.472], Avg:  -173.943 (0.455) <0-12:56:33> ({'r_t':   348.2308, 'eps':     0.4548, 'len': 34277.9300, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  492000, Reward:   323.887 [  33.770], Avg:  -172.933 (0.455) <0-12:58:04> ({'r_t':   256.5417, 'eps':     0.4548, 'len': 34312.6620, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  493000, Reward:   319.431 [  24.695], Avg:  -171.936 (0.455) <0-12:59:34> ({'r_t':    51.6938, 'eps':     0.4548, 'len': 34347.7020, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  494000, Reward:   308.321 [  29.514], Avg:  -170.966 (0.455) <0-13:01:04> ({'r_t':   215.5742, 'eps':     0.4548, 'len': 34381.8930, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  495000, Reward:   324.171 [  28.620], Avg:  -169.968 (0.455) <0-13:02:35> ({'r_t':   284.3721, 'eps':     0.4548, 'len': 34415.5000, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  496000, Reward:   323.075 [  24.419], Avg:  -168.976 (0.455) <0-13:04:05> ({'r_t':    60.8738, 'eps':     0.4548, 'len': 34448.2580, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  497000, Reward:   333.546 [  23.556], Avg:  -167.967 (0.455) <0-13:05:35> ({'r_t':   234.5266, 'eps':     0.4548, 'len': 34481.5330, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  498000, Reward:   196.635 [ 360.880], Avg:  -167.236 (0.455) <0-13:07:06> ({'r_t':    99.6743, 'eps':     0.4548, 'len': 34515.3040, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  499000, Reward:   326.873 [  36.475], Avg:  -166.248 (0.455) <0-13:08:36> ({'r_t':   203.1996, 'eps':     0.4548, 'len': 34549.4250, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:  500000, Reward:   326.410 [  25.198], Avg:  -165.264 (0.446) <0-13:17:14> ({'r_t':   240.5314, 'eps':     0.4457, 'len': 34582.4000, 'dyn_loss':    14.3888, 'dot_loss':     1.8409, 'ddot_loss':     4.3552, 'rew_loss':   168.8583, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  501000, Reward:   271.382 [ 126.280], Avg:  -164.395 (0.446) <0-13:18:46> ({'r_t':   133.0850, 'eps':     0.4457, 'len': 34615.3770, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  502000, Reward:   318.510 [  26.846], Avg:  -163.435 (0.446) <0-13:20:18> ({'r_t':   389.8677, 'eps':     0.4457, 'len': 34650.8040, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  503000, Reward:   310.978 [  41.981], Avg:  -162.493 (0.446) <0-13:21:51> ({'r_t':   246.6363, 'eps':     0.4457, 'len': 34683.3300, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  504000, Reward:   315.053 [  28.293], Avg:  -161.548 (0.446) <0-13:23:23> ({'r_t':   376.6782, 'eps':     0.4457, 'len': 34715.4780, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  505000, Reward:   314.300 [  36.310], Avg:  -160.607 (0.446) <0-13:24:55> ({'r_t':   300.4979, 'eps':     0.4457, 'len': 34747.7570, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  506000, Reward:   323.409 [  30.151], Avg:  -159.653 (0.446) <0-13:26:27> ({'r_t':   308.6173, 'eps':     0.4457, 'len': 34781.8370, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  507000, Reward:   296.285 [  72.211], Avg:  -158.755 (0.446) <0-13:28:00> ({'r_t':   328.1130, 'eps':     0.4457, 'len': 34814.7070, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  508000, Reward:   330.012 [  29.092], Avg:  -157.795 (0.446) <0-13:29:32> ({'r_t':   238.7480, 'eps':     0.4457, 'len': 34847.5250, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  509000, Reward:   318.406 [  27.109], Avg:  -156.861 (0.446) <0-13:31:04> ({'r_t':   384.3976, 'eps':     0.4457, 'len': 34879.7510, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:  510000, Reward:   245.349 [ 384.724], Avg:  -156.074 (0.437) <0-13:39:38> ({'r_t':   389.2644, 'eps':     0.4368, 'len': 34914.3170, 'dyn_loss':    14.2471, 'dot_loss':     1.8310, 'ddot_loss':     4.3270, 'rew_loss':   183.3919, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  511000, Reward:   346.139 [  31.041], Avg:  -155.093 (0.437) <0-13:41:11> ({'r_t':   213.8131, 'eps':     0.4368, 'len': 34947.4340, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  512000, Reward:   329.876 [  85.053], Avg:  -154.148 (0.437) <0-13:42:43> ({'r_t':   396.9322, 'eps':     0.4368, 'len': 34980.8170, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  513000, Reward:   364.854 [  26.220], Avg:  -153.138 (0.437) <0-13:44:15> ({'r_t':    25.9191, 'eps':     0.4368, 'len': 35013.1600, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  514000, Reward:   274.379 [ 342.622], Avg:  -152.308 (0.437) <0-13:45:47> ({'r_t':   194.8723, 'eps':     0.4368, 'len': 35046.2300, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  515000, Reward:   355.557 [  32.582], Avg:  -151.324 (0.437) <0-13:47:19> ({'r_t':   264.0798, 'eps':     0.4368, 'len': 35080.3280, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  516000, Reward:   259.373 [ 374.789], Avg:  -150.529 (0.437) <0-13:48:50> ({'r_t':   224.2417, 'eps':     0.4368, 'len': 35113.4140, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  517000, Reward:   262.236 [ 367.319], Avg:  -149.732 (0.437) <0-13:50:22> ({'r_t':   183.9011, 'eps':     0.4368, 'len': 35147.0310, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  518000, Reward:   370.092 [  34.331], Avg:  -148.731 (0.437) <0-13:51:54> ({'r_t':   406.0442, 'eps':     0.4368, 'len': 35179.8210, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  519000, Reward:   276.484 [ 343.863], Avg:  -147.913 (0.437) <0-13:53:26> ({'r_t':   339.2230, 'eps':     0.4368, 'len': 35213.0020, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:  520000, Reward:   304.579 [ 381.962], Avg:  -147.045 (0.428) <0-14:02:05> ({'r_t':   353.8280, 'eps':     0.4281, 'len': 35247.6420, 'dyn_loss':    14.2815, 'dot_loss':     1.8410, 'ddot_loss':     4.3561, 'rew_loss':   169.0136, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  521000, Reward:   362.519 [  45.619], Avg:  -146.068 (0.428) <0-14:03:37> ({'r_t':   453.0360, 'eps':     0.4281, 'len': 35281.4460, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  522000, Reward:   359.458 [  48.327], Avg:  -145.102 (0.428) <0-14:05:08> ({'r_t':   411.4600, 'eps':     0.4281, 'len': 35316.9890, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  523000, Reward:   375.325 [  44.785], Avg:  -144.109 (0.428) <0-14:06:40> ({'r_t':   341.0905, 'eps':     0.4281, 'len': 35352.1130, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  524000, Reward:   278.947 [ 356.547], Avg:  -143.303 (0.428) <0-14:08:12> ({'r_t':   258.0766, 'eps':     0.4281, 'len': 35384.1340, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  525000, Reward:   340.813 [ 129.623], Avg:  -142.382 (0.428) <0-14:09:44> ({'r_t':   211.2804, 'eps':     0.4281, 'len': 35417.3060, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  526000, Reward:   383.318 [  23.712], Avg:  -141.385 (0.428) <0-14:11:16> ({'r_t':   372.8010, 'eps':     0.4281, 'len': 35450.2330, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  527000, Reward:   391.473 [  55.023], Avg:  -140.376 (0.428) <0-14:12:47> ({'r_t':   111.2956, 'eps':     0.4281, 'len': 35483.8170, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  528000, Reward:   375.857 [  34.189], Avg:  -139.400 (0.428) <0-14:14:19> ({'r_t':   432.8360, 'eps':     0.4281, 'len': 35517.0870, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  529000, Reward:   331.168 [ 113.076], Avg:  -138.512 (0.428) <0-14:15:52> ({'r_t':   207.7980, 'eps':     0.4281, 'len': 35549.6950, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
Step:  530000, Reward:   418.517 [  26.205], Avg:  -137.463 (0.419) <0-14:24:35> ({'r_t':   173.4043, 'eps':     0.4195, 'len': 35581.9740, 'dyn_loss':    14.7113, 'dot_loss':     1.8665, 'ddot_loss':     4.4254, 'rew_loss':   169.2887, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  531000, Reward:   412.884 [  35.009], Avg:  -136.429 (0.419) <0-14:26:09> ({'r_t':     9.9348, 'eps':     0.4195, 'len': 35614.2500, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  532000, Reward:   303.944 [ 360.503], Avg:  -135.602 (0.419) <0-14:27:43> ({'r_t':   182.3486, 'eps':     0.4195, 'len': 35646.4400, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  533000, Reward:   427.027 [  27.220], Avg:  -134.549 (0.419) <0-14:29:16> ({'r_t':   208.0403, 'eps':     0.4195, 'len': 35680.3340, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  534000, Reward:   403.205 [  33.259], Avg:  -133.544 (0.419) <0-14:30:49> ({'r_t':   364.5187, 'eps':     0.4195, 'len': 35713.7360, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  535000, Reward:   326.840 [ 367.362], Avg:  -132.685 (0.419) <0-14:32:23> ({'r_t':    82.1330, 'eps':     0.4195, 'len': 35747.0380, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  536000, Reward:   419.660 [  22.250], Avg:  -131.656 (0.419) <0-14:33:56> ({'r_t':   282.4374, 'eps':     0.4195, 'len': 35779.2280, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  537000, Reward:   410.462 [  46.570], Avg:  -130.648 (0.419) <0-14:35:31> ({'r_t':   368.3341, 'eps':     0.4195, 'len': 35811.7140, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  538000, Reward:   411.543 [  29.715], Avg:  -129.642 (0.419) <0-14:37:05> ({'r_t':   -14.5522, 'eps':     0.4195, 'len': 35845.2750, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  539000, Reward:   120.280 [ 576.362], Avg:  -129.180 (0.419) <0-14:38:40> ({'r_t':   380.7568, 'eps':     0.4195, 'len': 35879.0270, 'lr':   5.00e-05, 'eps_e':     0.4195, 'lr_e':   5.00e-05})
Step:  540000, Reward:   379.407 [ 161.440], Avg:  -128.240 (0.411) <0-14:47:28> ({'r_t':   226.1670, 'eps':     0.4111, 'len': 35912.7120, 'dyn_loss':    13.9249, 'dot_loss':     1.7976, 'ddot_loss':     4.2764, 'rew_loss':   176.9032, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  541000, Reward:   340.486 [ 365.898], Avg:  -127.375 (0.411) <0-14:49:02> ({'r_t':   326.2391, 'eps':     0.4111, 'len': 35945.5110, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  542000, Reward:   427.510 [  22.802], Avg:  -126.353 (0.411) <0-14:50:35> ({'r_t':   370.0995, 'eps':     0.4111, 'len': 35977.7980, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  543000, Reward:   420.942 [  36.277], Avg:  -125.347 (0.411) <0-14:52:09> ({'r_t':   443.1243, 'eps':     0.4111, 'len': 36010.1660, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  544000, Reward:   242.868 [ 734.825], Avg:  -124.671 (0.411) <0-14:53:42> ({'r_t':   551.7279, 'eps':     0.4111, 'len': 36042.8880, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  545000, Reward:   420.273 [  34.455], Avg:  -123.673 (0.411) <0-14:55:16> ({'r_t':   324.3284, 'eps':     0.4111, 'len': 36075.5800, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  546000, Reward:   427.984 [  39.254], Avg:  -122.665 (0.411) <0-14:56:50> ({'r_t':   379.5117, 'eps':     0.4111, 'len': 36108.5880, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  547000, Reward:   423.841 [  29.142], Avg:  -121.667 (0.411) <0-14:58:24> ({'r_t':   438.0242, 'eps':     0.4111, 'len': 36142.9880, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  548000, Reward:   392.967 [ 125.846], Avg:  -120.730 (0.411) <0-14:59:59> ({'r_t':   556.3950, 'eps':     0.4111, 'len': 36176.2810, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  549000, Reward:   423.645 [  28.184], Avg:  -119.740 (0.411) <0-15:01:33> ({'r_t':   517.3030, 'eps':     0.4111, 'len': 36209.8100, 'lr':   5.00e-05, 'eps_e':     0.4111, 'lr_e':   5.00e-05})
Step:  550000, Reward:   409.286 [  32.392], Avg:  -118.780 (0.403) <0-15:10:25> ({'r_t':   361.0026, 'eps':     0.4029, 'len': 36245.9920, 'dyn_loss':    13.8647, 'dot_loss':     1.7786, 'ddot_loss':     4.2322, 'rew_loss':   161.6071, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  551000, Reward:   231.738 [ 496.891], Avg:  -118.145 (0.403) <0-15:11:59> ({'r_t':   427.5948, 'eps':     0.4029, 'len': 36279.5000, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  552000, Reward:   413.512 [  55.276], Avg:  -117.184 (0.403) <0-15:13:32> ({'r_t':   220.5682, 'eps':     0.4029, 'len': 36311.5300, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  553000, Reward:   225.574 [ 502.946], Avg:  -116.565 (0.403) <0-15:15:06> ({'r_t':   390.8336, 'eps':     0.4029, 'len': 36344.2410, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  554000, Reward:   207.236 [ 493.365], Avg:  -115.982 (0.403) <0-15:16:39> ({'r_t':   427.5530, 'eps':     0.4029, 'len': 36376.7730, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  555000, Reward:   234.303 [ 505.648], Avg:  -115.352 (0.403) <0-15:18:13> ({'r_t':   249.8287, 'eps':     0.4029, 'len': 36409.1140, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  556000, Reward:   411.988 [  50.611], Avg:  -114.405 (0.403) <0-15:19:47> ({'r_t':   541.5905, 'eps':     0.4029, 'len': 36441.9850, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  557000, Reward:   374.182 [  88.110], Avg:  -113.529 (0.403) <0-15:21:21> ({'r_t':   120.8826, 'eps':     0.4029, 'len': 36474.0560, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  558000, Reward:   408.224 [  51.722], Avg:  -112.596 (0.403) <0-15:22:56> ({'r_t':   281.5692, 'eps':     0.4029, 'len': 36507.3800, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  559000, Reward:   237.885 [ 487.458], Avg:  -111.970 (0.403) <0-15:24:30> ({'r_t':   316.0050, 'eps':     0.4029, 'len': 36540.1660, 'lr':   5.00e-05, 'eps_e':     0.4029, 'lr_e':   5.00e-05})
Step:  560000, Reward:   259.627 [ 376.603], Avg:  -111.308 (0.395) <0-15:33:24> ({'r_t':   444.0165, 'eps':     0.3948, 'len': 36572.3520, 'dyn_loss':    13.7343, 'dot_loss':     1.7720, 'ddot_loss':     4.2117, 'rew_loss':   195.6676, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  561000, Reward:   364.818 [ 146.626], Avg:  -110.460 (0.395) <0-15:34:59> ({'r_t':   610.7841, 'eps':     0.3948, 'len': 36604.9450, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  562000, Reward:   416.062 [  55.166], Avg:  -109.525 (0.395) <0-15:36:35> ({'r_t':   483.7984, 'eps':     0.3948, 'len': 36637.3090, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  563000, Reward:   420.172 [  52.870], Avg:  -108.586 (0.395) <0-15:38:10> ({'r_t':   459.9395, 'eps':     0.3948, 'len': 36671.0600, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  564000, Reward:   397.148 [  58.537], Avg:  -107.691 (0.395) <0-15:39:46> ({'r_t':   359.0021, 'eps':     0.3948, 'len': 36703.8420, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  565000, Reward:   309.061 [ 362.092], Avg:  -106.955 (0.395) <0-15:41:21> ({'r_t':   390.2791, 'eps':     0.3948, 'len': 36736.6590, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  566000, Reward:   399.063 [  47.289], Avg:  -106.062 (0.395) <0-15:42:57> ({'r_t':   496.4614, 'eps':     0.3948, 'len': 36769.9490, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  567000, Reward:   423.572 [  32.928], Avg:  -105.130 (0.395) <0-15:44:33> ({'r_t':   296.8418, 'eps':     0.3948, 'len': 36802.4350, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  568000, Reward:   432.779 [  27.080], Avg:  -104.184 (0.395) <0-15:46:09> ({'r_t':   345.8887, 'eps':     0.3948, 'len': 36834.4690, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  569000, Reward:   430.880 [  26.648], Avg:  -103.246 (0.395) <0-15:47:44> ({'r_t':   628.6893, 'eps':     0.3948, 'len': 36868.0710, 'lr':   5.00e-05, 'eps_e':     0.3948, 'lr_e':   5.00e-05})
Step:  570000, Reward:   456.828 [  23.935], Avg:  -102.265 (0.387) <0-15:56:53> ({'r_t':   538.7568, 'eps':     0.3869, 'len': 36900.2750, 'dyn_loss':    14.2363, 'dot_loss':     1.7901, 'ddot_loss':     4.2569, 'rew_loss':   158.9938, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  571000, Reward:   372.023 [ 385.738], Avg:  -101.436 (0.387) <0-15:58:28> ({'r_t':   282.9939, 'eps':     0.3869, 'len': 36932.3590, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  572000, Reward:   463.488 [  20.605], Avg:  -100.450 (0.387) <0-16:00:04> ({'r_t':   289.8531, 'eps':     0.3869, 'len': 36964.5950, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  573000, Reward:   367.076 [ 371.554], Avg:   -99.635 (0.387) <0-16:01:40> ({'r_t':   292.1949, 'eps':     0.3869, 'len': 36997.8570, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  574000, Reward:   422.160 [ 108.265], Avg:   -98.728 (0.387) <0-16:03:18> ({'r_t':   226.5827, 'eps':     0.3869, 'len': 37030.9380, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  575000, Reward:   451.428 [  36.639], Avg:   -97.773 (0.387) <0-16:04:53> ({'r_t':   496.1596, 'eps':     0.3869, 'len': 37063.6220, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  576000, Reward:   437.134 [  47.534], Avg:   -96.845 (0.387) <0-16:06:30> ({'r_t':   443.5207, 'eps':     0.3869, 'len': 37096.7090, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  577000, Reward:   392.820 [ 145.970], Avg:   -95.998 (0.387) <0-16:08:06> ({'r_t':   289.1893, 'eps':     0.3869, 'len': 37129.8030, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  578000, Reward:   436.571 [  52.776], Avg:   -95.079 (0.387) <0-16:09:43> ({'r_t':   345.5698, 'eps':     0.3869, 'len': 37162.8040, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  579000, Reward:   458.469 [  32.937], Avg:   -94.124 (0.387) <0-16:11:19> ({'r_t':   470.2950, 'eps':     0.3869, 'len': 37194.7810, 'lr':   5.00e-05, 'eps_e':     0.3869, 'lr_e':   5.00e-05})
Step:  580000, Reward:   465.581 [  30.052], Avg:   -93.161 (0.379) <0-16:20:22> ({'r_t':   364.5649, 'eps':     0.3792, 'len': 37227.2380, 'dyn_loss':    14.1485, 'dot_loss':     1.7868, 'ddot_loss':     4.2565, 'rew_loss':   160.3014, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  581000, Reward:   457.827 [  36.443], Avg:   -92.214 (0.379) <0-16:21:58> ({'r_t':   460.0033, 'eps':     0.3792, 'len': 37260.0290, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  582000, Reward:   464.099 [  33.413], Avg:   -91.260 (0.379) <0-16:23:33> ({'r_t':   633.3078, 'eps':     0.3792, 'len': 37293.2260, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  583000, Reward:   457.812 [  29.203], Avg:   -90.320 (0.379) <0-16:25:08> ({'r_t':   447.5128, 'eps':     0.3792, 'len': 37325.7840, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  584000, Reward:   464.465 [  28.971], Avg:   -89.371 (0.379) <0-16:26:43> ({'r_t':   635.5574, 'eps':     0.3792, 'len': 37357.7940, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  585000, Reward:   471.563 [  25.702], Avg:   -88.414 (0.379) <0-16:28:20> ({'r_t':   671.4191, 'eps':     0.3792, 'len': 37390.6020, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  586000, Reward:   472.860 [  24.507], Avg:   -87.458 (0.379) <0-16:29:57> ({'r_t':   440.0254, 'eps':     0.3792, 'len': 37423.5030, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  587000, Reward:   465.888 [  24.050], Avg:   -86.517 (0.379) <0-16:31:32> ({'r_t':   441.5639, 'eps':     0.3792, 'len': 37457.1150, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  588000, Reward:   427.479 [ 135.081], Avg:   -85.644 (0.379) <0-16:33:09> ({'r_t':   567.3690, 'eps':     0.3792, 'len': 37489.6780, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  589000, Reward:   443.326 [ 109.002], Avg:   -84.748 (0.379) <0-16:34:45> ({'r_t':   394.0525, 'eps':     0.3792, 'len': 37523.5580, 'lr':   5.00e-05, 'eps_e':     0.3792, 'lr_e':   5.00e-05})
Step:  590000, Reward:   449.743 [  40.737], Avg:   -83.843 (0.372) <0-16:43:49> ({'r_t':   445.9518, 'eps':     0.3716, 'len': 37555.6140, 'dyn_loss':    13.9871, 'dot_loss':     1.7771, 'ddot_loss':     4.2379, 'rew_loss':   170.7227, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  591000, Reward:   364.894 [ 352.277], Avg:   -83.085 (0.372) <0-16:45:28> ({'r_t':   444.7143, 'eps':     0.3716, 'len': 37589.3930, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  592000, Reward:   388.997 [ 370.091], Avg:   -82.289 (0.372) <0-16:47:05> ({'r_t':   705.6946, 'eps':     0.3716, 'len': 37621.8960, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  593000, Reward:   480.389 [  26.910], Avg:   -81.342 (0.372) <0-16:48:44> ({'r_t':   465.5484, 'eps':     0.3716, 'len': 37654.0270, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  594000, Reward:   449.182 [ 126.898], Avg:   -80.450 (0.372) <0-16:50:21> ({'r_t':   580.2305, 'eps':     0.3716, 'len': 37687.5270, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  595000, Reward:   490.203 [  23.322], Avg:   -79.493 (0.372) <0-16:51:58> ({'r_t':   596.3884, 'eps':     0.3716, 'len': 37720.4200, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  596000, Reward:   479.183 [  25.647], Avg:   -78.557 (0.372) <0-16:53:36> ({'r_t':   160.4032, 'eps':     0.3716, 'len': 37753.8770, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  597000, Reward:   442.349 [ 148.852], Avg:   -77.686 (0.372) <0-16:55:13> ({'r_t':   679.8139, 'eps':     0.3716, 'len': 37785.9540, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  598000, Reward:   459.030 [  43.788], Avg:   -76.790 (0.372) <0-16:56:50> ({'r_t':   597.8481, 'eps':     0.3716, 'len': 37819.2850, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  599000, Reward:   476.198 [  35.025], Avg:   -75.868 (0.372) <0-16:58:27> ({'r_t':   658.5025, 'eps':     0.3716, 'len': 37852.7590, 'lr':   5.00e-05, 'eps_e':     0.3716, 'lr_e':   5.00e-05})
Step:  600000, Reward:   397.747 [ 180.662], Avg:   -75.080 (0.364) <0-17:07:41> ({'r_t':   523.9855, 'eps':     0.3642, 'len': 37885.1020, 'dyn_loss':    14.0522, 'dot_loss':     1.7796, 'ddot_loss':     4.2568, 'rew_loss':   171.5242, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  601000, Reward:   474.906 [  30.479], Avg:   -74.167 (0.364) <0-17:09:18> ({'r_t':   649.0840, 'eps':     0.3642, 'len': 37917.6040, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  602000, Reward:   492.623 [  19.568], Avg:   -73.227 (0.364) <0-17:10:54> ({'r_t':   691.9834, 'eps':     0.3642, 'len': 37950.2060, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  603000, Reward:   378.685 [ 366.404], Avg:   -72.478 (0.364) <0-17:12:32> ({'r_t':   742.6718, 'eps':     0.3642, 'len': 37982.9860, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  604000, Reward:   484.548 [  30.038], Avg:   -71.558 (0.364) <0-17:14:09> ({'r_t':   509.2650, 'eps':     0.3642, 'len': 38015.2350, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  605000, Reward:   463.829 [  42.636], Avg:   -70.674 (0.364) <0-17:15:46> ({'r_t':   623.7291, 'eps':     0.3642, 'len': 38048.1120, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  606000, Reward:   444.685 [ 109.647], Avg:   -69.825 (0.364) <0-17:17:22> ({'r_t':   649.3975, 'eps':     0.3642, 'len': 38080.2740, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  607000, Reward:   398.912 [ 370.034], Avg:   -69.054 (0.364) <0-17:18:59> ({'r_t':   670.7019, 'eps':     0.3642, 'len': 38113.1630, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  608000, Reward:   453.037 [ 128.192], Avg:   -68.197 (0.364) <0-17:20:36> ({'r_t':   439.9943, 'eps':     0.3642, 'len': 38145.5990, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  609000, Reward:   452.327 [ 133.381], Avg:   -67.344 (0.364) <0-17:22:13> ({'r_t':   391.4985, 'eps':     0.3642, 'len': 38177.9620, 'lr':   5.00e-05, 'eps_e':     0.3642, 'lr_e':   5.00e-05})
Step:  610000, Reward:   376.766 [ 375.681], Avg:   -66.617 (0.357) <0-17:31:37> ({'r_t':   495.9721, 'eps':     0.3569, 'len': 38209.9540, 'dyn_loss':    14.2698, 'dot_loss':     1.7977, 'ddot_loss':     4.2948, 'rew_loss':   192.3380, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  611000, Reward:   343.847 [ 370.375], Avg:   -65.946 (0.357) <0-17:33:14> ({'r_t':   534.3046, 'eps':     0.3569, 'len': 38241.9480, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  612000, Reward:   380.559 [ 357.115], Avg:   -65.218 (0.357) <0-17:34:51> ({'r_t':   429.8942, 'eps':     0.3569, 'len': 38273.8840, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  613000, Reward:   361.902 [ 225.853], Avg:   -64.522 (0.357) <0-17:36:28> ({'r_t':   461.2602, 'eps':     0.3569, 'len': 38305.8280, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  614000, Reward:   361.586 [ 364.845], Avg:   -63.829 (0.357) <0-17:38:06> ({'r_t':   625.4264, 'eps':     0.3569, 'len': 38337.7640, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  615000, Reward:   463.414 [  28.534], Avg:   -62.973 (0.357) <0-17:39:43> ({'r_t':   627.2242, 'eps':     0.3569, 'len': 38369.7080, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  616000, Reward:   398.317 [ 190.952], Avg:   -62.226 (0.357) <0-17:41:20> ({'r_t':   529.0805, 'eps':     0.3569, 'len': 38401.9130, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  617000, Reward:   467.150 [  29.541], Avg:   -61.369 (0.357) <0-17:42:57> ({'r_t':   693.0829, 'eps':     0.3569, 'len': 38433.8740, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  618000, Reward:   427.083 [ 151.991], Avg:   -60.580 (0.357) <0-17:44:33> ({'r_t':   681.7168, 'eps':     0.3569, 'len': 38465.8110, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  619000, Reward:   372.610 [ 357.487], Avg:   -59.881 (0.357) <0-17:46:10> ({'r_t':   383.5600, 'eps':     0.3569, 'len': 38498.6840, 'lr':   5.00e-05, 'eps_e':     0.3569, 'lr_e':   5.00e-05})
Step:  620000, Reward:   482.414 [  15.212], Avg:   -59.008 (0.350) <0-17:55:27> ({'r_t':   581.9405, 'eps':     0.3497, 'len': 38530.7580, 'dyn_loss':    14.3297, 'dot_loss':     1.8071, 'ddot_loss':     4.3225, 'rew_loss':   154.4908, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  621000, Reward:   494.273 [  16.771], Avg:   -58.118 (0.350) <0-17:57:06> ({'r_t':   524.0652, 'eps':     0.3497, 'len': 38563.2900, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  622000, Reward:   474.965 [  28.497], Avg:   -57.263 (0.350) <0-17:58:45> ({'r_t':   688.1825, 'eps':     0.3497, 'len': 38595.8580, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  623000, Reward:   488.253 [  24.645], Avg:   -56.389 (0.350) <0-18:00:24> ({'r_t':   497.1224, 'eps':     0.3497, 'len': 38627.8980, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  624000, Reward:   488.093 [  18.523], Avg:   -55.517 (0.350) <0-18:02:03> ({'r_t':   815.4387, 'eps':     0.3497, 'len': 38659.8580, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  625000, Reward:   417.485 [ 167.656], Avg:   -54.762 (0.350) <0-18:03:44> ({'r_t':   573.8505, 'eps':     0.3497, 'len': 38692.0300, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  626000, Reward:   469.676 [  69.413], Avg:   -53.925 (0.350) <0-18:05:24> ({'r_t':   596.1296, 'eps':     0.3497, 'len': 38724.0460, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  627000, Reward:   362.355 [ 380.881], Avg:   -53.263 (0.350) <0-18:07:03> ({'r_t':   614.1223, 'eps':     0.3497, 'len': 38756.0010, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  628000, Reward:   486.676 [  19.270], Avg:   -52.404 (0.350) <0-18:08:43> ({'r_t':   545.8723, 'eps':     0.3497, 'len': 38788.5980, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  629000, Reward:   491.671 [  19.904], Avg:   -51.541 (0.350) <0-18:10:22> ({'r_t':   476.1710, 'eps':     0.3497, 'len': 38821.2720, 'lr':   5.00e-05, 'eps_e':     0.3497, 'lr_e':   5.00e-05})
Step:  630000, Reward:   495.397 [  18.123], Avg:   -50.674 (0.343) <0-18:19:57> ({'r_t':   697.1129, 'eps':     0.3428, 'len': 38853.2080, 'dyn_loss':    14.3096, 'dot_loss':     1.7901, 'ddot_loss':     4.2889, 'rew_loss':   165.0085, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  631000, Reward:   298.847 [ 500.850], Avg:   -50.121 (0.343) <0-18:21:36> ({'r_t':   769.3393, 'eps':     0.3428, 'len': 38886.2230, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  632000, Reward:   489.895 [  24.825], Avg:   -49.268 (0.343) <0-18:23:14> ({'r_t':   748.6612, 'eps':     0.3428, 'len': 38919.4570, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  633000, Reward:   451.059 [ 150.496], Avg:   -48.478 (0.343) <0-18:24:53> ({'r_t':   710.1331, 'eps':     0.3428, 'len': 38952.3250, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  634000, Reward:   463.204 [  64.064], Avg:   -47.673 (0.343) <0-18:26:32> ({'r_t':   767.3890, 'eps':     0.3428, 'len': 38984.3130, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  635000, Reward:   392.058 [ 357.744], Avg:   -46.981 (0.343) <0-18:28:11> ({'r_t':   745.3670, 'eps':     0.3428, 'len': 39016.5630, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  636000, Reward:   490.232 [  14.025], Avg:   -46.138 (0.343) <0-18:29:49> ({'r_t':   645.3024, 'eps':     0.3428, 'len': 39049.7690, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  637000, Reward:   395.017 [ 371.293], Avg:   -45.446 (0.343) <0-18:31:28> ({'r_t':   618.0633, 'eps':     0.3428, 'len': 39082.2280, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  638000, Reward:   485.613 [  36.065], Avg:   -44.615 (0.343) <0-18:33:07> ({'r_t':   727.0011, 'eps':     0.3428, 'len': 39114.5760, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  639000, Reward:   479.879 [  18.232], Avg:   -43.796 (0.343) <0-18:34:45> ({'r_t':   432.1703, 'eps':     0.3428, 'len': 39147.6000, 'lr':   5.00e-05, 'eps_e':     0.3428, 'lr_e':   5.00e-05})
Step:  640000, Reward:   479.837 [  49.417], Avg:   -42.979 (0.336) <0-18:44:32> ({'r_t':   626.3375, 'eps':     0.3359, 'len': 39179.5380, 'dyn_loss':    14.1936, 'dot_loss':     1.7892, 'ddot_loss':     4.2853, 'rew_loss':   177.9382, 'lr':   5.00e-05, 'eps_e':     0.3359, 'lr_e':   5.00e-05})
Step:  641000, Reward:   461.058 [ 120.760], Avg:   -42.194 (0.336) <0-18:46:12> ({'r_t':   685.8352, 'eps':     0.3359, 'len': 39211.7180, 'lr':   5.00e-05, 'eps_e':     0.3359, 'lr_e':   5.00e-05})
