Model: <class 'src.models.pytorch.mpc.mppi.MPPIAgent'>, Env: CartPole-v0, Date: 02/06/2020 22:34:12
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: 7ab06f66e0334f41638191fec0a495ee88f27799
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.98
   NUM_STEPS = 20
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 5000
   TARGET_UPDATE_RATE = 0.0004
   BATCH_SIZE = 100
   DYN_EPOCHS = 10
   TRAIN_EVERY = 1000
   ENV_MODEL = dfrntl
   MPC = 
      NSAMPLES = 100
      HORIZON = 20
      LAMBDA = 0.5
      CONTROL_FREQ = 1
   dynamics_size = 4
   state_size = (4,)
   action_size = [2]
   env_name = CartPole-v0
   rank = 0
   size = 17
   split = 17
   model = mppi
   framework = pt
   train_prop = 1.0
   tcp_ports = [9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 200000
   render = False
   trial = False
   icm = False
   rs = False
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.001
      TRANSITION_HIDDEN = 256
      REWARD_HIDDEN = 256
      BETA_DYN = 0.1
      BETA_DOT = 1
      BETA_DDOT = 1,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7ff4842e0e90> 
	env = <GymEnv<TimeLimit<CartPoleEnv<CartPole-v0>>>> 
		env = <TimeLimit<CartPoleEnv<CartPole-v0>>> 
			env = <CartPoleEnv<CartPole-v0>> 
				gravity = 9.8
				masscart = 1.0
				masspole = 0.1
				total_mass = 1.1
				length = 0.5
				polemass_length = 0.05
				force_mag = 10.0
				tau = 0.02
				kinematics_integrator = euler
				theta_threshold_radians = 0.20943951023931953
				x_threshold = 2.4
				action_space = Discrete(2) 
					n = 2
					shape = ()
					dtype = int64
					np_random = RandomState(MT19937)
				observation_space = Box(4,) 
					dtype = float32
					shape = (4,)
					low = [-4.800e+00 -3.403e+38 -4.189e-01 -3.403e+38]
					high = [ 4.800e+00  3.403e+38  4.189e-01  3.403e+38]
					bounded_below = [ True  True  True  True]
					bounded_above = [ True  True  True  True]
					np_random = RandomState(MT19937)
				np_random = RandomState(MT19937)
				viewer = None
				state = None
				steps_beyond_done = None
				spec = EnvSpec(CartPole-v0) 
					id = CartPole-v0
					entry_point = gym.envs.classic_control:CartPoleEnv
					reward_threshold = 195.0
					nondeterministic = False
					max_episode_steps = 200
				verbose = 0
			action_space = Discrete(2) 
				n = 2
				shape = ()
				dtype = int64
				np_random = RandomState(MT19937)
			observation_space = Box(4,) 
				dtype = float32
				shape = (4,)
				low = [-4.800e+00 -3.403e+38 -4.189e-01 -3.403e+38]
				high = [ 4.800e+00  3.403e+38  4.189e-01  3.403e+38]
				bounded_below = [ True  True  True  True]
				bounded_above = [ True  True  True  True]
				np_random = RandomState(MT19937)
			reward_range = (-inf, inf)
			metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}
		action_space = Discrete(2) 
			n = 2
			shape = ()
			dtype = int64
			np_random = RandomState(MT19937)
		observation_space = Box(4,) 
			dtype = float32
			shape = (4,)
			low = [-4.800e+00 -3.403e+38 -4.189e-01 -3.403e+38]
			high = [ 4.800e+00  3.403e+38  4.189e-01  3.403e+38]
			bounded_below = [ True  True  True  True]
			bounded_above = [ True  True  True  True]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7ff484188c90> 
			observation_space = Box(4,) 
				dtype = float32
				shape = (4,)
				low = [-4.800e+00 -3.403e+38 -4.189e-01 -3.403e+38]
				high = [ 4.800e+00  3.403e+38  4.189e-01  3.403e+38]
				bounded_below = [ True  True  True  True]
				bounded_above = [ True  True  True  True]
				np_random = RandomState(MT19937)
	state_size = (4,)
	action_size = [2]
	action_space = Discrete(2) 
		n = 2
		shape = ()
		dtype = int64
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7ff484188d50> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {9001: <socket.socket fd=34, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 52512), raddr=('127.0.0.1', 9001)>, 9002: <socket.socket fd=35, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 47696), raddr=('127.0.0.1', 9002)>, 9003: <socket.socket fd=43, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 39184), raddr=('127.0.0.1', 9003)>, 9004: <socket.socket fd=45, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 50908), raddr=('127.0.0.1', 9004)>, 9005: <socket.socket fd=46, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46332), raddr=('127.0.0.1', 9005)>, 9006: <socket.socket fd=55, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49082), raddr=('127.0.0.1', 9006)>, 9007: <socket.socket fd=57, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 39400), raddr=('127.0.0.1', 9007)>, 9008: <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42410), raddr=('127.0.0.1', 9008)>, 9009: <socket.socket fd=75, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 56056), raddr=('127.0.0.1', 9009)>, 9010: <socket.socket fd=76, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 58870), raddr=('127.0.0.1', 9010)>, 9011: <socket.socket fd=84, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42824), raddr=('127.0.0.1', 9011)>, 9012: <socket.socket fd=86, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 57184), raddr=('127.0.0.1', 9012)>, 9013: <socket.socket fd=88, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59254), raddr=('127.0.0.1', 9013)>, 9014: <socket.socket fd=89, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 34094), raddr=('127.0.0.1', 9014)>, 9015: <socket.socket fd=91, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 49406), raddr=('127.0.0.1', 9015)>, 9016: <socket.socket fd=92, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48232), raddr=('127.0.0.1', 9016)>}
	num_envs = 16
	max_steps = 200,
agent: <src.models.wrappers.ParallelAgent object at 0x7ff48418d250> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7ff48418d2d0> 
		state_size = (4,)
	agent = <src.models.pytorch.mpc.mppi.MPPIAgent object at 0x7ff48418d8d0> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7ff48419ef50> 
			size = [2]
			dt = 0.2
			action = [ 0.063 -1.000]
			daction_dt = [-0.167 -1.474]
		discrete = True
		action_size = [2]
		state_size = (4,)
		config = <src.utils.config.Config object at 0x7ff48c0ddf90> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.98
			NUM_STEPS = 20
			MAX_BUFFER_SIZE = 100000
			REPLAY_BATCH_SIZE = 5000
			TARGET_UPDATE_RATE = 0.0004
			BATCH_SIZE = 100
			DYN_EPOCHS = 10
			TRAIN_EVERY = 1000
			ENV_MODEL = dfrntl
			MPC = <src.utils.config.Config object at 0x7ff52e2c2890> 
				NSAMPLES = 100
				HORIZON = 20
				LAMBDA = 0.5
				CONTROL_FREQ = 1
			dynamics_size = 4
			state_size = (4,)
			action_size = [2]
			env_name = CartPole-v0
			rank = 0
			size = 17
			split = 17
			model = mppi
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 200000
			render = False
			trial = False
			icm = False
			rs = False
			DYN = <src.utils.config.Config object at 0x7ff48c0ced10> 
				REG_LAMBDA = 1e-06
				FACTOR = 0.5
				PATIENCE = 5
				LEARN_RATE = 0.001
				TRANSITION_HIDDEN = 256
				REWARD_HIDDEN = 256
				BETA_DYN = 0.1
				BETA_DOT = 1
				BETA_DDOT = 1
		stats = <src.utils.logger.Stats object at 0x7ff4841367d0> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = MPPIController() 
			training = True
			tau = 0.0004
			name = mppi
			stats = <src.utils.logger.Stats object at 0x7ff484136850> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7ff48c0ddf90> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.98
				NUM_STEPS = 20
				MAX_BUFFER_SIZE = 100000
				REPLAY_BATCH_SIZE = 5000
				TARGET_UPDATE_RATE = 0.0004
				BATCH_SIZE = 100
				DYN_EPOCHS = 10
				TRAIN_EVERY = 1000
				ENV_MODEL = dfrntl
				MPC = <src.utils.config.Config object at 0x7ff52e2c2890> 
					NSAMPLES = 100
					HORIZON = 20
					LAMBDA = 0.5
					CONTROL_FREQ = 1
				dynamics_size = 4
				state_size = (4,)
				action_size = [2]
				env_name = CartPole-v0
				rank = 0
				size = 17
				split = 17
				model = mppi
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 200000
				render = False
				trial = False
				icm = False
				rs = False
				DYN = <src.utils.config.Config object at 0x7ff48c0ced10> 
					REG_LAMBDA = 1e-06
					FACTOR = 0.5
					PATIENCE = 5
					LEARN_RATE = 0.001
					TRANSITION_HIDDEN = 256
					REWARD_HIDDEN = 256
					BETA_DYN = 0.1
					BETA_DOT = 1
					BETA_DDOT = 1
			device = cuda
			envmodel = <src.models.pytorch.mpc.EnvModel object at 0x7ff484136890> 
				network = DifferentialEnv(
					  (reward): RewardModel(
					    (linear1): Linear(in_features=10, out_features=256, bias=True)
					    (linear2): Linear(in_features=256, out_features=256, bias=True)
					    (linear3): Linear(in_features=256, out_features=1, bias=True)
					  )
					  (dynamics): TransitionModel(
					    (gru): GRUCell(10, 256)
					    (linear1): Linear(in_features=256, out_features=256, bias=True)
					    (linear2): Linear(in_features=256, out_features=256, bias=True)
					    (state_ddot): Linear(in_features=256, out_features=4, bias=True)
					  )
					) 
					training = True
					tau = 0.0004
					name = dfrntl
					stats = <src.utils.logger.Stats object at 0x7ff484136910> 
						mean_dict = {}
						sum_dict = {}
					config = <src.utils.config.Config object at 0x7ff48c0ddf90> 
						TRIAL_AT = 1000
						SAVE_AT = 1
						SEED = 0
						REG_LAMBDA = 1e-06
						LEARN_RATE = 0.0001
						DISCOUNT_RATE = 0.99
						ADVANTAGE_DECAY = 0.95
						INPUT_LAYER = 512
						ACTOR_HIDDEN = 256
						CRITIC_HIDDEN = 1024
						EPS_MAX = 1.0
						EPS_MIN = 0.1
						EPS_DECAY = 0.98
						NUM_STEPS = 20
						MAX_BUFFER_SIZE = 100000
						REPLAY_BATCH_SIZE = 5000
						TARGET_UPDATE_RATE = 0.0004
						BATCH_SIZE = 100
						DYN_EPOCHS = 10
						TRAIN_EVERY = 1000
						ENV_MODEL = dfrntl
						MPC = <src.utils.config.Config object at 0x7ff52e2c2890> 
							NSAMPLES = 100
							HORIZON = 20
							LAMBDA = 0.5
							CONTROL_FREQ = 1
						dynamics_size = 4
						state_size = (4,)
						action_size = [2]
						env_name = CartPole-v0
						rank = 0
						size = 17
						split = 17
						model = mppi
						framework = pt
						train_prop = 1.0
						tcp_ports = <list len=17>
						tcp_rank = 0
						num_envs = 1
						nsteps = 200000
						render = False
						trial = False
						icm = False
						rs = False
						DYN = <src.utils.config.Config object at 0x7ff48c0ced10> 
							REG_LAMBDA = 1e-06
							FACTOR = 0.5
							PATIENCE = 5
							LEARN_RATE = 0.001
							TRANSITION_HIDDEN = 256
							REWARD_HIDDEN = 256
							BETA_DYN = 0.1
							BETA_DOT = 1
							BETA_DDOT = 1
					device = cuda
					state_size = (4,)
					action_size = [2]
					discrete = True
					dyn_index = 4
					optimizer = Adam (
					Parameter Group 0
					    amsgrad: False
					    betas: (0.9, 0.999)
					    eps: 1e-08
					    lr: 0.001
					    weight_decay: 1e-06
					)
					scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff484136e90>
				state_size = (4,)
				action_size = [2]
			mu = [ 0.000  0.000]
			cov = [[ 0.500  0.000]
			 [ 0.000  0.500]]
			icov = [[ 2.000  0.000]
			 [ 0.000  2.000]]
			lamda = 0.5
			horizon = 20
			nsamples = 100
			action_size = [2]
			control = [[[ 0.038  0.737]
			  [-0.119 -0.499]
			  [-0.308 -0.158]
			  [ 0.727 -0.542]
			  [ 0.687 -0.408]
			  [-0.821  0.954]
			  [-0.978 -0.059]
			  [-0.202  0.969]
			  [-0.952 -0.342]
			  [-0.807 -0.088]
			  [-0.717  0.317]
			  [-0.305 -0.984]
			  [-0.719 -0.585]
			  [-0.752 -1.000]
			  [-0.280  0.975]
			  [-0.925 -0.854]
			  [-0.784  0.894]
			  [ 0.337  0.857]
			  [-0.620  0.779]
			  [-0.301 -0.326]]]
			noise = [[[[-0.835 -0.387]
			   [ 0.707 -0.150]
			   [-0.338  0.731]
			   ...
			   [ 0.913  0.175]
			   [ 0.166 -1.773]
			   [-0.055  0.167]]
			
			  [[ 0.209  0.762]
			   [ 1.004 -0.349]
			   [-0.472 -0.436]
			   ...
			   [-0.526 -0.423]
			   [ 0.969 -0.598]
			   [-1.231 -0.092]]
			
			  [[ 1.695  0.234]
			   [ 0.344  1.424]
			   [-0.480  0.589]
			   ...
			   [ 0.149  0.864]
			   [ 0.925 -1.040]
			   [ 0.098  0.061]]
			
			  ...
			
			  [[-1.009 -0.451]
			   [-1.862 -0.307]
			   [-0.044  0.205]
			   ...
			   [ 1.102 -0.235]
			   [ 0.541  0.713]
			   [-0.827  1.473]]
			
			  [[-0.248  0.207]
			   [ 0.859 -0.878]
			   [-0.269 -0.512]
			   ...
			   [ 0.864 -0.822]
			   [ 0.619  0.628]
			   [-0.337  1.180]]
			
			  [[-0.097 -1.311]
			   [-0.628  0.157]
			   [-0.772  1.036]
			   ...
			   [-1.086  0.451]
			   [ 0.234  1.409]
			   [-0.983  0.045]]]]
			init_cost = [[-10.202  -6.410   3.122  -1.170  10.541  -7.895  -7.163   7.674   3.849   9.411  -6.045  10.653   2.460  -7.102   1.856   0.676  14.922  12.731 -10.479  -3.308   7.959   5.010  -1.502   1.082   1.614   1.708  -2.833   1.355   7.966  -1.979  -7.148 -10.389  16.684   1.637  -7.907   5.347   5.817  -1.230  -6.193  -0.838   3.032  -1.520   4.493 -13.428 -10.247   6.123  -2.099   1.358  -0.702   6.091   6.890  -6.796   7.209  -3.197   3.030  -1.887   2.080   4.610 -10.981   4.317  -3.563   7.258 -11.477   4.820  -6.219   5.929   9.681   5.633   1.660   5.107  -1.376  -0.052   2.971  -0.909  -1.341   5.080   5.431  -2.033  -6.655   7.512   4.181  -1.329   1.972 -21.358   1.083   4.934  -7.022  -7.076  -0.523  -3.537   2.320  -1.319   3.628   0.339   2.174   4.747 -10.205  -4.019  -3.649  -5.067]]
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7ff484136f10> 
			buffer = deque([], maxlen=100000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7ff484136f50> 
		size = [2]
		dt = 0.2
		action = [-1.000  0.920]
		daction_dt = [-0.578  1.981]
	discrete = True
	action_size = [2]
	state_size = (4,)
	config = <src.utils.config.Config object at 0x7ff48c0ddf90> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.98
		NUM_STEPS = 20
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 5000
		TARGET_UPDATE_RATE = 0.0004
		BATCH_SIZE = 100
		DYN_EPOCHS = 10
		TRAIN_EVERY = 1000
		ENV_MODEL = dfrntl
		MPC = <src.utils.config.Config object at 0x7ff52e2c2890> 
			NSAMPLES = 100
			HORIZON = 20
			LAMBDA = 0.5
			CONTROL_FREQ = 1
		dynamics_size = 4
		state_size = (4,)
		action_size = [2]
		env_name = CartPole-v0
		rank = 0
		size = 17
		split = 17
		model = mppi
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 200000
		render = False
		trial = False
		icm = False
		rs = False
		DYN = <src.utils.config.Config object at 0x7ff48c0ced10> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.001
			TRANSITION_HIDDEN = 256
			REWARD_HIDDEN = 256
			BETA_DYN = 0.1
			BETA_DOT = 1
			BETA_DDOT = 1
	stats = <src.utils.logger.Stats object at 0x7ff4841480d0> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import tqdm
import torch
import random
import numpy as np
import scipy as sp
from scipy.stats import multivariate_normal
from src.utils.rand import RandomAgent, ReplayBuffer
from ..agents.base import PTNetwork, PTAgent, Conv, one_hot_from_indices
from . import EnvModel

class MPPIController(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="mppi"):
		super().__init__(config, gpu=gpu, name=name)
		self.envmodel = EnvModel(state_size, action_size, config, load=load, gpu=gpu)
		self.mu = np.zeros(action_size)
		self.cov = np.diag(np.ones(action_size))*0.5
		self.icov = np.linalg.inv(self.cov)
		self.lamda = config.MPC.LAMBDA
		self.horizon = config.MPC.HORIZON
		self.nsamples = config.MPC.NSAMPLES
		self.config = config
		self.action_size = action_size
		self.init_control()

	def get_action(self, state, eps=None, sample=True):
		batch = state.shape[:-1]
		if len(batch) and self.control.shape[0] != batch[0]: self.init_control(batch[0])
		x = torch.Tensor(state).view(*batch, 1,-1).repeat_interleave(self.nsamples, -2)
		controls = np.clip(self.control[:,None,:,:] + self.noise, -1, 1)
		self.states, rewards = self.envmodel.rollout(controls, x, numpy=True)
		costs = -np.sum(rewards, -1) #+ self.lamda * np.copy(self.init_cost)
		beta = np.min(costs, -1, keepdims=True)
		costs_norm = -(costs - beta)/self.lamda
		weights = sp.special.softmax(costs_norm)
		self.control += np.sum(weights[:,:,None,None]*self.noise, len(batch))
		action = self.control[...,0,:]
		self.control = np.roll(self.control, -1, axis=-2)
		self.control[...,-1,:] = 0
		return action

	def init_control(self, batch_size=1):
		self.control = np.random.uniform(-1, 1, size=[batch_size, self.horizon, *self.action_size])
		self.noise = np.random.multivariate_normal(self.mu, self.cov, size=[batch_size, self.nsamples, self.horizon])
		self.init_cost = np.sum(self.control[:,None,:,None,:] @ self.icov[None,None,None,:,:] @ self.noise[:,:,:,:,None], axis=(2,3,4))

	def optimize(self, states, actions, next_states, rewards, dones):
		return self.envmodel.optimize(states, actions, next_states, rewards, dones)

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.save_model(dirname, name, net)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.load_model(dirname, name, net)

	def get_stats(self):
		return {**super().get_stats(), **self.envmodel.get_stats()}

class MPPIAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, MPPIController, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		eps = self.eps if eps is None else eps
		if random.random() < eps: return super().get_action(state)
		action = self.network.get_action(np.array(state))
		return np.clip(action, -1, 1)

	def partition(self, x):
		num_splits = x.shape[0]//self.config.NUM_STEPS
		if num_splits == 0:
			arr = np.zeros([self.config.NUM_STEPS, *x.shape[1:]])
			arr[-x.shape[0]:] = x
			num_splits = 1
			x = arr
		arr = x[-num_splits*self.config.NUM_STEPS:].reshape(num_splits, self.config.NUM_STEPS, *x.shape[1:])
		return arr

	def train(self, state, action, next_state, reward, done):
		self.time = getattr(self, "time", 0) + 1
		if not hasattr(self, "buffers"): self.buffers = [[] for _ in done]
		for buffer, s, a, ns, r, d in zip(self.buffers, state, action, next_state, reward, done):
			buffer.append((s, a, ns, r, d))
			if not d: continue
			states, actions, next_states, rewards, dones = map(np.array, zip(*buffer))
			states, actions, next_states, rewards, dones = [self.partition(x) for x in (states, actions, next_states, rewards, dones)]
			buffer.clear()
			self.replay_buffer.extend(list(zip(states, actions, next_states, rewards, dones)), shuffle=False)
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE and self.time % self.config.TRAIN_EVERY == 0:
			pbar = tqdm.trange(self.config.DYN_EPOCHS*self.config.REPLAY_BATCH_SIZE//self.config.BATCH_SIZE)
			for _ in pbar:
				transform = lambda x: self.to_tensor(x).transpose(0,1)
				states, actions, next_states, rewards, dones = self.replay_buffer.next_batch(self.config.BATCH_SIZE, dtype=transform)[0]
				loss = self.network.optimize(states, actions, next_states, rewards, dones)
				pbar.set_postfix_str(f"Loss: {loss:.4f}")
			self.eps = max(self.eps * self.config.EPS_DECAY, self.config.EPS_MIN)


Step:       0, Reward:    13.625 [   5.372], Avg:    13.625 (1.000) <0-00:00:00> ({'r_t':     1.0000, 'eps':     1.0000, 'lr':     0.0010, 'eps_e':     1.0000, 'lr_e':     0.0010})
Step:    1000, Reward:    16.062 [   7.901], Avg:    14.844 (1.000) <0-00:00:01> ({'r_t':  1000.0000, 'eps':     1.0000, 'lr':     0.0010, 'eps_e':     1.0000, 'lr_e':     0.0010})
Step:    2000, Reward:    12.688 [   4.496], Avg:    14.125 (1.000) <0-00:00:01> ({'r_t':  1000.0000, 'eps':     1.0000, 'lr':     0.0010, 'eps_e':     1.0000, 'lr_e':     0.0010})
Step:    3000, Reward:    13.000 [   7.874], Avg:    13.844 (1.000) <0-00:00:02> ({'r_t':  1000.0000, 'eps':     1.0000, 'lr':     0.0010, 'eps_e':     1.0000, 'lr_e':     0.0010})
Step:    4000, Reward:    15.438 [  11.784], Avg:    14.162 (1.000) <0-00:00:02> ({'r_t':  1000.0000, 'eps':     1.0000, 'lr':     0.0010, 'eps_e':     1.0000, 'lr_e':     0.0010})
Step:    5000, Reward:    11.000 [   3.623], Avg:    13.635 (0.980) <0-00:01:08> ({'r_t':  1000.0000, 'eps':     0.9800, 'dyn_loss': 522904.1562, 'dot_loss':   486.5580, 'ddot_loss':     8.5292, 'rew_loss':     0.1793, 'lr':     0.0010, 'eps_e':     0.9800, 'lr_e':     0.0010})
Step:    6000, Reward:    15.125 [   8.652], Avg:    13.848 (0.960) <0-00:02:15> ({'r_t':  1000.0000, 'eps':     0.9604, 'dyn_loss':   848.7314, 'dot_loss':    13.6391, 'ddot_loss':     3.3177, 'rew_loss':     0.0207, 'lr':     0.0010, 'eps_e':     0.9604, 'lr_e':     0.0010})
Step:    7000, Reward:    13.562 [   4.690], Avg:    13.812 (0.941) <0-00:03:22> ({'r_t':  1000.0000, 'eps':     0.9412, 'dyn_loss':   411.5663, 'dot_loss':     6.9452, 'ddot_loss':     2.1368, 'rew_loss':     0.0191, 'lr':     0.0010, 'eps_e':     0.9412, 'lr_e':     0.0010})
Step:    8000, Reward:    14.438 [   7.937], Avg:    13.882 (0.922) <0-00:04:31> ({'r_t':  1000.0000, 'eps':     0.9224, 'dyn_loss':   245.6820, 'dot_loss':     4.5676, 'ddot_loss':     1.7579, 'rew_loss':     0.0189, 'lr':     0.0010, 'eps_e':     0.9224, 'lr_e':     0.0010})
Step:    9000, Reward:    17.000 [   8.761], Avg:    14.194 (0.904) <0-00:05:39> ({'r_t':  1000.0000, 'eps':     0.9039, 'dyn_loss':   161.7049, 'dot_loss':     3.4310, 'ddot_loss':     1.5600, 'rew_loss':     0.0186, 'lr':     0.0010, 'eps_e':     0.9039, 'lr_e':     0.0010})
Step:   10000, Reward:    17.688 [  11.049], Avg:    14.511 (0.886) <0-00:06:48> ({'r_t':  1000.0000, 'eps':     0.8858, 'dyn_loss':   109.7428, 'dot_loss':     2.5661, 'ddot_loss':     1.3984, 'rew_loss':     0.0181, 'lr':     0.0010, 'eps_e':     0.8858, 'lr_e':     0.0010})
Step:   11000, Reward:    16.000 [  10.223], Avg:    14.635 (0.868) <0-00:07:57> ({'r_t':  1000.0000, 'eps':     0.8681, 'dyn_loss':    77.5830, 'dot_loss':     2.0032, 'ddot_loss':     1.2779, 'rew_loss':     0.0181, 'lr':     0.0010, 'eps_e':     0.8681, 'lr_e':     0.0010})
Step:   12000, Reward:    13.688 [   5.300], Avg:    14.562 (0.851) <0-00:09:05> ({'r_t':  1000.0000, 'eps':     0.8508, 'dyn_loss':    54.3640, 'dot_loss':     1.6170, 'ddot_loss':     1.1855, 'rew_loss':     0.0178, 'lr':     0.0010, 'eps_e':     0.8508, 'lr_e':     0.0010})
Step:   13000, Reward:    12.188 [   4.216], Avg:    14.393 (0.834) <0-00:10:15> ({'r_t':  1000.0000, 'eps':     0.8337, 'dyn_loss':    37.8078, 'dot_loss':     1.2963, 'ddot_loss':     1.1080, 'rew_loss':     0.0179, 'lr':     0.0010, 'eps_e':     0.8337, 'lr_e':     0.0010})
Step:   14000, Reward:    15.438 [   8.853], Avg:    14.463 (0.817) <0-00:11:24> ({'r_t':  1000.0000, 'eps':     0.8171, 'dyn_loss':    27.6353, 'dot_loss':     1.0828, 'ddot_loss':     1.0486, 'rew_loss':     0.0179, 'lr':     0.0010, 'eps_e':     0.8171, 'lr_e':     0.0010})
Step:   15000, Reward:    15.250 [   9.284], Avg:    14.512 (0.801) <0-00:12:34> ({'r_t':  1000.0000, 'eps':     0.8007, 'dyn_loss':    21.5743, 'dot_loss':     0.9459, 'ddot_loss':     1.0106, 'rew_loss':     0.0177, 'lr':     0.0010, 'eps_e':     0.8007, 'lr_e':     0.0010})
Step:   16000, Reward:    15.500 [   5.895], Avg:    14.570 (0.785) <0-00:13:44> ({'r_t':  1000.0000, 'eps':     0.7847, 'dyn_loss':    17.0207, 'dot_loss':     0.8385, 'ddot_loss':     0.9811, 'rew_loss':     0.0176, 'lr':     0.0010, 'eps_e':     0.7847, 'lr_e':     0.0010})
Step:   17000, Reward:    10.312 [   1.758], Avg:    14.333 (0.769) <0-00:14:55> ({'r_t':  1000.0000, 'eps':     0.7690, 'dyn_loss':    13.6722, 'dot_loss':     0.7633, 'ddot_loss':     0.9614, 'rew_loss':     0.0175, 'lr':     0.0010, 'eps_e':     0.7690, 'lr_e':     0.0010})
Step:   18000, Reward:    13.938 [   4.918], Avg:    14.312 (0.754) <0-00:16:06> ({'r_t':  1000.0000, 'eps':     0.7536, 'dyn_loss':    11.1596, 'dot_loss':     0.7035, 'ddot_loss':     0.9355, 'rew_loss':     0.0172, 'lr':     0.0010, 'eps_e':     0.7536, 'lr_e':     0.0010})
Step:   19000, Reward:    12.438 [   4.458], Avg:    14.219 (0.739) <0-00:17:17> ({'r_t':  1000.0000, 'eps':     0.7386, 'dyn_loss':     9.3314, 'dot_loss':     0.6604, 'ddot_loss':     0.9188, 'rew_loss':     0.0168, 'lr':     0.0010, 'eps_e':     0.7386, 'lr_e':     0.0010})
Step:   20000, Reward:    14.812 [   8.932], Avg:    14.247 (0.724) <0-00:18:29> ({'r_t':  1000.0000, 'eps':     0.7238, 'dyn_loss':     7.9845, 'dot_loss':     0.6319, 'ddot_loss':     0.9137, 'rew_loss':     0.0169, 'lr':     0.0010, 'eps_e':     0.7238, 'lr_e':     0.0010})
Step:   21000, Reward:    14.062 [   5.595], Avg:    14.239 (0.709) <0-00:19:40> ({'r_t':  1000.0000, 'eps':     0.7093, 'dyn_loss':     6.8124, 'dot_loss':     0.5966, 'ddot_loss':     0.8968, 'rew_loss':     0.0167, 'lr':     0.0010, 'eps_e':     0.7093, 'lr_e':     0.0010})
Step:   22000, Reward:    15.312 [   5.997], Avg:    14.285 (0.695) <0-00:20:50> ({'r_t':  1000.0000, 'eps':     0.6951, 'dyn_loss':     5.8361, 'dot_loss':     0.5687, 'ddot_loss':     0.8859, 'rew_loss':     0.0166, 'lr':     0.0010, 'eps_e':     0.6951, 'lr_e':     0.0010})
Step:   23000, Reward:    17.438 [   9.334], Avg:    14.417 (0.681) <0-00:22:05> ({'r_t':  1000.0000, 'eps':     0.6812, 'dyn_loss':     4.9723, 'dot_loss':     0.5419, 'ddot_loss':     0.8740, 'rew_loss':     0.0163, 'lr':     0.0010, 'eps_e':     0.6812, 'lr_e':     0.0010})
Step:   24000, Reward:    11.562 [   1.968], Avg:    14.303 (0.668) <0-00:23:18> ({'r_t':  1000.0000, 'eps':     0.6676, 'dyn_loss':     4.2401, 'dot_loss':     0.5214, 'ddot_loss':     0.8622, 'rew_loss':     0.0159, 'lr':     0.0010, 'eps_e':     0.6676, 'lr_e':     0.0010})
Step:   25000, Reward:    16.438 [   8.093], Avg:    14.385 (0.654) <0-00:24:32> ({'r_t':  1000.0000, 'eps':     0.6543, 'dyn_loss':     3.6581, 'dot_loss':     0.5061, 'ddot_loss':     0.8654, 'rew_loss':     0.0160, 'lr':     0.0010, 'eps_e':     0.6543, 'lr_e':     0.0010})
Step:   26000, Reward:    11.188 [   3.893], Avg:    14.266 (0.641) <0-00:25:42> ({'r_t':  1000.0000, 'eps':     0.6412, 'dyn_loss':     3.1249, 'dot_loss':     0.4912, 'ddot_loss':     0.8573, 'rew_loss':     0.0156, 'lr':     0.0010, 'eps_e':     0.6412, 'lr_e':     0.0010})
Step:   27000, Reward:    14.375 [   4.859], Avg:    14.270 (0.628) <0-00:26:54> ({'r_t':  1000.0000, 'eps':     0.6283, 'dyn_loss':     2.6672, 'dot_loss':     0.4700, 'ddot_loss':     0.8374, 'rew_loss':     0.0149, 'lr':     0.0010, 'eps_e':     0.6283, 'lr_e':     0.0010})
Step:   28000, Reward:    18.188 [  10.418], Avg:    14.405 (0.616) <0-00:28:07> ({'r_t':  1000.0000, 'eps':     0.6158, 'dyn_loss':     2.3832, 'dot_loss':     0.4670, 'ddot_loss':     0.8475, 'rew_loss':     0.0154, 'lr':     0.0010, 'eps_e':     0.6158, 'lr_e':     0.0010})
Step:   29000, Reward:    13.938 [   5.771], Avg:    14.390 (0.603) <0-00:29:19> ({'r_t':  1000.0000, 'eps':     0.6035, 'dyn_loss':     2.0733, 'dot_loss':     0.4516, 'ddot_loss':     0.8270, 'rew_loss':     0.0146, 'lr':     0.0010, 'eps_e':     0.6035, 'lr_e':     0.0010})
Step:   30000, Reward:    14.000 [   5.755], Avg:    14.377 (0.591) <0-00:30:31> ({'r_t':  1000.0000, 'eps':     0.5914, 'dyn_loss':     1.8847, 'dot_loss':     0.4485, 'ddot_loss':     0.8348, 'rew_loss':     0.0152, 'lr':     0.0010, 'eps_e':     0.5914, 'lr_e':     0.0010})
Step:   31000, Reward:    16.562 [   7.348], Avg:    14.445 (0.580) <0-00:31:42> ({'r_t':  1000.0000, 'eps':     0.5796, 'dyn_loss':     1.6642, 'dot_loss':     0.4354, 'ddot_loss':     0.8146, 'rew_loss':     0.0140, 'lr':     0.0010, 'eps_e':     0.5796, 'lr_e':     0.0010})
Step:   32000, Reward:    13.938 [   6.388], Avg:    14.430 (0.568) <0-00:32:53> ({'r_t':  1000.0000, 'eps':     0.5680, 'dyn_loss':     1.5955, 'dot_loss':     0.4399, 'ddot_loss':     0.8373, 'rew_loss':     0.0152, 'lr':     0.0010, 'eps_e':     0.5680, 'lr_e':     0.0010})
Step:   33000, Reward:    15.438 [   8.674], Avg:    14.460 (0.557) <0-00:34:06> ({'r_t':  1000.0000, 'eps':     0.5566, 'dyn_loss':     1.4861, 'dot_loss':     0.4277, 'ddot_loss':     0.8151, 'rew_loss':     0.0144, 'lr':     0.0010, 'eps_e':     0.5566, 'lr_e':     0.0010})
Step:   34000, Reward:    20.500 [   9.899], Avg:    14.632 (0.545) <0-00:35:20> ({'r_t':  1000.0000, 'eps':     0.5455, 'dyn_loss':     1.4309, 'dot_loss':     0.4246, 'ddot_loss':     0.8106, 'rew_loss':     0.0138, 'lr':     0.0010, 'eps_e':     0.5455, 'lr_e':     0.0010})
Step:   35000, Reward:    15.062 [   5.651], Avg:    14.644 (0.535) <0-00:36:34> ({'r_t':  1000.0000, 'eps':     0.5346, 'dyn_loss':     1.4545, 'dot_loss':     0.4309, 'ddot_loss':     0.8317, 'rew_loss':     0.0149, 'lr':     0.0010, 'eps_e':     0.5346, 'lr_e':     0.0010})
Step:   36000, Reward:    19.875 [  10.398], Avg:    14.785 (0.524) <0-00:37:46> ({'r_t':  1000.0000, 'eps':     0.5239, 'dyn_loss':     1.4178, 'dot_loss':     0.4225, 'ddot_loss':     0.8155, 'rew_loss':     0.0145, 'lr':     0.0010, 'eps_e':     0.5239, 'lr_e':     0.0010})
Step:   37000, Reward:    13.438 [   5.326], Avg:    14.750 (0.513) <0-00:38:59> ({'r_t':  1000.0000, 'eps':     0.5134, 'dyn_loss':     1.3981, 'dot_loss':     0.4194, 'ddot_loss':     0.8092, 'rew_loss':     0.0141, 'lr':     0.0010, 'eps_e':     0.5134, 'lr_e':     0.0010})
Step:   38000, Reward:    12.188 [   3.107], Avg:    14.684 (0.503) <0-00:40:11> ({'r_t':  1000.0000, 'eps':     0.5031, 'dyn_loss':     1.3776, 'dot_loss':     0.4187, 'ddot_loss':     0.8014, 'rew_loss':     0.0136, 'lr':     0.0010, 'eps_e':     0.5031, 'lr_e':     0.0010})
Step:   39000, Reward:    16.750 [  10.158], Avg:    14.736 (0.493) <0-00:41:22> ({'r_t':  1000.0000, 'eps':     0.4931, 'dyn_loss':     1.3673, 'dot_loss':     0.4183, 'ddot_loss':     0.7972, 'rew_loss':     0.0133, 'lr':     0.0010, 'eps_e':     0.4931, 'lr_e':     0.0010})
Step:   40000, Reward:    10.812 [   2.377], Avg:    14.640 (0.483) <0-00:42:32> ({'r_t':  1000.0000, 'eps':     0.4832, 'dyn_loss':     1.3606, 'dot_loss':     0.4209, 'ddot_loss':     0.7851, 'rew_loss':     0.0135, 'lr':     0.0010, 'eps_e':     0.4832, 'lr_e':     0.0010})
Step:   41000, Reward:    12.625 [   2.976], Avg:    14.592 (0.474) <0-00:43:45> ({'r_t':  1000.0000, 'eps':     0.4735, 'dyn_loss':     1.3707, 'dot_loss':     0.4218, 'ddot_loss':     0.7971, 'rew_loss':     0.0137, 'lr':     0.0010, 'eps_e':     0.4735, 'lr_e':     0.0010})
Step:   42000, Reward:    10.875 [   3.314], Avg:    14.506 (0.464) <0-00:44:56> ({'r_t':  1000.0000, 'eps':     0.4641, 'dyn_loss':     1.3648, 'dot_loss':     0.4202, 'ddot_loss':     0.7934, 'rew_loss':     0.0133, 'lr':     0.0010, 'eps_e':     0.4641, 'lr_e':     0.0010})
Step:   43000, Reward:    16.562 [   7.890], Avg:    14.553 (0.455) <0-00:46:08> ({'r_t':  1000.0000, 'eps':     0.4548, 'dyn_loss':     1.3572, 'dot_loss':     0.4197, 'ddot_loss':     0.7922, 'rew_loss':     0.0132, 'lr':     0.0010, 'eps_e':     0.4548, 'lr_e':     0.0010})
Step:   44000, Reward:    15.000 [   9.124], Avg:    14.562 (0.446) <0-00:47:20> ({'r_t':  1000.0000, 'eps':     0.4457, 'dyn_loss':     1.3526, 'dot_loss':     0.4189, 'ddot_loss':     0.7881, 'rew_loss':     0.0130, 'lr':     0.0010, 'eps_e':     0.4457, 'lr_e':     0.0010})
Step:   45000, Reward:    10.750 [   2.305], Avg:    14.480 (0.437) <0-00:48:33> ({'r_t':  1000.0000, 'eps':     0.4368, 'dyn_loss':     1.3454, 'dot_loss':     0.4178, 'ddot_loss':     0.7831, 'rew_loss':     0.0130, 'lr':     0.0010, 'eps_e':     0.4368, 'lr_e':     0.0010})
Step:   46000, Reward:    11.562 [   1.935], Avg:    14.418 (0.428) <0-00:49:45> ({'r_t':  1000.0000, 'eps':     0.4281, 'dyn_loss':     1.3348, 'dot_loss':     0.4176, 'ddot_loss':     0.7768, 'rew_loss':     0.0128, 'lr':     0.0010, 'eps_e':     0.4281, 'lr_e':     0.0010})
Step:   47000, Reward:    13.312 [   7.006], Avg:    14.395 (0.419) <0-00:50:57> ({'r_t':  1000.0000, 'eps':     0.4195, 'dyn_loss':     1.3261, 'dot_loss':     0.4171, 'ddot_loss':     0.7727, 'rew_loss':     0.0127, 'lr':     0.0010, 'eps_e':     0.4195, 'lr_e':     0.0010})
Step:   48000, Reward:    15.500 [   7.500], Avg:    14.417 (0.411) <0-00:52:11> ({'r_t':  1000.0000, 'eps':     0.4111, 'dyn_loss':     1.3206, 'dot_loss':     0.4174, 'ddot_loss':     0.7652, 'rew_loss':     0.0126, 'lr':     0.0010, 'eps_e':     0.4111, 'lr_e':     0.0010})
Step:   49000, Reward:    11.625 [   2.342], Avg:    14.361 (0.403) <0-00:53:23> ({'r_t':  1000.0000, 'eps':     0.4029, 'dyn_loss': 5932156.0000, 'dot_loss':  6539.5425, 'ddot_loss':   162.6326, 'rew_loss':     0.2865, 'lr':     0.0010, 'eps_e':     0.4029, 'lr_e':     0.0010})
Step:   50000, Reward:    13.250 [   4.070], Avg:    14.339 (0.395) <0-00:54:35> ({'r_t':  1000.0000, 'eps':     0.3948, 'dyn_loss': 8460089.0000, 'dot_loss': 19265.8594, 'ddot_loss':   491.7081, 'rew_loss':     0.2755, 'lr':     0.0010, 'eps_e':     0.3948, 'lr_e':     0.0010})
Step:   51000, Reward:    17.375 [  13.583], Avg:    14.398 (0.387) <0-00:55:47> ({'r_t':  1000.0000, 'eps':     0.3869, 'dyn_loss': 36592744.0000, 'dot_loss': 33074.3086, 'ddot_loss':   634.9772, 'rew_loss':     0.2357, 'lr':     0.0010, 'eps_e':     0.3869, 'lr_e':     0.0010})
Step:   52000, Reward:    14.062 [   4.351], Avg:    14.392 (0.379) <0-00:57:00> ({'r_t':  1000.0000, 'eps':     0.3792, 'dyn_loss': 16409062.0000, 'dot_loss': 17557.2324, 'ddot_loss':   544.4141, 'rew_loss':     0.2188, 'lr':     0.0010, 'eps_e':     0.3792, 'lr_e':     0.0010})
Step:   53000, Reward:    13.812 [  10.501], Avg:    14.381 (0.372) <0-00:58:12> ({'r_t':  1000.0000, 'eps':     0.3716, 'dyn_loss': 52019492.0000, 'dot_loss': 35223.9844, 'ddot_loss':   566.1326, 'rew_loss':     0.2127, 'lr':     0.0010, 'eps_e':     0.3716, 'lr_e':     0.0010})
Step:   54000, Reward:    12.875 [   3.480], Avg:    14.353 (0.364) <0-00:59:25> ({'r_t':  1000.0000, 'eps':     0.3642, 'dyn_loss': 5189383.0000, 'dot_loss': 10611.3965, 'ddot_loss':   641.4282, 'rew_loss':     0.2108, 'lr':     0.0010, 'eps_e':     0.3642, 'lr_e':     0.0010})
Step:   55000, Reward:    13.000 [   3.500], Avg:    14.329 (0.357) <0-01:00:37> ({'r_t':  1000.0000, 'eps':     0.3569, 'dyn_loss': 2167202.5000, 'dot_loss':  8222.7354, 'ddot_loss':   645.9797, 'rew_loss':     0.2020, 'lr':     0.0010, 'eps_e':     0.3569, 'lr_e':     0.0010})
Step:   56000, Reward:    11.062 [   1.298], Avg:    14.272 (0.350) <0-01:01:50> ({'r_t':  1000.0000, 'eps':     0.3497, 'dyn_loss': 1944438.7500, 'dot_loss':  8238.8320, 'ddot_loss':   695.9943, 'rew_loss':     0.2079, 'lr':     0.0010, 'eps_e':     0.3497, 'lr_e':     0.0010})
Step:   57000, Reward:    20.312 [  15.028], Avg:    14.376 (0.343) <0-01:03:04> ({'r_t':  1000.0000, 'eps':     0.3428, 'dyn_loss': 41850600.0000, 'dot_loss': 31666.0508, 'ddot_loss':   797.4642, 'rew_loss':     0.2251, 'lr':     0.0010, 'eps_e':     0.3428, 'lr_e':     0.0010})
Step:   58000, Reward:    14.562 [   8.888], Avg:    14.379 (0.336) <0-01:04:18> ({'r_t':  1000.0000, 'eps':     0.3359, 'dyn_loss': 4660872.0000, 'dot_loss': 16943.4121, 'ddot_loss':   794.9455, 'rew_loss':     0.2193, 'lr':     0.0010, 'eps_e':     0.3359, 'lr_e':     0.0010})
Step:   59000, Reward:    20.688 [  17.207], Avg:    14.484 (0.329) <0-01:05:32> ({'r_t':  1000.0000, 'eps':     0.3292, 'dyn_loss': 26960510.0000, 'dot_loss': 26799.3086, 'ddot_loss':   840.3838, 'rew_loss':     0.2209, 'lr':     0.0010, 'eps_e':     0.3292, 'lr_e':     0.0010})
Step:   60000, Reward:    15.000 [  10.834], Avg:    14.493 (0.323) <0-01:06:47> ({'r_t':  1000.0000, 'eps':     0.3226, 'dyn_loss': 547639744.0000, 'dot_loss': 409294.5938, 'ddot_loss':  1838.7385, 'rew_loss':     0.2290, 'lr':     0.0010, 'eps_e':     0.3226, 'lr_e':     0.0010})
Step:   61000, Reward:    11.062 [   2.657], Avg:    14.438 (0.316) <0-01:08:02> ({'r_t':  1000.0000, 'eps':     0.3161, 'dyn_loss': 289383232.0000, 'dot_loss': 424660.5938, 'ddot_loss':  3469.0142, 'rew_loss':     0.2315, 'lr':     0.0010, 'eps_e':     0.3161, 'lr_e':     0.0010})
Step:   62000, Reward:    15.438 [   7.786], Avg:    14.453 (0.310) <0-01:09:16> ({'r_t':  1000.0000, 'eps':     0.3098, 'dyn_loss': 287113408.0000, 'dot_loss': 280177.6562, 'ddot_loss':  2673.7625, 'rew_loss':     0.2260, 'lr':     0.0010, 'eps_e':     0.3098, 'lr_e':     0.0010})
Step:   63000, Reward:    10.500 [   2.318], Avg:    14.392 (0.304) <0-01:10:29> ({'r_t':  1000.0000, 'eps':     0.3036, 'dyn_loss': 99062400.0000, 'dot_loss': 115095.0703, 'ddot_loss':  1905.5985, 'rew_loss':     0.2209, 'lr':     0.0010, 'eps_e':     0.3036, 'lr_e':     0.0010})
Step:   64000, Reward:    13.500 [   8.761], Avg:    14.378 (0.298) <0-01:11:42> ({'r_t':  1000.0000, 'eps':     0.2976, 'dyn_loss': 303090432.0000, 'dot_loss': 402471.3750, 'ddot_loss':  2962.2073, 'rew_loss':     0.2269, 'lr':     0.0010, 'eps_e':     0.2976, 'lr_e':     0.0010})
Step:   65000, Reward:    10.562 [   3.517], Avg:    14.320 (0.292) <0-01:12:57> ({'r_t':  1000.0000, 'eps':     0.2916, 'dyn_loss': 341260512.0000, 'dot_loss': 412729.0312, 'ddot_loss':  2948.8369, 'rew_loss':     0.2201, 'lr':     0.0010, 'eps_e':     0.2916, 'lr_e':     0.0010})
Step:   66000, Reward:    13.938 [   3.363], Avg:    14.314 (0.286) <0-01:14:11> ({'r_t':  1000.0000, 'eps':     0.2858, 'dyn_loss': 86914288.0000, 'dot_loss': 73722.7969, 'ddot_loss':  1821.4358, 'rew_loss':     0.2230, 'lr':     0.0010, 'eps_e':     0.2858, 'lr_e':     0.0010})
Step:   67000, Reward:    10.938 [   1.298], Avg:    14.265 (0.280) <0-01:15:25> ({'r_t':  1000.0000, 'eps':     0.2801, 'dyn_loss': 107296976.0000, 'dot_loss': 101216.5859, 'ddot_loss':  1666.0861, 'rew_loss':     0.2274, 'lr':     0.0010, 'eps_e':     0.2801, 'lr_e':     0.0010})
Step:   68000, Reward:    13.000 [   3.182], Avg:    14.246 (0.274) <0-01:16:39> ({'r_t':  1000.0000, 'eps':     0.2745, 'dyn_loss': 36352628.0000, 'dot_loss': 89971.7969, 'ddot_loss':  1730.2645, 'rew_loss':     0.2158, 'lr':     0.0010, 'eps_e':     0.2745, 'lr_e':     0.0010})
Step:   69000, Reward:    11.312 [   2.256], Avg:    14.204 (0.269) <0-01:17:53> ({'r_t':  1000.0000, 'eps':     0.2690, 'dyn_loss': 65081784.0000, 'dot_loss': 73087.1797, 'ddot_loss':  1718.2167, 'rew_loss':     0.2169, 'lr':     0.0010, 'eps_e':     0.2690, 'lr_e':     0.0010})
Step:   70000, Reward:    11.250 [   3.631], Avg:    14.163 (0.264) <0-01:19:08> ({'r_t':  1000.0000, 'eps':     0.2636, 'dyn_loss': 759606208.0000, 'dot_loss': 506997.4062, 'ddot_loss':  2082.0256, 'rew_loss':     0.2270, 'lr':     0.0010, 'eps_e':     0.2636, 'lr_e':     0.0010})
