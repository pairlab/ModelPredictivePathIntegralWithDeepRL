Model: <class 'src.models.pytorch.mpc.mppi.MPPIAgent'>, Env: Pendulum-v0, Date: 10/06/2020 12:48:58
CPU: 20 Core, 0.0GHz, 377.59 GB, Linux-4.14.175-llgrid-10ms-x86_64-with-debian-buster-sid
GPU 0: Tesla V100-PCIE-32GB, 32.51 GB (Driver: 440.33.01)
GPU 1: Tesla V100-PCIE-32GB, 32.51 GB (Driver: 440.33.01)
Git URL: https://github.com/shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: 762d294d989a2ee63534a58d1363310463df4f0e
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 20
   MAX_BUFFER_SIZE = 1000000
   REPLAY_BATCH_SIZE = 2000
   TARGET_UPDATE_RATE = 0.0004
   TRAIN_EVERY = 2000
   BATCH_SIZE = 500
   ENV_MODEL = dfrntl
   MPC = 
      NSAMPLES = 100
      HORIZON = 20
      LAMBDA = 0.1
      COV = 1
   dynamics_size = 3
   state_size = (3,)
   action_size = (1,)
   env_name = Pendulum-v0
   rank = 0
   size = 17
   split = 17
   model = mppi
   framework = pt
   train_prop = 1.0
   tcp_ports = []
   tcp_rank = 0
   num_envs = 1
   nsteps = 500000
   render = False
   trial = False
   icm = False
   rs = False
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.97
      PATIENCE = 10
      LEARN_RATE = 0.0001
      TRANSITION_HIDDEN = 512
      REWARD_HIDDEN = 256
      BETA_DYN = 1
      BETA_DOT = 0
      BETA_DDOT = 0,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f30227f5ef0> 
	env = <GymEnv<TimeLimit<PendulumEnv<Pendulum-v0>>>> 
		env = <TimeLimit<PendulumEnv<Pendulum-v0>>> 
			env = <PendulumEnv<Pendulum-v0>> 
				max_speed = 8
				max_torque = 2.0
				dt = 0.05
				g = 10.0
				m = 1.0
				l = 1.0
				viewer = None
				action_space = Box(1,) 
					dtype = float32
					shape = (1,)
					low = [-2.000]
					high = [ 2.000]
					bounded_below = [ True]
					bounded_above = [ True]
					np_random = RandomState(MT19937)
				observation_space = Box(3,) 
					dtype = float32
					shape = (3,)
					low = [-1.000 -1.000 -8.000]
					high = [ 1.000  1.000  8.000]
					bounded_below = [ True  True  True]
					bounded_above = [ True  True  True]
					np_random = RandomState(MT19937)
				np_random = RandomState(MT19937)
				spec = EnvSpec(Pendulum-v0) 
					id = Pendulum-v0
					entry_point = gym.envs.classic_control:PendulumEnv
					reward_threshold = None
					nondeterministic = False
					max_episode_steps = 200
				verbose = 0
			action_space = Box(1,) 
				dtype = float32
				shape = (1,)
				low = [-2.000]
				high = [ 2.000]
				bounded_below = [ True]
				bounded_above = [ True]
				np_random = RandomState(MT19937)
			observation_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -8.000]
				high = [ 1.000  1.000  8.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			reward_range = (-inf, inf)
			metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}
		action_space = Box(1,) 
			dtype = float32
			shape = (1,)
			low = [-2.000]
			high = [ 2.000]
			bounded_below = [ True]
			bounded_above = [ True]
			np_random = RandomState(MT19937)
		observation_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -8.000]
			high = [ 1.000  1.000  8.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f3022767160> 
			observation_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -8.000]
				high = [ 1.000  1.000  8.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
	state_size = (3,)
	action_size = (1,)
	action_space = Box(1,) 
		dtype = float32
		shape = (1,)
		low = [-2.000]
		high = [ 2.000]
		bounded_below = [ True]
		bounded_above = [ True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.MPIConnection object at 0x7f30227677f0> 
		root = 0
		rank = 0
		cluster = <list len=16>
	num_envs = 16
	max_steps = 200,
agent: <src.models.wrappers.ParallelAgent object at 0x7f3022767828> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f302276cf60> 
		state_size = (3,)
	agent = <src.models.pytorch.mpc.mppi.MPPIAgent object at 0x7f302277e3c8> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f302277e400> 
			size = (1,)
			dt = 0.2
			action = [-0.085]
			daction_dt = [ 0.804]
		discrete = False
		action_size = (1,)
		state_size = (3,)
		config = <src.utils.config.Config object at 0x7f3022b1b1d0> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.998
			NUM_STEPS = 20
			MAX_BUFFER_SIZE = 1000000
			REPLAY_BATCH_SIZE = 2000
			TARGET_UPDATE_RATE = 0.0004
			TRAIN_EVERY = 2000
			BATCH_SIZE = 500
			ENV_MODEL = dfrntl
			MPC = <src.utils.config.Config object at 0x7f3048941550> 
				NSAMPLES = 100
				HORIZON = 20
				LAMBDA = 0.1
				COV = 1
			dynamics_size = 3
			state_size = (3,)
			action_size = (1,)
			env_name = Pendulum-v0
			rank = 0
			size = 17
			split = 17
			model = mppi
			framework = pt
			train_prop = 1.0
			tcp_ports = []
			tcp_rank = 0
			num_envs = 1
			nsteps = 500000
			render = False
			trial = False
			icm = False
			rs = False
			DYN = <src.utils.config.Config object at 0x7f3046db09b0> 
				REG_LAMBDA = 1e-06
				FACTOR = 0.97
				PATIENCE = 10
				LEARN_RATE = 0.0001
				TRANSITION_HIDDEN = 512
				REWARD_HIDDEN = 256
				BETA_DYN = 1
				BETA_DOT = 0
				BETA_DDOT = 0
		stats = <src.utils.logger.Stats object at 0x7f302277e438> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = MPPIController() 
			training = True
			tau = 0.0004
			name = mppi
			stats = <src.utils.logger.Stats object at 0x7f302277e4a8> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7f3022b1b1d0> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.998
				NUM_STEPS = 20
				MAX_BUFFER_SIZE = 1000000
				REPLAY_BATCH_SIZE = 2000
				TARGET_UPDATE_RATE = 0.0004
				TRAIN_EVERY = 2000
				BATCH_SIZE = 500
				ENV_MODEL = dfrntl
				MPC = <src.utils.config.Config object at 0x7f3048941550> 
					NSAMPLES = 100
					HORIZON = 20
					LAMBDA = 0.1
					COV = 1
				dynamics_size = 3
				state_size = (3,)
				action_size = (1,)
				env_name = Pendulum-v0
				rank = 0
				size = 17
				split = 17
				model = mppi
				framework = pt
				train_prop = 1.0
				tcp_ports = []
				tcp_rank = 0
				num_envs = 1
				nsteps = 500000
				render = False
				trial = False
				icm = False
				rs = False
				DYN = <src.utils.config.Config object at 0x7f3046db09b0> 
					REG_LAMBDA = 1e-06
					FACTOR = 0.97
					PATIENCE = 10
					LEARN_RATE = 0.0001
					TRANSITION_HIDDEN = 512
					REWARD_HIDDEN = 256
					BETA_DYN = 1
					BETA_DOT = 0
					BETA_DDOT = 0
			device = cuda
			envmodel = <src.models.pytorch.mpc.EnvModel object at 0x7f302277e4e0> 
				network = DifferentialEnv(
					  (reward): RewardModel(
					    (linear1): Linear(in_features=7, out_features=256, bias=True)
					    (drop1): Dropout(p=0.5, inplace=False)
					    (linear2): Linear(in_features=256, out_features=256, bias=True)
					    (drop2): Dropout(p=0.5, inplace=False)
					    (linear3): Linear(in_features=256, out_features=256, bias=True)
					    (linear4): Linear(in_features=256, out_features=1, bias=True)
					  )
					  (dynamics): TransitionModel(
					    (gru): GRUCell(7, 512)
					    (linear1): Linear(in_features=512, out_features=512, bias=True)
					    (drop1): Dropout(p=0.5, inplace=False)
					    (linear2): Linear(in_features=512, out_features=512, bias=True)
					    (drop2): Dropout(p=0.5, inplace=False)
					    (state_ddot): Linear(in_features=512, out_features=3, bias=True)
					  )
					) 
					training = True
					tau = 0.0004
					name = dfrntl
					stats = <src.utils.logger.Stats object at 0x7f302277e550> 
						mean_dict = {}
						sum_dict = {}
					config = <src.utils.config.Config object at 0x7f3022b1b1d0> 
						TRIAL_AT = 1000
						SAVE_AT = 1
						SEED = 0
						REG_LAMBDA = 1e-06
						LEARN_RATE = 0.0001
						DISCOUNT_RATE = 0.99
						ADVANTAGE_DECAY = 0.95
						INPUT_LAYER = 512
						ACTOR_HIDDEN = 256
						CRITIC_HIDDEN = 1024
						EPS_MAX = 1.0
						EPS_MIN = 0.1
						EPS_DECAY = 0.998
						NUM_STEPS = 20
						MAX_BUFFER_SIZE = 1000000
						REPLAY_BATCH_SIZE = 2000
						TARGET_UPDATE_RATE = 0.0004
						TRAIN_EVERY = 2000
						BATCH_SIZE = 500
						ENV_MODEL = dfrntl
						MPC = <src.utils.config.Config object at 0x7f3048941550> 
							NSAMPLES = 100
							HORIZON = 20
							LAMBDA = 0.1
							COV = 1
						dynamics_size = 3
						state_size = (3,)
						action_size = (1,)
						env_name = Pendulum-v0
						rank = 0
						size = 17
						split = 17
						model = mppi
						framework = pt
						train_prop = 1.0
						tcp_ports = []
						tcp_rank = 0
						num_envs = 1
						nsteps = 500000
						render = False
						trial = False
						icm = False
						rs = False
						DYN = <src.utils.config.Config object at 0x7f3046db09b0> 
							REG_LAMBDA = 1e-06
							FACTOR = 0.97
							PATIENCE = 10
							LEARN_RATE = 0.0001
							TRANSITION_HIDDEN = 512
							REWARD_HIDDEN = 256
							BETA_DYN = 1
							BETA_DOT = 0
							BETA_DDOT = 0
					device = cuda
					state_size = (3,)
					action_size = (1,)
					discrete = False
					dyn_index = 3
					optimizer = Adam (
					Parameter Group 0
					    amsgrad: False
					    betas: (0.9, 0.999)
					    eps: 1e-08
					    lr: 0.0001
					    weight_decay: 1e-06
					)
					scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f302277e8d0>
				state_size = (3,)
				action_size = (1,)
			mu = [ 0.000]
			cov = [[ 1.000]]
			icov = [[ 1.000]]
			lamda = 0.1
			horizon = 20
			nsamples = 100
			action_size = (1,)
			control = [[[ 0.497]
			  [ 0.496]
			  [-0.573]
			  [-0.245]
			  [ 0.309]
			  [-0.089]
			  [ 0.524]
			  [-0.069]
			  [-0.729]
			  [-0.672]
			  [-0.436]
			  [-0.203]
			  [-0.032]
			  [ 0.102]
			  [-0.703]
			  [-0.064]
			  [ 0.150]
			  [-0.407]
			  [ 0.093]
			  [ 0.948]]]
			noise = [[[[-1.223]
			   [-2.179]
			   [ 1.078]
			   ...
			   [-1.166]
			   [-0.328]
			   [ 0.818]]
			
			  [[ 0.843]
			   [-0.335]
			   [-0.994]
			   ...
			   [ 0.281]
			   [-0.200]
			   [ 1.591]]
			
			  [[-0.412]
			   [-0.024]
			   [-0.399]
			   ...
			   [ 0.304]
			   [ 0.361]
			   [ 0.401]]
			
			  ...
			
			  [[ 0.053]
			   [ 1.506]
			   [-0.129]
			   ...
			   [-2.312]
			   [ 1.013]
			   [ 1.137]]
			
			  [[-0.900]
			   [ 0.012]
			   [ 1.345]
			   ...
			   [-0.448]
			   [-1.635]
			   [-1.145]]
			
			  [[ 0.038]
			   [-1.112]
			   [ 0.420]
			   ...
			   [ 0.158]
			   [-0.282]
			   [-1.405]]]]
			init_cost = [[-0.124  0.163 -0.032 -0.066  0.262 -0.033 -0.046  0.022 -0.222  0.056  0.192  0.115 -0.115  0.096  0.158  0.057  0.055 -0.135 -0.024  0.119  0.086  0.006  0.094 -0.005  0.026 -0.009 -0.041  0.058  0.130 -0.018  0.055 -0.022 -0.048 -0.018 -0.093 -0.015 -0.003  0.105  0.097  0.062  0.181  0.066 -0.159  0.101 -0.103 -0.148 -0.011 -0.227  0.204  0.085 -0.113 -0.115 -0.011 -0.115 -0.203  0.032 -0.044  0.095  0.056 -0.013  0.053  0.069 -0.026 -0.092  0.024 -0.033  0.090  0.039 -0.077  0.119  0.002  0.154 -0.186  0.066 -0.085  0.041  0.020  0.117 -0.175 -0.062  0.114  0.061 -0.041  0.209 -0.102 -0.097 -0.087 -0.135  0.080  0.046 -0.051 -0.130 -0.009  0.065 -0.038 -0.030 -0.020  0.223 -0.273 -0.096]]
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f302277e908> 
			buffer = deque([], maxlen=1000000)
		buffer = []
		dataset = <class 'src.data.loaders.OnlineDataset'>
		ep_lens = deque([], maxlen=1000000)
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f30227a0400> 
		size = (1,)
		dt = 0.2
		action = [ 0.934]
		daction_dt = [-0.415]
	discrete = False
	action_size = (1,)
	state_size = (3,)
	config = <src.utils.config.Config object at 0x7f3022b1b1d0> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 20
		MAX_BUFFER_SIZE = 1000000
		REPLAY_BATCH_SIZE = 2000
		TARGET_UPDATE_RATE = 0.0004
		TRAIN_EVERY = 2000
		BATCH_SIZE = 500
		ENV_MODEL = dfrntl
		MPC = <src.utils.config.Config object at 0x7f3048941550> 
			NSAMPLES = 100
			HORIZON = 20
			LAMBDA = 0.1
			COV = 1
		dynamics_size = 3
		state_size = (3,)
		action_size = (1,)
		env_name = Pendulum-v0
		rank = 0
		size = 17
		split = 17
		model = mppi
		framework = pt
		train_prop = 1.0
		tcp_ports = []
		tcp_rank = 0
		num_envs = 1
		nsteps = 500000
		render = False
		trial = False
		icm = False
		rs = False
		DYN = <src.utils.config.Config object at 0x7f3046db09b0> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.97
			PATIENCE = 10
			LEARN_RATE = 0.0001
			TRANSITION_HIDDEN = 512
			REWARD_HIDDEN = 256
			BETA_DYN = 1
			BETA_DOT = 0
			BETA_DDOT = 0
	stats = <src.utils.logger.Stats object at 0x7f30227a0080> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import tqdm
import torch
import random
import numpy as np
import scipy as sp
from collections import deque
from scipy.stats import multivariate_normal
from src.utils.misc import load_module, pad
from src.utils.rand import RandomAgent, ReplayBuffer
from ..agents.base import PTNetwork, PTAgent, Conv, one_hot_from_indices
from . import EnvModel

class MPPIController(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="mppi"):
		super().__init__(config, gpu=gpu, name=name)
		self.envmodel = EnvModel(state_size, action_size, config, load=load, gpu=gpu)
		self.mu = np.zeros(action_size)
		self.cov = np.diag(np.ones(action_size))*config.MPC.COV
		self.icov = np.linalg.inv(self.cov)
		self.lamda = config.MPC.LAMBDA
		self.horizon = config.MPC.HORIZON
		self.nsamples = config.MPC.NSAMPLES
		self.action_size = action_size
		self.config = config
		self.init_control()

	def get_action(self, state, eps=None, sample=True):
		batch = state.shape[:-1]
		horizon = max(int((1-eps)*self.horizon),1) if eps else self.horizon
		if len(batch) and self.control.shape[0] != batch[0]: self.init_control(batch[0])
		x = torch.Tensor(state).view(*batch, 1,-1).repeat_interleave(self.nsamples, -2)
		noise = self.noise[...,:horizon,:] * max(eps if eps else 0, 0.1)
		controls = np.clip(self.control[:,None,:horizon,:] + noise, -1, 1)
		self.states, rewards = self.envmodel.rollout(controls, x, numpy=True)
		costs = -np.sum(rewards, -1) + self.lamda * np.copy(self.init_cost)
		beta = np.min(costs, -1, keepdims=True)
		costs_norm = -(costs - beta)/self.lamda
		weights = sp.special.softmax(costs_norm, axis=-1)
		self.control[...,:horizon,:] += np.sum(weights[:,:,None,None]*noise, len(batch))
		action = self.control[...,0,:]
		self.control = np.roll(self.control, -1, axis=-2)
		self.control[...,-1,:] = 0
		return action

	def init_control(self, batch_size=1):
		self.control = np.random.uniform(-1, 1, size=[batch_size, self.horizon, *self.action_size])
		self.noise = np.random.multivariate_normal(self.mu, self.cov, size=[batch_size, self.nsamples, self.horizon])
		self.init_cost = np.sum(self.control[:,None,:,None,:] @ self.icov[None,None,None,:,:] @ self.noise[:,:,:,:,None], axis=(2,3,4))/self.horizon

	def optimize(self, states, actions, next_states, rewards, dones):
		return self.envmodel.optimize(states, actions, next_states, rewards, dones)

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.save_model(dirname, name, net)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.load_model(dirname, name, net)

	def get_stats(self):
		return {**super().get_stats(), **self.envmodel.get_stats()}

class MPPIAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, MPPIController, gpu=gpu, load=load)
		self.dataset = load_module("src.data.loaders:OnlineDataset")
		self.ep_lens = deque(maxlen=config.MAX_BUFFER_SIZE)

	def get_action(self, state, eps=None, sample=True):
		action_random = super().get_action(state)
		if eps is None and not hasattr(self, "losses"): return action_random
		eps = self.eps if eps is None else eps
		action_greedy = self.network.get_action(np.array(state), eps)
		action = np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action

	def train(self, state, action, next_state, reward, done):
		self.time = getattr(self, "time", 0) + 1
		if not hasattr(self, "buffers"): self.buffers = [[] for _ in done]
		for buffer, s, a, ns, r, d in zip(self.buffers, state, action, next_state, reward, done):
			buffer.append((s, a, s if d else ns, r, d))
			if not d: continue
			self.ep_lens.append(len(buffer))
			states, actions, next_states, rewards, dones = map(lambda x: self.to_tensor(x)[None], zip(*buffer))
			buffer.clear()
			values = self.network.envmodel.network.reward(actions, states, next_states)[0]
			rewards = self.compute_gae(0*values[-1], rewards.transpose(0,1), dones.transpose(0,1), values)[0].transpose(0,1)
			states, actions, next_states, rewards, dones = map(lambda x: x.cpu().numpy(), [states, actions, next_states, rewards, dones])
			states, actions, next_states, rewards, dones = map(lambda x: pad(x[0], self.config.NUM_STEPS), [states, actions, next_states, rewards, dones])
			self.replay_buffer.extend(list(zip(states, actions, next_states, rewards, dones)), shuffle=False)
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE:# and self.time % self.config.TRAIN_EVERY == 0:
			self.losses = []
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.losses.append(self.network.optimize(states, actions, next_states, rewards, dones))
			# samples = list(self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=None)[0])
			# dataset = self.dataset(self.config, samples, seq_len=self.config.MPC.HORIZON)
			# loader = torch.utils.data.DataLoader(dataset, batch_size=self.config.BATCH_SIZE, shuffle=True)
			# pbar = tqdm.tqdm(loader)
			# for states, actions, next_states, rewards, dones in pbar:
			# 	self.losses.append(self.network.optimize(states, actions, next_states, rewards, dones))
			# 	pbar.set_postfix_str(f"Loss: {self.losses[-1]:.4f}")
			self.network.envmodel.network.schedule(np.mean(self.losses))
		self.eps = (self.time/np.mean(self.ep_lens))%1 if hasattr(self, "losses") else 1

	def get_stats(self):
		return {**super().get_stats(), "len":len(self.replay_buffer), "ep_len":np.mean(self.ep_lens)}


Step:       0, Reward: -1193.468 [ 123.389], Avg: -1193.468 (1.000) <0-00:00:00> ({'r_t':    -3.6172, 'eps':     1.0000, 'lr':     0.0001, 'len':   0.00e+00, 'ep_len':        nan, 'eps_e':     1.0000, 'lr_e':     0.0001, 'len_e':   0.00e+00, 'ep_len_e':        nan})
Step:    1000, Reward: -1242.376 [  99.854], Avg: -1217.922 (1.000) <0-00:00:10> ({'r_t': -6283.6961, 'eps':     1.0000, 'lr':     0.0001, 'len':   800.0000, 'ep_len':   200.0000, 'eps_e':     1.0000, 'lr_e':     0.0001, 'len_e':   800.0000, 'ep_len_e':   200.0000})
Step:    2000, Reward: -1244.839 [  69.578], Avg: -1226.894 (1.000) <0-00:00:21> ({'r_t': -6234.6180, 'eps':     1.0000, 'lr':     0.0001, 'len':  1600.0000, 'ep_len':   200.0000, 'eps_e':     1.0000, 'lr_e':     0.0001, 'len_e':  1600.0000, 'ep_len_e':   200.0000})
Step:    3000, Reward: -1541.033 [ 166.892], Avg: -1305.429 (0.005) <0-00:01:09> ({'r_t': -6615.5224, 'eps':     0.0050, 'dyn_loss':    36.9114, 'dot_loss':     2.1613, 'ddot_loss':     0.5810, 'rew_loss':  6991.5620, 'lr':   6.73e-05, 'len':  2400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   6.73e-05, 'len_e':  2400.0000, 'ep_len_e':   200.0000})
Step:    4000, Reward: -1691.011 [ 132.791], Avg: -1382.545 (0.005) <0-00:02:51> ({'r_t': -8022.8723, 'eps':     0.0050, 'dyn_loss':    11.9789, 'dot_loss':     0.6668, 'ddot_loss':     0.2825, 'rew_loss':  6470.6763, 'lr':   4.21e-06, 'len':  3200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   4.21e-06, 'len_e':  3200.0000, 'ep_len_e':   200.0000})
Step:    5000, Reward: -1652.670 [ 111.856], Avg: -1427.566 (0.005) <0-00:04:34> ({'r_t': -7981.7082, 'eps':     0.0050, 'dyn_loss':     9.3855, 'dot_loss':     0.5324, 'ddot_loss':     0.2407, 'rew_loss':  8100.4766, 'lr':   3.26e-07, 'len':  4000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  4000.0000, 'ep_len_e':   200.0000})
Step:    6000, Reward: -1663.174 [ 174.567], Avg: -1461.225 (0.005) <0-00:06:17> ({'r_t': -8012.0531, 'eps':     0.0050, 'dyn_loss':     8.2419, 'dot_loss':     0.4736, 'ddot_loss':     0.2177, 'rew_loss':  9320.7139, 'lr':   3.26e-07, 'len':  4800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  4800.0000, 'ep_len_e':   200.0000})
Step:    7000, Reward: -1688.168 [ 172.779], Avg: -1489.592 (0.005) <0-00:08:00> ({'r_t': -7877.8633, 'eps':     0.0050, 'dyn_loss':     7.3973, 'dot_loss':     0.4311, 'ddot_loss':     0.2013, 'rew_loss': 10172.0879, 'lr':   3.26e-07, 'len':  5600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  5600.0000, 'ep_len_e':   200.0000})
Step:    8000, Reward: -1558.219 [ 153.502], Avg: -1497.218 (0.005) <0-00:09:43> ({'r_t': -7856.2353, 'eps':     0.0050, 'dyn_loss':     6.7539, 'dot_loss':     0.4000, 'ddot_loss':     0.1898, 'rew_loss': 10743.1436, 'lr':   3.26e-07, 'len':  6400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  6400.0000, 'ep_len_e':   200.0000})
Step:    9000, Reward: -1487.099 [ 267.789], Avg: -1496.206 (0.005) <0-00:11:27> ({'r_t': -7843.6298, 'eps':     0.0050, 'dyn_loss':     6.2191, 'dot_loss':     0.3741, 'ddot_loss':     0.1810, 'rew_loss': 11153.5498, 'lr':   3.26e-07, 'len':  7200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  7200.0000, 'ep_len_e':   200.0000})
Step:   10000, Reward: -1400.553 [ 172.306], Avg: -1487.510 (0.005) <0-00:13:11> ({'r_t': -7721.0598, 'eps':     0.0050, 'dyn_loss':     5.7387, 'dot_loss':     0.3523, 'ddot_loss':     0.1744, 'rew_loss': 11443.8408, 'lr':   3.26e-07, 'len':  8000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  8000.0000, 'ep_len_e':   200.0000})
Step:   11000, Reward: -1421.963 [ 197.795], Avg: -1482.048 (0.005) <0-00:14:55> ({'r_t': -7572.3703, 'eps':     0.0050, 'dyn_loss':     5.2737, 'dot_loss':     0.3318, 'ddot_loss':     0.1686, 'rew_loss': 11640.2129, 'lr':   3.26e-07, 'len':  8800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  8800.0000, 'ep_len_e':   200.0000})
Step:   12000, Reward: -1457.731 [ 292.324], Avg: -1480.177 (0.005) <0-00:16:39> ({'r_t': -7418.5275, 'eps':     0.0050, 'dyn_loss':     4.8416, 'dot_loss':     0.3131, 'ddot_loss':     0.1640, 'rew_loss': 11762.2734, 'lr':   3.26e-07, 'len':  9600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e':  9600.0000, 'ep_len_e':   200.0000})
Step:   13000, Reward: -1281.936 [ 316.161], Avg: -1466.017 (0.005) <0-00:18:23> ({'r_t': -7361.9064, 'eps':     0.0050, 'dyn_loss':     4.4328, 'dot_loss':     0.2955, 'ddot_loss':     0.1601, 'rew_loss': 11809.5996, 'lr':   3.26e-07, 'len': 10400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 10400.0000, 'ep_len_e':   200.0000})
Step:   14000, Reward: -1292.223 [ 341.614], Avg: -1454.431 (0.005) <0-00:20:07> ({'r_t': -6905.5921, 'eps':     0.0050, 'dyn_loss':     4.0444, 'dot_loss':     0.2795, 'ddot_loss':     0.1572, 'rew_loss': 11802.6953, 'lr':   3.26e-07, 'len': 11200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 11200.0000, 'ep_len_e':   200.0000})
Step:   15000, Reward: -1216.047 [ 356.606], Avg: -1439.532 (0.005) <0-00:21:51> ({'r_t': -6837.2516, 'eps':     0.0050, 'dyn_loss':     3.7105, 'dot_loss':     0.2660, 'ddot_loss':     0.1555, 'rew_loss': 11706.4648, 'lr':   3.26e-07, 'len': 12000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 12000.0000, 'ep_len_e':   200.0000})
Step:   16000, Reward: -1118.024 [ 337.354], Avg: -1420.620 (0.005) <0-00:23:33> ({'r_t': -6684.9299, 'eps':     0.0050, 'dyn_loss':     3.3782, 'dot_loss':     0.2534, 'ddot_loss':     0.1542, 'rew_loss': 11627.8066, 'lr':   3.26e-07, 'len': 12800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 12800.0000, 'ep_len_e':   200.0000})
Step:   17000, Reward:  -948.483 [ 127.418], Avg: -1394.390 (0.005) <0-00:25:16> ({'r_t': -6615.0465, 'eps':     0.0050, 'dyn_loss':     3.0763, 'dot_loss':     0.2424, 'ddot_loss':     0.1535, 'rew_loss': 11518.6602, 'lr':   3.26e-07, 'len': 13600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 13600.0000, 'ep_len_e':   200.0000})
Step:   18000, Reward: -1046.594 [ 369.290], Avg: -1376.085 (0.005) <0-00:26:58> ({'r_t': -6628.8918, 'eps':     0.0050, 'dyn_loss':     2.7964, 'dot_loss':     0.2327, 'ddot_loss':     0.1534, 'rew_loss': 11421.1055, 'lr':   3.26e-07, 'len': 14400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 14400.0000, 'ep_len_e':   200.0000})
Step:   19000, Reward: -1098.148 [ 398.460], Avg: -1362.188 (0.005) <0-00:28:41> ({'r_t': -6614.1834, 'eps':     0.0050, 'dyn_loss':     2.5469, 'dot_loss':     0.2231, 'ddot_loss':     0.1528, 'rew_loss': 11329.1904, 'lr':   3.26e-07, 'len': 15200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 15200.0000, 'ep_len_e':   200.0000})
Step:   20000, Reward: -1268.429 [ 471.611], Avg: -1357.723 (0.005) <0-00:30:23> ({'r_t': -6689.4529, 'eps':     0.0050, 'dyn_loss':     2.3234, 'dot_loss':     0.2141, 'ddot_loss':     0.1521, 'rew_loss': 11258.6777, 'lr':   3.26e-07, 'len': 16000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 16000.0000, 'ep_len_e':   200.0000})
Step:   21000, Reward: -1422.474 [ 374.279], Avg: -1360.666 (0.005) <0-00:32:06> ({'r_t': -6788.2895, 'eps':     0.0050, 'dyn_loss':     2.1276, 'dot_loss':     0.2052, 'ddot_loss':     0.1510, 'rew_loss': 11195.9863, 'lr':   3.26e-07, 'len': 16800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 16800.0000, 'ep_len_e':   200.0000})
Step:   22000, Reward: -1265.215 [ 597.010], Avg: -1356.516 (0.005) <0-00:33:49> ({'r_t': -6585.7760, 'eps':     0.0050, 'dyn_loss':     1.9463, 'dot_loss':     0.1963, 'ddot_loss':     0.1493, 'rew_loss': 11166.8301, 'lr':   3.26e-07, 'len': 17600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 17600.0000, 'ep_len_e':   200.0000})
Step:   23000, Reward: -1382.701 [ 474.200], Avg: -1357.607 (0.005) <0-00:35:32> ({'r_t': -6237.9563, 'eps':     0.0050, 'dyn_loss':     1.8052, 'dot_loss':     0.1880, 'ddot_loss':     0.1474, 'rew_loss': 11100.1963, 'lr':   3.26e-07, 'len': 18400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 18400.0000, 'ep_len_e':   200.0000})
Step:   24000, Reward: -1407.302 [ 567.167], Avg: -1359.595 (0.005) <0-00:37:16> ({'r_t': -6658.4325, 'eps':     0.0050, 'dyn_loss':     1.6814, 'dot_loss':     0.1802, 'ddot_loss':     0.1454, 'rew_loss': 11048.7510, 'lr':   3.26e-07, 'len': 19200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 19200.0000, 'ep_len_e':   200.0000})
Step:   25000, Reward: -1360.902 [ 639.573], Avg: -1359.645 (0.005) <0-00:38:58> ({'r_t': -6661.3409, 'eps':     0.0050, 'dyn_loss':     1.5587, 'dot_loss':     0.1725, 'ddot_loss':     0.1431, 'rew_loss': 11036.3203, 'lr':   3.26e-07, 'len': 20000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 20000.0000, 'ep_len_e':   200.0000})
Step:   26000, Reward: -1665.476 [ 207.774], Avg: -1370.973 (0.005) <0-00:40:42> ({'r_t': -6267.1786, 'eps':     0.0050, 'dyn_loss':     1.4673, 'dot_loss':     0.1657, 'ddot_loss':     0.1411, 'rew_loss': 11001.0820, 'lr':   3.26e-07, 'len': 20800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 20800.0000, 'ep_len_e':   200.0000})
Step:   27000, Reward: -1375.652 [ 648.802], Avg: -1371.140 (0.005) <0-00:42:26> ({'r_t': -6515.5913, 'eps':     0.0050, 'dyn_loss':     1.3776, 'dot_loss':     0.1593, 'ddot_loss':     0.1390, 'rew_loss': 10976.3516, 'lr':   3.26e-07, 'len': 21600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 21600.0000, 'ep_len_e':   200.0000})
Step:   28000, Reward: -1528.561 [ 564.861], Avg: -1376.568 (0.005) <0-00:44:09> ({'r_t': -6595.4923, 'eps':     0.0050, 'dyn_loss':     1.2962, 'dot_loss':     0.1537, 'ddot_loss':     0.1375, 'rew_loss': 10949.5908, 'lr':   3.26e-07, 'len': 22400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 22400.0000, 'ep_len_e':   200.0000})
Step:   29000, Reward: -1309.166 [ 663.572], Avg: -1374.321 (0.005) <0-00:45:52> ({'r_t': -6234.1434, 'eps':     0.0050, 'dyn_loss':     1.2227, 'dot_loss':     0.1485, 'ddot_loss':     0.1360, 'rew_loss': 10931.2324, 'lr':   3.26e-07, 'len': 23200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 23200.0000, 'ep_len_e':   200.0000})
Step:   30000, Reward: -1384.677 [ 585.963], Avg: -1374.655 (0.005) <0-00:47:36> ({'r_t': -6804.9659, 'eps':     0.0050, 'dyn_loss':     1.1552, 'dot_loss':     0.1438, 'ddot_loss':     0.1347, 'rew_loss': 10901.3486, 'lr':   3.26e-07, 'len': 24000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 24000.0000, 'ep_len_e':   200.0000})
Step:   31000, Reward: -1493.270 [ 531.656], Avg: -1378.362 (0.005) <0-00:49:20> ({'r_t': -6541.3682, 'eps':     0.0050, 'dyn_loss':     1.0875, 'dot_loss':     0.1394, 'ddot_loss':     0.1335, 'rew_loss': 10879.5605, 'lr':   3.26e-07, 'len': 24800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 24800.0000, 'ep_len_e':   200.0000})
Step:   32000, Reward: -1485.824 [ 579.016], Avg: -1381.618 (0.005) <0-00:51:05> ({'r_t': -6333.1709, 'eps':     0.0050, 'dyn_loss':     1.0290, 'dot_loss':     0.1354, 'ddot_loss':     0.1326, 'rew_loss': 10861.7617, 'lr':   3.26e-07, 'len': 25600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 25600.0000, 'ep_len_e':   200.0000})
Step:   33000, Reward: -1128.898 [ 734.188], Avg: -1374.185 (0.005) <0-00:52:49> ({'r_t': -6934.7222, 'eps':     0.0050, 'dyn_loss':     0.9769, 'dot_loss':     0.1316, 'ddot_loss':     0.1316, 'rew_loss': 10847.0156, 'lr':   3.26e-07, 'len': 26400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 26400.0000, 'ep_len_e':   200.0000})
Step:   34000, Reward: -1033.100 [ 765.737], Avg: -1364.440 (0.005) <0-00:54:33> ({'r_t': -6278.2245, 'eps':     0.0050, 'dyn_loss':     0.9243, 'dot_loss':     0.1284, 'ddot_loss':     0.1310, 'rew_loss': 10840.1562, 'lr':   3.26e-07, 'len': 27200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 27200.0000, 'ep_len_e':   200.0000})
Step:   35000, Reward: -1290.901 [ 672.278], Avg: -1362.397 (0.005) <0-00:56:18> ({'r_t': -6824.2487, 'eps':     0.0050, 'dyn_loss':     0.8742, 'dot_loss':     0.1250, 'ddot_loss':     0.1302, 'rew_loss': 10821.3398, 'lr':   3.26e-07, 'len': 28000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 28000.0000, 'ep_len_e':   200.0000})
Step:   36000, Reward: -1262.737 [ 690.975], Avg: -1359.704 (0.005) <0-00:58:02> ({'r_t': -6143.3326, 'eps':     0.0050, 'dyn_loss':     0.8284, 'dot_loss':     0.1219, 'ddot_loss':     0.1295, 'rew_loss': 10827.5947, 'lr':   3.26e-07, 'len': 28800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 28800.0000, 'ep_len_e':   200.0000})
Step:   37000, Reward: -1399.160 [ 577.254], Avg: -1360.742 (0.005) <0-00:59:47> ({'r_t': -6650.2636, 'eps':     0.0050, 'dyn_loss':     0.7906, 'dot_loss':     0.1192, 'ddot_loss':     0.1290, 'rew_loss': 10782.5762, 'lr':   3.26e-07, 'len': 29600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 29600.0000, 'ep_len_e':   200.0000})
Step:   38000, Reward: -1388.350 [ 641.660], Avg: -1361.450 (0.005) <0-01:01:32> ({'r_t': -6795.1517, 'eps':     0.0050, 'dyn_loss':     0.7497, 'dot_loss':     0.1164, 'ddot_loss':     0.1283, 'rew_loss': 10776.6543, 'lr':   3.26e-07, 'len': 30400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 30400.0000, 'ep_len_e':   200.0000})
Step:   39000, Reward: -1339.366 [ 646.136], Avg: -1360.898 (0.005) <0-01:03:17> ({'r_t': -6296.6229, 'eps':     0.0050, 'dyn_loss':     0.7143, 'dot_loss':     0.1138, 'ddot_loss':     0.1278, 'rew_loss': 10762.4541, 'lr':   3.26e-07, 'len': 31200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 31200.0000, 'ep_len_e':   200.0000})
Step:   40000, Reward: -1223.498 [ 656.653], Avg: -1357.547 (0.005) <0-01:05:02> ({'r_t': -6706.2387, 'eps':     0.0050, 'dyn_loss':     0.6820, 'dot_loss':     0.1114, 'ddot_loss':     0.1273, 'rew_loss': 10752.0195, 'lr':   3.26e-07, 'len': 32000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 32000.0000, 'ep_len_e':   200.0000})
Step:   41000, Reward: -1277.491 [ 690.178], Avg: -1355.641 (0.005) <0-01:06:47> ({'r_t': -6040.0502, 'eps':     0.0050, 'dyn_loss':     0.6529, 'dot_loss':     0.1090, 'ddot_loss':     0.1267, 'rew_loss': 10737.3057, 'lr':   3.26e-07, 'len': 32800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 32800.0000, 'ep_len_e':   200.0000})
Step:   42000, Reward: -1449.787 [ 532.987], Avg: -1357.830 (0.005) <0-01:08:33> ({'r_t': -6365.3110, 'eps':     0.0050, 'dyn_loss':     0.6247, 'dot_loss':     0.1068, 'ddot_loss':     0.1262, 'rew_loss': 10702.7998, 'lr':   3.26e-07, 'len': 33600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 33600.0000, 'ep_len_e':   200.0000})
Step:   43000, Reward: -1104.032 [ 769.880], Avg: -1352.062 (0.005) <0-01:10:18> ({'r_t': -6674.1338, 'eps':     0.0050, 'dyn_loss':     0.5954, 'dot_loss':     0.1046, 'ddot_loss':     0.1256, 'rew_loss': 10688.7217, 'lr':   3.26e-07, 'len': 34400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 34400.0000, 'ep_len_e':   200.0000})
Step:   44000, Reward: -1413.719 [ 518.619], Avg: -1353.432 (0.005) <0-01:12:03> ({'r_t': -6090.5368, 'eps':     0.0050, 'dyn_loss':     0.5720, 'dot_loss':     0.1027, 'ddot_loss':     0.1252, 'rew_loss': 10670.1943, 'lr':   3.26e-07, 'len': 35200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 35200.0000, 'ep_len_e':   200.0000})
Step:   45000, Reward: -1375.052 [ 599.970], Avg: -1353.902 (0.005) <0-01:13:48> ({'r_t': -6302.7230, 'eps':     0.0050, 'dyn_loss':     0.5487, 'dot_loss':     0.1007, 'ddot_loss':     0.1246, 'rew_loss': 10652.2021, 'lr':   3.26e-07, 'len': 36000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 36000.0000, 'ep_len_e':   200.0000})
Step:   46000, Reward:  -939.059 [ 736.801], Avg: -1345.076 (0.005) <0-01:15:34> ({'r_t': -6155.7735, 'eps':     0.0050, 'dyn_loss':     0.5306, 'dot_loss':     0.0991, 'ddot_loss':     0.1243, 'rew_loss': 10611.5156, 'lr':   3.26e-07, 'len': 36800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 36800.0000, 'ep_len_e':   200.0000})
Step:   47000, Reward: -1071.118 [ 759.938], Avg: -1339.368 (0.005) <0-01:17:19> ({'r_t': -6418.7441, 'eps':     0.0050, 'dyn_loss':     0.5072, 'dot_loss':     0.0972, 'ddot_loss':     0.1239, 'rew_loss': 10596.9482, 'lr':   3.26e-07, 'len': 37600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 37600.0000, 'ep_len_e':   200.0000})
Step:   48000, Reward:  -973.635 [ 743.403], Avg: -1331.904 (0.005) <0-01:19:05> ({'r_t': -6572.0043, 'eps':     0.0050, 'dyn_loss':     0.4898, 'dot_loss':     0.0957, 'ddot_loss':     0.1236, 'rew_loss': 10583.8857, 'lr':   3.26e-07, 'len': 38400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 38400.0000, 'ep_len_e':   200.0000})
Step:   49000, Reward: -1468.164 [ 469.918], Avg: -1334.629 (0.005) <0-01:20:51> ({'r_t': -6618.3647, 'eps':     0.0050, 'dyn_loss':     0.4699, 'dot_loss':     0.0938, 'ddot_loss':     0.1230, 'rew_loss': 10574.2539, 'lr':   3.26e-07, 'len': 39200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 39200.0000, 'ep_len_e':   200.0000})
Step:   50000, Reward:  -928.265 [ 753.507], Avg: -1326.662 (0.005) <0-01:22:37> ({'r_t': -6397.7454, 'eps':     0.0050, 'dyn_loss':     0.4539, 'dot_loss':     0.0925, 'ddot_loss':     0.1227, 'rew_loss': 10551.0586, 'lr':   3.26e-07, 'len': 40000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 40000.0000, 'ep_len_e':   200.0000})
Step:   51000, Reward: -1313.186 [ 622.709], Avg: -1326.402 (0.005) <0-01:24:23> ({'r_t': -6621.9151, 'eps':     0.0050, 'dyn_loss':     0.4351, 'dot_loss':     0.0908, 'ddot_loss':     0.1222, 'rew_loss': 10548.0479, 'lr':   3.26e-07, 'len': 40800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 40800.0000, 'ep_len_e':   200.0000})
Step:   52000, Reward: -1499.295 [ 450.162], Avg: -1329.665 (0.005) <0-01:26:09> ({'r_t': -6660.8102, 'eps':     0.0050, 'dyn_loss':     0.4187, 'dot_loss':     0.0892, 'ddot_loss':     0.1217, 'rew_loss': 10544.7041, 'lr':   3.26e-07, 'len': 41600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 41600.0000, 'ep_len_e':   200.0000})
Step:   53000, Reward: -1430.655 [ 503.503], Avg: -1331.535 (0.005) <0-01:27:55> ({'r_t': -6353.7899, 'eps':     0.0050, 'dyn_loss':     0.4042, 'dot_loss':     0.0878, 'ddot_loss':     0.1211, 'rew_loss': 10544.2549, 'lr':   3.26e-07, 'len': 42400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 42400.0000, 'ep_len_e':   200.0000})
Step:   54000, Reward:  -988.061 [ 799.589], Avg: -1325.290 (0.005) <0-01:29:41> ({'r_t': -6461.3078, 'eps':     0.0050, 'dyn_loss':     0.3906, 'dot_loss':     0.0863, 'ddot_loss':     0.1205, 'rew_loss': 10522.4922, 'lr':   3.26e-07, 'len': 43200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 43200.0000, 'ep_len_e':   200.0000})
Step:   55000, Reward: -1364.089 [ 576.746], Avg: -1325.983 (0.005) <0-01:31:28> ({'r_t': -6362.2584, 'eps':     0.0050, 'dyn_loss':     0.3780, 'dot_loss':     0.0851, 'ddot_loss':     0.1202, 'rew_loss': 10516.4385, 'lr':   3.26e-07, 'len': 44000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 44000.0000, 'ep_len_e':   200.0000})
Step:   56000, Reward: -1243.573 [ 712.158], Avg: -1324.537 (0.005) <0-01:33:12> ({'r_t': -6471.0074, 'eps':     0.0050, 'dyn_loss':     0.3637, 'dot_loss':     0.0837, 'ddot_loss':     0.1197, 'rew_loss': 10500.6035, 'lr':   3.26e-07, 'len': 44800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 44800.0000, 'ep_len_e':   200.0000})
Step:   57000, Reward: -1260.220 [ 609.003], Avg: -1323.428 (0.005) <0-01:34:56> ({'r_t': -6213.9632, 'eps':     0.0050, 'dyn_loss':     0.3529, 'dot_loss':     0.0826, 'ddot_loss':     0.1194, 'rew_loss': 10458.5420, 'lr':   3.26e-07, 'len': 45600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 45600.0000, 'ep_len_e':   200.0000})
Step:   58000, Reward: -1352.328 [ 641.134], Avg: -1323.918 (0.005) <0-01:36:40> ({'r_t': -6131.2208, 'eps':     0.0050, 'dyn_loss':     0.3415, 'dot_loss':     0.0814, 'ddot_loss':     0.1190, 'rew_loss': 10447.4395, 'lr':   3.26e-07, 'len': 46400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 46400.0000, 'ep_len_e':   200.0000})
Step:   59000, Reward: -1375.392 [ 595.017], Avg: -1324.776 (0.005) <0-01:38:25> ({'r_t': -6706.2418, 'eps':     0.0050, 'dyn_loss':     0.3306, 'dot_loss':     0.0801, 'ddot_loss':     0.1183, 'rew_loss': 10436.0225, 'lr':   3.26e-07, 'len': 47200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 47200.0000, 'ep_len_e':   200.0000})
Step:   60000, Reward: -1290.455 [ 683.165], Avg: -1324.213 (0.005) <0-01:40:09> ({'r_t': -6146.0984, 'eps':     0.0050, 'dyn_loss':     0.3215, 'dot_loss':     0.0791, 'ddot_loss':     0.1181, 'rew_loss': 10412.3115, 'lr':   3.26e-07, 'len': 48000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 48000.0000, 'ep_len_e':   200.0000})
Step:   61000, Reward: -1296.365 [ 644.438], Avg: -1323.764 (0.005) <0-01:41:53> ({'r_t': -6526.6227, 'eps':     0.0050, 'dyn_loss':     0.3114, 'dot_loss':     0.0780, 'ddot_loss':     0.1177, 'rew_loss': 10386.6582, 'lr':   3.26e-07, 'len': 48800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 48800.0000, 'ep_len_e':   200.0000})
Step:   62000, Reward: -1144.847 [ 720.046], Avg: -1320.924 (0.005) <0-01:43:37> ({'r_t': -6203.1828, 'eps':     0.0050, 'dyn_loss':     0.3027, 'dot_loss':     0.0769, 'ddot_loss':     0.1171, 'rew_loss': 10376.5166, 'lr':   3.26e-07, 'len': 49600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 49600.0000, 'ep_len_e':   200.0000})
Step:   63000, Reward:  -947.711 [ 704.786], Avg: -1315.092 (0.005) <0-01:45:21> ({'r_t': -6061.0705, 'eps':     0.0050, 'dyn_loss':     0.2932, 'dot_loss':     0.0758, 'ddot_loss':     0.1166, 'rew_loss': 10361.1504, 'lr':   3.26e-07, 'len': 50400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 50400.0000, 'ep_len_e':   200.0000})
Step:   64000, Reward: -1412.467 [ 604.317], Avg: -1316.591 (0.005) <0-01:47:06> ({'r_t': -6186.7762, 'eps':     0.0050, 'dyn_loss':     0.2861, 'dot_loss':     0.0750, 'ddot_loss':     0.1164, 'rew_loss': 10321.3682, 'lr':   3.26e-07, 'len': 51200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 51200.0000, 'ep_len_e':   200.0000})
Step:   65000, Reward: -1207.706 [ 784.631], Avg: -1314.941 (0.005) <0-01:48:51> ({'r_t': -6326.3818, 'eps':     0.0050, 'dyn_loss':     0.2788, 'dot_loss':     0.0739, 'ddot_loss':     0.1157, 'rew_loss': 10309.9229, 'lr':   3.26e-07, 'len': 52000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 52000.0000, 'ep_len_e':   200.0000})
Step:   66000, Reward:  -999.481 [ 794.005], Avg: -1310.232 (0.005) <0-01:50:35> ({'r_t': -5972.4970, 'eps':     0.0050, 'dyn_loss':     0.2702, 'dot_loss':     0.0730, 'ddot_loss':     0.1154, 'rew_loss': 10289.8027, 'lr':   3.26e-07, 'len': 52800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 52800.0000, 'ep_len_e':   200.0000})
Step:   67000, Reward: -1486.159 [ 508.611], Avg: -1312.820 (0.005) <0-01:52:20> ({'r_t': -6479.6872, 'eps':     0.0050, 'dyn_loss':     0.2638, 'dot_loss':     0.0720, 'ddot_loss':     0.1149, 'rew_loss': 10275.2363, 'lr':   3.26e-07, 'len': 53600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 53600.0000, 'ep_len_e':   200.0000})
Step:   68000, Reward: -1190.055 [ 746.034], Avg: -1311.040 (0.005) <0-01:54:05> ({'r_t': -6638.2676, 'eps':     0.0050, 'dyn_loss':     0.2561, 'dot_loss':     0.0711, 'ddot_loss':     0.1144, 'rew_loss': 10246.7520, 'lr':   3.26e-07, 'len': 54400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 54400.0000, 'ep_len_e':   200.0000})
Step:   69000, Reward: -1249.808 [ 722.586], Avg: -1310.166 (0.005) <0-01:55:51> ({'r_t': -6648.6074, 'eps':     0.0050, 'dyn_loss':     0.2492, 'dot_loss':     0.0702, 'ddot_loss':     0.1140, 'rew_loss': 10250.2822, 'lr':   3.26e-07, 'len': 55200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 55200.0000, 'ep_len_e':   200.0000})
Step:   70000, Reward: -1198.815 [ 717.054], Avg: -1308.597 (0.005) <0-01:57:36> ({'r_t': -6165.2069, 'eps':     0.0050, 'dyn_loss':     0.2425, 'dot_loss':     0.0693, 'ddot_loss':     0.1134, 'rew_loss': 10234.0410, 'lr':   3.26e-07, 'len': 56000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 56000.0000, 'ep_len_e':   200.0000})
Step:   71000, Reward: -1430.903 [ 625.126], Avg: -1310.296 (0.005) <0-01:59:22> ({'r_t': -6088.2435, 'eps':     0.0050, 'dyn_loss':     0.2361, 'dot_loss':     0.0684, 'ddot_loss':     0.1128, 'rew_loss': 10218.4717, 'lr':   3.26e-07, 'len': 56800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 56800.0000, 'ep_len_e':   200.0000})
Step:   72000, Reward: -1585.057 [ 497.115], Avg: -1314.060 (0.005) <0-02:01:08> ({'r_t': -6383.4958, 'eps':     0.0050, 'dyn_loss':     0.2303, 'dot_loss':     0.0676, 'ddot_loss':     0.1124, 'rew_loss': 10193.9600, 'lr':   3.26e-07, 'len': 57600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 57600.0000, 'ep_len_e':   200.0000})
Step:   73000, Reward: -1147.295 [ 741.959], Avg: -1311.806 (0.005) <0-02:02:54> ({'r_t': -6479.3589, 'eps':     0.0050, 'dyn_loss':     0.2232, 'dot_loss':     0.0667, 'ddot_loss':     0.1118, 'rew_loss': 10181.5645, 'lr':   3.26e-07, 'len': 58400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 58400.0000, 'ep_len_e':   200.0000})
Step:   74000, Reward:  -874.399 [ 765.892], Avg: -1305.974 (0.005) <0-02:04:39> ({'r_t': -6693.3989, 'eps':     0.0050, 'dyn_loss':     0.2190, 'dot_loss':     0.0659, 'ddot_loss':     0.1113, 'rew_loss': 10181.4004, 'lr':   3.26e-07, 'len': 59200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 59200.0000, 'ep_len_e':   200.0000})
Step:   75000, Reward: -1101.411 [ 796.675], Avg: -1303.283 (0.005) <0-02:06:25> ({'r_t': -6188.4778, 'eps':     0.0050, 'dyn_loss':     0.2129, 'dot_loss':     0.0651, 'ddot_loss':     0.1107, 'rew_loss': 10166.5381, 'lr':   3.26e-07, 'len': 60000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 60000.0000, 'ep_len_e':   200.0000})
Step:   76000, Reward: -1112.318 [ 788.295], Avg: -1300.802 (0.005) <0-02:08:11> ({'r_t': -6397.2763, 'eps':     0.0050, 'dyn_loss':     0.2081, 'dot_loss':     0.0643, 'ddot_loss':     0.1101, 'rew_loss': 10150.5342, 'lr':   3.26e-07, 'len': 60800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 60800.0000, 'ep_len_e':   200.0000})
Step:   77000, Reward: -1165.699 [ 741.748], Avg: -1299.070 (0.005) <0-02:09:59> ({'r_t': -6552.7276, 'eps':     0.0050, 'dyn_loss':     0.2028, 'dot_loss':     0.0636, 'ddot_loss':     0.1095, 'rew_loss': 10141.9180, 'lr':   3.26e-07, 'len': 61600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 61600.0000, 'ep_len_e':   200.0000})
Step:   78000, Reward:  -840.950 [ 784.137], Avg: -1293.271 (0.005) <0-02:11:45> ({'r_t': -6760.9056, 'eps':     0.0050, 'dyn_loss':     0.1991, 'dot_loss':     0.0628, 'ddot_loss':     0.1090, 'rew_loss': 10131.4658, 'lr':   3.26e-07, 'len': 62400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 62400.0000, 'ep_len_e':   200.0000})
Step:   79000, Reward: -1432.101 [ 657.955], Avg: -1295.007 (0.005) <0-02:13:32> ({'r_t': -6178.1283, 'eps':     0.0050, 'dyn_loss':     0.1933, 'dot_loss':     0.0620, 'ddot_loss':     0.1085, 'rew_loss': 10125.3623, 'lr':   3.26e-07, 'len': 63200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 63200.0000, 'ep_len_e':   200.0000})
Step:   80000, Reward: -1225.806 [ 755.051], Avg: -1294.152 (0.005) <0-02:15:18> ({'r_t': -6914.8583, 'eps':     0.0050, 'dyn_loss':     0.1901, 'dot_loss':     0.0614, 'ddot_loss':     0.1080, 'rew_loss': 10113.1143, 'lr':   3.26e-07, 'len': 64000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 64000.0000, 'ep_len_e':   200.0000})
Step:   81000, Reward: -1289.312 [ 670.653], Avg: -1294.093 (0.005) <0-02:17:05> ({'r_t': -6799.6051, 'eps':     0.0050, 'dyn_loss':     0.1847, 'dot_loss':     0.0606, 'ddot_loss':     0.1072, 'rew_loss': 10116.7168, 'lr':   3.26e-07, 'len': 64800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 64800.0000, 'ep_len_e':   200.0000})
Step:   82000, Reward: -1350.602 [ 721.701], Avg: -1294.774 (0.005) <0-02:18:52> ({'r_t': -7133.5558, 'eps':     0.0050, 'dyn_loss':     0.1807, 'dot_loss':     0.0598, 'ddot_loss':     0.1065, 'rew_loss': 10115.5059, 'lr':   3.26e-07, 'len': 65600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 65600.0000, 'ep_len_e':   200.0000})
Step:   83000, Reward:  -997.246 [ 817.562], Avg: -1291.232 (0.005) <0-02:20:38> ({'r_t': -6478.9899, 'eps':     0.0050, 'dyn_loss':     0.1766, 'dot_loss':     0.0591, 'ddot_loss':     0.1059, 'rew_loss': 10116.3574, 'lr':   3.26e-07, 'len': 66400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 66400.0000, 'ep_len_e':   200.0000})
Step:   84000, Reward: -1283.909 [ 788.489], Avg: -1291.146 (0.005) <0-02:22:25> ({'r_t': -6888.7556, 'eps':     0.0050, 'dyn_loss':     0.1724, 'dot_loss':     0.0584, 'ddot_loss':     0.1053, 'rew_loss': 10105.3496, 'lr':   3.26e-07, 'len': 67200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 67200.0000, 'ep_len_e':   200.0000})
Step:   85000, Reward:  -813.498 [ 806.759], Avg: -1285.592 (0.005) <0-02:24:12> ({'r_t': -6491.9281, 'eps':     0.0050, 'dyn_loss':     0.1687, 'dot_loss':     0.0578, 'ddot_loss':     0.1048, 'rew_loss': 10106.0244, 'lr':   3.26e-07, 'len': 68000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 68000.0000, 'ep_len_e':   200.0000})
Step:   86000, Reward: -1481.576 [ 667.867], Avg: -1287.845 (0.005) <0-02:25:59> ({'r_t': -6545.6573, 'eps':     0.0050, 'dyn_loss':     0.1652, 'dot_loss':     0.0570, 'ddot_loss':     0.1040, 'rew_loss': 10094.0283, 'lr':   3.26e-07, 'len': 68800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 68800.0000, 'ep_len_e':   200.0000})
Step:   87000, Reward: -1376.959 [ 743.085], Avg: -1288.857 (0.005) <0-02:27:47> ({'r_t': -6914.4758, 'eps':     0.0050, 'dyn_loss':     0.1613, 'dot_loss':     0.0564, 'ddot_loss':     0.1034, 'rew_loss': 10102.3467, 'lr':   3.26e-07, 'len': 69600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 69600.0000, 'ep_len_e':   200.0000})
Step:   88000, Reward:  -973.312 [ 800.810], Avg: -1285.312 (0.005) <0-02:29:34> ({'r_t': -6829.8613, 'eps':     0.0050, 'dyn_loss':     0.1583, 'dot_loss':     0.0558, 'ddot_loss':     0.1029, 'rew_loss': 10103.3496, 'lr':   3.26e-07, 'len': 70400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 70400.0000, 'ep_len_e':   200.0000})
Step:   89000, Reward: -1470.359 [ 678.224], Avg: -1287.368 (0.005) <0-02:31:21> ({'r_t': -6679.1306, 'eps':     0.0050, 'dyn_loss':     0.1552, 'dot_loss':     0.0552, 'ddot_loss':     0.1022, 'rew_loss': 10084.3340, 'lr':   3.26e-07, 'len': 71200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 71200.0000, 'ep_len_e':   200.0000})
Step:   90000, Reward: -1123.921 [ 828.090], Avg: -1285.572 (0.005) <0-02:33:09> ({'r_t': -6421.0657, 'eps':     0.0050, 'dyn_loss':     0.1513, 'dot_loss':     0.0545, 'ddot_loss':     0.1016, 'rew_loss': 10079.8604, 'lr':   3.26e-07, 'len': 72000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 72000.0000, 'ep_len_e':   200.0000})
Step:   91000, Reward: -1388.544 [ 762.471], Avg: -1286.691 (0.005) <0-02:34:57> ({'r_t': -6492.5769, 'eps':     0.0050, 'dyn_loss':     0.1493, 'dot_loss':     0.0541, 'ddot_loss':     0.1012, 'rew_loss': 10059.3477, 'lr':   3.26e-07, 'len': 72800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 72800.0000, 'ep_len_e':   200.0000})
Step:   92000, Reward: -1014.325 [ 805.653], Avg: -1283.762 (0.005) <0-02:36:45> ({'r_t': -6722.3503, 'eps':     0.0050, 'dyn_loss':     0.1454, 'dot_loss':     0.0533, 'ddot_loss':     0.1004, 'rew_loss': 10060.8818, 'lr':   3.26e-07, 'len': 73600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 73600.0000, 'ep_len_e':   200.0000})
Step:   93000, Reward: -1469.892 [ 655.757], Avg: -1285.743 (0.005) <0-02:38:33> ({'r_t': -6625.1835, 'eps':     0.0050, 'dyn_loss':     0.1431, 'dot_loss':     0.0528, 'ddot_loss':     0.0998, 'rew_loss': 10055.7275, 'lr':   3.26e-07, 'len': 74400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 74400.0000, 'ep_len_e':   200.0000})
Step:   94000, Reward:  -975.926 [ 788.103], Avg: -1282.481 (0.005) <0-02:40:21> ({'r_t': -6526.4312, 'eps':     0.0050, 'dyn_loss':     0.1401, 'dot_loss':     0.0523, 'ddot_loss':     0.0992, 'rew_loss': 10048.5537, 'lr':   3.26e-07, 'len': 75200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 75200.0000, 'ep_len_e':   200.0000})
Step:   95000, Reward: -1085.783 [ 774.121], Avg: -1280.432 (0.005) <0-02:42:09> ({'r_t': -6186.7921, 'eps':     0.0050, 'dyn_loss':     0.1378, 'dot_loss':     0.0517, 'ddot_loss':     0.0987, 'rew_loss': 10027.5898, 'lr':   3.26e-07, 'len': 76000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 76000.0000, 'ep_len_e':   200.0000})
Step:   96000, Reward: -1359.030 [ 747.366], Avg: -1281.243 (0.005) <0-02:43:57> ({'r_t': -6435.7621, 'eps':     0.0050, 'dyn_loss':     0.1347, 'dot_loss':     0.0512, 'ddot_loss':     0.0981, 'rew_loss': 10007.9111, 'lr':   3.26e-07, 'len': 76800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 76800.0000, 'ep_len_e':   200.0000})
Step:   97000, Reward: -1267.480 [ 759.534], Avg: -1281.102 (0.005) <0-02:45:45> ({'r_t': -7015.6602, 'eps':     0.0050, 'dyn_loss':     0.1323, 'dot_loss':     0.0506, 'ddot_loss':     0.0974, 'rew_loss': 10010.4023, 'lr':   3.26e-07, 'len': 77600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 77600.0000, 'ep_len_e':   200.0000})
Step:   98000, Reward: -1178.398 [ 820.724], Avg: -1280.065 (0.005) <0-02:47:34> ({'r_t': -6500.6952, 'eps':     0.0050, 'dyn_loss':     0.1305, 'dot_loss':     0.0502, 'ddot_loss':     0.0969, 'rew_loss': 10007.7979, 'lr':   3.26e-07, 'len': 78400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 78400.0000, 'ep_len_e':   200.0000})
Step:   99000, Reward: -1349.657 [ 718.203], Avg: -1280.761 (0.005) <0-02:49:21> ({'r_t': -6676.0355, 'eps':     0.0050, 'dyn_loss':     0.1277, 'dot_loss':     0.0496, 'ddot_loss':     0.0962, 'rew_loss': 10000.7607, 'lr':   3.26e-07, 'len': 79200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 79200.0000, 'ep_len_e':   200.0000})
Step:  100000, Reward: -1665.995 [ 453.398], Avg: -1284.575 (0.005) <0-02:51:07> ({'r_t': -6444.4895, 'eps':     0.0050, 'dyn_loss':     0.1250, 'dot_loss':     0.0490, 'ddot_loss':     0.0955, 'rew_loss':  9993.3076, 'lr':   3.26e-07, 'len': 80000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 80000.0000, 'ep_len_e':   200.0000})
Step:  101000, Reward: -1369.188 [ 711.488], Avg: -1285.405 (0.005) <0-02:52:53> ({'r_t': -7159.5418, 'eps':     0.0050, 'dyn_loss':     0.1230, 'dot_loss':     0.0485, 'ddot_loss':     0.0949, 'rew_loss':  9993.1738, 'lr':   3.26e-07, 'len': 80800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 80800.0000, 'ep_len_e':   200.0000})
Step:  102000, Reward: -1400.436 [ 726.264], Avg: -1286.521 (0.005) <0-02:54:39> ({'r_t': -7025.3268, 'eps':     0.0050, 'dyn_loss':     0.1202, 'dot_loss':     0.0479, 'ddot_loss':     0.0942, 'rew_loss':  9986.7197, 'lr':   3.26e-07, 'len': 81600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 81600.0000, 'ep_len_e':   200.0000})
Step:  103000, Reward: -1296.794 [ 746.296], Avg: -1286.620 (0.005) <0-02:56:25> ({'r_t': -6471.1800, 'eps':     0.0050, 'dyn_loss':     0.1186, 'dot_loss':     0.0474, 'ddot_loss':     0.0935, 'rew_loss':  9989.0518, 'lr':   3.26e-07, 'len': 82400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 82400.0000, 'ep_len_e':   200.0000})
Step:  104000, Reward: -1168.821 [ 811.432], Avg: -1285.498 (0.005) <0-02:58:11> ({'r_t': -6647.9737, 'eps':     0.0050, 'dyn_loss':     0.1163, 'dot_loss':     0.0470, 'ddot_loss':     0.0930, 'rew_loss':  9973.2363, 'lr':   3.26e-07, 'len': 83200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 83200.0000, 'ep_len_e':   200.0000})
Step:  105000, Reward: -1297.203 [ 814.492], Avg: -1285.609 (0.005) <0-02:59:57> ({'r_t': -6804.9866, 'eps':     0.0050, 'dyn_loss':     0.1143, 'dot_loss':     0.0464, 'ddot_loss':     0.0923, 'rew_loss':  9969.1035, 'lr':   3.26e-07, 'len': 84000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 84000.0000, 'ep_len_e':   200.0000})
Step:  106000, Reward: -1355.240 [ 716.730], Avg: -1286.259 (0.005) <0-03:01:43> ({'r_t': -6283.6214, 'eps':     0.0050, 'dyn_loss':     0.1122, 'dot_loss':     0.0460, 'ddot_loss':     0.0917, 'rew_loss':  9965.5381, 'lr':   3.26e-07, 'len': 84800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 84800.0000, 'ep_len_e':   200.0000})
Step:  107000, Reward: -1232.726 [ 775.955], Avg: -1285.764 (0.005) <0-03:03:29> ({'r_t': -5940.0584, 'eps':     0.0050, 'dyn_loss':     0.1102, 'dot_loss':     0.0455, 'ddot_loss':     0.0912, 'rew_loss':  9941.9199, 'lr':   3.26e-07, 'len': 85600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 85600.0000, 'ep_len_e':   200.0000})
Step:  108000, Reward: -1086.184 [ 867.665], Avg: -1283.933 (0.005) <0-03:05:16> ({'r_t': -6393.6474, 'eps':     0.0050, 'dyn_loss':     0.1088, 'dot_loss':     0.0451, 'ddot_loss':     0.0906, 'rew_loss':  9923.3799, 'lr':   3.26e-07, 'len': 86400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 86400.0000, 'ep_len_e':   200.0000})
Step:  109000, Reward: -1297.345 [ 768.005], Avg: -1284.055 (0.005) <0-03:07:02> ({'r_t': -7055.2618, 'eps':     0.0050, 'dyn_loss':     0.1075, 'dot_loss':     0.0446, 'ddot_loss':     0.0900, 'rew_loss':  9910.0703, 'lr':   3.26e-07, 'len': 87200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 87200.0000, 'ep_len_e':   200.0000})
Step:  110000, Reward: -1473.937 [ 665.908], Avg: -1285.765 (0.005) <0-03:08:50> ({'r_t': -6778.4845, 'eps':     0.0050, 'dyn_loss':     0.1055, 'dot_loss':     0.0442, 'ddot_loss':     0.0895, 'rew_loss':  9913.0654, 'lr':   3.26e-07, 'len': 88000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 88000.0000, 'ep_len_e':   200.0000})
Step:  111000, Reward: -1276.078 [ 803.510], Avg: -1285.679 (0.005) <0-03:10:36> ({'r_t': -6434.7523, 'eps':     0.0050, 'dyn_loss':     0.1033, 'dot_loss':     0.0437, 'ddot_loss':     0.0887, 'rew_loss':  9905.1162, 'lr':   3.26e-07, 'len': 88800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 88800.0000, 'ep_len_e':   200.0000})
Step:  112000, Reward: -1432.214 [ 662.410], Avg: -1286.976 (0.005) <0-03:12:24> ({'r_t': -6830.2924, 'eps':     0.0050, 'dyn_loss':     0.1020, 'dot_loss':     0.0433, 'ddot_loss':     0.0882, 'rew_loss':  9898.0723, 'lr':   3.26e-07, 'len': 89600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 89600.0000, 'ep_len_e':   200.0000})
Step:  113000, Reward: -1358.230 [ 728.705], Avg: -1287.601 (0.005) <0-03:14:11> ({'r_t': -6981.2994, 'eps':     0.0050, 'dyn_loss':     0.1001, 'dot_loss':     0.0428, 'ddot_loss':     0.0875, 'rew_loss':  9902.9072, 'lr':   3.26e-07, 'len': 90400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 90400.0000, 'ep_len_e':   200.0000})
Step:  114000, Reward: -1153.847 [ 808.584], Avg: -1286.438 (0.005) <0-03:15:59> ({'r_t': -6555.0325, 'eps':     0.0050, 'dyn_loss':     0.0985, 'dot_loss':     0.0424, 'ddot_loss':     0.0869, 'rew_loss':  9884.9902, 'lr':   3.26e-07, 'len': 91200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 91200.0000, 'ep_len_e':   200.0000})
Step:  115000, Reward: -1328.347 [ 714.780], Avg: -1286.799 (0.005) <0-03:17:46> ({'r_t': -6734.7546, 'eps':     0.0050, 'dyn_loss':     0.0969, 'dot_loss':     0.0420, 'ddot_loss':     0.0863, 'rew_loss':  9878.5098, 'lr':   3.26e-07, 'len': 92000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 92000.0000, 'ep_len_e':   200.0000})
Step:  116000, Reward: -1686.953 [ 429.514], Avg: -1290.219 (0.005) <0-03:19:34> ({'r_t': -6429.3731, 'eps':     0.0050, 'dyn_loss':     0.0955, 'dot_loss':     0.0416, 'ddot_loss':     0.0857, 'rew_loss':  9871.4863, 'lr':   3.26e-07, 'len': 92800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 92800.0000, 'ep_len_e':   200.0000})
Step:  117000, Reward: -1000.065 [ 819.945], Avg: -1287.760 (0.005) <0-03:21:22> ({'r_t': -6095.6193, 'eps':     0.0050, 'dyn_loss':     0.0940, 'dot_loss':     0.0412, 'ddot_loss':     0.0852, 'rew_loss':  9860.5078, 'lr':   3.26e-07, 'len': 93600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 93600.0000, 'ep_len_e':   200.0000})
Step:  118000, Reward: -1458.463 [ 577.397], Avg: -1289.194 (0.005) <0-03:23:10> ({'r_t': -6480.2197, 'eps':     0.0050, 'dyn_loss':     0.0925, 'dot_loss':     0.0408, 'ddot_loss':     0.0847, 'rew_loss':  9847.6797, 'lr':   3.26e-07, 'len': 94400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 94400.0000, 'ep_len_e':   200.0000})
Step:  119000, Reward: -1197.089 [ 850.115], Avg: -1288.427 (0.005) <0-03:24:58> ({'r_t': -6452.9690, 'eps':     0.0050, 'dyn_loss':     0.0916, 'dot_loss':     0.0405, 'ddot_loss':     0.0841, 'rew_loss':  9818.2324, 'lr':   3.26e-07, 'len': 95200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 95200.0000, 'ep_len_e':   200.0000})
Step:  120000, Reward: -1397.385 [ 656.013], Avg: -1289.327 (0.005) <0-03:26:47> ({'r_t': -7178.8914, 'eps':     0.0050, 'dyn_loss':     0.0902, 'dot_loss':     0.0401, 'ddot_loss':     0.0835, 'rew_loss':  9809.8428, 'lr':   3.26e-07, 'len': 96000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 96000.0000, 'ep_len_e':   200.0000})
Step:  121000, Reward: -1328.084 [ 756.695], Avg: -1289.645 (0.005) <0-03:28:34> ({'r_t': -6939.2242, 'eps':     0.0050, 'dyn_loss':     0.0885, 'dot_loss':     0.0396, 'ddot_loss':     0.0828, 'rew_loss':  9818.8320, 'lr':   3.26e-07, 'len': 96800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 96800.0000, 'ep_len_e':   200.0000})
Step:  122000, Reward: -1609.048 [ 565.838], Avg: -1292.242 (0.005) <0-03:30:23> ({'r_t': -6807.0027, 'eps':     0.0050, 'dyn_loss':     0.0870, 'dot_loss':     0.0393, 'ddot_loss':     0.0823, 'rew_loss':  9811.3496, 'lr':   3.26e-07, 'len': 97600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 97600.0000, 'ep_len_e':   200.0000})
Step:  123000, Reward: -1133.088 [ 796.706], Avg: -1290.958 (0.005) <0-03:32:11> ({'r_t': -6783.5948, 'eps':     0.0050, 'dyn_loss':     0.0857, 'dot_loss':     0.0389, 'ddot_loss':     0.0817, 'rew_loss':  9798.3418, 'lr':   3.26e-07, 'len': 98400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 98400.0000, 'ep_len_e':   200.0000})
Step:  124000, Reward:  -857.739 [ 812.721], Avg: -1287.493 (0.005) <0-03:34:00> ({'r_t': -6476.9946, 'eps':     0.0050, 'dyn_loss':     0.0844, 'dot_loss':     0.0385, 'ddot_loss':     0.0811, 'rew_loss':  9792.0020, 'lr':   3.26e-07, 'len': 99200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 99200.0000, 'ep_len_e':   200.0000})
Step:  125000, Reward: -1382.394 [ 755.430], Avg: -1288.246 (0.005) <0-03:35:48> ({'r_t': -6186.3652, 'eps':     0.0050, 'dyn_loss':     0.0830, 'dot_loss':     0.0381, 'ddot_loss':     0.0805, 'rew_loss':  9791.8740, 'lr':   3.26e-07, 'len': 100000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 100000.0000, 'ep_len_e':   200.0000})
Step:  126000, Reward: -1459.353 [ 656.634], Avg: -1289.593 (0.005) <0-03:37:37> ({'r_t': -6532.6227, 'eps':     0.0050, 'dyn_loss':     0.0816, 'dot_loss':     0.0377, 'ddot_loss':     0.0799, 'rew_loss':  9772.9883, 'lr':   3.26e-07, 'len': 100800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 100800.0000, 'ep_len_e':   200.0000})
Step:  127000, Reward: -1221.777 [ 745.869], Avg: -1289.063 (0.005) <0-03:39:26> ({'r_t': -6553.6710, 'eps':     0.0050, 'dyn_loss':     0.0806, 'dot_loss':     0.0374, 'ddot_loss':     0.0793, 'rew_loss':  9761.3994, 'lr':   3.26e-07, 'len': 101600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 101600.0000, 'ep_len_e':   200.0000})
Step:  128000, Reward: -1066.854 [ 801.923], Avg: -1287.341 (0.005) <0-03:41:15> ({'r_t': -6592.8883, 'eps':     0.0050, 'dyn_loss':     0.0799, 'dot_loss':     0.0371, 'ddot_loss':     0.0789, 'rew_loss':  9738.0498, 'lr':   3.26e-07, 'len': 102400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 102400.0000, 'ep_len_e':   200.0000})
Step:  129000, Reward: -1196.301 [ 779.966], Avg: -1286.640 (0.005) <0-03:43:04> ({'r_t': -6612.9171, 'eps':     0.0050, 'dyn_loss':     0.0785, 'dot_loss':     0.0367, 'ddot_loss':     0.0782, 'rew_loss':  9732.7217, 'lr':   3.26e-07, 'len': 103200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 103200.0000, 'ep_len_e':   200.0000})
Step:  130000, Reward:  -969.207 [ 754.997], Avg: -1284.217 (0.005) <0-03:44:54> ({'r_t': -6945.4712, 'eps':     0.0050, 'dyn_loss':     0.0774, 'dot_loss':     0.0364, 'ddot_loss':     0.0777, 'rew_loss':  9727.2305, 'lr':   3.26e-07, 'len': 104000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 104000.0000, 'ep_len_e':   200.0000})
Step:  131000, Reward: -1390.186 [ 643.140], Avg: -1285.020 (0.005) <0-03:46:44> ({'r_t': -6309.3679, 'eps':     0.0050, 'dyn_loss':     0.0764, 'dot_loss':     0.0361, 'ddot_loss':     0.0772, 'rew_loss':  9710.0479, 'lr':   3.26e-07, 'len': 104800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 104800.0000, 'ep_len_e':   200.0000})
Step:  132000, Reward: -1113.351 [ 803.103], Avg: -1283.729 (0.005) <0-03:48:33> ({'r_t': -6728.2005, 'eps':     0.0050, 'dyn_loss':     0.0752, 'dot_loss':     0.0357, 'ddot_loss':     0.0767, 'rew_loss':  9695.5879, 'lr':   3.26e-07, 'len': 105600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 105600.0000, 'ep_len_e':   200.0000})
Step:  133000, Reward: -1329.642 [ 689.610], Avg: -1284.072 (0.005) <0-03:50:23> ({'r_t': -7073.0705, 'eps':     0.0050, 'dyn_loss':     0.0741, 'dot_loss':     0.0353, 'ddot_loss':     0.0759, 'rew_loss':  9689.9824, 'lr':   3.26e-07, 'len': 106400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 106400.0000, 'ep_len_e':   200.0000})
Step:  134000, Reward: -1010.318 [ 782.679], Avg: -1282.044 (0.005) <0-03:52:13> ({'r_t': -6678.7010, 'eps':     0.0050, 'dyn_loss':     0.0730, 'dot_loss':     0.0350, 'ddot_loss':     0.0755, 'rew_loss':  9690.1260, 'lr':   3.26e-07, 'len': 107200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 107200.0000, 'ep_len_e':   200.0000})
Step:  135000, Reward: -1436.704 [ 658.322], Avg: -1283.181 (0.005) <0-03:54:03> ({'r_t': -6454.1386, 'eps':     0.0050, 'dyn_loss':     0.0720, 'dot_loss':     0.0347, 'ddot_loss':     0.0750, 'rew_loss':  9677.5742, 'lr':   3.26e-07, 'len': 108000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 108000.0000, 'ep_len_e':   200.0000})
Step:  136000, Reward: -1399.536 [ 630.878], Avg: -1284.031 (0.005) <0-03:55:52> ({'r_t': -6719.2192, 'eps':     0.0050, 'dyn_loss':     0.0712, 'dot_loss':     0.0344, 'ddot_loss':     0.0744, 'rew_loss':  9658.4678, 'lr':   3.26e-07, 'len': 108800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 108800.0000, 'ep_len_e':   200.0000})
Step:  137000, Reward: -1397.246 [ 621.685], Avg: -1284.851 (0.005) <0-03:57:43> ({'r_t': -7082.3021, 'eps':     0.0050, 'dyn_loss':     0.0699, 'dot_loss':     0.0340, 'ddot_loss':     0.0738, 'rew_loss':  9649.3477, 'lr':   3.26e-07, 'len': 109600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 109600.0000, 'ep_len_e':   200.0000})
Step:  138000, Reward: -1430.159 [ 528.280], Avg: -1285.896 (0.005) <0-03:59:32> ({'r_t': -6489.5716, 'eps':     0.0050, 'dyn_loss':     0.0690, 'dot_loss':     0.0337, 'ddot_loss':     0.0732, 'rew_loss':  9650.2324, 'lr':   3.26e-07, 'len': 110400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 110400.0000, 'ep_len_e':   200.0000})
Step:  139000, Reward: -1199.797 [ 713.499], Avg: -1285.281 (0.005) <0-04:01:23> ({'r_t': -6417.9764, 'eps':     0.0050, 'dyn_loss':     0.0681, 'dot_loss':     0.0334, 'ddot_loss':     0.0727, 'rew_loss':  9632.2334, 'lr':   3.26e-07, 'len': 111200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 111200.0000, 'ep_len_e':   200.0000})
Step:  140000, Reward: -1108.393 [ 713.588], Avg: -1284.027 (0.005) <0-04:03:12> ({'r_t': -6357.9098, 'eps':     0.0050, 'dyn_loss':     0.0672, 'dot_loss':     0.0331, 'ddot_loss':     0.0721, 'rew_loss':  9611.2344, 'lr':   3.26e-07, 'len': 112000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 112000.0000, 'ep_len_e':   200.0000})
Step:  141000, Reward: -1040.273 [ 706.965], Avg: -1282.310 (0.005) <0-04:05:03> ({'r_t': -6634.7700, 'eps':     0.0050, 'dyn_loss':     0.0666, 'dot_loss':     0.0328, 'ddot_loss':     0.0717, 'rew_loss':  9605.0654, 'lr':   3.26e-07, 'len': 112800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 112800.0000, 'ep_len_e':   200.0000})
Step:  142000, Reward: -1304.435 [ 682.416], Avg: -1282.465 (0.005) <0-04:06:53> ({'r_t': -6250.8920, 'eps':     0.0050, 'dyn_loss':     0.0655, 'dot_loss':     0.0325, 'ddot_loss':     0.0711, 'rew_loss':  9589.2139, 'lr':   3.26e-07, 'len': 113600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 113600.0000, 'ep_len_e':   200.0000})
Step:  143000, Reward: -1218.809 [ 703.636], Avg: -1282.023 (0.005) <0-04:08:42> ({'r_t': -6104.5092, 'eps':     0.0050, 'dyn_loss':     0.0646, 'dot_loss':     0.0322, 'ddot_loss':     0.0706, 'rew_loss':  9576.0059, 'lr':   3.26e-07, 'len': 114400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 114400.0000, 'ep_len_e':   200.0000})
Step:  144000, Reward: -1085.446 [ 777.786], Avg: -1280.667 (0.005) <0-04:10:30> ({'r_t': -6241.9608, 'eps':     0.0050, 'dyn_loss':     0.0640, 'dot_loss':     0.0319, 'ddot_loss':     0.0700, 'rew_loss':  9554.1035, 'lr':   3.26e-07, 'len': 115200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 115200.0000, 'ep_len_e':   200.0000})
Step:  145000, Reward:  -970.341 [ 779.515], Avg: -1278.542 (0.005) <0-04:12:18> ({'r_t': -6859.7456, 'eps':     0.0050, 'dyn_loss':     0.0628, 'dot_loss':     0.0316, 'ddot_loss':     0.0695, 'rew_loss':  9551.1748, 'lr':   3.26e-07, 'len': 116000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 116000.0000, 'ep_len_e':   200.0000})
Step:  146000, Reward: -1350.949 [ 601.106], Avg: -1279.034 (0.005) <0-04:14:06> ({'r_t': -6294.9395, 'eps':     0.0050, 'dyn_loss':     0.0625, 'dot_loss':     0.0314, 'ddot_loss':     0.0691, 'rew_loss':  9519.5137, 'lr':   3.26e-07, 'len': 116800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 116800.0000, 'ep_len_e':   200.0000})
Step:  147000, Reward: -1169.928 [ 727.114], Avg: -1278.297 (0.005) <0-04:15:54> ({'r_t': -6756.7951, 'eps':     0.0050, 'dyn_loss':     0.0615, 'dot_loss':     0.0311, 'ddot_loss':     0.0687, 'rew_loss':  9513.1465, 'lr':   3.26e-07, 'len': 117600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 117600.0000, 'ep_len_e':   200.0000})
Step:  148000, Reward: -1227.187 [ 650.057], Avg: -1277.954 (0.005) <0-04:17:43> ({'r_t': -6879.7045, 'eps':     0.0050, 'dyn_loss':     0.0607, 'dot_loss':     0.0308, 'ddot_loss':     0.0680, 'rew_loss':  9506.8457, 'lr':   3.26e-07, 'len': 118400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 118400.0000, 'ep_len_e':   200.0000})
Step:  149000, Reward: -1338.702 [ 614.950], Avg: -1278.359 (0.005) <0-04:19:31> ({'r_t': -6584.9065, 'eps':     0.0050, 'dyn_loss':     0.0597, 'dot_loss':     0.0305, 'ddot_loss':     0.0675, 'rew_loss':  9495.7021, 'lr':   3.26e-07, 'len': 119200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 119200.0000, 'ep_len_e':   200.0000})
Step:  150000, Reward: -1166.172 [ 724.367], Avg: -1277.616 (0.005) <0-04:21:19> ({'r_t': -6545.4658, 'eps':     0.0050, 'dyn_loss':     0.0588, 'dot_loss':     0.0302, 'ddot_loss':     0.0669, 'rew_loss':  9485.5176, 'lr':   3.26e-07, 'len': 120000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 120000.0000, 'ep_len_e':   200.0000})
Step:  151000, Reward:  -878.627 [ 730.939], Avg: -1274.991 (0.005) <0-04:23:08> ({'r_t': -6666.4867, 'eps':     0.0050, 'dyn_loss':     0.0584, 'dot_loss':     0.0300, 'ddot_loss':     0.0665, 'rew_loss':  9471.3936, 'lr':   3.26e-07, 'len': 120800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 120800.0000, 'ep_len_e':   200.0000})
Step:  152000, Reward: -1364.855 [ 600.834], Avg: -1275.579 (0.005) <0-04:24:56> ({'r_t': -6991.6717, 'eps':     0.0050, 'dyn_loss':     0.0578, 'dot_loss':     0.0297, 'ddot_loss':     0.0659, 'rew_loss':  9472.8359, 'lr':   3.26e-07, 'len': 121600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 121600.0000, 'ep_len_e':   200.0000})
Step:  153000, Reward: -1200.894 [ 675.337], Avg: -1275.094 (0.005) <0-04:26:44> ({'r_t': -6813.7739, 'eps':     0.0050, 'dyn_loss':     0.0570, 'dot_loss':     0.0293, 'ddot_loss':     0.0654, 'rew_loss':  9458.6436, 'lr':   3.26e-07, 'len': 122400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 122400.0000, 'ep_len_e':   200.0000})
Step:  154000, Reward: -1176.772 [ 727.495], Avg: -1274.459 (0.005) <0-04:28:32> ({'r_t': -6702.6953, 'eps':     0.0050, 'dyn_loss':     0.0560, 'dot_loss':     0.0291, 'ddot_loss':     0.0650, 'rew_loss':  9440.6855, 'lr':   3.26e-07, 'len': 123200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 123200.0000, 'ep_len_e':   200.0000})
Step:  155000, Reward:  -887.088 [ 796.622], Avg: -1271.976 (0.005) <0-04:30:22> ({'r_t': -6573.0959, 'eps':     0.0050, 'dyn_loss':     0.0555, 'dot_loss':     0.0289, 'ddot_loss':     0.0645, 'rew_loss':  9430.1084, 'lr':   3.26e-07, 'len': 124000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 124000.0000, 'ep_len_e':   200.0000})
Step:  156000, Reward: -1258.860 [ 644.708], Avg: -1271.893 (0.005) <0-04:32:10> ({'r_t': -6580.8878, 'eps':     0.0050, 'dyn_loss':     0.0550, 'dot_loss':     0.0286, 'ddot_loss':     0.0640, 'rew_loss':  9428.6787, 'lr':   3.26e-07, 'len': 124800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 124800.0000, 'ep_len_e':   200.0000})
Step:  157000, Reward: -1262.780 [ 643.899], Avg: -1271.835 (0.005) <0-04:33:59> ({'r_t': -5977.6441, 'eps':     0.0050, 'dyn_loss':     0.0543, 'dot_loss':     0.0284, 'ddot_loss':     0.0636, 'rew_loss':  9392.0859, 'lr':   3.26e-07, 'len': 125600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 125600.0000, 'ep_len_e':   200.0000})
Step:  158000, Reward: -1252.819 [ 658.146], Avg: -1271.715 (0.005) <0-04:35:49> ({'r_t': -6757.6485, 'eps':     0.0050, 'dyn_loss':     0.0535, 'dot_loss':     0.0281, 'ddot_loss':     0.0632, 'rew_loss':  9394.8584, 'lr':   3.26e-07, 'len': 126400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 126400.0000, 'ep_len_e':   200.0000})
Step:  159000, Reward:  -981.848 [ 775.596], Avg: -1269.904 (0.005) <0-04:37:38> ({'r_t': -6589.8846, 'eps':     0.0050, 'dyn_loss':     0.0528, 'dot_loss':     0.0278, 'ddot_loss':     0.0626, 'rew_loss':  9370.3584, 'lr':   3.26e-07, 'len': 127200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 127200.0000, 'ep_len_e':   200.0000})
Step:  160000, Reward: -1267.706 [ 684.051], Avg: -1269.890 (0.005) <0-04:39:27> ({'r_t': -6100.8560, 'eps':     0.0050, 'dyn_loss':     0.0522, 'dot_loss':     0.0276, 'ddot_loss':     0.0622, 'rew_loss':  9367.1943, 'lr':   3.26e-07, 'len': 128000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 128000.0000, 'ep_len_e':   200.0000})
Step:  161000, Reward: -1034.015 [ 743.040], Avg: -1268.434 (0.005) <0-04:41:17> ({'r_t': -6809.8364, 'eps':     0.0050, 'dyn_loss':     0.0516, 'dot_loss':     0.0274, 'ddot_loss':     0.0618, 'rew_loss':  9343.8730, 'lr':   3.26e-07, 'len': 128800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 128800.0000, 'ep_len_e':   200.0000})
Step:  162000, Reward: -1464.864 [ 514.565], Avg: -1269.639 (0.005) <0-04:43:07> ({'r_t': -6542.5189, 'eps':     0.0050, 'dyn_loss':     0.0509, 'dot_loss':     0.0271, 'ddot_loss':     0.0613, 'rew_loss':  9340.4346, 'lr':   3.26e-07, 'len': 129600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 129600.0000, 'ep_len_e':   200.0000})
Step:  163000, Reward: -1252.846 [ 670.880], Avg: -1269.537 (0.005) <0-04:44:57> ({'r_t': -6859.6105, 'eps':     0.0050, 'dyn_loss':     0.0505, 'dot_loss':     0.0269, 'ddot_loss':     0.0608, 'rew_loss':  9319.8574, 'lr':   3.26e-07, 'len': 130400.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 130400.0000, 'ep_len_e':   200.0000})
Step:  164000, Reward: -1144.990 [ 669.142], Avg: -1268.782 (0.005) <0-04:46:47> ({'r_t': -6531.0586, 'eps':     0.0050, 'dyn_loss':     0.0498, 'dot_loss':     0.0267, 'ddot_loss':     0.0604, 'rew_loss':  9309.2324, 'lr':   3.26e-07, 'len': 131200.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 131200.0000, 'ep_len_e':   200.0000})
Step:  165000, Reward: -1104.112 [ 687.151], Avg: -1267.790 (0.005) <0-04:48:38> ({'r_t': -6903.0867, 'eps':     0.0050, 'dyn_loss':     0.0494, 'dot_loss':     0.0264, 'ddot_loss':     0.0598, 'rew_loss':  9304.9717, 'lr':   3.26e-07, 'len': 132000.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 132000.0000, 'ep_len_e':   200.0000})
Step:  166000, Reward: -1331.373 [ 589.943], Avg: -1268.171 (0.005) <0-04:50:27> ({'r_t': -6442.8812, 'eps':     0.0050, 'dyn_loss':     0.0487, 'dot_loss':     0.0262, 'ddot_loss':     0.0595, 'rew_loss':  9285.6025, 'lr':   3.26e-07, 'len': 132800.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 132800.0000, 'ep_len_e':   200.0000})
Step:  167000, Reward: -1029.207 [ 742.342], Avg: -1266.748 (0.005) <0-04:52:17> ({'r_t': -6653.6008, 'eps':     0.0050, 'dyn_loss':     0.0481, 'dot_loss':     0.0260, 'ddot_loss':     0.0590, 'rew_loss':  9276.5156, 'lr':   3.26e-07, 'len': 133600.0000, 'ep_len':   200.0000, 'eps_e':     0.0050, 'lr_e':   3.26e-07, 'len_e': 133600.0000, 'ep_len_e':   200.0000})
