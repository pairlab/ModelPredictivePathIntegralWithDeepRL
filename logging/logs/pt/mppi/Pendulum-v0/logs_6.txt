Model: <class 'src.models.pytorch.mpc.mppi.MPPIAgent'>, Env: Pendulum-v0, Date: 04/06/2020 16:46:31
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: c5ac20e17d74f53946f224da38da23cdc46e034e
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.98
   NUM_STEPS = None
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 2500
   TARGET_UPDATE_RATE = 0.0004
   BATCH_SIZE = 250
   DYN_EPOCHS = 1
   TRAIN_EVERY = 1000
   ENV_MODEL = dfrntl
   MPC = 
      NSAMPLES = 100
      HORIZON = 40
      LAMBDA = 0.1
      COV = 0.5
   dynamics_size = 3
   state_size = (3,)
   action_size = (1,)
   env_name = Pendulum-v0
   rank = 0
   size = 17
   split = 17
   model = mppi
   framework = pt
   train_prop = 1.0
   tcp_ports = [9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 100000
   render = False
   trial = False
   icm = False
   rs = False
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.0001
      TRANSITION_HIDDEN = 512
      REWARD_HIDDEN = 256
      BETA_DYN = 1
      BETA_DOT = 0
      BETA_DDOT = 0,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f6cd80cbe50> 
	env = <GymEnv<TimeLimit<PendulumEnv<Pendulum-v0>>>> 
		env = <TimeLimit<PendulumEnv<Pendulum-v0>>> 
			env = <PendulumEnv<Pendulum-v0>> 
				max_speed = 8
				max_torque = 2.0
				dt = 0.05
				g = 10.0
				m = 1.0
				l = 1.0
				viewer = None
				action_space = Box(1,) 
					dtype = float32
					shape = (1,)
					low = [-2.000]
					high = [ 2.000]
					bounded_below = [ True]
					bounded_above = [ True]
					np_random = RandomState(MT19937)
				observation_space = Box(3,) 
					dtype = float32
					shape = (3,)
					low = [-1.000 -1.000 -8.000]
					high = [ 1.000  1.000  8.000]
					bounded_below = [ True  True  True]
					bounded_above = [ True  True  True]
					np_random = RandomState(MT19937)
				np_random = RandomState(MT19937)
				spec = EnvSpec(Pendulum-v0) 
					id = Pendulum-v0
					entry_point = gym.envs.classic_control:PendulumEnv
					reward_threshold = None
					nondeterministic = False
					max_episode_steps = 200
				verbose = 0
			action_space = Box(1,) 
				dtype = float32
				shape = (1,)
				low = [-2.000]
				high = [ 2.000]
				bounded_below = [ True]
				bounded_above = [ True]
				np_random = RandomState(MT19937)
			observation_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -8.000]
				high = [ 1.000  1.000  8.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			reward_range = (-inf, inf)
			metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}
		action_space = Box(1,) 
			dtype = float32
			shape = (1,)
			low = [-2.000]
			high = [ 2.000]
			bounded_below = [ True]
			bounded_above = [ True]
			np_random = RandomState(MT19937)
		observation_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -8.000]
			high = [ 1.000  1.000  8.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f6cd2285a10> 
			observation_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -8.000]
				high = [ 1.000  1.000  8.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
	state_size = (3,)
	action_size = (1,)
	action_space = Box(1,) 
		dtype = float32
		shape = (1,)
		low = [-2.000]
		high = [ 2.000]
		bounded_below = [ True]
		bounded_above = [ True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7f6cd2285390> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {9001: <socket.socket fd=34, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46008), raddr=('127.0.0.1', 9001)>, 9002: <socket.socket fd=35, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 33338), raddr=('127.0.0.1', 9002)>, 9003: <socket.socket fd=91, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46770), raddr=('127.0.0.1', 9003)>, 9004: <socket.socket fd=93, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 35510), raddr=('127.0.0.1', 9004)>, 9005: <socket.socket fd=95, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46086), raddr=('127.0.0.1', 9005)>, 9006: <socket.socket fd=96, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 50700), raddr=('127.0.0.1', 9006)>, 9007: <socket.socket fd=115, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 50902), raddr=('127.0.0.1', 9007)>, 9008: <socket.socket fd=117, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 41222), raddr=('127.0.0.1', 9008)>, 9009: <socket.socket fd=120, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 33936), raddr=('127.0.0.1', 9009)>, 9010: <socket.socket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 41104), raddr=('127.0.0.1', 9010)>, 9011: <socket.socket fd=198, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 45518), raddr=('127.0.0.1', 9011)>, 9012: <socket.socket fd=199, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48474), raddr=('127.0.0.1', 9012)>, 9013: <socket.socket fd=200, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 43740), raddr=('127.0.0.1', 9013)>, 9014: <socket.socket fd=201, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 58212), raddr=('127.0.0.1', 9014)>, 9015: <socket.socket fd=202, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 56424), raddr=('127.0.0.1', 9015)>, 9016: <socket.socket fd=203, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 43062), raddr=('127.0.0.1', 9016)>}
	num_envs = 16
	max_steps = 200,
agent: <src.models.wrappers.ParallelAgent object at 0x7f6ce1ae6a90> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f6cd213d210> 
		state_size = (3,)
	agent = <src.models.pytorch.mpc.mppi.MPPIAgent object at 0x7f6cd213d290> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f6cd2157fd0> 
			size = (1,)
			dt = 0.2
			action = [ 1.000]
			daction_dt = [-0.519]
		discrete = False
		action_size = (1,)
		state_size = (3,)
		config = <src.utils.config.Config object at 0x7f6cd80e5650> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.98
			NUM_STEPS = None
			MAX_BUFFER_SIZE = 100000
			REPLAY_BATCH_SIZE = 2500
			TARGET_UPDATE_RATE = 0.0004
			BATCH_SIZE = 250
			DYN_EPOCHS = 1
			TRAIN_EVERY = 1000
			ENV_MODEL = dfrntl
			MPC = <src.utils.config.Config object at 0x7f6d781aad50> 
				NSAMPLES = 100
				HORIZON = 40
				LAMBDA = 0.1
				COV = 0.5
			dynamics_size = 3
			state_size = (3,)
			action_size = (1,)
			env_name = Pendulum-v0
			rank = 0
			size = 17
			split = 17
			model = mppi
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 100000
			render = False
			trial = False
			icm = False
			rs = False
			DYN = <src.utils.config.Config object at 0x7f6cd80d1bd0> 
				REG_LAMBDA = 1e-06
				FACTOR = 0.5
				PATIENCE = 5
				LEARN_RATE = 0.0001
				TRANSITION_HIDDEN = 512
				REWARD_HIDDEN = 256
				BETA_DYN = 1
				BETA_DOT = 0
				BETA_DDOT = 0
		stats = <src.utils.logger.Stats object at 0x7f6cd2298f50> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = MPPIController() 
			training = True
			tau = 0.0004
			name = mppi
			stats = <src.utils.logger.Stats object at 0x7f6cd1eb1790> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7f6cd80e5650> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.98
				NUM_STEPS = None
				MAX_BUFFER_SIZE = 100000
				REPLAY_BATCH_SIZE = 2500
				TARGET_UPDATE_RATE = 0.0004
				BATCH_SIZE = 250
				DYN_EPOCHS = 1
				TRAIN_EVERY = 1000
				ENV_MODEL = dfrntl
				MPC = <src.utils.config.Config object at 0x7f6d781aad50> 
					NSAMPLES = 100
					HORIZON = 40
					LAMBDA = 0.1
					COV = 0.5
				dynamics_size = 3
				state_size = (3,)
				action_size = (1,)
				env_name = Pendulum-v0
				rank = 0
				size = 17
				split = 17
				model = mppi
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 100000
				render = False
				trial = False
				icm = False
				rs = False
				DYN = <src.utils.config.Config object at 0x7f6cd80d1bd0> 
					REG_LAMBDA = 1e-06
					FACTOR = 0.5
					PATIENCE = 5
					LEARN_RATE = 0.0001
					TRANSITION_HIDDEN = 512
					REWARD_HIDDEN = 256
					BETA_DYN = 1
					BETA_DOT = 0
					BETA_DDOT = 0
			device = cuda
			envmodel = <src.models.pytorch.mpc.EnvModel object at 0x7f6cd1eb17d0> 
				network = DifferentialEnv(
					  (reward): RewardModel(
					    (linear1): Linear(in_features=7, out_features=256, bias=True)
					    (drop1): Dropout(p=0.5, inplace=False)
					    (linear2): Linear(in_features=256, out_features=256, bias=True)
					    (drop2): Dropout(p=0.5, inplace=False)
					    (linear3): Linear(in_features=256, out_features=256, bias=True)
					    (linear4): Linear(in_features=256, out_features=1, bias=True)
					  )
					  (dynamics): TransitionModel(
					    (gru): GRUCell(7, 512)
					    (linear1): Linear(in_features=512, out_features=512, bias=True)
					    (drop1): Dropout(p=0.5, inplace=False)
					    (linear2): Linear(in_features=512, out_features=512, bias=True)
					    (drop2): Dropout(p=0.5, inplace=False)
					    (state_ddot): Linear(in_features=512, out_features=3, bias=True)
					  )
					) 
					training = True
					tau = 0.0004
					name = dfrntl
					stats = <src.utils.logger.Stats object at 0x7f6cd1eb1850> 
						mean_dict = {}
						sum_dict = {}
					config = <src.utils.config.Config object at 0x7f6cd80e5650> 
						TRIAL_AT = 1000
						SAVE_AT = 1
						SEED = 0
						REG_LAMBDA = 1e-06
						LEARN_RATE = 0.0001
						DISCOUNT_RATE = 0.99
						ADVANTAGE_DECAY = 0.95
						INPUT_LAYER = 512
						ACTOR_HIDDEN = 256
						CRITIC_HIDDEN = 1024
						EPS_MAX = 1.0
						EPS_MIN = 0.1
						EPS_DECAY = 0.98
						NUM_STEPS = None
						MAX_BUFFER_SIZE = 100000
						REPLAY_BATCH_SIZE = 2500
						TARGET_UPDATE_RATE = 0.0004
						BATCH_SIZE = 250
						DYN_EPOCHS = 1
						TRAIN_EVERY = 1000
						ENV_MODEL = dfrntl
						MPC = <src.utils.config.Config object at 0x7f6d781aad50> 
							NSAMPLES = 100
							HORIZON = 40
							LAMBDA = 0.1
							COV = 0.5
						dynamics_size = 3
						state_size = (3,)
						action_size = (1,)
						env_name = Pendulum-v0
						rank = 0
						size = 17
						split = 17
						model = mppi
						framework = pt
						train_prop = 1.0
						tcp_ports = <list len=17>
						tcp_rank = 0
						num_envs = 1
						nsteps = 100000
						render = False
						trial = False
						icm = False
						rs = False
						DYN = <src.utils.config.Config object at 0x7f6cd80d1bd0> 
							REG_LAMBDA = 1e-06
							FACTOR = 0.5
							PATIENCE = 5
							LEARN_RATE = 0.0001
							TRANSITION_HIDDEN = 512
							REWARD_HIDDEN = 256
							BETA_DYN = 1
							BETA_DOT = 0
							BETA_DDOT = 0
					device = cuda
					state_size = (3,)
					action_size = (1,)
					discrete = False
					dyn_index = 3
					optimizer = Adam (
					Parameter Group 0
					    amsgrad: False
					    betas: (0.9, 0.999)
					    eps: 1e-08
					    lr: 0.0001
					    weight_decay: 1e-06
					)
					scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6cd222f8d0>
				state_size = (3,)
				action_size = (1,)
			mu = [ 0.000]
			cov = [[ 0.500]]
			icov = [[ 2.000]]
			lamda = 0.1
			horizon = 40
			nsamples = 100
			action_size = (1,)
			control = [[[ 8.294e-01]
			  [ 4.550e-04]
			  [ 1.968e-01]
			  [-8.368e-01]
			  [ 7.319e-01]
			  [ 4.384e-01]
			  [-1.022e-01]
			  [ 7.010e-01]
			  [-1.800e-01]
			  [ 1.093e-01]
			  [-7.425e-01]
			  [-4.122e-01]
			  [ 1.701e-01]
			  [-9.027e-01]
			  [ 1.423e-01]
			  [ 5.595e-01]
			  [ 3.348e-01]
			  [-7.984e-01]
			  [-8.243e-01]
			  [ 9.785e-01]
			  [-5.052e-01]
			  [-2.572e-01]
			  [-9.959e-01]
			  [-6.973e-01]
			  [-8.253e-01]
			  [-2.794e-01]
			  [ 8.149e-01]
			  [-7.471e-01]
			  [ 4.451e-01]
			  [ 1.855e-01]
			  [-6.929e-01]
			  [ 9.881e-02]
			  [ 4.785e-01]
			  [ 6.261e-01]
			  [-9.436e-01]
			  [ 3.269e-01]
			  [ 2.269e-01]
			  [-1.085e-02]
			  [ 6.512e-01]
			  [ 6.176e-01]]]
			noise = [[[[ 0.152]
			   [ 1.321]
			   [-0.159]
			   ...
			   [-0.005]
			   [ 0.125]
			   [ 0.148]]
			
			  [[-0.080]
			   [ 0.697]
			   [ 0.694]
			   ...
			   [ 0.486]
			   [ 0.408]
			   [ 0.820]]
			
			  [[-0.516]
			   [ 0.100]
			   [-0.705]
			   ...
			   [-0.137]
			   [ 1.297]
			   [-0.682]]
			
			  ...
			
			  [[ 0.267]
			   [-0.067]
			   [-0.653]
			   ...
			   [-0.500]
			   [-0.234]
			   [ 0.079]]
			
			  [[-1.058]
			   [ 0.503]
			   [-0.924]
			   ...
			   [ 0.448]
			   [ 0.075]
			   [-0.443]]
			
			  [[ 0.685]
			   [ 0.415]
			   [ 0.579]
			   ...
			   [ 0.741]
			   [-1.327]
			   [-0.616]]]]
			init_cost = [[-2.762e+00  2.277e-01  7.352e+00 -5.949e+00 -6.614e-01  5.079e-01 -2.758e+00 -7.975e+00 -6.937e+00  1.501e+00  5.169e+00 -6.435e+00  2.034e+00  7.918e+00 -1.900e+01  6.454e+00  3.322e-01 -1.069e+01 -1.085e+00 -4.647e+00 -3.377e+00 -1.301e+00  9.782e+00  8.633e-01 -3.806e+00 -3.098e-01  1.270e+00 -7.000e-01 -4.227e+00  1.025e+01  1.774e+00  2.420e+00 -1.138e+00 -6.064e-02 -1.408e+01  1.243e+00 -9.460e+00 -1.948e-01 -2.171e+00  1.879e+00  3.039e+00  7.887e+00  7.463e+00 -8.620e+00 -4.121e+00  1.042e+00 -1.855e+00 -4.255e+00  3.667e+00 -5.536e+00 -3.024e-01 -3.947e+00  6.545e+00 -1.012e+01  2.272e+00  1.225e-02  5.747e-01 -3.822e+00 -1.901e+00  1.943e+00  4.386e+00 -2.736e+00  3.235e+00  8.379e-01 -7.926e+00 -1.872e+00 -5.526e+00 -1.334e+00 -4.095e+00 -3.232e+00 -1.196e+00 -1.116e+01  1.841e+00  1.512e+00  8.921e+00  2.212e+00 -7.109e+00  1.804e+00  2.572e+00  8.280e+00 -8.513e+00  6.441e+00  2.046e+00  7.772e+00 -1.061e+00  8.636e+00  1.177e+01  9.793e-02 -2.851e+00  2.042e+00  5.354e+00 -5.287e+00 -5.148e+00  9.290e-01  1.902e+00 -7.680e-01  8.490e+00  8.612e+00  3.099e+00 -3.462e+00]]
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f6cd1eb1f10> 
			buffer = deque([], maxlen=100000)
		buffer = []
		dataset = <class 'src.data.loaders.OnlineDataset'>
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f6cd1ec66d0> 
		size = (1,)
		dt = 0.2
		action = [ 0.651]
		daction_dt = [-0.087]
	discrete = False
	action_size = (1,)
	state_size = (3,)
	config = <src.utils.config.Config object at 0x7f6cd80e5650> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.98
		NUM_STEPS = None
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 2500
		TARGET_UPDATE_RATE = 0.0004
		BATCH_SIZE = 250
		DYN_EPOCHS = 1
		TRAIN_EVERY = 1000
		ENV_MODEL = dfrntl
		MPC = <src.utils.config.Config object at 0x7f6d781aad50> 
			NSAMPLES = 100
			HORIZON = 40
			LAMBDA = 0.1
			COV = 0.5
		dynamics_size = 3
		state_size = (3,)
		action_size = (1,)
		env_name = Pendulum-v0
		rank = 0
		size = 17
		split = 17
		model = mppi
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 100000
		render = False
		trial = False
		icm = False
		rs = False
		DYN = <src.utils.config.Config object at 0x7f6cd80d1bd0> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.0001
			TRANSITION_HIDDEN = 512
			REWARD_HIDDEN = 256
			BETA_DYN = 1
			BETA_DOT = 0
			BETA_DDOT = 0
	stats = <src.utils.logger.Stats object at 0x7f6cd0cf2410> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import tqdm
import torch
import random
import numpy as np
import scipy as sp
from scipy.stats import multivariate_normal
from src.utils.rand import RandomAgent, ReplayBuffer
from src.utils.misc import load_module
from ..agents.base import PTNetwork, PTAgent, Conv, one_hot_from_indices
from . import EnvModel

class MPPIController(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="mppi"):
		super().__init__(config, gpu=gpu, name=name)
		self.envmodel = EnvModel(state_size, action_size, config, load=load, gpu=gpu)
		self.mu = np.zeros(action_size)
		self.cov = np.diag(np.ones(action_size))*config.MPC.COV
		self.icov = np.linalg.inv(self.cov)
		self.lamda = config.MPC.LAMBDA
		self.horizon = config.MPC.HORIZON
		self.nsamples = config.MPC.NSAMPLES
		self.config = config
		self.action_size = action_size
		self.init_control()

	def get_action(self, state, eps=None, sample=True):
		batch = state.shape[:-1]
		horizon = max(int((1-eps)*self.horizon),1) if eps else self.horizon
		if len(batch) and self.control.shape[0] != batch[0]: self.init_control(batch[0])
		x = torch.Tensor(state).view(*batch, 1,-1).repeat_interleave(self.nsamples, -2)
		controls = np.clip(self.control[:,None,:,:] + self.noise, -1, 1)
		self.states, rewards = self.envmodel.rollout(controls[...,:horizon,:], x, numpy=True)
		costs = -np.sum(rewards, -1) #+ self.lamda * np.copy(self.init_cost)
		beta = np.min(costs, -1, keepdims=True)
		costs_norm = -(costs - beta)/self.lamda
		weights = sp.special.softmax(costs_norm)
		self.control += np.sum(weights[:,:,None,None]*self.noise, len(batch))
		action = self.control[...,0,:]
		self.control = np.roll(self.control, -1, axis=-2)
		self.control[...,-1,:] = 0
		return action

	def init_control(self, batch_size=1):
		self.control = np.random.uniform(-1, 1, size=[batch_size, self.horizon, *self.action_size])
		self.noise = np.random.multivariate_normal(self.mu, self.cov, size=[batch_size, self.nsamples, self.horizon])
		self.init_cost = np.sum(self.control[:,None,:,None,:] @ self.icov[None,None,None,:,:] @ self.noise[:,:,:,:,None], axis=(2,3,4))

	def optimize(self, states, actions, next_states, rewards, dones):
		return self.envmodel.optimize(states, actions, next_states, rewards, dones)

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.save_model(dirname, name, net)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.load_model(dirname, name, net)

	def get_stats(self):
		return {**super().get_stats(), **self.envmodel.get_stats()}

class MPPIAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, MPPIController, gpu=gpu, load=load)
		self.dataset = load_module("src.data.loaders:OnlineDataset")

	def get_action(self, state, eps=None, sample=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		action_greedy = self.network.get_action(np.array(state), eps)
		action = np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action

	def partition(self, x):
		if self.config.NUM_STEPS is None:
			return x[None,...]
		num_splits = x.shape[0]//self.config.NUM_STEPS
		if num_splits == 0:
			arr = np.zeros([self.config.NUM_STEPS, *x.shape[1:]])
			arr[-x.shape[0]:] = x
			num_splits = 1
			x = arr
		arr = x[:num_splits*self.config.NUM_STEPS].reshape(num_splits, self.config.NUM_STEPS, *x.shape[1:])
		return arr

	def train(self, state, action, next_state, reward, done):
		self.time = getattr(self, "time", 0) + 1
		if not hasattr(self, "buffers"): self.buffers = [[] for _ in done]
		for buffer, s, a, ns, r, d in zip(self.buffers, state, action, next_state, reward, done):
			buffer.append((s, a, s if d else ns, r, d))
			if not d: continue
			states, actions, next_states, rewards, dones = map(np.array, zip(*buffer))
			states, actions, next_states, rewards, dones = [self.partition(x) for x in (states, actions, next_states, rewards, dones)]
			buffer.clear()
			self.replay_buffer.extend(list(zip(states, actions, next_states, rewards, dones)), shuffle=False)
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE and self.time % self.config.TRAIN_EVERY == 0:
			losses = []
			samples = list(self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=None)[0])
			dataset = self.dataset(self.config, samples, seq_len=self.config.MPC.HORIZON)
			loader = torch.utils.data.DataLoader(dataset, batch_size=self.config.BATCH_SIZE, shuffle=True)
			pbar = tqdm.tqdm(loader)
			for states, actions, next_states, rewards, dones in pbar:
				losses.append(self.network.optimize(states, actions, next_states, rewards, dones))
				pbar.set_postfix_str(f"Loss: {losses[-1]:.4f}")
			self.network.envmodel.network.schedule(np.mean(losses))
			self.eps = max(self.eps * self.config.EPS_DECAY, self.config.EPS_MIN)


Step:       0, Reward: -1282.468 [ 228.556], Avg: -1282.468 (1.000) <0-00:00:00> ({'r_t':    -3.8649, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    1000, Reward: -1242.562 [ 232.909], Avg: -1262.515 (1.000) <0-00:00:12> ({'r_t': -6208.6317, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    2000, Reward: -1248.557 [ 251.820], Avg: -1257.862 (1.000) <0-00:00:24> ({'r_t': -6178.8853, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    3000, Reward: -1272.503 [ 255.577], Avg: -1261.523 (1.000) <0-00:00:36> ({'r_t': -6170.5595, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    4000, Reward: -1349.149 [ 150.290], Avg: -1279.048 (1.000) <0-00:00:48> ({'r_t': -6031.1925, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    5000, Reward: -1260.432 [ 189.568], Avg: -1275.945 (1.000) <0-00:01:00> ({'r_t': -6321.4478, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    6000, Reward: -1373.414 [ 239.899], Avg: -1289.869 (1.000) <0-00:01:12> ({'r_t': -6161.1719, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    7000, Reward: -1226.050 [ 227.153], Avg: -1281.892 (1.000) <0-00:01:24> ({'r_t': -6271.6947, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    8000, Reward: -1370.325 [ 283.568], Avg: -1291.718 (1.000) <0-00:01:36> ({'r_t': -6082.5192, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    9000, Reward: -1439.661 [ 223.467], Avg: -1306.512 (1.000) <0-00:01:49> ({'r_t': -6096.0196, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   10000, Reward: -1354.883 [ 236.370], Avg: -1310.909 (1.000) <0-00:02:01> ({'r_t': -6200.2571, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   11000, Reward: -1127.563 [ 206.589], Avg: -1295.631 (1.000) <0-00:02:14> ({'r_t': -6205.6789, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   12000, Reward: -1260.910 [ 252.158], Avg: -1292.960 (1.000) <0-00:02:26> ({'r_t': -6263.4165, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   13000, Reward: -1330.171 [ 273.015], Avg: -1295.618 (1.000) <0-00:02:39> ({'r_t': -6182.7447, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   14000, Reward: -1267.097 [ 233.792], Avg: -1293.716 (1.000) <0-00:02:51> ({'r_t': -6123.7013, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   15000, Reward: -1326.922 [ 225.961], Avg: -1295.792 (1.000) <0-00:03:04> ({'r_t': -6254.5988, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   16000, Reward: -1204.720 [ 217.019], Avg: -1290.435 (1.000) <0-00:03:16> ({'r_t': -6147.4911, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   17000, Reward: -1214.547 [ 257.088], Avg: -1286.219 (1.000) <0-00:03:29> ({'r_t': -6250.1628, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   18000, Reward: -1261.051 [ 241.394], Avg: -1284.894 (1.000) <0-00:03:42> ({'r_t': -6102.6128, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   19000, Reward: -1288.985 [ 254.193], Avg: -1285.099 (1.000) <0-00:03:54> ({'r_t': -6244.1806, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   20000, Reward: -1303.043 [ 174.272], Avg: -1285.953 (1.000) <0-00:04:07> ({'r_t': -6208.3270, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   21000, Reward: -1436.698 [ 233.888], Avg: -1292.805 (1.000) <0-00:04:20> ({'r_t': -6126.9081, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   22000, Reward: -1283.370 [ 286.226], Avg: -1292.395 (1.000) <0-00:04:33> ({'r_t': -6213.4386, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   23000, Reward: -1223.968 [ 212.634], Avg: -1289.544 (1.000) <0-00:04:45> ({'r_t': -6157.1749, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   24000, Reward: -1258.550 [ 194.809], Avg: -1288.304 (1.000) <0-00:04:58> ({'r_t': -6154.1965, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   25000, Reward: -1296.883 [ 228.973], Avg: -1288.634 (1.000) <0-00:05:11> ({'r_t': -6112.6909, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   26000, Reward: -1206.141 [ 234.741], Avg: -1285.579 (1.000) <0-00:05:24> ({'r_t': -6293.0528, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   27000, Reward: -1286.659 [ 204.698], Avg: -1285.617 (1.000) <0-00:05:36> ({'r_t': -6154.3101, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   28000, Reward: -1276.808 [ 194.168], Avg: -1285.314 (1.000) <0-00:05:49> ({'r_t': -6255.7742, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   29000, Reward: -1234.288 [ 211.470], Avg: -1283.613 (1.000) <0-00:06:02> ({'r_t': -6034.6506, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   30000, Reward: -1315.397 [ 271.577], Avg: -1284.638 (1.000) <0-00:06:15> ({'r_t': -6063.7186, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   31000, Reward: -1319.709 [ 279.604], Avg: -1285.734 (1.000) <0-00:06:28> ({'r_t': -6177.4135, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:   32000, Reward: -1325.397 [ 322.673], Avg: -1286.936 (0.980) <0-00:08:29> ({'r_t': -6301.6778, 'eps':     0.9800, 'dyn_loss':   710.2130, 'dot_loss':    11.5984, 'ddot_loss':     1.7893, 'rew_loss':    29.7797, 'lr':     0.0001, 'eps_e':     0.9800, 'lr_e':     0.0001})
Step:   33000, Reward: -1371.746 [ 368.327], Avg: -1289.430 (0.960) <0-00:10:31> ({'r_t': -6195.4833, 'eps':     0.9604, 'dyn_loss':    22.7683, 'dot_loss':     1.2939, 'ddot_loss':     0.4699, 'rew_loss':    14.7088, 'lr':     0.0001, 'eps_e':     0.9604, 'lr_e':     0.0001})
Step:   34000, Reward: -1277.180 [ 322.909], Avg: -1289.080 (0.941) <0-00:12:33> ({'r_t': -6074.2234, 'eps':     0.9412, 'dyn_loss':    19.1211, 'dot_loss':     0.9887, 'ddot_loss':     0.3831, 'rew_loss':    14.5170, 'lr':     0.0001, 'eps_e':     0.9412, 'lr_e':     0.0001})
Step:   35000, Reward: -1275.241 [ 342.422], Avg: -1288.696 (0.922) <0-00:14:36> ({'r_t': -6226.2953, 'eps':     0.9224, 'dyn_loss':    17.2626, 'dot_loss':     0.8313, 'ddot_loss':     0.3417, 'rew_loss':    14.4073, 'lr':     0.0001, 'eps_e':     0.9224, 'lr_e':     0.0001})
Step:   36000, Reward: -1199.885 [ 150.695], Avg: -1286.296 (0.904) <0-00:16:46> ({'r_t': -6267.9639, 'eps':     0.9039, 'dyn_loss':    15.9576, 'dot_loss':     0.7171, 'ddot_loss':     0.3121, 'rew_loss':    14.4048, 'lr':     0.0001, 'eps_e':     0.9039, 'lr_e':     0.0001})
Step:   37000, Reward: -1221.680 [ 212.454], Avg: -1284.595 (0.886) <0-00:18:50> ({'r_t': -6181.4632, 'eps':     0.8858, 'dyn_loss':    14.3366, 'dot_loss':     0.6017, 'ddot_loss':     0.2808, 'rew_loss':    14.2212, 'lr':     0.0001, 'eps_e':     0.8858, 'lr_e':     0.0001})
Step:   38000, Reward:  -876.273 [ 107.512], Avg: -1274.125 (0.868) <0-00:20:57> ({'r_t': -6179.8906, 'eps':     0.8681, 'dyn_loss':    10.6835, 'dot_loss':     0.4906, 'ddot_loss':     0.2749, 'rew_loss':    12.9276, 'lr':     0.0001, 'eps_e':     0.8681, 'lr_e':     0.0001})
Step:   39000, Reward:  -968.399 [  89.252], Avg: -1266.482 (0.851) <0-00:23:05> ({'r_t': -6117.5687, 'eps':     0.8508, 'dyn_loss':     5.7572, 'dot_loss':     0.3810, 'ddot_loss':     0.2889, 'rew_loss':     9.8926, 'lr':     0.0001, 'eps_e':     0.8508, 'lr_e':     0.0001})
Step:   40000, Reward:  -722.557 [  80.145], Avg: -1253.216 (0.834) <0-00:25:14> ({'r_t': -6140.8215, 'eps':     0.8337, 'dyn_loss':     3.0617, 'dot_loss':     0.2878, 'ddot_loss':     0.2940, 'rew_loss':     6.1487, 'lr':     0.0001, 'eps_e':     0.8337, 'lr_e':     0.0001})
Step:   41000, Reward:  -784.925 [  75.350], Avg: -1242.066 (0.817) <0-00:27:24> ({'r_t': -6131.7905, 'eps':     0.8171, 'dyn_loss':     1.9617, 'dot_loss':     0.2283, 'ddot_loss':     0.2883, 'rew_loss':     4.1025, 'lr':     0.0001, 'eps_e':     0.8171, 'lr_e':     0.0001})
Step:   42000, Reward:  -798.009 [ 125.112], Avg: -1231.739 (0.801) <0-00:29:35> ({'r_t': -6146.4443, 'eps':     0.8007, 'dyn_loss':     1.4323, 'dot_loss':     0.1889, 'ddot_loss':     0.2769, 'rew_loss':     3.0102, 'lr':     0.0001, 'eps_e':     0.8007, 'lr_e':     0.0001})
Step:   43000, Reward:  -926.322 [ 119.830], Avg: -1224.798 (0.785) <0-00:31:48> ({'r_t': -6165.8805, 'eps':     0.7847, 'dyn_loss':     1.1116, 'dot_loss':     0.1611, 'ddot_loss':     0.2627, 'rew_loss':     2.4378, 'lr':     0.0001, 'eps_e':     0.7847, 'lr_e':     0.0001})
Step:   44000, Reward:  -827.166 [ 100.490], Avg: -1215.961 (0.769) <0-00:34:00> ({'r_t': -6182.5783, 'eps':     0.7690, 'dyn_loss':     0.9725, 'dot_loss':     0.1405, 'ddot_loss':     0.2449, 'rew_loss':     2.1221, 'lr':     0.0001, 'eps_e':     0.7690, 'lr_e':     0.0001})
Step:   45000, Reward:  -836.062 [ 152.617], Avg: -1207.703 (0.754) <0-00:36:13> ({'r_t': -5958.8747, 'eps':     0.7536, 'dyn_loss':     0.9075, 'dot_loss':     0.1253, 'ddot_loss':     0.2256, 'rew_loss':     1.9772, 'lr':     0.0001, 'eps_e':     0.7536, 'lr_e':     0.0001})
Step:   46000, Reward:  -773.731 [ 125.783], Avg: -1198.469 (0.739) <0-00:38:29> ({'r_t': -5949.1393, 'eps':     0.7386, 'dyn_loss':     0.8055, 'dot_loss':     0.1092, 'ddot_loss':     0.2025, 'rew_loss':     1.7701, 'lr':     0.0001, 'eps_e':     0.7386, 'lr_e':     0.0001})
Step:   47000, Reward:  -789.659 [  86.257], Avg: -1189.952 (0.724) <0-00:40:45> ({'r_t': -6177.3211, 'eps':     0.7238, 'dyn_loss':     0.6997, 'dot_loss':     0.0928, 'ddot_loss':     0.1753, 'rew_loss':     1.5842, 'lr':     0.0001, 'eps_e':     0.7238, 'lr_e':     0.0001})
Step:   48000, Reward:  -881.757 [  93.165], Avg: -1183.663 (0.709) <0-00:42:59> ({'r_t': -5863.1833, 'eps':     0.7093, 'dyn_loss':     0.6824, 'dot_loss':     0.0825, 'ddot_loss':     0.1535, 'rew_loss':     1.5209, 'lr':     0.0001, 'eps_e':     0.7093, 'lr_e':     0.0001})
Step:   49000, Reward:  -827.457 [ 136.636], Avg: -1176.539 (0.695) <0-00:45:17> ({'r_t': -5926.5258, 'eps':     0.6951, 'dyn_loss':     0.6632, 'dot_loss':     0.0744, 'ddot_loss':     0.1373, 'rew_loss':     1.4597, 'lr':     0.0001, 'eps_e':     0.6951, 'lr_e':     0.0001})
Step:   50000, Reward:  -850.463 [ 118.062], Avg: -1170.145 (0.681) <0-00:47:39> ({'r_t': -5971.3615, 'eps':     0.6812, 'dyn_loss':     0.5946, 'dot_loss':     0.0671, 'ddot_loss':     0.1241, 'rew_loss':     1.3425, 'lr':     0.0001, 'eps_e':     0.6812, 'lr_e':     0.0001})
Step:   51000, Reward:  -807.381 [ 114.410], Avg: -1163.169 (0.668) <0-00:50:01> ({'r_t': -5961.7204, 'eps':     0.6676, 'dyn_loss':     0.5635, 'dot_loss':     0.0602, 'ddot_loss':     0.1111, 'rew_loss':     1.2689, 'lr':     0.0001, 'eps_e':     0.6676, 'lr_e':     0.0001})
Step:   52000, Reward:  -866.029 [ 114.188], Avg: -1157.562 (0.654) <0-00:52:20> ({'r_t': -5794.1791, 'eps':     0.6543, 'dyn_loss':     0.5601, 'dot_loss':     0.0568, 'ddot_loss':     0.1044, 'rew_loss':     1.2344, 'lr':     0.0001, 'eps_e':     0.6543, 'lr_e':     0.0001})
Step:   53000, Reward:  -804.895 [ 138.401], Avg: -1151.032 (0.641) <0-00:54:43> ({'r_t': -5907.4578, 'eps':     0.6412, 'dyn_loss':     0.5296, 'dot_loss':     0.0524, 'ddot_loss':     0.0959, 'rew_loss':     1.1811, 'lr':     0.0001, 'eps_e':     0.6412, 'lr_e':     0.0001})
Step:   54000, Reward:  -818.226 [ 132.944], Avg: -1144.980 (0.628) <0-00:57:07> ({'r_t': -5643.2255, 'eps':     0.6283, 'dyn_loss':     0.5183, 'dot_loss':     0.0498, 'ddot_loss':     0.0910, 'rew_loss':     1.1580, 'lr':     0.0001, 'eps_e':     0.6283, 'lr_e':     0.0001})
Step:   55000, Reward:  -776.234 [  86.912], Avg: -1138.396 (0.616) <0-00:59:40> ({'r_t': -5746.1699, 'eps':     0.6158, 'dyn_loss':     0.5082, 'dot_loss':     0.0453, 'ddot_loss':     0.0821, 'rew_loss':     1.1047, 'lr':     0.0001, 'eps_e':     0.6158, 'lr_e':     0.0001})
Step:   56000, Reward:  -744.120 [ 130.696], Avg: -1131.479 (0.603) <0-01:02:03> ({'r_t': -5741.3173, 'eps':     0.6035, 'dyn_loss':     0.5182, 'dot_loss':     0.0445, 'ddot_loss':     0.0791, 'rew_loss':     1.1172, 'lr':     0.0001, 'eps_e':     0.6035, 'lr_e':     0.0001})
Step:   57000, Reward:  -832.619 [ 139.819], Avg: -1126.326 (0.591) <0-01:04:22> ({'r_t': -5735.8084, 'eps':     0.5914, 'dyn_loss':     0.4785, 'dot_loss':     0.0415, 'ddot_loss':     0.0743, 'rew_loss':     1.0536, 'lr':     0.0001, 'eps_e':     0.5914, 'lr_e':     0.0001})
Step:   58000, Reward:  -856.814 [  98.414], Avg: -1121.758 (0.580) <0-01:06:43> ({'r_t': -5573.9717, 'eps':     0.5796, 'dyn_loss':     0.4669, 'dot_loss':     0.0390, 'ddot_loss':     0.0692, 'rew_loss':     1.0161, 'lr':     0.0001, 'eps_e':     0.5796, 'lr_e':     0.0001})
Step:   59000, Reward:  -803.052 [ 125.912], Avg: -1116.446 (0.568) <0-01:09:03> ({'r_t': -5783.7031, 'eps':     0.5680, 'dyn_loss':     0.4883, 'dot_loss':     0.0377, 'ddot_loss':     0.0657, 'rew_loss':     1.0253, 'lr':     0.0001, 'eps_e':     0.5680, 'lr_e':     0.0001})
Step:   60000, Reward:  -765.298 [ 105.251], Avg: -1110.690 (0.557) <0-01:11:46> ({'r_t': -5754.4881, 'eps':     0.5566, 'dyn_loss':     0.4326, 'dot_loss':     0.0357, 'ddot_loss':     0.0634, 'rew_loss':     0.9548, 'lr':     0.0001, 'eps_e':     0.5566, 'lr_e':     0.0001})
Step:   61000, Reward:  -802.395 [ 137.513], Avg: -1105.717 (0.545) <0-01:14:11> ({'r_t': -5583.3489, 'eps':     0.5455, 'dyn_loss':     0.4261, 'dot_loss':     0.0338, 'ddot_loss':     0.0593, 'rew_loss':     0.9196, 'lr':     0.0001, 'eps_e':     0.5455, 'lr_e':     0.0001})
Step:   62000, Reward:  -775.193 [ 107.826], Avg: -1100.471 (0.535) <0-01:16:38> ({'r_t': -5591.1637, 'eps':     0.5346, 'dyn_loss':     0.4395, 'dot_loss':     0.0334, 'ddot_loss':     0.0588, 'rew_loss':     0.9398, 'lr':     0.0001, 'eps_e':     0.5346, 'lr_e':     0.0001})
Step:   63000, Reward:  -863.563 [ 136.548], Avg: -1096.769 (0.524) <0-01:19:00> ({'r_t': -5481.2901, 'eps':     0.5239, 'dyn_loss':     0.4387, 'dot_loss':     0.0318, 'ddot_loss':     0.0554, 'rew_loss':     0.9363, 'lr':     0.0001, 'eps_e':     0.5239, 'lr_e':     0.0001})
Step:   64000, Reward:  -778.848 [ 112.434], Avg: -1091.878 (0.513) <0-01:21:24> ({'r_t': -5532.7418, 'eps':     0.5134, 'dyn_loss':     0.4194, 'dot_loss':     0.0306, 'ddot_loss':     0.0535, 'rew_loss':     0.8991, 'lr':     0.0001, 'eps_e':     0.5134, 'lr_e':     0.0001})
Step:   65000, Reward:  -802.599 [ 139.669], Avg: -1087.495 (0.503) <0-01:23:49> ({'r_t': -5359.9943, 'eps':     0.5031, 'dyn_loss':     0.4178, 'dot_loss':     0.0296, 'ddot_loss':     0.0508, 'rew_loss':     0.9000, 'lr':     0.0001, 'eps_e':     0.5031, 'lr_e':     0.0001})
Step:   66000, Reward:  -802.324 [  96.354], Avg: -1083.239 (0.493) <0-01:26:16> ({'r_t': -5586.0785, 'eps':     0.4931, 'dyn_loss':     0.4245, 'dot_loss':     0.0291, 'ddot_loss':     0.0494, 'rew_loss':     0.8997, 'lr':     0.0001, 'eps_e':     0.4931, 'lr_e':     0.0001})
Step:   67000, Reward:  -811.314 [ 113.588], Avg: -1079.240 (0.483) <0-01:28:44> ({'r_t': -5491.2188, 'eps':     0.4832, 'dyn_loss':     0.4227, 'dot_loss':     0.0280, 'ddot_loss':     0.0470, 'rew_loss':     0.8989, 'lr':     0.0001, 'eps_e':     0.4832, 'lr_e':     0.0001})
Step:   68000, Reward:  -803.421 [  97.173], Avg: -1075.242 (0.474) <0-01:31:21> ({'r_t': -5352.8963, 'eps':     0.4735, 'dyn_loss':     0.4055, 'dot_loss':     0.0269, 'ddot_loss':     0.0455, 'rew_loss':     0.8663, 'lr':     0.0001, 'eps_e':     0.4735, 'lr_e':     0.0001})
Step:   69000, Reward:  -800.821 [ 140.314], Avg: -1071.322 (0.464) <0-01:33:51> ({'r_t': -5246.3454, 'eps':     0.4641, 'dyn_loss':     0.4161, 'dot_loss':     0.0267, 'ddot_loss':     0.0447, 'rew_loss':     0.8669, 'lr':     0.0001, 'eps_e':     0.4641, 'lr_e':     0.0001})
Step:   70000, Reward:  -778.083 [  78.608], Avg: -1067.192 (0.455) <0-01:36:14> ({'r_t': -5210.1601, 'eps':     0.4548, 'dyn_loss':     0.3946, 'dot_loss':     0.0251, 'ddot_loss':     0.0419, 'rew_loss':     0.8394, 'lr':     0.0001, 'eps_e':     0.4548, 'lr_e':     0.0001})
Step:   71000, Reward:  -789.768 [ 123.483], Avg: -1063.339 (0.446) <0-01:38:46> ({'r_t': -5226.6543, 'eps':     0.4457, 'dyn_loss':     0.3967, 'dot_loss':     0.0251, 'ddot_loss':     0.0426, 'rew_loss':     0.8353, 'lr':     0.0001, 'eps_e':     0.4457, 'lr_e':     0.0001})
Step:   72000, Reward:  -843.641 [ 110.206], Avg: -1060.329 (0.437) <0-01:41:29> ({'r_t': -5109.2421, 'eps':     0.4368, 'dyn_loss':     0.4004, 'dot_loss':     0.0246, 'ddot_loss':     0.0413, 'rew_loss':     0.8308, 'lr':     0.0001, 'eps_e':     0.4368, 'lr_e':     0.0001})
Step:   73000, Reward:  -819.139 [ 148.082], Avg: -1057.070 (0.428) <0-01:43:58> ({'r_t': -5176.8906, 'eps':     0.4281, 'dyn_loss':     0.3956, 'dot_loss':     0.0239, 'ddot_loss':     0.0395, 'rew_loss':     0.8302, 'lr':     0.0001, 'eps_e':     0.4281, 'lr_e':     0.0001})
