Model: <class 'src.models.pytorch.mpc.mppi.MPPIAgent'>, Env: Pendulum-v0, Date: 04/06/2020 11:42:55
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: 6b45e60fd9407cf6551acce4378c896d71efc5c8
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.99
   NUM_STEPS = 40
   MAX_BUFFER_SIZE = 1000000
   REPLAY_BATCH_SIZE = 1000
   TARGET_UPDATE_RATE = 0.0004
   BATCH_SIZE = 250
   DYN_EPOCHS = 10
   TRAIN_EVERY = 1000
   ENV_MODEL = dfrntl
   MPC = 
      NSAMPLES = 100
      HORIZON = 40
      LAMBDA = 0.1
      COV = 0.5
   dynamics_size = 3
   state_size = (3,)
   action_size = (1,)
   env_name = Pendulum-v0
   rank = 0
   size = 17
   split = 17
   model = mppi
   framework = pt
   train_prop = 1.0
   tcp_ports = [9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 200000
   render = False
   trial = False
   icm = False
   rs = False
   DYN = 
      REG_LAMBDA = 1e-06
      FACTOR = 0.5
      PATIENCE = 5
      LEARN_RATE = 0.0001
      TRANSITION_HIDDEN = 512
      REWARD_HIDDEN = 256
      BETA_DYN = 0.1
      BETA_DOT = 1
      BETA_DDOT = 1,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7ff98385ef90> 
	env = <GymEnv<TimeLimit<PendulumEnv<Pendulum-v0>>>> 
		env = <TimeLimit<PendulumEnv<Pendulum-v0>>> 
			env = <PendulumEnv<Pendulum-v0>> 
				max_speed = 8
				max_torque = 2.0
				dt = 0.05
				g = 10.0
				m = 1.0
				l = 1.0
				viewer = None
				action_space = Box(1,) 
					dtype = float32
					shape = (1,)
					low = [-2.000]
					high = [ 2.000]
					bounded_below = [ True]
					bounded_above = [ True]
					np_random = RandomState(MT19937)
				observation_space = Box(3,) 
					dtype = float32
					shape = (3,)
					low = [-1.000 -1.000 -8.000]
					high = [ 1.000  1.000  8.000]
					bounded_below = [ True  True  True]
					bounded_above = [ True  True  True]
					np_random = RandomState(MT19937)
				np_random = RandomState(MT19937)
				spec = EnvSpec(Pendulum-v0) 
					id = Pendulum-v0
					entry_point = gym.envs.classic_control:PendulumEnv
					reward_threshold = None
					nondeterministic = False
					max_episode_steps = 200
				verbose = 0
			action_space = Box(1,) 
				dtype = float32
				shape = (1,)
				low = [-2.000]
				high = [ 2.000]
				bounded_below = [ True]
				bounded_above = [ True]
				np_random = RandomState(MT19937)
			observation_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -8.000]
				high = [ 1.000  1.000  8.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			reward_range = (-inf, inf)
			metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}
		action_space = Box(1,) 
			dtype = float32
			shape = (1,)
			low = [-2.000]
			high = [ 2.000]
			bounded_below = [ True]
			bounded_above = [ True]
			np_random = RandomState(MT19937)
		observation_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -8.000]
			high = [ 1.000  1.000  8.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7ff983878ad0> 
			observation_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -8.000]
				high = [ 1.000  1.000  8.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
	state_size = (3,)
	action_size = (1,)
	action_space = Box(1,) 
		dtype = float32
		shape = (1,)
		low = [-2.000]
		high = [ 2.000]
		bounded_below = [ True]
		bounded_above = [ True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7ff983878f90> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {9001: <socket.socket fd=35, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 41580), raddr=('127.0.0.1', 9001)>, 9002: <socket.socket fd=91, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 57142), raddr=('127.0.0.1', 9002)>, 9003: <socket.socket fd=93, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42342), raddr=('127.0.0.1', 9003)>, 9004: <socket.socket fd=95, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59314), raddr=('127.0.0.1', 9004)>, 9005: <socket.socket fd=96, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 41658), raddr=('127.0.0.1', 9005)>, 9006: <socket.socket fd=115, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46272), raddr=('127.0.0.1', 9006)>, 9007: <socket.socket fd=117, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46474), raddr=('127.0.0.1', 9007)>, 9008: <socket.socket fd=120, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 36794), raddr=('127.0.0.1', 9008)>, 9009: <socket.socket fd=181, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 57740), raddr=('127.0.0.1', 9009)>, 9010: <socket.socket fd=198, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 36676), raddr=('127.0.0.1', 9010)>, 9011: <socket.socket fd=199, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 41090), raddr=('127.0.0.1', 9011)>, 9012: <socket.socket fd=200, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 44046), raddr=('127.0.0.1', 9012)>, 9013: <socket.socket fd=201, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 39312), raddr=('127.0.0.1', 9013)>, 9014: <socket.socket fd=202, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 53784), raddr=('127.0.0.1', 9014)>, 9015: <socket.socket fd=203, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 51996), raddr=('127.0.0.1', 9015)>, 9016: <socket.socket fd=204, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 38634), raddr=('127.0.0.1', 9016)>}
	num_envs = 16
	max_steps = 200,
agent: <src.models.wrappers.ParallelAgent object at 0x7ff9839ccd90> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7ff983879050> 
		state_size = (3,)
	agent = <src.models.pytorch.mpc.mppi.MPPIAgent object at 0x7ff983879650> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7ff983827450> 
			size = (1,)
			dt = 0.2
			action = [-0.527]
			daction_dt = [-1.699]
		discrete = False
		action_size = (1,)
		state_size = (3,)
		config = <src.utils.config.Config object at 0x7ff98814fa90> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.99
			NUM_STEPS = 40
			MAX_BUFFER_SIZE = 1000000
			REPLAY_BATCH_SIZE = 1000
			TARGET_UPDATE_RATE = 0.0004
			BATCH_SIZE = 250
			DYN_EPOCHS = 10
			TRAIN_EVERY = 1000
			ENV_MODEL = dfrntl
			MPC = <src.utils.config.Config object at 0x7ffa2b3d3710> 
				NSAMPLES = 100
				HORIZON = 40
				LAMBDA = 0.1
				COV = 0.5
			dynamics_size = 3
			state_size = (3,)
			action_size = (1,)
			env_name = Pendulum-v0
			rank = 0
			size = 17
			split = 17
			model = mppi
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 200000
			render = False
			trial = False
			icm = False
			rs = False
			DYN = <src.utils.config.Config object at 0x7ff988138d90> 
				REG_LAMBDA = 1e-06
				FACTOR = 0.5
				PATIENCE = 5
				LEARN_RATE = 0.0001
				TRANSITION_HIDDEN = 512
				REWARD_HIDDEN = 256
				BETA_DYN = 0.1
				BETA_DOT = 1
				BETA_DDOT = 1
		stats = <src.utils.logger.Stats object at 0x7ff983827490> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = MPPIController() 
			training = True
			tau = 0.0004
			name = mppi
			stats = <src.utils.logger.Stats object at 0x7ff983827510> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7ff98814fa90> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.99
				NUM_STEPS = 40
				MAX_BUFFER_SIZE = 1000000
				REPLAY_BATCH_SIZE = 1000
				TARGET_UPDATE_RATE = 0.0004
				BATCH_SIZE = 250
				DYN_EPOCHS = 10
				TRAIN_EVERY = 1000
				ENV_MODEL = dfrntl
				MPC = <src.utils.config.Config object at 0x7ffa2b3d3710> 
					NSAMPLES = 100
					HORIZON = 40
					LAMBDA = 0.1
					COV = 0.5
				dynamics_size = 3
				state_size = (3,)
				action_size = (1,)
				env_name = Pendulum-v0
				rank = 0
				size = 17
				split = 17
				model = mppi
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 200000
				render = False
				trial = False
				icm = False
				rs = False
				DYN = <src.utils.config.Config object at 0x7ff988138d90> 
					REG_LAMBDA = 1e-06
					FACTOR = 0.5
					PATIENCE = 5
					LEARN_RATE = 0.0001
					TRANSITION_HIDDEN = 512
					REWARD_HIDDEN = 256
					BETA_DYN = 0.1
					BETA_DOT = 1
					BETA_DDOT = 1
			device = cuda
			envmodel = <src.models.pytorch.mpc.EnvModel object at 0x7ff983827550> 
				network = DifferentialEnv(
					  (reward): RewardModel(
					    (linear1): Linear(in_features=7, out_features=256, bias=True)
					    (linear2): Linear(in_features=256, out_features=256, bias=True)
					    (linear3): Linear(in_features=256, out_features=1, bias=True)
					  )
					  (dynamics): TransitionModel(
					    (gru): GRUCell(7, 512)
					    (linear1): Linear(in_features=512, out_features=512, bias=True)
					    (linear2): Linear(in_features=512, out_features=512, bias=True)
					    (state_ddot): Linear(in_features=512, out_features=3, bias=True)
					  )
					) 
					training = True
					tau = 0.0004
					name = dfrntl
					stats = <src.utils.logger.Stats object at 0x7ff9838275d0> 
						mean_dict = {}
						sum_dict = {}
					config = <src.utils.config.Config object at 0x7ff98814fa90> 
						TRIAL_AT = 1000
						SAVE_AT = 1
						SEED = 0
						REG_LAMBDA = 1e-06
						LEARN_RATE = 0.0001
						DISCOUNT_RATE = 0.99
						ADVANTAGE_DECAY = 0.95
						INPUT_LAYER = 512
						ACTOR_HIDDEN = 256
						CRITIC_HIDDEN = 1024
						EPS_MAX = 1.0
						EPS_MIN = 0.1
						EPS_DECAY = 0.99
						NUM_STEPS = 40
						MAX_BUFFER_SIZE = 1000000
						REPLAY_BATCH_SIZE = 1000
						TARGET_UPDATE_RATE = 0.0004
						BATCH_SIZE = 250
						DYN_EPOCHS = 10
						TRAIN_EVERY = 1000
						ENV_MODEL = dfrntl
						MPC = <src.utils.config.Config object at 0x7ffa2b3d3710> 
							NSAMPLES = 100
							HORIZON = 40
							LAMBDA = 0.1
							COV = 0.5
						dynamics_size = 3
						state_size = (3,)
						action_size = (1,)
						env_name = Pendulum-v0
						rank = 0
						size = 17
						split = 17
						model = mppi
						framework = pt
						train_prop = 1.0
						tcp_ports = <list len=17>
						tcp_rank = 0
						num_envs = 1
						nsteps = 200000
						render = False
						trial = False
						icm = False
						rs = False
						DYN = <src.utils.config.Config object at 0x7ff988138d90> 
							REG_LAMBDA = 1e-06
							FACTOR = 0.5
							PATIENCE = 5
							LEARN_RATE = 0.0001
							TRANSITION_HIDDEN = 512
							REWARD_HIDDEN = 256
							BETA_DYN = 0.1
							BETA_DOT = 1
							BETA_DDOT = 1
					device = cuda
					state_size = (3,)
					action_size = (1,)
					discrete = False
					dyn_index = 3
					optimizer = Adam (
					Parameter Group 0
					    amsgrad: False
					    betas: (0.9, 0.999)
					    eps: 1e-08
					    lr: 0.0001
					    weight_decay: 1e-06
					)
					scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff983827b90>
				state_size = (3,)
				action_size = (1,)
			mu = [ 0.000]
			cov = [[ 0.500]]
			icov = [[ 2.000]]
			lamda = 0.1
			horizon = 40
			nsamples = 100
			action_size = (1,)
			control = [[[ 0.359]
			  [ 0.138]
			  [-0.739]
			  [-0.399]
			  [-0.274]
			  [-0.364]
			  [-0.999]
			  [-0.238]
			  [ 0.253]
			  [ 0.212]
			  [ 0.950]
			  [ 0.586]
			  [ 0.216]
			  [-0.812]
			  [-0.667]
			  [-0.819]
			  [ 0.100]
			  [-0.222]
			  [ 0.532]
			  [-0.865]
			  [-0.267]
			  [-0.835]
			  [ 0.686]
			  [ 0.754]
			  [ 0.311]
			  [ 0.270]
			  [-0.380]
			  [-0.428]
			  [ 0.234]
			  [ 0.671]
			  [-0.213]
			  [ 0.305]
			  [ 0.360]
			  [-0.191]
			  [ 0.142]
			  [-0.073]
			  [ 0.686]
			  [-0.283]
			  [-0.433]
			  [ 0.345]]]
			noise = [[[[ 0.040]
			   [-0.170]
			   [ 1.009]
			   ...
			   [ 0.042]
			   [ 0.534]
			   [ 0.017]]
			
			  [[-0.729]
			   [ 0.440]
			   [ 0.548]
			   ...
			   [-0.672]
			   [-0.452]
			   [-0.507]]
			
			  [[ 1.484]
			   [ 0.503]
			   [ 0.300]
			   ...
			   [ 1.269]
			   [-0.638]
			   [ 0.645]]
			
			  ...
			
			  [[ 0.744]
			   [ 0.560]
			   [ 1.292]
			   ...
			   [ 0.772]
			   [-1.197]
			   [ 0.259]]
			
			  [[-1.363]
			   [ 0.155]
			   [ 1.027]
			   ...
			   [ 0.002]
			   [-1.067]
			   [-0.008]]
			
			  [[ 0.378]
			   [ 0.431]
			   [ 0.030]
			   ...
			   [ 0.705]
			   [ 1.159]
			   [-1.117]]]]
			init_cost = [[-6.728  1.663 -3.113 -1.607 -2.228 -1.133  4.093 -0.038 -0.921 -4.292 -5.759  0.112  3.217 -4.762  7.692 -4.850  2.325 -4.556 -1.460  5.416  2.640 -3.572  0.592  3.121 -0.332 -6.524 -3.881  3.902  0.622  1.039  0.229 -2.172 -1.452 -7.299 -5.245 -2.050 -6.341  2.673  4.726 -1.414  1.179 -3.927  9.062 -3.114  0.166  9.307  5.070  4.697  2.849 -0.208  0.103 -0.170 -0.589 -3.444 -7.061 -3.615  3.135 -1.214  2.266 -2.775 -2.619 -5.104  7.892 -2.038  8.988 -4.294 -9.779  8.663  0.128 -1.165  8.562 -6.652  0.640 -1.209  1.091 -1.792 -2.980 -0.385 -5.270 -3.393 -7.579  1.585 12.904 -6.898 -6.876  4.774  3.130  0.470  4.154 -1.143  6.931 -1.392 -1.860  0.785  2.867 -2.039  5.032 -3.577  1.142  2.450]]
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7ff9839cc290> 
			buffer = deque([], maxlen=1000000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7ff983827c10> 
		size = (1,)
		dt = 0.2
		action = [ 0.813]
		daction_dt = [-0.729]
	discrete = False
	action_size = (1,)
	state_size = (3,)
	config = <src.utils.config.Config object at 0x7ff98814fa90> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.99
		NUM_STEPS = 40
		MAX_BUFFER_SIZE = 1000000
		REPLAY_BATCH_SIZE = 1000
		TARGET_UPDATE_RATE = 0.0004
		BATCH_SIZE = 250
		DYN_EPOCHS = 10
		TRAIN_EVERY = 1000
		ENV_MODEL = dfrntl
		MPC = <src.utils.config.Config object at 0x7ffa2b3d3710> 
			NSAMPLES = 100
			HORIZON = 40
			LAMBDA = 0.1
			COV = 0.5
		dynamics_size = 3
		state_size = (3,)
		action_size = (1,)
		env_name = Pendulum-v0
		rank = 0
		size = 17
		split = 17
		model = mppi
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 200000
		render = False
		trial = False
		icm = False
		rs = False
		DYN = <src.utils.config.Config object at 0x7ff988138d90> 
			REG_LAMBDA = 1e-06
			FACTOR = 0.5
			PATIENCE = 5
			LEARN_RATE = 0.0001
			TRANSITION_HIDDEN = 512
			REWARD_HIDDEN = 256
			BETA_DYN = 0.1
			BETA_DOT = 1
			BETA_DDOT = 1
	stats = <src.utils.logger.Stats object at 0x7ff9838393d0> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import tqdm
import torch
import random
import numpy as np
import scipy as sp
from scipy.stats import multivariate_normal
from src.utils.rand import RandomAgent, ReplayBuffer
from ..agents.base import PTNetwork, PTAgent, Conv, one_hot_from_indices
from . import EnvModel

class MPPIController(PTNetwork):
	def __init__(self, state_size, action_size, config, load="", gpu=True, name="mppi"):
		super().__init__(config, gpu=gpu, name=name)
		self.envmodel = EnvModel(state_size, action_size, config, load=load, gpu=gpu)
		self.mu = np.zeros(action_size)
		self.cov = np.diag(np.ones(action_size))*config.MPC.COV
		self.icov = np.linalg.inv(self.cov)
		self.lamda = config.MPC.LAMBDA
		self.horizon = config.MPC.HORIZON
		self.nsamples = config.MPC.NSAMPLES
		self.config = config
		self.action_size = action_size
		self.init_control()

	def get_action(self, state, eps=None, sample=True):
		batch = state.shape[:-1]
		if len(batch) and self.control.shape[0] != batch[0]: self.init_control(batch[0])
		x = torch.Tensor(state).view(*batch, 1,-1).repeat_interleave(self.nsamples, -2)
		controls = np.clip(self.control[:,None,:,:] + self.noise, -1, 1)
		self.states, rewards = self.envmodel.rollout(controls, x, numpy=True)
		costs = -np.sum(rewards, -1) #+ self.lamda * np.copy(self.init_cost)
		beta = np.min(costs, -1, keepdims=True)
		costs_norm = -(costs - beta)/self.lamda
		weights = sp.special.softmax(costs_norm)
		self.control += np.sum(weights[:,:,None,None]*self.noise, len(batch))
		action = self.control[...,0,:]
		self.control = np.roll(self.control, -1, axis=-2)
		self.control[...,-1,:] = 0
		return action

	def init_control(self, batch_size=1):
		self.control = np.random.uniform(-1, 1, size=[batch_size, self.horizon, *self.action_size])
		self.noise = np.random.multivariate_normal(self.mu, self.cov, size=[batch_size, self.nsamples, self.horizon])
		self.init_cost = np.sum(self.control[:,None,:,None,:] @ self.icov[None,None,None,:,:] @ self.noise[:,:,:,:,None], axis=(2,3,4))

	def optimize(self, states, actions, next_states, rewards, dones):
		return self.envmodel.optimize(states, actions, next_states, rewards, dones)

	def save_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.save_model(dirname, name, net)
		
	def load_model(self, dirname="pytorch", name="checkpoint", net=None):
		return self.envmodel.load_model(dirname, name, net)

	def get_stats(self):
		return {**super().get_stats(), **self.envmodel.get_stats()}

class MPPIAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, MPPIController, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		action_greedy = self.network.get_action(np.array(state))
		action = np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action

	def partition(self, x):
		num_splits = x.shape[0]//self.config.NUM_STEPS
		if num_splits == 0:
			arr = np.zeros([self.config.NUM_STEPS, *x.shape[1:]])
			arr[-x.shape[0]:] = x
			num_splits = 1
			x = arr
		arr = x[:num_splits*self.config.NUM_STEPS].reshape(num_splits, self.config.NUM_STEPS, *x.shape[1:])
		return arr

	def train(self, state, action, next_state, reward, done):
		self.time = getattr(self, "time", 0) + 1
		if not hasattr(self, "buffers"): self.buffers = [[] for _ in done]
		for buffer, s, a, ns, r, d in zip(self.buffers, state, action, next_state, reward, done):
			buffer.append((s, a, s if d else ns, r, d))
			if not d: continue
			states, actions, next_states, rewards, dones = map(np.array, zip(*buffer))
			states, actions, next_states, rewards, dones = [self.partition(x) for x in (states, actions, next_states, rewards, dones)]
			buffer.clear()
			self.replay_buffer.extend(list(zip(states, actions, next_states, rewards, dones)), shuffle=False)
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE and self.time % self.config.TRAIN_EVERY == 0:
			losses = []
			pbar = tqdm.trange(self.config.DYN_EPOCHS*self.config.REPLAY_BATCH_SIZE//self.config.BATCH_SIZE)
			for _ in pbar:
				states, actions, next_states, rewards, dones = self.replay_buffer.sample(self.config.BATCH_SIZE, dtype=self.to_tensor)[0]
				losses.append(self.network.optimize(states, actions, next_states, rewards, dones))
				pbar.set_postfix_str(f"Loss: {losses[-1]:.4f}")
			self.network.envmodel.network.schedule(np.mean(losses))
			self.eps = max(self.eps * self.config.EPS_DECAY, self.config.EPS_MIN)


Step:       0, Reward: -1189.495 [ 108.181], Avg: -1189.495 (1.000) <0-00:00:00> ({'r_t':    -3.6054, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    1000, Reward: -1250.211 [ 158.834], Avg: -1219.853 (1.000) <0-00:00:59> ({'r_t': -6206.6231, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    2000, Reward: -1249.967 [ 176.388], Avg: -1229.891 (1.000) <0-00:02:01> ({'r_t': -6244.9354, 'eps':     1.0000, 'lr':     0.0001, 'eps_e':     1.0000, 'lr_e':     0.0001})
Step:    3000, Reward: -1300.018 [ 155.688], Avg: -1247.423 (0.990) <0-00:03:07> ({'r_t': -6247.1786, 'eps':     0.9900, 'dyn_loss':  4903.1187, 'dot_loss':    44.1145, 'ddot_loss':     1.3273, 'rew_loss':    49.3431, 'lr':     0.0001, 'eps_e':     0.9900, 'lr_e':     0.0001})
Step:    4000, Reward: -1258.566 [ 167.383], Avg: -1249.651 (0.980) <0-00:04:13> ({'r_t': -6204.1194, 'eps':     0.9801, 'dyn_loss':    61.9332, 'dot_loss':     4.6593, 'ddot_loss':     0.6287, 'rew_loss':    29.5737, 'lr':     0.0001, 'eps_e':     0.9801, 'lr_e':     0.0001})
Step:    5000, Reward: -1198.171 [ 134.118], Avg: -1241.071 (0.970) <0-00:05:20> ({'r_t': -6219.2058, 'eps':     0.9703, 'dyn_loss':    38.1206, 'dot_loss':     2.2258, 'ddot_loss':     0.2866, 'rew_loss':    19.6087, 'lr':     0.0001, 'eps_e':     0.9703, 'lr_e':     0.0001})
Step:    6000, Reward: -1246.090 [ 150.983], Avg: -1241.788 (0.961) <0-00:06:26> ({'r_t': -6127.4297, 'eps':     0.9606, 'dyn_loss':    28.8653, 'dot_loss':     1.3874, 'ddot_loss':     0.1782, 'rew_loss':    15.5258, 'lr':     0.0001, 'eps_e':     0.9606, 'lr_e':     0.0001})
Step:    7000, Reward: -1283.718 [ 128.245], Avg: -1247.029 (0.951) <0-00:07:33> ({'r_t': -6196.1427, 'eps':     0.9510, 'dyn_loss':    24.9118, 'dot_loss':     1.0066, 'ddot_loss':     0.1284, 'rew_loss':    14.4190, 'lr':     0.0001, 'eps_e':     0.9510, 'lr_e':     0.0001})
Step:    8000, Reward: -1157.444 [ 144.897], Avg: -1237.076 (0.941) <0-00:08:45> ({'r_t': -6110.9193, 'eps':     0.9415, 'dyn_loss':    23.2418, 'dot_loss':     0.8251, 'ddot_loss':     0.1046, 'rew_loss':    14.4696, 'lr':     0.0001, 'eps_e':     0.9415, 'lr_e':     0.0001})
Step:    9000, Reward: -1317.570 [ 158.285], Avg: -1245.125 (0.932) <0-00:09:52> ({'r_t': -6062.1326, 'eps':     0.9321, 'dyn_loss':    21.5634, 'dot_loss':     0.6905, 'ddot_loss':     0.0869, 'rew_loss':    14.2224, 'lr':     0.0001, 'eps_e':     0.9321, 'lr_e':     0.0001})
Step:   10000, Reward: -1279.078 [ 182.154], Avg: -1248.212 (0.923) <0-00:10:57> ({'r_t': -6192.5110, 'eps':     0.9227, 'dyn_loss':    20.4658, 'dot_loss':     0.6203, 'ddot_loss':     0.0770, 'rew_loss':    14.1276, 'lr':     0.0001, 'eps_e':     0.9227, 'lr_e':     0.0001})
Step:   11000, Reward: -1223.143 [ 163.762], Avg: -1246.123 (0.914) <0-00:12:04> ({'r_t': -6108.7090, 'eps':     0.9135, 'dyn_loss':    20.1380, 'dot_loss':     0.5762, 'ddot_loss':     0.0707, 'rew_loss':    14.3189, 'lr':     0.0001, 'eps_e':     0.9135, 'lr_e':     0.0001})
Step:   12000, Reward: -1230.667 [ 150.313], Avg: -1244.934 (0.904) <0-00:13:17> ({'r_t': -6178.5001, 'eps':     0.9044, 'dyn_loss':    19.7082, 'dot_loss':     0.5451, 'ddot_loss':     0.0666, 'rew_loss':    14.2597, 'lr':     0.0001, 'eps_e':     0.9044, 'lr_e':     0.0001})
Step:   13000, Reward: -1204.013 [ 142.038], Avg: -1242.011 (0.895) <0-00:14:23> ({'r_t': -6189.5947, 'eps':     0.8953, 'dyn_loss':    19.0925, 'dot_loss':     0.5116, 'ddot_loss':     0.0623, 'rew_loss':    14.3515, 'lr':     0.0001, 'eps_e':     0.8953, 'lr_e':     0.0001})
Step:   14000, Reward: -1269.153 [ 196.343], Avg: -1243.820 (0.886) <0-00:15:33> ({'r_t': -6152.9248, 'eps':     0.8864, 'dyn_loss':    18.6020, 'dot_loss':     0.4888, 'ddot_loss':     0.0589, 'rew_loss':    14.0905, 'lr':     0.0001, 'eps_e':     0.8864, 'lr_e':     0.0001})
Step:   15000, Reward: -1231.886 [ 170.595], Avg: -1243.074 (0.878) <0-00:16:38> ({'r_t': -6278.2838, 'eps':     0.8775, 'dyn_loss':    18.1689, 'dot_loss':     0.4709, 'ddot_loss':     0.0565, 'rew_loss':    14.0341, 'lr':     0.0001, 'eps_e':     0.8775, 'lr_e':     0.0001})
Step:   16000, Reward: -1216.880 [ 187.080], Avg: -1241.534 (0.869) <0-00:17:45> ({'r_t': -6187.0516, 'eps':     0.8687, 'dyn_loss':    17.7356, 'dot_loss':     0.4548, 'ddot_loss':     0.0546, 'rew_loss':    14.0027, 'lr':     0.0001, 'eps_e':     0.8687, 'lr_e':     0.0001})
Step:   17000, Reward: -1210.905 [ 154.697], Avg: -1239.832 (0.860) <0-00:18:51> ({'r_t': -6046.8750, 'eps':     0.8601, 'dyn_loss':    17.5223, 'dot_loss':     0.4422, 'ddot_loss':     0.0529, 'rew_loss':    14.1713, 'lr':     0.0001, 'eps_e':     0.8601, 'lr_e':     0.0001})
Step:   18000, Reward: -1196.039 [ 187.984], Avg: -1237.527 (0.851) <0-00:19:56> ({'r_t': -6034.9415, 'eps':     0.8515, 'dyn_loss':    17.4411, 'dot_loss':     0.4369, 'ddot_loss':     0.0516, 'rew_loss':    14.0779, 'lr':     0.0001, 'eps_e':     0.8515, 'lr_e':     0.0001})
Step:   19000, Reward: -1216.391 [ 195.259], Avg: -1236.470 (0.843) <0-00:21:02> ({'r_t': -6138.1932, 'eps':     0.8429, 'dyn_loss':    16.9512, 'dot_loss':     0.4230, 'ddot_loss':     0.0503, 'rew_loss':    13.9897, 'lr':     0.0001, 'eps_e':     0.8429, 'lr_e':     0.0001})
Step:   20000, Reward: -1184.521 [ 182.080], Avg: -1233.997 (0.835) <0-00:22:14> ({'r_t': -6095.3881, 'eps':     0.8345, 'dyn_loss':    16.7504, 'dot_loss':     0.4149, 'ddot_loss':     0.0492, 'rew_loss':    13.9955, 'lr':     0.0001, 'eps_e':     0.8345, 'lr_e':     0.0001})
Step:   21000, Reward: -1269.580 [ 217.190], Avg: -1235.614 (0.826) <0-00:23:21> ({'r_t': -5934.5085, 'eps':     0.8262, 'dyn_loss':    16.8194, 'dot_loss':     0.4119, 'ddot_loss':     0.0487, 'rew_loss':    14.1112, 'lr':     0.0001, 'eps_e':     0.8262, 'lr_e':     0.0001})
Step:   22000, Reward: -1340.243 [ 214.644], Avg: -1240.163 (0.818) <0-00:24:27> ({'r_t': -6259.0414, 'eps':     0.8179, 'dyn_loss':    16.2807, 'dot_loss':     0.4004, 'ddot_loss':     0.0472, 'rew_loss':    14.0161, 'lr':     0.0001, 'eps_e':     0.8179, 'lr_e':     0.0001})
Step:   23000, Reward: -1309.687 [ 184.782], Avg: -1243.060 (0.810) <0-00:25:33> ({'r_t': -6258.8869, 'eps':     0.8097, 'dyn_loss':    16.1953, 'dot_loss':     0.3946, 'ddot_loss':     0.0465, 'rew_loss':    14.0291, 'lr':     0.0001, 'eps_e':     0.8097, 'lr_e':     0.0001})
Step:   24000, Reward: -1144.027 [ 120.275], Avg: -1239.099 (0.802) <0-00:26:38> ({'r_t': -6180.3404, 'eps':     0.8016, 'dyn_loss':    15.9388, 'dot_loss':     0.3870, 'ddot_loss':     0.0454, 'rew_loss':    13.8715, 'lr':     0.0001, 'eps_e':     0.8016, 'lr_e':     0.0001})
Step:   25000, Reward: -1253.537 [ 174.158], Avg: -1239.654 (0.794) <0-00:27:46> ({'r_t': -6141.9155, 'eps':     0.7936, 'dyn_loss':    15.8337, 'dot_loss':     0.3829, 'ddot_loss':     0.0449, 'rew_loss':    13.8891, 'lr':     0.0001, 'eps_e':     0.7936, 'lr_e':     0.0001})
Step:   26000, Reward: -1274.634 [ 175.363], Avg: -1240.949 (0.786) <0-00:28:54> ({'r_t': -6055.9562, 'eps':     0.7857, 'dyn_loss':    15.7932, 'dot_loss':     0.3792, 'ddot_loss':     0.0446, 'rew_loss':    13.8886, 'lr':     0.0001, 'eps_e':     0.7857, 'lr_e':     0.0001})
Step:   27000, Reward: -1166.258 [ 125.980], Avg: -1238.282 (0.778) <0-00:30:03> ({'r_t': -6177.3058, 'eps':     0.7778, 'dyn_loss':    15.5240, 'dot_loss':     0.3729, 'ddot_loss':     0.0437, 'rew_loss':    13.8288, 'lr':     0.0001, 'eps_e':     0.7778, 'lr_e':     0.0001})
Step:   28000, Reward: -1330.010 [ 226.959], Avg: -1241.445 (0.770) <0-00:31:10> ({'r_t': -6210.4650, 'eps':     0.7700, 'dyn_loss':    15.2251, 'dot_loss':     0.3659, 'ddot_loss':     0.0426, 'rew_loss':    13.7061, 'lr':     0.0001, 'eps_e':     0.7700, 'lr_e':     0.0001})
Step:   29000, Reward: -1275.262 [ 198.571], Avg: -1242.572 (0.762) <0-00:32:18> ({'r_t': -6546.7243, 'eps':     0.7623, 'dyn_loss':    14.9368, 'dot_loss':     0.3594, 'ddot_loss':     0.0419, 'rew_loss':    13.7408, 'lr':     0.0001, 'eps_e':     0.7623, 'lr_e':     0.0001})
Step:   30000, Reward: -1262.666 [ 193.102], Avg: -1243.220 (0.755) <0-00:33:26> ({'r_t': -6197.9027, 'eps':     0.7547, 'dyn_loss':    14.9586, 'dot_loss':     0.3560, 'ddot_loss':     0.0415, 'rew_loss':    13.7855, 'lr':     0.0001, 'eps_e':     0.7547, 'lr_e':     0.0001})
Step:   31000, Reward: -1132.940 [ 155.112], Avg: -1239.774 (0.747) <0-00:34:43> ({'r_t': -6087.0207, 'eps':     0.7472, 'dyn_loss':    14.4910, 'dot_loss':     0.3476, 'ddot_loss':     0.0401, 'rew_loss':    13.4993, 'lr':     0.0001, 'eps_e':     0.7472, 'lr_e':     0.0001})
