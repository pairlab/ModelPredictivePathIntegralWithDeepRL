Model: <class 'src.models.pytorch.agents.ddpg.DDPGAgent'>, Env: CarRacing-v1, Date: 01/06/2020 10:22:20
CPU: 8 Core, 5.0GHz, 62.66 GB, Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid
GPU 0: GeForce RTX 2070, 7.98 GB (Driver: 440.64.00)
Git URL: git@github.com:shawnmanuel000/ModelPredictivePathIntegralWithDeepRL.git
Hash: f49760a1503c280235bea170083f10c4af2abbf0
Branch: master

config: 
   TRIAL_AT = 1000
   SAVE_AT = 1
   SEED = 0
   REG_LAMBDA = 1e-06
   LEARN_RATE = 0.0001
   DISCOUNT_RATE = 0.99
   ADVANTAGE_DECAY = 0.95
   INPUT_LAYER = 512
   ACTOR_HIDDEN = 256
   CRITIC_HIDDEN = 1024
   EPS_MAX = 1.0
   EPS_MIN = 0.1
   EPS_DECAY = 0.998
   NUM_STEPS = 500
   MAX_BUFFER_SIZE = 100000
   REPLAY_BATCH_SIZE = 32
   TARGET_UPDATE_RATE = 0.0004
   dynamics_size = 13
   state_size = (80,)
   action_size = (3,)
   env_name = CarRacing-v1
   rank = 0
   size = 17
   split = 17
   model = ddpg
   framework = pt
   train_prop = 1.0
   tcp_ports = [10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015, 10016]
   tcp_rank = 0
   num_envs = 1
   nsteps = 1000000
   render = False
   trial = False
   icm = False
   rs = False,
num_envs: 16,
envs: <src.utils.envs.EnvManager object at 0x7f6fe810ffd0> 
	env = <GymEnv<CarRacing<CarRacing-v1>>> 
		env = <CarRacing<CarRacing-v1>> 
			channel = <mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel object at 0x7f6fe1ea2b50>
			scale_sim = <function CarRacing.__init__.<locals>.<lambda> at 0x7f6fe1d1db00>
			env = <UnityToGymWrapper instance> 
				visual_obs = None
				game_over = False
				name = CarBehavior?team=0
				group_spec = BehaviorSpec(observation_shapes=[(30,)], action_type=<ActionType.CONTINUOUS: 1>, action_shape=3)
				use_visual = False
				uint8_visual = False
			cost_model = <src.envs.CarRacing.objective.cost.CostModel object at 0x7f6fe804ead0> 
				track = <src.envs.CarRacing.objective.track.Track object at 0x7f6fe1d1bd90> 
					track = <list len=500>
					X = (1.540585208684206, 1.5814536064863205, 1.6016383588314056, 1.6350171357393264, 1.6559478223323822, 1.6717498254776002, 1.709812204837799, 1.7354034245014192, 1.7725858569145203, 1.8077154874801635, 1.958074402809143, 2.0178433418273927, 2.1851138830184937, 2.258661150932312, 2.3439700841903686, 2.452700424194336, 2.586679172515869, 2.782884216308594, 3.047244071960449, 3.4783129692077637, 3.9734771251678467, 4.596014499664307, 5.29957389831543, 6.05716609954834, 6.824328422546387, 7.646727561950684, 8.59219741821289, 9.675070762634277, 10.77119255065918, 11.868535041809082, 12.83842658996582, 13.727555274963379, 14.569844245910645, 15.391722679138184, 16.204023361206055, 17.02372169494629, 17.626384735107422, 18.072078704833984, 18.462026596069336, 18.803436279296875, 19.08125877380371, 19.200590133666992, 19.074377059936523, 18.833162307739258, 18.582487106323242, 18.339160919189453, 17.97744369506836, 17.59515380859375, 17.09140968322754, 16.50218391418457, 15.817791938781738, 14.983868598937988, 13.986822128295898, 12.817933082580566, 11.528505325317383, 10.241579055786133, 8.946599960327148, 7.588953971862793, 6.2032341957092285, 4.799948692321777, 3.3720505237579346, 1.9454675912857056, 0.4815756678581238, -0.9242660999298096, -2.3082480430603027, -3.7190709114074707, -5.090760231018066, -6.490819931030273, -7.933252811431885, -9.48039722442627, -11.141877174377441, -12.927711486816406, -14.796602249145508, -16.603300094604492, -18.390233993530273, -20.1385498046875, -21.805997848510742, -23.41408920288086, -25.02754783630371, -26.801597595214844, -28.776451110839844, -30.972705841064453, -33.385520935058594, -35.90762710571289, -38.527618408203125, -41.362369537353516, -44.435585021972656, -47.831398010253906, -51.587188720703125, -55.642662048339844, -59.980804443359375, -64.55036163330078, -69.1060562133789, -73.4732666015625, -77.65788269042969, -81.6474380493164, -85.45370483398438, -89.12055206298828, -92.67816925048828, -96.15220642089844, -99.54827117919922, -102.86875915527344, -106.01786804199219, -109.03597259521484, -111.96282958984375, -114.75870513916016, -117.48453521728516, -120.2335205078125, -123.01750946044922, -125.81232452392578, -128.56246948242188, -131.20936584472656, -133.767333984375, -136.21359252929688, -138.6573486328125, -141.0603485107422, -143.3613739013672, -145.4899444580078, -147.5723114013672, -149.41514587402344, -150.9908905029297, -152.32089233398438, -153.6006622314453, -154.83030700683594, -156.0063018798828, -157.14691162109375, -158.23680114746094, -159.30880737304688, -160.30152893066406, -161.2411651611328, -162.03582763671875, -162.72186279296875, -163.28753662109375, -163.81460571289062, -164.31549072265625, -164.78814697265625, -165.1201171875, -165.26596069335938, -165.24961853027344, -165.20376586914062, -165.07931518554688, -165.0469512939453, -165.03262329101562, -164.86660766601562, -164.62220764160156, -164.3842315673828, -164.145263671875, -163.90011596679688, -163.64981079101562, -163.3218231201172, -162.726318359375, -161.83493041992188, -160.71856689453125, -159.4139862060547, -157.9736328125, -156.54212951660156, -155.10464477539062, -153.63636779785156, -152.13641357421875, -150.6412811279297, -149.1659698486328, -147.64437866210938, -146.01336669921875, -144.21286010742188, -142.3518829345703, -140.49502563476562, -138.6591796875, -136.8135986328125, -134.9413604736328, -132.9547882080078, -130.7132110595703, -128.1597137451172, -125.3279037475586, -122.26266479492188, -118.97386932373047, -115.49871826171875, -111.90750122070312, -108.16539764404297, -104.34297180175781, -100.58757781982422, -96.96247863769531, -93.51396942138672, -90.1981201171875, -86.93607330322266, -83.70171356201172, -80.58210754394531, -77.49177551269531, -74.4620132446289, -71.53809356689453, -68.60317993164062, -65.52932739257812, -62.46957778930664, -59.48895263671875, -56.56187057495117, -53.813289642333984, -51.1711311340332, -48.648197174072266, -46.242332458496094, -43.94118118286133, -41.766075134277344, -39.70472717285156, -37.813140869140625, -36.01365280151367, -34.269657135009766, -32.50520706176758, -30.680166244506836, -28.837051391601562, -27.001256942749023, -25.25333023071289, -23.701873779296875, -22.668081283569336, -22.199195861816406, -22.169893264770508, -22.46630859375, -23.134033203125, -24.32797622680664, -26.001781463623047, -27.869766235351562, -29.80392074584961, -31.775949478149414, -33.793365478515625, -35.771907806396484, -37.70563888549805, -39.61886215209961, -41.516029357910156, -43.41127014160156, -45.27768325805664, -47.11109924316406, -48.94091796875, -50.77583694458008, -52.619163513183594, -54.48332977294922, -56.314815521240234, -58.103755950927734, -59.823333740234375, -61.56585693359375, -63.30061340332031, -64.97642517089844, -66.51130676269531, -67.94270324707031, -69.3357925415039, -70.66708374023438, -71.93402099609375, -73.18978118896484, -74.31753540039062, -75.23255920410156, -75.95966339111328, -76.61920166015625, -77.26768493652344, -77.9359130859375, -78.5946273803711, -79.26289367675781, -79.79534912109375, -80.2015380859375, -80.60335540771484, -81.02714538574219, -81.53772735595703, -82.04193878173828, -82.53047180175781, -83.04158020019531, -83.56088256835938, -84.14714813232422, -84.81393432617188, -85.55133056640625, -86.36656188964844, -87.24837493896484, -88.13751983642578, -88.99240112304688, -89.81124877929688, -90.60415649414062, -91.33631896972656, -92.02133178710938, -92.65229034423828, -93.23121643066406, -93.7853012084961, -94.3372573852539, -94.88070678710938, -95.41710662841797, -95.84803771972656, -96.24778747558594, -96.6568374633789, -97.0496826171875, -97.41992950439453, -97.77052307128906, -97.91485595703125, -97.96147155761719, -97.87026977539062, -97.53227233886719, -96.85386657714844, -95.81302642822266, -94.54135131835938, -93.15739440917969, -91.603271484375, -89.95466613769531, -88.35015106201172, -86.80291748046875, -85.39144134521484, -84.07344055175781, -82.86149597167969, -81.5972671508789, -80.11182403564453, -78.36345672607422, -76.40621948242188, -74.32894134521484, -72.0761489868164, -69.69659423828125, -67.17849731445312, -64.48152160644531, -61.61235046386719, -58.499427795410156, -55.10073471069336, -51.55522918701172, -47.74736785888672, -43.832923889160156, -39.801971435546875, -35.743858337402344, -31.80649757385254, -28.028738021850586, -24.38759994506836, -20.836519241333008, -17.374597549438477, -14.002902030944824, -10.617079734802246, -7.34421443939209, -4.187110424041748, -1.115414023399353, 2.037353277206421, 5.401520252227783, 8.870983123779297, 12.423381805419922, 16.180818557739258, 20.157392501831055, 24.33769989013672, 28.77823829650879, 33.3828010559082, 38.12346267700195, 42.767642974853516, 47.21396255493164, 51.497074127197266, 55.640106201171875, 59.61445999145508, 63.45794677734375, 67.16992950439453, 70.71627044677734, 74.12809753417969, 77.53622436523438, 80.97876739501953, 84.45626068115234, 87.9986572265625, 91.61026000976562, 95.1865234375, 98.68260192871094, 102.08172607421875, 105.37554168701172, 108.5978012084961, 111.72406005859375, 114.72969818115234, 117.6103515625, 120.28418731689453, 122.77039337158203, 125.10813903808594, 127.35991668701172, 129.5707550048828, 131.73577880859375, 133.8451385498047, 135.88076782226562, 137.81361389160156, 139.69195556640625, 141.56494140625, 143.51321411132812, 145.43582153320312, 147.37954711914062, 149.30592346191406, 151.1349334716797, 152.76832580566406, 154.18382263183594, 155.40008544921875, 156.48155212402344, 157.39840698242188, 158.19866943359375, 158.91281127929688, 159.4974822998047, 160.02337646484375, 160.31883239746094, 160.23129272460938, 159.7694854736328, 159.0675506591797, 158.11312866210938, 157.08311462402344, 155.8784942626953, 154.47816467285156, 152.8489990234375, 151.00660705566406, 149.11109924316406, 147.24368286132812, 145.35427856445312, 143.4554443359375, 141.39073181152344, 139.07090759277344, 136.57705688476562, 134.08177185058594, 131.63348388671875, 129.23263549804688, 126.91446685791016, 124.63007354736328, 122.27965545654297, 119.90943145751953, 117.51732635498047, 115.1493148803711, 112.83964538574219, 110.53994750976562, 108.22462463378906, 105.85285949707031, 103.4562759399414, 101.13794708251953, 98.82323455810547, 96.44384765625, 93.94629669189453, 91.3570556640625, 88.73168182373047, 86.05917358398438, 83.26211547851562, 80.25263214111328, 77.10718536376953, 73.97905731201172, 70.96484375, 68.1133804321289, 65.44701385498047, 62.890159606933594, 60.41355514526367, 57.95263671875, 55.59248352050781, 53.20044708251953, 50.7462272644043, 48.28958511352539, 45.88505935668945, 43.5562744140625, 41.31084442138672, 39.171634674072266, 37.183380126953125, 35.43268966674805, 33.800804138183594, 32.20466613769531, 30.66669273376465, 29.13826560974121, 27.552635192871094, 25.97852325439453, 24.294662475585938, 22.565439224243164, 20.874217987060547, 19.30082893371582, 17.831933975219727, 16.408084869384766, 15.044317245483398, 13.766607284545898, 12.577005386352539, 11.475253105163574, 10.496495246887207, 9.622332572937012, 8.769275665283203, 7.927954196929932, 7.112521648406982, 6.322704315185547, 5.563619136810303, 4.829586982727051, 4.113427639007568, 3.3697121143341064, 2.5567243099212646, 1.7977246046066284, 1.0246542692184448, 0.2572939395904541, -0.4480553865432739, -1.1242897510528564, -1.6556841135025024, -2.0525705814361572, -2.214649200439453, -2.169621467590332, -2.035892963409424, -1.9102517366409302, -1.7909443378448486, -1.7162281274795532, -1.651557445526123, -1.5775796175003052, -1.5097243785858154, -1.4451829195022583, -1.3808107376098633, -1.3076838254928589, -1.1195673942565918, -0.8252816200256348, -0.5349398255348206, -0.2580118477344513, 0.009828831069171429, 0.2716897428035736, 0.5349469780921936, 0.7902784943580627, 1.052398443222046, 1.31592857837677, 1.570581078529358, 1.6137370109558105, 1.6365979194641114)
					Z = (-0.8819639682769775, -0.8812801241874695, -0.8804802298545837, -0.8791921734809875, -0.8777425289154053, -0.8758563995361328, -0.873963475227356, -0.8539403676986694, -0.7802032232284546, -0.761174201965332, -0.7716957926750183, -0.8395041823387146, -0.8772552609443665, -0.8344407081604004, -0.788372814655304, -0.80742347240448, -0.8527643084526062, -0.8346409797668457, -0.824370265007019, -0.8134136199951172, -0.7967275381088257, -0.7752544283866882, -0.7417746782302856, -0.6927484273910522, -0.633834719657898, -0.5747796297073364, -0.5113369226455688, -0.4433113932609558, -0.3737497925758362, -0.3008161187171936, -0.2312106341123581, -0.16523221135139465, -0.09990986436605453, -0.033577218651771545, 0.03842548280954361, 0.11881522089242935, 0.1981208622455597, 0.28177762031555176, 0.38250869512557983, 0.5017393231391907, 0.625041127204895, 0.7394312620162964, 0.8367793560028076, 0.9279725551605225, 1.0242633819580078, 1.1258037090301514, 1.2272775173187256, 1.3421326875686646, 1.4506069421768188, 1.561546802520752, 1.6706804037094116, 1.7743912935256958, 1.8515067100524902, 1.9097793102264404, 1.948763370513916, 1.9814872741699219, 2.0233898162841797, 2.07637095451355, 2.132861375808716, 2.17509126663208, 2.2180161476135254, 2.274773597717285, 2.3546767234802246, 2.4420950412750244, 2.5328733921051025, 2.6344215869903564, 2.7358694076538086, 2.8366494178771973, 2.9418249130249023, 3.0620920658111572, 3.1827614307403564, 3.30625581741333, 3.427833080291748, 3.5489587783813477, 3.675954818725586, 3.79117488861084, 3.901960849761963, 4.005653381347656, 4.107993125915527, 4.2158284187316895, 4.328779220581055, 4.445080280303955, 4.569532871246338, 4.690032005310059, 4.799752712249756, 4.872299671173096, 4.92843770980835, 4.985036849975586, 5.057000637054443, 5.13352108001709, 5.213327884674072, 5.295718193054199, 5.3766703605651855, 5.451817512512207, 5.519579887390137, 5.582165718078613, 5.639312267303467, 5.692175388336182, 5.7414727210998535, 5.787367820739746, 5.830183506011963, 5.869744300842285, 5.905086994171143, 5.936120986938477, 5.963281154632568, 5.987318992614746, 6.008669376373291, 6.027542591094971, 6.044310569763184, 6.057828903198242, 6.067286968231201, 6.074985504150391, 6.081448554992676, 6.086737155914307, 6.091536998748779, 6.096595764160156, 6.1012773513793945, 6.104137420654297, 6.10720682144165, 6.105283260345459, 6.09289026260376, 6.069871425628662, 6.042582988739014, 6.011574745178223, 5.977062702178955, 5.945542812347412, 5.9195661544799805, 5.900696277618408, 5.875031471252441, 5.850343227386475, 5.822032451629639, 5.787215232849121, 5.749323844909668, 5.708043575286865, 5.672667503356934, 5.640613079071045, 5.58774995803833, 5.510519504547119, 5.4132280349731445, 5.318352222442627, 5.21757173538208, 5.129578113555908, 5.049224376678467, 4.955892086029053, 4.855170726776123, 4.759181022644043, 4.6699957847595215, 4.590251922607422, 4.507761478424072, 4.420248508453369, 4.298507213592529, 4.1367998123168945, 3.954977035522461, 3.7536673545837402, 3.5393548011779785, 3.336235761642456, 3.13871431350708, 2.941469192504883, 2.743802785873413, 2.5500059127807617, 2.362222671508789, 2.172161817550659, 1.9712504148483276, 1.7527763843536377, 1.5335578918457031, 1.3216581344604492, 1.11974036693573, 0.924856424331665, 0.7362942099571228, 0.548167884349823, 0.3510936498641968, 0.14911779761314392, -0.04503828287124634, -0.22794248163700104, -0.3905165493488312, -0.5209499597549438, -0.6174218654632568, -0.6916936039924622, -0.7458155751228333, -0.7768694162368774, -0.7899942994117737, -0.7893635630607605, -0.7789414525032043, -0.7635725736618042, -0.7461717128753662, -0.7283236980438232, -0.704211413860321, -0.6622856855392456, -0.5993924140930176, -0.5216199159622192, -0.426088809967041, -0.3150973916053772, -0.1974087506532669, -0.07835512608289719, 0.03133012354373932, 0.13556505739688873, 0.24022513628005981, 0.3493971824645996, 0.45991453528404236, 0.5715771317481995, 0.6827750205993652, 0.7940959930419922, 0.907843291759491, 1.025125503540039, 1.148614764213562, 1.2811535596847534, 1.417541265487671, 1.5532535314559937, 1.6824359893798828, 1.7986339330673218, 1.8819316625595093, 1.9304401874542236, 1.9543043375015259, 1.9636659622192383, 1.9588732719421387, 1.916387915611267, 1.8345577716827393, 1.7349056005477905, 1.6296110153198242, 1.5208213329315186, 1.405418872833252, 1.2866981029510498, 1.16438889503479, 1.0394600629806519, 0.9107307195663452, 0.7798608541488647, 0.6512886881828308, 0.5262399315834045, 0.4030036926269531, 0.2815271019935608, 0.16398224234580994, 0.05072043836116791, -0.05590145289897919, -0.15327762067317963, -0.24135041236877441, -0.3243723213672638, -0.3988741636276245, -0.4620799124240875, -0.542617678642273, -0.646656334400177, -0.7287228107452393, -0.7844877243041992, -0.806078314781189, -0.8148013949394226, -0.8116025924682617, -0.8039451837539673, -0.7978506088256836, -0.8006065487861633, -0.8066939115524292, -0.8129818439483643, -0.8215823173522949, -0.8290983438491821, -0.8362972736358643, -0.8428731560707092, -0.8489797711372375, -0.8558133840560913, -0.8626493811607361, -0.8682581186294556, -0.8741699457168579, -0.879978597164154, -0.8859436511993408, -0.8909560441970825, -0.8937748670578003, -0.8939367532730103, -0.8897822499275208, -0.8787690997123718, -0.8593403697013855, -0.8307321667671204, -0.8021003603935242, -0.7821503281593323, -0.7700151801109314, -0.7592963576316833, -0.7492351531982422, -0.7390634417533875, -0.7314242720603943, -0.7212424278259277, -0.7080341577529907, -0.6888165473937988, -0.66937655210495, -0.6463529467582703, -0.6128187775611877, -0.5654257535934448, -0.5037499666213989, -0.42715343832969666, -0.34471648931503296, -0.25006303191185, -0.14578062295913696, -0.03818090260028839, 0.0759134441614151, 0.21288788318634033, 0.35622480511665344, 0.515775203704834, 0.6532223224639893, 0.7738814949989319, 0.8932506442070007, 1.0421302318572998, 1.2146294116973877, 1.385721206665039, 1.5515326261520386, 1.7406084537506104, 1.9566478729248047, 2.214561700820923, 2.5135207176208496, 2.8274102210998535, 3.160696268081665, 3.501220941543579, 3.8431997299194336, 4.200472354888916, 4.574350357055664, 4.894090175628662, 5.0936360359191895, 5.216364860534668, 5.390469074249268, 5.586197853088379, 5.784314155578613, 5.985593795776367, 6.1828765869140625, 6.373883247375488, 6.556783199310303, 6.733740329742432, 6.906088829040527, 7.071183204650879, 7.233142852783203, 7.3868231773376465, 7.530625343322754, 7.665377616882324, 7.797634124755859, 7.930730819702148, 8.059279441833496, 8.180848121643066, 8.296680450439453, 8.406368255615234, 8.505520820617676, 8.589674949645996, 8.655287742614746, 8.70052719116211, 8.722027778625488, 8.70865249633789, 8.652679443359375, 8.560135841369629, 8.443024635314941, 8.307100296020508, 8.149582862854004, 7.971302032470703, 7.780361175537109, 7.575259685516357, 7.355491638183594, 7.124767303466797, 6.885737419128418, 6.638427257537842, 6.395895481109619, 6.166090488433838, 5.953654766082764, 5.738729953765869, 5.529703140258789, 5.342148303985596, 5.179572105407715, 5.024766445159912, 4.851255416870117, 4.646117210388184, 4.430662155151367, 4.217848777770996, 4.0131144523620605, 3.7878849506378174, 3.559556245803833, 3.3353841304779053, 3.1190574169158936, 2.9180359840393066, 2.7267343997955322, 2.5381720066070557, 2.3227102756500244, 2.0959630012512207, 1.8809078931808472, 1.6847819089889526, 1.495663046836853, 1.3055880069732666, 1.1171165704727173, 0.9520562887191772, 0.8042331337928772, 0.681337833404541, 0.5795820951461792, 0.5025584101676941, 0.46133852005004883, 0.4328932762145996, 0.3858243227005005, 0.3234015107154846, 0.2624247372150421, 0.19709435105323792, 0.15313704311847687, 0.11826862394809723, 0.08544927090406418, 0.04712279140949249, 0.0015682056546211243, -0.026410788297653198, -0.03486667573451996, -0.027389593422412872, -0.0065015703439712524, 0.0059362053871154785, 0.002570606768131256, -0.006264716386795044, -0.013282939791679382, -0.018584154546260834, -0.022372961044311523, -0.0232115238904953, -0.02133723348379135, -0.030498042702674866, -0.057736508548259735, -0.09805164486169815, -0.13833804428577423, -0.17615404725074768, -0.21290594339370728, -0.24737012386322021, -0.26589956879615784, -0.2773838937282562, -0.2822290062904358, -0.2861996591091156, -0.2940981388092041, -0.2990141808986664, -0.3035801351070404, -0.3050832152366638, -0.3049992024898529, -0.30373987555503845, -0.3003387153148651, -0.29614898562431335, -0.2985635995864868, -0.31389492750167847, -0.34401920437812805, -0.3844596743583679, -0.4300534129142761, -0.4741150140762329, -0.5105020999908447, -0.5354415774345398, -0.552415132522583, -0.5600359439849854, -0.5654557943344116, -0.5681073665618896, -0.5666967630386353, -0.5622239112854004, -0.5597591996192932, -0.5650179386138916, -0.579081654548645, -0.5969113707542419, -0.6101321578025818, -0.622231125831604, -0.6340838074684143, -0.6458472609519958, -0.657522976398468, -0.6685013771057129, -0.6801296472549438, -0.6912583708763123, -0.7032382488250732, -0.7155491709709167, -0.7265709042549133, -0.7348979115486145, -0.7445682287216187, -0.7536845207214355, -0.761847198009491, -0.7706142067909241, -0.7806366682052612, -0.7898868322372437, -0.7978246212005615, -0.8051745295524597, -0.8114349842071533, -0.8171375393867493, -0.821597158908844, -0.8264663219451904, -0.8312869071960449, -0.8363567590713501, -0.8399266004562378, -0.8434712290763855, -0.8482410907745361, -0.8517320156097412, -0.8557907342910767, -0.8605977296829224, -0.864855170249939, -0.8680832982063293, -0.869952917098999, -0.8720065951347351, -0.8741781711578369, -0.8759156465530396, -0.8775535821914673, -0.8793764710426331, -0.8817098140716553, -0.8832718729972839, -0.8847836852073669, -0.8870889544487, -0.8891378045082092, -0.8896875977516174, -0.8895387649536133, -0.8889559507369995, -0.8881706595420837, -0.8874912261962891, -0.8865614533424377, -0.8851791024208069, -0.8832001686096191, -0.8809881806373596, -0.8781297206878662, -0.8746054172515869, -0.8718098402023315, -0.8688086271286011)
					Y = (0.24426956474781036, 0.4990326166152954, 0.819128692150116, 1.153626799583435, 1.5026447772979736, 1.8859440088272095, 2.373248815536499, 2.968236207962036, 3.61586332321167, 4.355114459991455, 5.173743724822998, 6.038478374481201, 6.951005458831787, 7.899267673492432, 8.918261528015137, 10.051026344299316, 11.312947273254395, 12.90755558013916, 14.871548652648926, 17.198680877685547, 19.908754348754883, 22.898487091064453, 26.10063934326172, 29.397844314575195, 32.636375427246094, 35.74137878417969, 38.707183837890625, 41.484439849853516, 44.07951736450195, 46.60736846923828, 49.15201187133789, 51.65317916870117, 54.06341552734375, 56.4561882019043, 58.852813720703125, 61.29132080078125, 63.84211730957031, 66.49172973632812, 69.07376861572266, 71.62057495117188, 74.08918762207031, 76.49169158935547, 78.78299713134766, 80.95753479003906, 83.06936645507812, 85.1029281616211, 87.12429809570312, 89.12969970703125, 91.03314971923828, 92.87902069091797, 94.55635070800781, 96.09061431884766, 97.33863830566406, 98.26770782470703, 98.91900634765625, 99.34143829345703, 99.79500579833984, 100.22048950195312, 100.46652221679688, 100.50714111328125, 100.43055725097656, 100.3218765258789, 100.27439880371094, 100.24840545654297, 100.22171020507812, 100.19712829589844, 100.16851043701172, 100.09687042236328, 100.02641296386719, 99.95970153808594, 99.8285140991211, 99.58265686035156, 99.25724792480469, 98.94861602783203, 98.7610855102539, 98.6032943725586, 98.43841552734375, 98.27819061279297, 98.11662292480469, 97.93367004394531, 97.72758483886719, 97.4378662109375, 97.10028839111328, 96.74153900146484, 96.36189270019531, 95.95005798339844, 95.50723266601562, 95.01679229736328, 94.47090911865234, 93.8803482055664, 93.24833679199219, 92.5796127319336, 91.90768432617188, 91.14244079589844, 90.31917572021484, 89.48597717285156, 88.64861297607422, 87.82418823242188, 87.01628875732422, 86.22871398925781, 85.56230163574219, 84.96900177001953, 84.57625579833984, 84.36016082763672, 84.20700073242188, 84.08193969726562, 83.97764587402344, 83.87611389160156, 83.92423248291016, 84.14193725585938, 84.41809844970703, 84.70330810546875, 85.00025939941406, 85.29436492919922, 85.68895721435547, 86.27693176269531, 87.06804656982422, 88.0323715209961, 89.15747833251953, 90.61774444580078, 92.43035125732422, 94.46464538574219, 96.57106018066406, 98.82080078125, 101.0973129272461, 103.33666229248047, 105.50848388671875, 107.6570053100586, 109.891357421875, 112.15137481689453, 114.42011260986328, 116.68489074707031, 118.90473175048828, 121.11170959472656, 123.25049591064453, 125.32403564453125, 127.53121185302734, 129.89825439453125, 132.2855987548828, 134.6158905029297, 136.92697143554688, 139.15802001953125, 141.3134002685547, 143.4351806640625, 145.5569305419922, 147.65158081054688, 149.7096405029297, 151.71261596679688, 153.65261840820312, 155.51608276367188, 157.31924438476562, 159.11117553710938, 160.7533416748047, 162.2732696533203, 163.74002075195312, 165.19287109375, 166.6624298095703, 168.05679321289062, 169.36721801757812, 170.6645965576172, 171.94862365722656, 173.23680114746094, 174.46946716308594, 175.60227966308594, 176.68606567382812, 177.7667236328125, 178.8304901123047, 179.89537048339844, 180.9698944091797, 182.1023712158203, 183.38099670410156, 184.83396911621094, 186.4405059814453, 188.17733764648438, 190.03277587890625, 191.99041748046875, 193.9769287109375, 195.76626586914062, 197.2998809814453, 198.64427185058594, 199.84442138671875, 201.0236358642578, 202.19769287109375, 203.31591796875, 204.40118408203125, 205.4407196044922, 206.46392822265625, 207.45944213867188, 208.4150848388672, 209.36993408203125, 210.36520385742188, 211.35165405273438, 212.19497680664062, 212.80360412597656, 212.99081420898438, 212.8595428466797, 212.59893798828125, 212.30372619628906, 211.88113403320312, 211.2249298095703, 210.27505493164062, 209.16802978515625, 207.95042419433594, 206.6737060546875, 205.3536376953125, 203.98805236816406, 202.4827117919922, 200.79603576660156, 198.84075927734375, 196.52613830566406, 193.94662475585938, 191.1892852783203, 188.33187866210938, 185.4967803955078, 182.7758331298828, 180.3319091796875, 178.08534240722656, 175.87472534179688, 173.57350158691406, 171.1052703857422, 168.51658630371094, 165.9554443359375, 163.4188995361328, 160.97314453125, 158.5869903564453, 156.26071166992188, 154.0010223388672, 151.86273193359375, 149.84214782714844, 147.8561553955078, 145.87100219726562, 143.8812255859375, 141.9394073486328, 140.04071044921875, 138.22088623046875, 136.38259887695312, 134.54953002929688, 132.78271484375, 130.9574737548828, 129.08750915527344, 127.25975799560547, 125.4315185546875, 123.64933013916016, 121.882080078125, 120.05531311035156, 118.18463134765625, 116.25498962402344, 114.34269714355469, 112.4908447265625, 110.6985092163086, 108.94164276123047, 107.16153717041016, 105.32911682128906, 103.44462585449219, 101.6138916015625, 99.76459503173828, 97.91300964355469, 96.16510772705078, 94.41311645507812, 92.58258056640625, 90.4946517944336, 88.02781677246094, 85.19628143310547, 82.00907135009766, 78.48986053466797, 74.69635772705078, 70.86166381835938, 67.15168762207031, 63.572113037109375, 60.10674285888672, 56.803375244140625, 53.6189079284668, 50.549373626708984, 47.61164474487305, 44.77302932739258, 41.92876434326172, 39.06986999511719, 36.2219352722168, 33.32758331298828, 30.242610931396484, 26.973918914794922, 23.662368774414062, 20.41046714782715, 17.231449127197266, 14.126823425292969, 11.168815612792969, 8.347853660583496, 5.706920623779297, 3.3018741607666016, 1.2335699796676636, -0.5328974723815918, -2.043576717376709, -3.110535144805908, -3.740983486175537, -4.098943710327148, -4.4906511306762695, -4.8972249031066895, -5.2530198097229, -5.577995777130127, -5.934023857116699, -6.255759239196777, -6.630918025970459, -7.013139724731445, -7.412384033203125, -7.725191116333008, -8.017799377441406, -8.335323333740234, -8.662646293640137, -9.008383750915527, -9.383427619934082, -9.718378067016602, -10.013775825500488, -10.301630973815918, -10.562592506408691, -10.815587997436523, -11.065951347351074, -11.301687240600586, -11.448249816894531, -11.537090301513672, -11.524465560913086, -11.443005561828613, -11.383244514465332, -11.339241981506348, -11.295818328857422, -11.257658004760742, -11.223909378051758, -11.219079971313477, -11.304905891418457, -11.446738243103027, -11.616390228271484, -11.812542915344238, -12.02774429321289, -12.266841888427734, -12.534515380859375, -12.815123558044434, -13.006359100341797, -13.117430686950684, -13.182148933410645, -13.210461616516113, -13.223767280578613, -13.236565589904785, -13.257308006286621, -13.364906311035156, -13.60283374786377, -13.906349182128906, -14.247852325439453, -14.630463600158691, -15.034890174865723, -15.458684921264648, -15.909191131591797, -16.372478485107422, -16.83634376525879, -17.298728942871094, -17.954330444335938, -18.74985694885254, -19.579227447509766, -20.42566680908203, -21.43193817138672, -22.800357818603516, -24.44293212890625, -26.13048553466797, -27.82823944091797, -29.55722427368164, -31.477741241455078, -33.487709045410156, -35.511478424072266, -37.493263244628906, -39.456016540527344, -41.433685302734375, -43.504295349121094, -45.86669158935547, -48.45779037475586, -51.14822006225586, -53.83092498779297, -56.52829360961914, -59.291015625, -62.107452392578125, -64.86852264404297, -67.60960388183594, -70.36067199707031, -73.03939819335938, -75.66210174560547, -78.23661041259766, -80.80587005615234, -83.38500213623047, -85.95026397705078, -88.392578125, -90.68785095214844, -92.96864318847656, -95.2093505859375, -97.35236358642578, -99.36150360107422, -101.18042755126953, -102.92134857177734, -104.60369110107422, -106.27859497070312, -107.93692779541016, -109.50454711914062, -110.95790100097656, -112.26480102539062, -113.4476318359375, -114.55032348632812, -115.59841918945312, -116.59353637695312, -117.56787872314453, -118.43424987792969, -119.07018280029297, -119.529541015625, -119.9432144165039, -120.33118438720703, -120.70291137695312, -121.06876373291016, -121.57264709472656, -122.14915466308594, -122.72602844238281, -123.31329345703125, -123.84371948242188, -124.38484191894531, -124.94699096679688, -125.50639343261719, -126.06773376464844, -126.62725067138672, -127.21639251708984, -127.76771545410156, -128.14712524414062, -128.24986267089844, -128.0001220703125, -127.45743560791016, -126.70941925048828, -125.85266876220703, -124.98062133789062, -124.1561508178711, -123.36287689208984, -122.56819915771484, -121.65084838867188, -120.66740417480469, -119.70370483398438, -118.76301574707031, -117.76809692382812, -116.55887603759766, -115.09596252441406, -113.52935028076172, -111.99527740478516, -110.50000762939453, -108.9967041015625, -107.39553833007812, -105.7052001953125, -103.86796569824219, -101.89085388183594, -99.83897399902344, -97.75530242919922, -95.71993255615234, -93.73746490478516, -91.82310485839844, -89.95047760009766, -88.10604858398438, -86.26592254638672, -84.39051818847656, -82.42990112304688, -80.4601821899414, -78.54206085205078, -76.67953491210938, -74.87965393066406, -73.13782501220703, -71.447998046875, -69.79700469970703, -68.07174682617188, -66.20356750488281, -64.17756652832031, -62.02452850341797, -59.78955841064453, -57.599979400634766, -55.49079895019531, -53.38170623779297, -51.32799530029297, -49.24906539916992, -47.25999069213867, -45.2713508605957, -43.23389434814453, -41.17817687988281, -39.17205047607422, -37.22850799560547, -35.21967697143555, -33.25495910644531, -31.328039169311523, -29.30510902404785, -27.14748191833496, -24.93663215637207, -22.68917465209961, -20.511201858520508, -18.440406799316406, -16.442750930786133, -14.476696014404297, -12.49740982055664, -10.538829803466797, -8.549440383911133, -6.5612688064575195, -4.653802394866943, -2.830416679382324, -1.0931862592697144)
					Xmap = [-215.266 -214.266 -213.266 -212.266 -211.266 -210.266 -209.266 -208.266 -207.266 -206.266 -205.266 -204.266 -203.266 -202.266 -201.266 -200.266 -199.266 -198.266 -197.266 -196.266 -195.266 -194.266 -193.266 -192.266 -191.266 -190.266 -189.266 -188.266 -187.266 -186.266 -185.266 -184.266 -183.266 -182.266 -181.266 -180.266 -179.266 -178.266 -177.266 -176.266 -175.266 -174.266 -173.266 -172.266 -171.266 -170.266 -169.266 -168.266 -167.266 -166.266 -165.266 -164.266 -163.266 -162.266 -161.266 -160.266 -159.266 -158.266 -157.266 -156.266 -155.266 -154.266 -153.266 -152.266 -151.266 -150.266 -149.266 -148.266 -147.266 -146.266 -145.266 -144.266 -143.266 -142.266 -141.266 -140.266 -139.266 -138.266 -137.266 -136.266 -135.266 -134.266 -133.266 -132.266 -131.266 -130.266 -129.266 -128.266 -127.266 -126.266 -125.266 -124.266 -123.266 -122.266 -121.266 -120.266 -119.266 -118.266 -117.266 -116.266 -115.266 -114.266 -113.266 -112.266 -111.266 -110.266 -109.266 -108.266 -107.266 -106.266 -105.266 -104.266 -103.266 -102.266 -101.266 -100.266  -99.266  -98.266  -97.266  -96.266  -95.266  -94.266  -93.266  -92.266  -91.266  -90.266  -89.266  -88.266  -87.266  -86.266  -85.266  -84.266  -83.266  -82.266  -81.266  -80.266  -79.266  -78.266  -77.266  -76.266  -75.266  -74.266  -73.266  -72.266  -71.266  -70.266  -69.266  -68.266  -67.266  -66.266  -65.266  -64.266  -63.266  -62.266  -61.266  -60.266  -59.266  -58.266  -57.266  -56.266  -55.266  -54.266  -53.266  -52.266  -51.266  -50.266  -49.266  -48.266  -47.266  -46.266  -45.266  -44.266  -43.266  -42.266  -41.266  -40.266  -39.266  -38.266  -37.266  -36.266  -35.266  -34.266  -33.266  -32.266  -31.266  -30.266  -29.266  -28.266  -27.266  -26.266  -25.266  -24.266  -23.266  -22.266  -21.266  -20.266  -19.266  -18.266  -17.266  -16.266  -15.266  -14.266  -13.266  -12.266  -11.266  -10.266   -9.266   -8.266   -7.266   -6.266   -5.266   -4.266   -3.266   -2.266   -1.266   -0.266    0.734    1.734    2.734    3.734    4.734    5.734
					    6.734    7.734    8.734    9.734   10.734   11.734   12.734   13.734   14.734   15.734   16.734   17.734   18.734   19.734   20.734   21.734   22.734   23.734   24.734   25.734   26.734   27.734   28.734   29.734   30.734   31.734   32.734   33.734   34.734   35.734   36.734   37.734   38.734   39.734   40.734   41.734   42.734   43.734   44.734   45.734   46.734   47.734   48.734   49.734   50.734   51.734   52.734   53.734   54.734   55.734   56.734   57.734   58.734   59.734   60.734   61.734   62.734   63.734   64.734   65.734   66.734   67.734   68.734   69.734   70.734   71.734   72.734   73.734   74.734   75.734   76.734   77.734   78.734   79.734   80.734   81.734   82.734   83.734   84.734   85.734   86.734   87.734   88.734   89.734   90.734   91.734   92.734   93.734   94.734   95.734   96.734   97.734   98.734   99.734  100.734  101.734  102.734  103.734  104.734  105.734  106.734  107.734  108.734  109.734  110.734  111.734  112.734  113.734  114.734  115.734  116.734  117.734  118.734  119.734  120.734  121.734  122.734  123.734  124.734  125.734  126.734  127.734  128.734  129.734  130.734  131.734  132.734  133.734  134.734  135.734  136.734  137.734  138.734  139.734  140.734  141.734  142.734  143.734  144.734  145.734  146.734  147.734  148.734  149.734  150.734  151.734  152.734  153.734  154.734  155.734  156.734  157.734  158.734  159.734  160.734  161.734  162.734  163.734  164.734  165.734  166.734  167.734  168.734  169.734  170.734  171.734  172.734  173.734  174.734  175.734  176.734  177.734  178.734  179.734  180.734  181.734  182.734  183.734  184.734  185.734  186.734  187.734  188.734  189.734  190.734  191.734  192.734  193.734  194.734  195.734  196.734  197.734  198.734  199.734  200.734  201.734  202.734  203.734  204.734  205.734  206.734  207.734  208.734  209.734]
					Ymap = [-1.782e+02 -1.772e+02 -1.762e+02 -1.752e+02 -1.742e+02 -1.732e+02 -1.722e+02 -1.712e+02 -1.702e+02 -1.692e+02 -1.682e+02 -1.672e+02 -1.662e+02 -1.652e+02 -1.642e+02 -1.632e+02 -1.622e+02 -1.612e+02 -1.602e+02 -1.592e+02 -1.582e+02 -1.572e+02 -1.562e+02 -1.552e+02 -1.542e+02 -1.532e+02 -1.522e+02 -1.512e+02 -1.502e+02 -1.492e+02 -1.482e+02 -1.472e+02 -1.462e+02 -1.452e+02 -1.442e+02 -1.432e+02 -1.422e+02 -1.412e+02 -1.402e+02 -1.392e+02 -1.382e+02 -1.372e+02 -1.362e+02 -1.352e+02 -1.342e+02 -1.332e+02 -1.322e+02 -1.312e+02 -1.302e+02 -1.292e+02 -1.282e+02 -1.272e+02 -1.262e+02 -1.252e+02 -1.242e+02 -1.232e+02 -1.222e+02 -1.212e+02 -1.202e+02 -1.192e+02 -1.182e+02 -1.172e+02 -1.162e+02 -1.152e+02 -1.142e+02 -1.132e+02 -1.122e+02 -1.112e+02 -1.102e+02 -1.092e+02 -1.082e+02 -1.072e+02 -1.062e+02 -1.052e+02 -1.042e+02 -1.032e+02 -1.022e+02 -1.012e+02 -1.002e+02 -9.925e+01 -9.825e+01 -9.725e+01 -9.625e+01 -9.525e+01 -9.425e+01 -9.325e+01 -9.225e+01 -9.125e+01 -9.025e+01 -8.925e+01 -8.825e+01 -8.725e+01 -8.625e+01 -8.525e+01 -8.425e+01 -8.325e+01 -8.225e+01 -8.125e+01 -8.025e+01 -7.925e+01 -7.825e+01 -7.725e+01 -7.625e+01 -7.525e+01 -7.425e+01 -7.325e+01 -7.225e+01 -7.125e+01 -7.025e+01 -6.925e+01 -6.825e+01 -6.725e+01 -6.625e+01 -6.525e+01 -6.425e+01 -6.325e+01 -6.225e+01 -6.125e+01 -6.025e+01 -5.925e+01 -5.825e+01 -5.725e+01 -5.625e+01 -5.525e+01 -5.425e+01 -5.325e+01 -5.225e+01 -5.125e+01 -5.025e+01 -4.925e+01 -4.825e+01 -4.725e+01 -4.625e+01 -4.525e+01 -4.425e+01 -4.325e+01 -4.225e+01 -4.125e+01 -4.025e+01 -3.925e+01 -3.825e+01 -3.725e+01 -3.625e+01 -3.525e+01 -3.425e+01 -3.325e+01 -3.225e+01 -3.125e+01 -3.025e+01 -2.925e+01 -2.825e+01 -2.725e+01 -2.625e+01 -2.525e+01 -2.425e+01 -2.325e+01 -2.225e+01 -2.125e+01 -2.025e+01 -1.925e+01 -1.825e+01 -1.725e+01 -1.625e+01 -1.525e+01 -1.425e+01 -1.325e+01 -1.225e+01 -1.125e+01 -1.025e+01 -9.250e+00 -8.250e+00 -7.250e+00 -6.250e+00 -5.250e+00 -4.250e+00 -3.250e+00 -2.250e+00 -1.250e+00 -2.499e-01  7.501e-01  1.750e+00
					  2.750e+00  3.750e+00  4.750e+00  5.750e+00  6.750e+00  7.750e+00  8.750e+00  9.750e+00  1.075e+01  1.175e+01  1.275e+01  1.375e+01  1.475e+01  1.575e+01  1.675e+01  1.775e+01  1.875e+01  1.975e+01  2.075e+01  2.175e+01  2.275e+01  2.375e+01  2.475e+01  2.575e+01  2.675e+01  2.775e+01  2.875e+01  2.975e+01  3.075e+01  3.175e+01  3.275e+01  3.375e+01  3.475e+01  3.575e+01  3.675e+01  3.775e+01  3.875e+01  3.975e+01  4.075e+01  4.175e+01  4.275e+01  4.375e+01  4.475e+01  4.575e+01  4.675e+01  4.775e+01  4.875e+01  4.975e+01  5.075e+01  5.175e+01  5.275e+01  5.375e+01  5.475e+01  5.575e+01  5.675e+01  5.775e+01  5.875e+01  5.975e+01  6.075e+01  6.175e+01  6.275e+01  6.375e+01  6.475e+01  6.575e+01  6.675e+01  6.775e+01  6.875e+01  6.975e+01  7.075e+01  7.175e+01  7.275e+01  7.375e+01  7.475e+01  7.575e+01  7.675e+01  7.775e+01  7.875e+01  7.975e+01  8.075e+01  8.175e+01  8.275e+01  8.375e+01  8.475e+01  8.575e+01  8.675e+01  8.775e+01  8.875e+01  8.975e+01  9.075e+01  9.175e+01  9.275e+01  9.375e+01  9.475e+01  9.575e+01  9.675e+01  9.775e+01  9.875e+01  9.975e+01  1.008e+02  1.018e+02  1.028e+02  1.038e+02  1.048e+02  1.058e+02  1.068e+02  1.078e+02  1.088e+02  1.098e+02  1.108e+02  1.118e+02  1.128e+02  1.138e+02  1.148e+02  1.158e+02  1.168e+02  1.178e+02  1.188e+02  1.198e+02  1.208e+02  1.218e+02  1.228e+02  1.238e+02  1.248e+02  1.258e+02  1.268e+02  1.278e+02  1.288e+02  1.298e+02  1.308e+02  1.318e+02  1.328e+02  1.338e+02  1.348e+02  1.358e+02  1.368e+02  1.378e+02  1.388e+02  1.398e+02  1.408e+02  1.418e+02  1.428e+02  1.438e+02  1.448e+02  1.458e+02  1.468e+02  1.478e+02  1.488e+02  1.498e+02  1.508e+02  1.518e+02  1.528e+02  1.538e+02  1.548e+02  1.558e+02  1.568e+02  1.578e+02  1.588e+02  1.598e+02  1.608e+02  1.618e+02  1.628e+02  1.638e+02  1.648e+02  1.658e+02  1.668e+02  1.678e+02  1.688e+02  1.698e+02  1.708e+02  1.718e+02  1.728e+02  1.738e+02  1.748e+02  1.758e+02  1.768e+02  1.778e+02  1.788e+02  1.798e+02  1.808e+02  1.818e+02  1.828e+02
					  1.838e+02  1.848e+02  1.858e+02  1.868e+02  1.878e+02  1.888e+02  1.898e+02  1.908e+02  1.918e+02  1.928e+02  1.938e+02  1.948e+02  1.958e+02  1.968e+02  1.978e+02  1.988e+02  1.998e+02  2.008e+02  2.018e+02  2.028e+02  2.038e+02  2.048e+02  2.058e+02  2.068e+02  2.078e+02  2.088e+02  2.098e+02  2.108e+02  2.118e+02  2.128e+02  2.138e+02  2.148e+02  2.158e+02  2.168e+02  2.178e+02  2.188e+02  2.198e+02  2.208e+02  2.218e+02  2.228e+02  2.238e+02  2.248e+02  2.258e+02  2.268e+02  2.278e+02  2.288e+02  2.298e+02  2.308e+02  2.318e+02  2.328e+02  2.338e+02  2.348e+02  2.358e+02  2.368e+02  2.378e+02  2.388e+02  2.398e+02  2.408e+02  2.418e+02  2.428e+02  2.438e+02  2.448e+02  2.458e+02  2.468e+02  2.478e+02  2.488e+02  2.498e+02  2.508e+02  2.518e+02  2.528e+02  2.538e+02  2.548e+02  2.558e+02  2.568e+02  2.578e+02  2.588e+02  2.598e+02  2.608e+02  2.618e+02  2.628e+02]
					Zmap = [-5.894 -4.894 -3.894 -2.894 -1.894 -0.894  0.106  1.106  2.106  3.106  4.106  5.106  6.106  7.106  8.106  9.106 10.106 11.106 12.106 13.106]
					point_map = [[[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]]
					
					 [[291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  [291 291 291 ... 291 291 291]
					  ...
					  [161 161 161 ... 161 161 161]
					  [161 161 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 [[291 291 291 ... 292 292 292]
					  [291 291 291 ... 291 292 292]
					  [291 291 291 ... 291 291 291]
					  ...
					  [162 162 161 ... 161 161 161]
					  [162 162 162 ... 161 161 161]
					  [162 162 162 ... 161 161 161]]
					
					 ...
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]
					
					 [[395 395 395 ... 395 395 395]
					  [395 395 395 ... 395 395 395]
					  [394 394 394 ... 394 394 394]
					  ...
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]
					  [210 210 210 ... 210 210 210]]]
					res = 1
					min_point = [-215.266 -178.250   -5.894]
					max_point = [ 209.734  262.750   13.106]
				X = [-215.266 -215.166 -215.066 ...  210.034  210.134  210.234]
				Y = [-178.250 -178.150 -178.050 ...  262.750  262.850  262.950]
				Z = [-0.894  8.722]
				cost_map = [[[ 214.381  214.381]
				  [ 214.299  214.299]
				  [ 214.217  214.217]
				  ...
				  [ 112.184  112.184]
				  [ 112.264  112.264]
				  [ 112.344  112.344]]
				
				 [[ 214.324  214.324]
				  [ 214.242  214.242]
				  [ 214.160  214.160]
				  ...
				  [ 112.124  112.124]
				  [ 112.204  112.204]
				  [ 112.284  112.284]]
				
				 [[ 214.267  214.267]
				  [ 214.185  214.185]
				  [ 214.103  214.103]
				  ...
				  [ 112.064  112.064]
				  [ 112.144  112.144]
				  [ 112.224  112.224]]
				
				 ...
				
				 [[  96.764   96.764]
				  [  96.690   96.690]
				  [  96.616   96.616]
				  ...
				  [ 242.661  242.661]
				  [ 242.689  242.689]
				  [ 242.717  242.717]]
				
				 [[  96.831   96.831]
				  [  96.757   96.757]
				  [  96.683   96.683]
				  ...
				  [ 242.757  242.757]
				  [ 242.785  242.785]
				  [ 242.813  242.813]]
				
				 [[  96.898   96.898]
				  [  96.824   96.824]
				  [  96.750   96.750]
				  ...
				  [ 242.852  242.852]
				  [ 242.881  242.881]
				  [ 242.909  242.909]]]
				res = 0.1
				min_point = [-215.266 -178.250   -0.894]
				max_point = [ 210.234  262.950    8.722]
				src = 
						def get_cost(self, state, prevstate=None):
							prevstate = state if prevstate is None else prevstate
							prevpos = prevstate["pos"][...,[0,2,1]]
							pos = state["pos"][...,[0,2,1]]
							vy = state["vel"][...,-1]
							cost = self.get_point_cost(pos, transform=True)
							progress = self.track.get_progress(prevpos, pos)
							reward = np.minimum(progress,0) + 2*progress + np.tanh(vy/self.vtarget)-np.power(self.vtarget-vy,2)/self.vtarget**2 - cost
							# reward = progress + np.tanh(vy/self.vtarget) - cost
				
				vtarget = 20
			action_space = Box(3,) 
				dtype = float32
				shape = (3,)
				low = [-1.000 -1.000 -1.000]
				high = [ 1.000  1.000  1.000]
				bounded_below = [ True  True  True]
				bounded_above = [ True  True  True]
				np_random = RandomState(MT19937)
			cost_queries = <list len=25>
			dynamics_size = 13
			obs = [ 1.617e-09 -3.908e-03 -7.273e-09  1.777e-12 -1.954e-01  3.555e-13  0.000e+00  0.000e+00  0.000e+00  1.000e+00  9.095e-13 -1.164e-10 -4.547e-12  0.000e+00  2.000e-02  3.657e-01  4.017e-01  4.572e-01  5.260e-01  6.036e-01  2.700e-01  3.171e-01  3.850e-01  4.646e-01  5.509e-01  1.792e-01  2.444e-01  3.277e-01  4.184e-01  5.125e-01  1.063e-01  1.973e-01  2.942e-01  3.927e-01  4.918e-01  1.024e-01  1.953e-01  2.929e-01  3.917e-01  4.910e-01]
			observation_space = Box(80,) 
				dtype = float32
				shape = (80,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
			src = 		return state
				
					def step(self, action):
						self.time += 1
						next_state, reward, done, info = self.env.step(action)
						idle = next_state[29]
						done = done or idle>self.idle_timeout or self.time > self.max_time
						next_state, next_spec = self.observation(next_state)
						terminal = -(1-self.time/self.max_time)*int(done)
						reward = -self.cost_model.get_cost(next_spec, self.spec) + terminal
						self.spec = next_spec
			
			max_time = 500
			time = 0
			idle_timeout = 10
			spec = EnvSpec(CarRacing-v1) 
				id = CarRacing-v1
				entry_point = <class 'src.envs.CarRacing.car_racing.CarRacing'> 
					reset = <function CarRacing.reset at 0x7f7061a86680>
					step = <function CarRacing.step at 0x7f7061a865f0>
					render = <function CarRacing.render at 0x7f708bef0b90>
					dynamics_spec = <staticmethod object at 0x7f7061a87bd0>
					track_spec = <function CarRacing.track_spec at 0x7f708bef0cb0>
					observation = <function CarRacing.observation at 0x7f708bef0d40>
					dynamics_keys = <staticmethod object at 0x7f7061a87ad0>
					observation_spec = <staticmethod object at 0x7f7061a87b10>
					close = <function CarRacing.close at 0x7f708bef0ef0>
					id = 2
				reward_threshold = None
				nondeterministic = False
				max_episode_steps = None
			verbose = 0
		action_space = Box(3,) 
			dtype = float32
			shape = (3,)
			low = [-1.000 -1.000 -1.000]
			high = [ 1.000  1.000  1.000]
			bounded_below = [ True  True  True]
			bounded_above = [ True  True  True]
			np_random = RandomState(MT19937)
		observation_space = Box(80,) 
			dtype = float32
			shape = (80,)
			low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
			high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
			bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
			np_random = RandomState(MT19937)
		reward_range = (-inf, inf)
		metadata = {'render.modes': []}
		preprocess = <src.envs.wrappers.RawPreprocess object at 0x7f6fe1d1b690> 
			observation_space = Box(80,) 
				dtype = float32
				shape = (80,)
				low = [-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]
				high = [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]
				bounded_below = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				bounded_above = [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False]
				np_random = RandomState(MT19937)
	state_size = (80,)
	action_size = (3,)
	action_space = Box(3,) 
		dtype = float32
		shape = (3,)
		low = [-1.000 -1.000 -1.000]
		high = [ 1.000  1.000  1.000]
		bounded_below = [ True  True  True]
		bounded_above = [ True  True  True]
		np_random = RandomState(MT19937)
	server_ports = <list len=16>
	conn = <src.utils.multiprocess.TCPClient object at 0x7f6fe1d1bd50> 
		num_clients = 16
		client_ranks = <list len=16>
		client_ports = <list len=16>
		client_sockets = {10001: <socket.socket fd=35, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 46026), raddr=('127.0.0.1', 10001)>, 10002: <socket.socket fd=36, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 41746), raddr=('127.0.0.1', 10002)>, 10003: <socket.socket fd=37, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59740), raddr=('127.0.0.1', 10003)>, 10004: <socket.socket fd=38, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 56338), raddr=('127.0.0.1', 10004)>, 10005: <socket.socket fd=39, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 47216), raddr=('127.0.0.1', 10005)>, 10006: <socket.socket fd=40, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 42372), raddr=('127.0.0.1', 10006)>, 10007: <socket.socket fd=41, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 35064), raddr=('127.0.0.1', 10007)>, 10008: <socket.socket fd=42, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59476), raddr=('127.0.0.1', 10008)>, 10009: <socket.socket fd=43, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 35520), raddr=('127.0.0.1', 10009)>, 10010: <socket.socket fd=44, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 54380), raddr=('127.0.0.1', 10010)>, 10011: <socket.socket fd=45, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 44552), raddr=('127.0.0.1', 10011)>, 10012: <socket.socket fd=46, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 59490), raddr=('127.0.0.1', 10012)>, 10013: <socket.socket fd=47, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 37602), raddr=('127.0.0.1', 10013)>, 10014: <socket.socket fd=48, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 38710), raddr=('127.0.0.1', 10014)>, 10015: <socket.socket fd=49, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 48988), raddr=('127.0.0.1', 10015)>, 10016: <socket.socket fd=50, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 33664), raddr=('127.0.0.1', 10016)>}
	num_envs = 16
	max_steps = 1000,
agent: <src.models.wrappers.ParallelAgent object at 0x7f6fe1d1b450> 
	icm = None
	stack = <src.models.wrappers.RawState object at 0x7f6fe1d77f90> 
		state_size = (80,)
	agent = <src.models.pytorch.agents.ddpg.DDPGAgent object at 0x7f6fe1d77f50> 
		noise_process = <src.utils.rand.BrownianNoise object at 0x7f6fe1d4a0d0> 
			size = (3,)
			dt = 0.2
			action = [ 0.151 -0.432 -0.129]
			daction_dt = [-0.978  0.075  0.224]
		discrete = False
		action_size = (3,)
		state_size = (80,)
		config = <src.utils.config.Config object at 0x7f6fe816b550> 
			TRIAL_AT = 1000
			SAVE_AT = 1
			SEED = 0
			REG_LAMBDA = 1e-06
			LEARN_RATE = 0.0001
			DISCOUNT_RATE = 0.99
			ADVANTAGE_DECAY = 0.95
			INPUT_LAYER = 512
			ACTOR_HIDDEN = 256
			CRITIC_HIDDEN = 1024
			EPS_MAX = 1.0
			EPS_MIN = 0.1
			EPS_DECAY = 0.998
			NUM_STEPS = 500
			MAX_BUFFER_SIZE = 100000
			REPLAY_BATCH_SIZE = 32
			TARGET_UPDATE_RATE = 0.0004
			dynamics_size = 13
			state_size = (80,)
			action_size = (3,)
			env_name = CarRacing-v1
			rank = 0
			size = 17
			split = 17
			model = ddpg
			framework = pt
			train_prop = 1.0
			tcp_ports = <list len=17>
			tcp_rank = 0
			num_envs = 1
			nsteps = 1000000
			render = False
			trial = False
			icm = False
			rs = False
		stats = <src.utils.logger.Stats object at 0x7f6fe1d4a390> 
			mean_dict = {}
			sum_dict = {}
		eps = 1.0
		network = DDPGNetwork(
			  (actor_local): DDPGActor(
			    (layer1): Linear(in_features=80, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=3, bias=True)
			    (action_sig): Linear(in_features=256, out_features=3, bias=True)
			  )
			  (actor_target): DDPGActor(
			    (layer1): Linear(in_features=80, out_features=512, bias=True)
			    (layer2): Linear(in_features=512, out_features=256, bias=True)
			    (layer3): Linear(in_features=256, out_features=256, bias=True)
			    (action_mu): Linear(in_features=256, out_features=3, bias=True)
			    (action_sig): Linear(in_features=256, out_features=3, bias=True)
			  )
			  (critic_local): DDPGCritic(
			    (net_state): Linear(in_features=80, out_features=512, bias=True)
			    (net_action): Linear(in_features=3, out_features=512, bias=True)
			    (net_layer1): Linear(in_features=1024, out_features=1024, bias=True)
			    (net_layer2): Linear(in_features=1024, out_features=1024, bias=True)
			    (q_value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			  (critic_target): DDPGCritic(
			    (net_state): Linear(in_features=80, out_features=512, bias=True)
			    (net_action): Linear(in_features=3, out_features=512, bias=True)
			    (net_layer1): Linear(in_features=1024, out_features=1024, bias=True)
			    (net_layer2): Linear(in_features=1024, out_features=1024, bias=True)
			    (q_value): Linear(in_features=1024, out_features=1, bias=True)
			  )
			) 
			discrete = False
			training = True
			tau = 0.0004
			name = ddpg
			stats = <src.utils.logger.Stats object at 0x7f6fe1d4a250> 
				mean_dict = {}
				sum_dict = {}
			config = <src.utils.config.Config object at 0x7f6fe816b550> 
				TRIAL_AT = 1000
				SAVE_AT = 1
				SEED = 0
				REG_LAMBDA = 1e-06
				LEARN_RATE = 0.0001
				DISCOUNT_RATE = 0.99
				ADVANTAGE_DECAY = 0.95
				INPUT_LAYER = 512
				ACTOR_HIDDEN = 256
				CRITIC_HIDDEN = 1024
				EPS_MAX = 1.0
				EPS_MIN = 0.1
				EPS_DECAY = 0.998
				NUM_STEPS = 500
				MAX_BUFFER_SIZE = 100000
				REPLAY_BATCH_SIZE = 32
				TARGET_UPDATE_RATE = 0.0004
				dynamics_size = 13
				state_size = (80,)
				action_size = (3,)
				env_name = CarRacing-v1
				rank = 0
				size = 17
				split = 17
				model = ddpg
				framework = pt
				train_prop = 1.0
				tcp_ports = <list len=17>
				tcp_rank = 0
				num_envs = 1
				nsteps = 1000000
				render = False
				trial = False
				icm = False
				rs = False
			device = cuda
			src = ['class DDPGActor(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config):\n\t\tsuper().__init__()\n\t\tinput_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN\n\t\tself.discrete = type(action_size) != tuple\n\t\tself.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.layer2 = torch.nn.Linear(input_layer, actor_hidden)\n\t\tself.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)\n\t\tself.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.action_sig = torch.nn.Linear(actor_hidden, action_size[-1])\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\n\tdef forward(self, state, sample=True):\n\t\tstate = self.layer1(state).relu() \n\t\tstate = self.layer2(state).relu() \n\t\tstate = self.layer3(state).relu() \n\t\taction_mu = self.action_mu(state)\n\t\taction_sig = self.action_sig(state).exp()\n\t\tepsilon = torch.randn_like(action_sig)\n\t\taction = action_mu + epsilon.mul(action_sig) if sample else action_mu\n\t\treturn action.tanh() if not self.discrete else gsoftmax(action)\n', 'class DDPGCritic(torch.nn.Module):\n\tdef __init__(self, state_size, action_size, config):\n\t\tsuper().__init__()\n\t\tinput_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN\n\t\tself.net_state = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)\n\t\tself.net_action = torch.nn.Linear(action_size[-1], input_layer)\n\t\tself.net_layer1 = torch.nn.Linear(2*input_layer, critic_hidden)\n\t\tself.net_layer2 = torch.nn.Linear(critic_hidden, critic_hidden)\n\t\tself.q_value = torch.nn.Linear(critic_hidden, 1)\n\t\tself.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)\n\n\tdef forward(self, state, action):\n\t\tstate = self.net_state(state).relu()\n\t\tnet_action = self.net_action(action).relu()\n\t\tnet_layer = torch.cat([state, net_action], dim=-1)\n\t\tnet_layer = self.net_layer1(net_layer).relu()\n\t\tnet_layer = self.net_layer2(net_layer).relu()\n\t\tq_value = self.q_value(net_layer)\n\t\treturn q_value\n']
			actor_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
			critic_optimizer = Adam (
			Parameter Group 0
			    amsgrad: False
			    betas: (0.9, 0.999)
			    eps: 1e-08
			    lr: 0.0001
			    weight_decay: 1e-06
			)
		replay_buffer = <src.utils.rand.ReplayBuffer object at 0x7f6fe1aaaad0> 
			buffer = deque([], maxlen=100000)
		buffer = []
	noise_process = <src.utils.rand.BrownianNoise object at 0x7f6fe1aaaa50> 
		size = (3,)
		dt = 0.2
		action = [-0.170  1.000 -1.000]
		daction_dt = [-1.392 -0.889  0.132]
	discrete = False
	action_size = (3,)
	state_size = (80,)
	config = <src.utils.config.Config object at 0x7f6fe816b550> 
		TRIAL_AT = 1000
		SAVE_AT = 1
		SEED = 0
		REG_LAMBDA = 1e-06
		LEARN_RATE = 0.0001
		DISCOUNT_RATE = 0.99
		ADVANTAGE_DECAY = 0.95
		INPUT_LAYER = 512
		ACTOR_HIDDEN = 256
		CRITIC_HIDDEN = 1024
		EPS_MAX = 1.0
		EPS_MIN = 0.1
		EPS_DECAY = 0.998
		NUM_STEPS = 500
		MAX_BUFFER_SIZE = 100000
		REPLAY_BATCH_SIZE = 32
		TARGET_UPDATE_RATE = 0.0004
		dynamics_size = 13
		state_size = (80,)
		action_size = (3,)
		env_name = CarRacing-v1
		rank = 0
		size = 17
		split = 17
		model = ddpg
		framework = pt
		train_prop = 1.0
		tcp_ports = <list len=17>
		tcp_rank = 0
		num_envs = 1
		nsteps = 1000000
		render = False
		trial = False
		icm = False
		rs = False
	stats = <src.utils.logger.Stats object at 0x7f6fe1aaab50> 
		mean_dict = {}
		sum_dict = {},
conn: None,

import torch
import random
import numpy as np
from .base import PTACNetwork, PTAgent, PTCritic, Conv, gsoftmax, one_hot
from src.utils.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		input_layer, actor_hidden = config.INPUT_LAYER, config.ACTOR_HIDDEN
		self.discrete = type(action_size) != tuple
		self.layer1 = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.layer2 = torch.nn.Linear(input_layer, actor_hidden)
		self.layer3 = torch.nn.Linear(actor_hidden, actor_hidden)
		self.action_mu = torch.nn.Linear(actor_hidden, action_size[-1])
		self.action_sig = torch.nn.Linear(actor_hidden, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh() if not self.discrete else gsoftmax(action)
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size, config):
		super().__init__()
		input_layer, critic_hidden = config.INPUT_LAYER, config.CRITIC_HIDDEN
		self.net_state = torch.nn.Linear(state_size[-1], input_layer) if len(state_size)!=3 else Conv(state_size, input_layer)
		self.net_action = torch.nn.Linear(action_size[-1], input_layer)
		self.net_layer1 = torch.nn.Linear(2*input_layer, critic_hidden)
		self.net_layer2 = torch.nn.Linear(critic_hidden, critic_hidden)
		self.q_value = torch.nn.Linear(critic_hidden, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, config, actor=DDPGActor, critic=DDPGCritic, gpu=True, load=None, name="ddpg"): 
		self.discrete = type(action_size)!=tuple
		super().__init__(state_size, action_size, config, actor, critic if not self.discrete else lambda s,a,c: PTCritic(s,a,c), gpu=gpu, load=load, name=name)

	def get_action(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False, probs=False):
		with torch.enable_grad() if grad else torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			q_value = critic(state) if self.discrete else critic(state, action)
			q_value = q_value.gather(-1, action.argmax(-1, keepdim=True)) if self.discrete and not probs else q_value
			return q_value.cpu().numpy() if numpy else q_value
	
	def optimize(self, states, actions, q_targets):
		actions = one_hot(actions) if self.actor_local.discrete else actions
		q_values = self.get_q_value(states, actions, grad=True, probs=False)
		critic_loss = (q_values - q_targets.detach()).pow(2).mean()
		self.step(self.critic_optimizer, critic_loss)
		self.soft_copy(self.critic_local, self.critic_target)

		actor_action = self.actor_local(states)
		q_actions = self.get_q_value(states, actor_action, grad=True, probs=True)
		q_actions = (actor_action*q_actions).sum(-1) if self.discrete else q_actions
		q_baseline = q_targets if self.discrete else q_values
		actor_loss = -(q_actions - q_baseline.detach()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())
		self.soft_copy(self.actor_local, self.actor_target)
		self.stats.mean(critic_loss=critic_loss, actor_loss=actor_loss)
		
class DDPGAgent(PTAgent):
	def __init__(self, state_size, action_size, config, gpu=True, load=None):
		super().__init__(state_size, action_size, config, DDPGNetwork, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if self.discrete and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), numpy=True, sample=sample)
		action = np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.config.NUM_STEPS:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			states = torch.cat([states, self.to_tensor(next_state).unsqueeze(0)], dim=0)
			actions = torch.cat([actions, self.network.get_action(states[-1], use_target=True).unsqueeze(0)], dim=0)
			values = self.network.get_q_value(states, actions, use_target=True)
			targets = self.compute_gae(values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), values[:-1])[0]
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states[:-1], actions[:-1], targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=False)	
		if len(self.replay_buffer) > self.config.REPLAY_BATCH_SIZE:
			states, actions, targets = self.replay_buffer.sample(self.config.REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, targets)
			if np.any(done[0]): self.eps = max(self.eps * self.config.EPS_DECAY, self.config.EPS_MIN)


Step:       0, Reward:  -306.717 [ 112.578], Avg:  -306.717 (1.000) <0-00:00:00> ({'r_t':    -1.1339, 'eps':     1.0000, 'eps_e':     1.0000})
Step:    1000, Reward:  -216.290 [ 157.068], Avg:  -261.503 (0.986) <0-00:00:29> ({'r_t': -2114.6386, 'eps':     0.9861, 'critic_loss':  7247.0088, 'actor_loss':    -1.1485, 'eps_e':     0.9861})
Step:    2000, Reward:  -224.531 [ 195.763], Avg:  -249.179 (0.974) <0-00:00:58> ({'r_t': -1701.0429, 'eps':     0.9743, 'critic_loss':  3609.4761, 'actor_loss':    -4.2926, 'eps_e':     0.9743})
Step:    3000, Reward:  -306.816 [ 526.870], Avg:  -263.588 (0.963) <0-00:01:24> ({'r_t': -1850.3695, 'eps':     0.9627, 'critic_loss':  3154.1487, 'actor_loss':    -6.7572, 'eps_e':     0.9627})
Step:    4000, Reward:  -347.901 [ 215.664], Avg:  -280.451 (0.953) <0-00:01:51> ({'r_t': -1884.8702, 'eps':     0.9531, 'critic_loss':  3169.8647, 'actor_loss':    -6.4882, 'eps_e':     0.9531})
Step:    5000, Reward:  -212.144 [ 180.358], Avg:  -269.066 (0.942) <0-00:02:16> ({'r_t': -1487.3171, 'eps':     0.9417, 'critic_loss':  2992.3491, 'actor_loss':    -5.8806, 'eps_e':     0.9417})
Step:    6000, Reward:  -164.249 [  73.920], Avg:  -254.093 (0.925) <0-00:02:41> ({'r_t': -1721.7131, 'eps':     0.9249, 'critic_loss':  2969.6985, 'actor_loss':    -6.1597, 'eps_e':     0.9249})
Step:    7000, Reward:  -266.278 [ 195.958], Avg:  -255.616 (0.910) <0-00:03:08> ({'r_t': -1768.0455, 'eps':     0.9102, 'critic_loss':  2626.4004, 'actor_loss':    -5.7651, 'eps_e':     0.9102})
Step:    8000, Reward:  -351.532 [ 518.827], Avg:  -266.273 (0.896) <0-00:03:38> ({'r_t': -1687.6768, 'eps':     0.8957, 'critic_loss':  2734.2407, 'actor_loss':    -5.2925, 'eps_e':     0.8957})
Step:    9000, Reward:  -190.808 [ 113.519], Avg:  -258.727 (0.882) <0-00:04:04> ({'r_t': -1447.2482, 'eps':     0.8815, 'critic_loss':  2646.3613, 'actor_loss':    -4.4153, 'eps_e':     0.8815})
Step:   10000, Reward:  -144.581 [ 112.043], Avg:  -248.350 (0.871) <0-00:04:30> ({'r_t': -1349.6788, 'eps':     0.8710, 'critic_loss':  2479.4441, 'actor_loss':    -5.5864, 'eps_e':     0.8710})
Step:   11000, Reward:  -236.360 [ 228.951], Avg:  -247.351 (0.862) <0-00:04:56> ({'r_t': -1440.7506, 'eps':     0.8623, 'critic_loss':  2012.9769, 'actor_loss':    -5.4408, 'eps_e':     0.8623})
Step:   12000, Reward:  -193.400 [ 116.707], Avg:  -243.201 (0.854) <0-00:05:22> ({'r_t': -1333.2878, 'eps':     0.8537, 'critic_loss':  1850.2875, 'actor_loss':    -5.0690, 'eps_e':     0.8537})
Step:   13000, Reward:  -179.661 [  81.945], Avg:  -238.662 (0.842) <0-00:05:46> ({'r_t': -1275.3083, 'eps':     0.8418, 'critic_loss':  1473.6262, 'actor_loss':    -4.6260, 'eps_e':     0.8418})
Step:   14000, Reward:  -203.303 [ 102.952], Avg:  -236.305 (0.835) <0-00:06:13> ({'r_t': -1274.3140, 'eps':     0.8351, 'critic_loss':  1141.6547, 'actor_loss':    -4.5886, 'eps_e':     0.8351})
Step:   15000, Reward:  -166.157 [  93.410], Avg:  -231.921 (0.820) <0-00:06:38> ({'r_t': -1335.0672, 'eps':     0.8202, 'critic_loss':   583.7350, 'actor_loss':    -4.2703, 'eps_e':     0.8202})
Step:   16000, Reward:  -175.613 [ 176.910], Avg:  -228.608 (0.810) <0-00:07:07> ({'r_t': -1334.9613, 'eps':     0.8104, 'critic_loss':   646.0706, 'actor_loss':    -4.3121, 'eps_e':     0.8104})
Step:   17000, Reward:  -140.971 [  90.146], Avg:  -223.740 (0.791) <0-00:07:31> ({'r_t': -1237.2344, 'eps':     0.7912, 'critic_loss':   504.5266, 'actor_loss':    -4.0754, 'eps_e':     0.7912})
Step:   18000, Reward:  -176.296 [ 108.752], Avg:  -221.243 (0.782) <0-00:07:57> ({'r_t': -1307.1098, 'eps':     0.7817, 'critic_loss':   348.1230, 'actor_loss':    -4.0173, 'eps_e':     0.7817})
Step:   19000, Reward:  -175.964 [ 121.372], Avg:  -218.979 (0.771) <0-00:08:23> ({'r_t': -1253.7889, 'eps':     0.7709, 'critic_loss':   513.1292, 'actor_loss':    -4.1735, 'eps_e':     0.7709})
Step:   20000, Reward:  -157.817 [  92.374], Avg:  -216.066 (0.760) <0-00:08:47> ({'r_t': -1266.7994, 'eps':     0.7601, 'critic_loss':   460.0068, 'actor_loss':    -4.3473, 'eps_e':     0.7601})
Step:   21000, Reward:  -148.674 [  73.487], Avg:  -213.003 (0.750) <0-00:09:10> ({'r_t': -1260.6846, 'eps':     0.7495, 'critic_loss':   428.1947, 'actor_loss':    -4.6083, 'eps_e':     0.7495})
Step:   22000, Reward:  -161.155 [ 109.373], Avg:  -210.749 (0.741) <0-00:09:35> ({'r_t': -1231.8762, 'eps':     0.7406, 'critic_loss':   416.6696, 'actor_loss':    -4.8327, 'eps_e':     0.7406})
Step:   23000, Reward:  -151.575 [ 122.138], Avg:  -208.283 (0.726) <0-00:10:01> ({'r_t': -1225.3075, 'eps':     0.7259, 'critic_loss':   235.0185, 'actor_loss':    -4.8436, 'eps_e':     0.7259})
Step:   24000, Reward:  -142.649 [  91.604], Avg:  -205.658 (0.713) <0-00:10:24> ({'r_t': -1197.2219, 'eps':     0.7130, 'critic_loss':   200.2029, 'actor_loss':    -4.9747, 'eps_e':     0.7130})
Step:   25000, Reward:  -146.152 [  73.958], Avg:  -203.369 (0.702) <0-00:10:47> ({'r_t': -1116.3045, 'eps':     0.7016, 'critic_loss':   147.1399, 'actor_loss':    -4.8850, 'eps_e':     0.7016})
Step:   26000, Reward:  -148.495 [  73.454], Avg:  -201.337 (0.689) <0-00:11:11> ({'r_t': -1147.7342, 'eps':     0.6891, 'critic_loss':   122.6036, 'actor_loss':    -4.7890, 'eps_e':     0.6891})
Step:   27000, Reward:  -162.717 [ 117.308], Avg:  -199.957 (0.680) <0-00:11:36> ({'r_t': -1137.3711, 'eps':     0.6795, 'critic_loss':   115.5620, 'actor_loss':    -4.9745, 'eps_e':     0.6795})
Step:   28000, Reward:  -126.917 [ 100.842], Avg:  -197.439 (0.667) <0-00:12:00> ({'r_t': -1149.0544, 'eps':     0.6674, 'critic_loss':   106.2757, 'actor_loss':    -5.0763, 'eps_e':     0.6674})
Step:   29000, Reward:   -86.649 [  48.246], Avg:  -193.746 (0.655) <0-00:12:23> ({'r_t': -1120.0964, 'eps':     0.6555, 'critic_loss':   109.8344, 'actor_loss':    -5.1620, 'eps_e':     0.6555})
Step:   30000, Reward:   -81.859 [  34.739], Avg:  -190.137 (0.642) <0-00:12:45> ({'r_t': -1057.2339, 'eps':     0.6425, 'critic_loss':   106.8100, 'actor_loss':    -5.3726, 'eps_e':     0.6425})
Step:   31000, Reward:   -84.628 [  40.925], Avg:  -186.839 (0.628) <0-00:13:08> ({'r_t': -1030.7924, 'eps':     0.6285, 'critic_loss':   105.3301, 'actor_loss':    -4.7847, 'eps_e':     0.6285})
Step:   32000, Reward:  -142.676 [ 154.526], Avg:  -185.501 (0.615) <0-00:13:37> ({'r_t':  -953.7602, 'eps':     0.6148, 'critic_loss':   103.5541, 'actor_loss':    -4.8151, 'eps_e':     0.6148})
Step:   33000, Reward:   -74.615 [  54.287], Avg:  -182.240 (0.601) <0-00:14:00> ({'r_t':  -926.3504, 'eps':     0.6014, 'critic_loss':    96.4398, 'actor_loss':    -4.5800, 'eps_e':     0.6014})
Step:   34000, Reward:   -82.006 [  36.393], Avg:  -179.376 (0.588) <0-00:14:23> ({'r_t':  -902.0050, 'eps':     0.5883, 'critic_loss':    93.8185, 'actor_loss':    -4.3184, 'eps_e':     0.5883})
Step:   35000, Reward:   -71.492 [  25.016], Avg:  -176.379 (0.575) <0-00:14:45> ({'r_t':  -819.8327, 'eps':     0.5755, 'critic_loss':    90.6832, 'actor_loss':    -4.0952, 'eps_e':     0.5755})
Step:   36000, Reward:   -75.190 [  33.868], Avg:  -173.644 (0.562) <0-00:15:07> ({'r_t':  -769.0668, 'eps':     0.5618, 'critic_loss':    87.9469, 'actor_loss':    -4.2163, 'eps_e':     0.5618})
Step:   37000, Reward:   -58.161 [  44.799], Avg:  -170.605 (0.550) <0-00:15:30> ({'r_t':  -674.8173, 'eps':     0.5496, 'critic_loss':    84.9342, 'actor_loss':    -4.1627, 'eps_e':     0.5496})
Step:   38000, Reward:   -39.468 [  24.150], Avg:  -167.243 (0.542) <0-00:15:52> ({'r_t':  -622.0822, 'eps':     0.5419, 'critic_loss':    95.7622, 'actor_loss':    -4.2466, 'eps_e':     0.5419})
Step:   39000, Reward:   -37.058 [  37.232], Avg:  -163.988 (0.532) <0-00:16:14> ({'r_t':  -540.6950, 'eps':     0.5323, 'critic_loss':   106.2221, 'actor_loss':    -4.7493, 'eps_e':     0.5323})
Step:   40000, Reward:   -40.683 [  37.542], Avg:  -160.981 (0.522) <0-00:16:36> ({'r_t':  -678.9034, 'eps':     0.5217, 'critic_loss':  1598.5165, 'actor_loss':    -5.0600, 'eps_e':     0.5217})
Step:   41000, Reward:    -6.313 [  46.920], Avg:  -157.298 (0.511) <0-00:16:58> ({'r_t':  -359.9579, 'eps':     0.5114, 'critic_loss':  1547.3929, 'actor_loss':    -5.7091, 'eps_e':     0.5114})
Step:   42000, Reward:   -11.389 [  40.630], Avg:  -153.905 (0.501) <0-00:17:21> ({'r_t':  -407.9207, 'eps':     0.5012, 'critic_loss':   398.0622, 'actor_loss':    -5.7120, 'eps_e':     0.5012})
Step:   43000, Reward:   -19.669 [  39.961], Avg:  -150.854 (0.492) <0-00:17:43> ({'r_t':  -189.6661, 'eps':     0.4923, 'critic_loss':  1462.0338, 'actor_loss':    -5.8626, 'eps_e':     0.4923})
Step:   44000, Reward:   -10.753 [  29.795], Avg:  -147.741 (0.482) <0-00:18:05> ({'r_t':   -63.2647, 'eps':     0.4816, 'critic_loss':  1253.9840, 'actor_loss':    -5.8657, 'eps_e':     0.4816})
Step:   45000, Reward:   -15.778 [  33.645], Avg:  -144.872 (0.471) <0-00:18:27> ({'r_t':  -246.5991, 'eps':     0.4711, 'critic_loss':  1531.2659, 'actor_loss':    -5.8494, 'eps_e':     0.4711})
Step:   46000, Reward:     8.923 [  36.413], Avg:  -141.600 (0.463) <0-00:18:49> ({'r_t':  -111.9517, 'eps':     0.4627, 'critic_loss':   882.3757, 'actor_loss':    -5.6244, 'eps_e':     0.4627})
Step:   47000, Reward:    17.728 [  45.731], Avg:  -138.280 (0.453) <0-00:19:13> ({'r_t':  -104.5656, 'eps':     0.4535, 'critic_loss':   341.8788, 'actor_loss':    -5.0958, 'eps_e':     0.4535})
Step:   48000, Reward:     2.654 [  44.306], Avg:  -135.404 (0.444) <0-00:19:35> ({'r_t':    57.0571, 'eps':     0.4436, 'critic_loss':   259.4264, 'actor_loss':    -4.6821, 'eps_e':     0.4436})
Step:   49000, Reward:    -7.127 [  40.584], Avg:  -132.839 (0.434) <0-00:19:57> ({'r_t':    67.5968, 'eps':     0.4339, 'critic_loss':   160.2446, 'actor_loss':    -4.4664, 'eps_e':     0.4339})
Step:   50000, Reward:    -6.921 [  41.481], Avg:  -130.370 (0.426) <0-00:20:20> ({'r_t':   -45.2617, 'eps':     0.4262, 'critic_loss':   211.8229, 'actor_loss':    -4.3802, 'eps_e':     0.4262})
Step:   51000, Reward:    18.307 [  25.524], Avg:  -127.511 (0.418) <0-00:20:41> ({'r_t':    31.7479, 'eps':     0.4177, 'critic_loss':   149.1824, 'actor_loss':    -4.3980, 'eps_e':     0.4177})
Step:   52000, Reward:    25.071 [  47.480], Avg:  -124.632 (0.409) <0-00:21:03> ({'r_t':   181.0796, 'eps':     0.4095, 'critic_loss':   113.0614, 'actor_loss':    -4.4954, 'eps_e':     0.4095})
Step:   53000, Reward:    29.594 [  43.408], Avg:  -121.776 (0.401) <0-00:21:25> ({'r_t':   295.5403, 'eps':     0.4014, 'critic_loss':   104.9464, 'actor_loss':    -4.8173, 'eps_e':     0.4014})
Step:   54000, Reward:    24.412 [  29.341], Avg:  -119.118 (0.393) <0-00:21:47> ({'r_t':   330.0634, 'eps':     0.3934, 'critic_loss':   111.0264, 'actor_loss':    -5.1679, 'eps_e':     0.3934})
Step:   55000, Reward:    36.643 [  47.693], Avg:  -116.336 (0.386) <0-00:22:09> ({'r_t':   332.8245, 'eps':     0.3856, 'critic_loss':   105.6403, 'actor_loss':    -5.3347, 'eps_e':     0.3856})
Step:   56000, Reward:    23.671 [  43.817], Avg:  -113.880 (0.378) <0-00:22:31> ({'r_t':   427.7585, 'eps':     0.3780, 'critic_loss':    93.8384, 'actor_loss':    -5.1752, 'eps_e':     0.3780})
Step:   57000, Reward:    57.621 [  28.531], Avg:  -110.923 (0.370) <0-00:22:52> ({'r_t':   437.8219, 'eps':     0.3705, 'critic_loss':    83.8544, 'actor_loss':    -4.9993, 'eps_e':     0.3705})
Step:   58000, Reward:    56.527 [  33.502], Avg:  -108.085 (0.363) <0-00:23:14> ({'r_t':   471.2026, 'eps':     0.3631, 'critic_loss':    84.5532, 'actor_loss':    -4.7193, 'eps_e':     0.3631})
Step:   59000, Reward:    55.806 [  37.500], Avg:  -105.353 (0.356) <0-00:23:37> ({'r_t':   520.2455, 'eps':     0.3559, 'critic_loss':    83.6888, 'actor_loss':    -4.4135, 'eps_e':     0.3559})
Step:   60000, Reward:    78.304 [  35.978], Avg:  -102.343 (0.349) <0-00:23:58> ({'r_t':   663.5843, 'eps':     0.3489, 'critic_loss':    88.1293, 'actor_loss':    -4.5127, 'eps_e':     0.3489})
Step:   61000, Reward:    80.876 [  23.964], Avg:   -99.387 (0.342) <0-00:24:20> ({'r_t':   687.6254, 'eps':     0.3420, 'critic_loss':    91.0359, 'actor_loss':    -4.5802, 'eps_e':     0.3420})
Step:   62000, Reward:    82.877 [  26.207], Avg:   -96.494 (0.335) <0-00:24:42> ({'r_t':   713.4938, 'eps':     0.3345, 'critic_loss':    88.1328, 'actor_loss':    -4.2491, 'eps_e':     0.3345})
Step:   63000, Reward:    66.794 [  54.530], Avg:   -93.943 (0.327) <0-00:25:03> ({'r_t':   774.7302, 'eps':     0.3272, 'critic_loss':    90.9648, 'actor_loss':    -4.0346, 'eps_e':     0.3272})
Step:   64000, Reward:    79.147 [  29.994], Avg:   -91.280 (0.321) <0-00:25:25> ({'r_t':   825.7085, 'eps':     0.3207, 'critic_loss':    91.1354, 'actor_loss':    -3.7739, 'eps_e':     0.3207})
Step:   65000, Reward:    78.534 [  23.770], Avg:   -88.707 (0.314) <0-00:25:47> ({'r_t':   855.0879, 'eps':     0.3144, 'critic_loss':    84.3412, 'actor_loss':    -3.7114, 'eps_e':     0.3144})
Step:   66000, Reward:    89.452 [  15.345], Avg:   -86.048 (0.308) <0-00:26:08> ({'r_t':   897.3176, 'eps':     0.3075, 'critic_loss':    78.3083, 'actor_loss':    -3.2032, 'eps_e':     0.3075})
Step:   67000, Reward:    92.854 [  39.554], Avg:   -83.417 (0.301) <0-00:26:30> ({'r_t':   909.4327, 'eps':     0.3014, 'critic_loss':    74.6764, 'actor_loss':    -3.0302, 'eps_e':     0.3014})
Step:   68000, Reward:    54.657 [  62.198], Avg:   -81.416 (0.295) <0-00:26:51> ({'r_t':   952.0443, 'eps':     0.2949, 'critic_loss':    68.2761, 'actor_loss':    -2.8516, 'eps_e':     0.2949})
Step:   69000, Reward:    99.034 [  15.645], Avg:   -78.838 (0.289) <0-00:27:13> ({'r_t':   960.6202, 'eps':     0.2890, 'critic_loss':    59.5425, 'actor_loss':    -2.7152, 'eps_e':     0.2890})
Step:   70000, Reward:    87.856 [  42.438], Avg:   -76.490 (0.283) <0-00:27:35> ({'r_t':   942.8907, 'eps':     0.2833, 'critic_loss':    49.0516, 'actor_loss':    -2.5728, 'eps_e':     0.2833})
Step:   71000, Reward:    84.935 [  34.879], Avg:   -74.248 (0.278) <0-00:27:57> ({'r_t':   868.9318, 'eps':     0.2777, 'critic_loss':    45.8867, 'actor_loss':    -2.4258, 'eps_e':     0.2777})
Step:   72000, Reward:    78.675 [  37.190], Avg:   -72.154 (0.273) <0-00:28:19> ({'r_t':   866.4350, 'eps':     0.2727, 'critic_loss':    43.5583, 'actor_loss':    -2.4669, 'eps_e':     0.2727})
Step:   73000, Reward:    99.467 [  22.255], Avg:   -69.834 (0.268) <0-00:28:41> ({'r_t':   857.1357, 'eps':     0.2679, 'critic_loss':    43.9191, 'actor_loss':    -2.4476, 'eps_e':     0.2679})
Step:   74000, Reward:    92.598 [  18.872], Avg:   -67.669 (0.263) <0-00:29:02> ({'r_t':   976.0192, 'eps':     0.2625, 'critic_loss':    37.5379, 'actor_loss':    -2.2905, 'eps_e':     0.2625})
Step:   75000, Reward:    85.744 [  24.134], Avg:   -65.650 (0.257) <0-00:29:24> ({'r_t':   968.3915, 'eps':     0.2573, 'critic_loss':    42.4104, 'actor_loss':    -2.2455, 'eps_e':     0.2573})
Step:   76000, Reward:    99.804 [  21.331], Avg:   -63.501 (0.252) <0-00:29:46> ({'r_t':  1062.1428, 'eps':     0.2522, 'critic_loss':    39.2480, 'actor_loss':    -2.4421, 'eps_e':     0.2522})
Step:   77000, Reward:   113.339 [  16.575], Avg:   -61.234 (0.247) <0-00:30:07> ({'r_t':  1062.8495, 'eps':     0.2472, 'critic_loss':    40.7781, 'actor_loss':    -2.6667, 'eps_e':     0.2472})
Step:   78000, Reward:   107.296 [  19.334], Avg:   -59.101 (0.242) <0-00:30:29> ({'r_t':  1119.0662, 'eps':     0.2423, 'critic_loss':    30.3606, 'actor_loss':    -2.5968, 'eps_e':     0.2423})
Step:   79000, Reward:   111.529 [  17.438], Avg:   -56.968 (0.238) <0-00:30:50> ({'r_t':  1062.5168, 'eps':     0.2375, 'critic_loss':    28.3889, 'actor_loss':    -2.4910, 'eps_e':     0.2375})
Step:   80000, Reward:   100.501 [  21.307], Avg:   -55.024 (0.233) <0-00:31:12> ({'r_t':  1090.7136, 'eps':     0.2328, 'critic_loss':    25.8516, 'actor_loss':    -2.1601, 'eps_e':     0.2328})
Step:   81000, Reward:   115.617 [  17.863], Avg:   -52.943 (0.228) <0-00:31:34> ({'r_t':  1064.8599, 'eps':     0.2282, 'critic_loss':    23.3619, 'actor_loss':    -2.1001, 'eps_e':     0.2282})
Step:   82000, Reward:    87.825 [  14.878], Avg:   -51.247 (0.224) <0-00:31:56> ({'r_t':   841.7423, 'eps':     0.2241, 'critic_loss':   278.5287, 'actor_loss':    -1.9268, 'eps_e':     0.2241})
Step:   83000, Reward:   116.163 [  16.182], Avg:   -49.254 (0.220) <0-00:32:17> ({'r_t':  1034.3393, 'eps':     0.2197, 'critic_loss':   214.6433, 'actor_loss':    -1.6250, 'eps_e':     0.2197})
Step:   84000, Reward:   104.498 [  17.339], Avg:   -47.445 (0.215) <0-00:32:39> ({'r_t':  1119.5863, 'eps':     0.2153, 'critic_loss':   735.5209, 'actor_loss':    -1.6315, 'eps_e':     0.2153})
Step:   85000, Reward:   101.633 [  14.251], Avg:   -45.712 (0.211) <0-00:33:00> ({'r_t':  1137.0298, 'eps':     0.2111, 'critic_loss':   573.6512, 'actor_loss':    -1.7978, 'eps_e':     0.2111})
Step:   86000, Reward:   108.019 [  14.332], Avg:   -43.945 (0.207) <0-00:33:22> ({'r_t':  1131.0085, 'eps':     0.2069, 'critic_loss':   455.5221, 'actor_loss':    -1.7846, 'eps_e':     0.2069})
Step:   87000, Reward:   116.629 [  12.477], Avg:   -42.120 (0.202) <0-00:33:43> ({'r_t':  1144.7230, 'eps':     0.2024, 'critic_loss':  1020.1931, 'actor_loss':    -1.5883, 'eps_e':     0.2024})
Step:   88000, Reward:   115.120 [  16.368], Avg:   -40.353 (0.198) <0-00:34:05> ({'r_t':  1206.2976, 'eps':     0.1984, 'critic_loss':   223.4824, 'actor_loss':    -1.4796, 'eps_e':     0.1984})
Step:   89000, Reward:   118.611 [  16.340], Avg:   -38.587 (0.194) <0-00:34:26> ({'r_t':  1251.6347, 'eps':     0.1944, 'critic_loss':    13.5163, 'actor_loss':    -1.2789, 'eps_e':     0.1944})
Step:   90000, Reward:   115.224 [  13.169], Avg:   -36.897 (0.190) <0-00:34:48> ({'r_t':  1215.3884, 'eps':     0.1902, 'critic_loss':    11.4950, 'actor_loss':    -1.2431, 'eps_e':     0.1902})
Step:   91000, Reward:   122.719 [  10.002], Avg:   -35.162 (0.186) <0-00:35:09> ({'r_t':  1213.7289, 'eps':     0.1864, 'critic_loss':    10.2370, 'actor_loss':    -1.2925, 'eps_e':     0.1864})
Step:   92000, Reward:   126.541 [  11.697], Avg:   -33.423 (0.183) <0-00:35:31> ({'r_t':  1232.9406, 'eps':     0.1827, 'critic_loss':     9.3335, 'actor_loss':    -1.3899, 'eps_e':     0.1827})
Step:   93000, Reward:   121.405 [  15.262], Avg:   -31.776 (0.179) <0-00:35:53> ({'r_t':  1242.0663, 'eps':     0.1795, 'critic_loss':     8.4461, 'actor_loss':    -1.4405, 'eps_e':     0.1795})
Step:   94000, Reward:   139.836 [  12.298], Avg:   -29.969 (0.176) <0-00:36:14> ({'r_t':  1233.7382, 'eps':     0.1759, 'critic_loss':     7.3731, 'actor_loss':    -1.3493, 'eps_e':     0.1759})
Step:   95000, Reward:   130.845 [   9.015], Avg:   -28.294 (0.172) <0-00:36:36> ({'r_t':  1233.2769, 'eps':     0.1724, 'critic_loss':     6.9091, 'actor_loss':    -1.3031, 'eps_e':     0.1724})
Step:   96000, Reward:   128.434 [  18.285], Avg:   -26.678 (0.169) <0-00:36:57> ({'r_t':  1291.1754, 'eps':     0.1694, 'critic_loss':     6.8867, 'actor_loss':    -1.3307, 'eps_e':     0.1694})
Step:   97000, Reward:    95.645 [   4.514], Avg:   -25.430 (0.166) <0-00:37:19> ({'r_t':   924.5929, 'eps':     0.1663, 'critic_loss':   199.8599, 'actor_loss':    -1.6831, 'eps_e':     0.1663})
Step:   98000, Reward:  -440.957 [ 726.411], Avg:   -29.628 (0.163) <0-00:37:41> ({'r_t':   972.2253, 'eps':     0.1630, 'critic_loss':   220.5152, 'actor_loss':    -1.3613, 'eps_e':     0.1630})
Step:   99000, Reward:    66.240 [   7.377], Avg:   -28.669 (0.161) <0-00:38:02> ({'r_t':   -53.2350, 'eps':     0.1611, 'critic_loss':  1294.1908, 'actor_loss':    -1.4843, 'eps_e':     0.1611})
Step:  100000, Reward:    62.633 [   6.750], Avg:   -27.765 (0.158) <0-00:38:24> ({'r_t':   992.2282, 'eps':     0.1576, 'critic_loss':  2226.2429, 'actor_loss':    -1.5506, 'eps_e':     0.1576})
Step:  101000, Reward:   111.405 [  11.424], Avg:   -26.400 (0.154) <0-00:38:45> ({'r_t':   901.1654, 'eps':     0.1545, 'critic_loss':  2260.6243, 'actor_loss':    -2.8226, 'eps_e':     0.1545})
Step:  102000, Reward:   100.582 [  11.258], Avg:   -25.168 (0.151) <0-00:39:07> ({'r_t':   971.8095, 'eps':     0.1511, 'critic_loss':  2236.5176, 'actor_loss':    -3.0852, 'eps_e':     0.1511})
Step:  103000, Reward:    82.193 [   6.612], Avg:   -24.135 (0.148) <0-00:39:28> ({'r_t':   972.6598, 'eps':     0.1481, 'critic_loss':  2214.4778, 'actor_loss':    -3.6010, 'eps_e':     0.1481})
Step:  104000, Reward:   111.276 [  11.278], Avg:   -22.846 (0.145) <0-00:39:49> ({'r_t':  1016.1852, 'eps':     0.1449, 'critic_loss':  2243.4832, 'actor_loss':    -2.7927, 'eps_e':     0.1449})
Step:  105000, Reward:    84.776 [  15.863], Avg:   -21.830 (0.141) <0-00:40:11> ({'r_t':  1005.5089, 'eps':     0.1414, 'critic_loss':  1018.6190, 'actor_loss':    -2.5864, 'eps_e':     0.1414})
Step:  106000, Reward:   118.448 [  18.132], Avg:   -20.519 (0.139) <0-00:40:32> ({'r_t':  1140.2065, 'eps':     0.1386, 'critic_loss':   100.0846, 'actor_loss':    -1.5618, 'eps_e':     0.1386})
Step:  107000, Reward:   125.584 [  10.959], Avg:   -19.167 (0.136) <0-00:40:54> ({'r_t':  1163.1354, 'eps':     0.1356, 'critic_loss':    19.2985, 'actor_loss':    -1.7082, 'eps_e':     0.1356})
Step:  108000, Reward:   131.613 [   9.926], Avg:   -17.783 (0.133) <0-00:41:15> ({'r_t':  1254.0977, 'eps':     0.1329, 'critic_loss':    15.8391, 'actor_loss':    -1.6280, 'eps_e':     0.1329})
Step:  109000, Reward:   117.240 [  10.525], Avg:   -16.556 (0.130) <0-00:41:37> ({'r_t':  1285.2098, 'eps':     0.1303, 'critic_loss':    14.5583, 'actor_loss':    -1.6764, 'eps_e':     0.1303})
Step:  110000, Reward:   127.641 [  12.495], Avg:   -15.257 (0.128) <0-00:41:58> ({'r_t':  1306.5881, 'eps':     0.1277, 'critic_loss':    13.5681, 'actor_loss':    -1.7433, 'eps_e':     0.1277})
Step:  111000, Reward:   131.667 [   9.583], Avg:   -13.945 (0.125) <0-00:42:20> ({'r_t':  1292.5582, 'eps':     0.1252, 'critic_loss':    12.1866, 'actor_loss':    -1.7985, 'eps_e':     0.1252})
Step:  112000, Reward:   122.950 [   3.166], Avg:   -12.733 (0.123) <0-00:42:41> ({'r_t':  1340.1824, 'eps':     0.1229, 'critic_loss':     9.6366, 'actor_loss':    -1.5856, 'eps_e':     0.1229})
Step:  113000, Reward:   125.433 [   5.299], Avg:   -11.521 (0.120) <0-00:43:03> ({'r_t':  1227.5667, 'eps':     0.1205, 'critic_loss':     8.2496, 'actor_loss':    -1.5188, 'eps_e':     0.1205})
Step:  114000, Reward:    -3.749 [ 360.445], Avg:   -11.454 (0.118) <0-00:43:25> ({'r_t':  1189.1095, 'eps':     0.1183, 'critic_loss':    31.5020, 'actor_loss':    -1.5081, 'eps_e':     0.1183})
Step:  115000, Reward:   126.841 [  18.162], Avg:   -10.262 (0.116) <0-00:43:47> ({'r_t':   620.1780, 'eps':     0.1162, 'critic_loss':   405.5584, 'actor_loss':    -1.2460, 'eps_e':     0.1162})
Step:  116000, Reward:   130.794 [   9.868], Avg:    -9.056 (0.114) <0-00:44:08> ({'r_t':   989.4953, 'eps':     0.1137, 'critic_loss':   982.1035, 'actor_loss':    -1.1149, 'eps_e':     0.1137})
Step:  117000, Reward:   190.123 [ 183.146], Avg:    -7.368 (0.112) <0-00:44:38> ({'r_t':  1096.1728, 'eps':     0.1117, 'critic_loss':  1459.1826, 'actor_loss':    -1.1983, 'eps_e':     0.1117})
Step:  118000, Reward:    95.196 [   7.975], Avg:    -6.506 (0.110) <0-00:44:59> ({'r_t':  1036.8763, 'eps':     0.1097, 'critic_loss':  1856.4435, 'actor_loss':    -1.2876, 'eps_e':     0.1097})
Step:  119000, Reward:   128.521 [  22.640], Avg:    -5.381 (0.108) <0-00:45:22> ({'r_t':  1123.8562, 'eps':     0.1077, 'critic_loss':  2123.3372, 'actor_loss':    -1.3324, 'eps_e':     0.1077})
Step:  120000, Reward:   192.138 [  28.522], Avg:    -3.749 (0.106) <0-00:45:45> ({'r_t':  1052.8498, 'eps':     0.1062, 'critic_loss':  2001.9303, 'actor_loss':    -1.4647, 'eps_e':     0.1062})
Step:  121000, Reward:   180.407 [  41.034], Avg:    -2.239 (0.105) <0-00:46:08> ({'r_t':  1436.4895, 'eps':     0.1045, 'critic_loss':  1695.6111, 'actor_loss':    -1.7135, 'eps_e':     0.1045})
Step:  122000, Reward:   200.953 [   8.105], Avg:    -0.587 (0.103) <0-00:46:30> ({'r_t':  1557.8370, 'eps':     0.1031, 'critic_loss':  1161.4912, 'actor_loss':    -1.9827, 'eps_e':     0.1031})
Step:  123000, Reward:   192.512 [  25.902], Avg:     0.970 (0.101) <0-00:46:53> ({'r_t':  1130.6214, 'eps':     0.1014, 'critic_loss':   468.6731, 'actor_loss':    -2.4079, 'eps_e':     0.1014})
Step:  124000, Reward:   124.939 [ 363.525], Avg:     1.962 (0.100) <0-00:47:16> ({'r_t':   539.3794, 'eps':     0.1000, 'critic_loss':  1495.2388, 'actor_loss':    -2.4112, 'eps_e':     0.1000})
Step:  125000, Reward:    94.054 [   3.388], Avg:     2.693 (0.100) <0-00:47:37> ({'r_t':  1040.6281, 'eps':     0.1000, 'critic_loss':  2115.8313, 'actor_loss':    -2.2157, 'eps_e':     0.1000})
Step:  126000, Reward:   157.889 [  67.289], Avg:     3.915 (0.100) <0-00:47:59> ({'r_t':  1334.4486, 'eps':     0.1000, 'critic_loss':  3129.2112, 'actor_loss':    -2.3557, 'eps_e':     0.1000})
Step:  127000, Reward:   146.018 [  29.782], Avg:     5.025 (0.100) <0-00:48:22> ({'r_t':  1506.5554, 'eps':     0.1000, 'critic_loss':  2817.8186, 'actor_loss':    -2.4190, 'eps_e':     0.1000})
Step:  128000, Reward:  -136.479 [ 315.521], Avg:     3.928 (0.100) <0-00:48:51> ({'r_t':  1310.0674, 'eps':     0.1000, 'critic_loss':  2183.5059, 'actor_loss':    -2.7026, 'eps_e':     0.1000})
Step:  129000, Reward:    90.544 [   7.311], Avg:     4.594 (0.100) <0-00:49:13> ({'r_t':  1075.3900, 'eps':     0.1000, 'critic_loss':  2959.6499, 'actor_loss':    -3.0422, 'eps_e':     0.1000})
Step:  130000, Reward:   235.039 [  16.516], Avg:     6.353 (0.100) <0-00:49:36> ({'r_t':  1240.3094, 'eps':     0.1000, 'critic_loss':  2897.9351, 'actor_loss':    -3.2857, 'eps_e':     0.1000})
Step:  131000, Reward:   224.961 [  25.534], Avg:     8.009 (0.100) <0-00:49:58> ({'r_t':  1693.8333, 'eps':     0.1000, 'critic_loss':   874.9244, 'actor_loss':    -2.6609, 'eps_e':     0.1000})
Step:  132000, Reward:   231.063 [  20.861], Avg:     9.687 (0.100) <0-00:50:21> ({'r_t':  1688.1429, 'eps':     0.1000, 'critic_loss':   189.0495, 'actor_loss':    -3.7654, 'eps_e':     0.1000})
Step:  133000, Reward:   239.518 [   8.747], Avg:    11.402 (0.100) <0-00:50:43> ({'r_t':  1764.6487, 'eps':     0.1000, 'critic_loss':    98.8421, 'actor_loss':    -4.9798, 'eps_e':     0.1000})
Step:  134000, Reward:   120.137 [ 129.326], Avg:    12.207 (0.100) <0-00:51:06> ({'r_t':  1690.6415, 'eps':     0.1000, 'critic_loss':    95.0666, 'actor_loss':    -6.0725, 'eps_e':     0.1000})
Step:  135000, Reward:   164.374 [ 114.359], Avg:    13.326 (0.100) <0-00:51:28> ({'r_t':  1698.3853, 'eps':     0.1000, 'critic_loss':   111.7757, 'actor_loss':    -5.9643, 'eps_e':     0.1000})
Step:  136000, Reward:   186.579 [  70.383], Avg:    14.591 (0.100) <0-00:51:50> ({'r_t':  1847.6049, 'eps':     0.1000, 'critic_loss':   121.0975, 'actor_loss':    -5.5739, 'eps_e':     0.1000})
Step:  137000, Reward:   237.055 [   8.055], Avg:    16.203 (0.100) <0-00:52:12> ({'r_t':  1880.1252, 'eps':     0.1000, 'critic_loss':   110.3981, 'actor_loss':    -4.5415, 'eps_e':     0.1000})
Step:  138000, Reward:   129.649 [ 363.467], Avg:    17.019 (0.100) <0-00:52:34> ({'r_t':  1659.0257, 'eps':     0.1000, 'critic_loss':   102.7424, 'actor_loss':    -3.8818, 'eps_e':     0.1000})
Step:  139000, Reward:   244.377 [  14.548], Avg:    18.643 (0.100) <0-00:52:56> ({'r_t':  1820.2781, 'eps':     0.1000, 'critic_loss':   517.8281, 'actor_loss':    -2.8675, 'eps_e':     0.1000})
Step:  140000, Reward:   237.325 [  34.116], Avg:    20.194 (0.100) <0-00:53:19> ({'r_t':  1850.5698, 'eps':     0.1000, 'critic_loss':   672.5285, 'actor_loss':    -2.5666, 'eps_e':     0.1000})
Step:  141000, Reward:   239.527 [  10.978], Avg:    21.738 (0.100) <0-00:53:41> ({'r_t':  1738.8098, 'eps':     0.1000, 'critic_loss':   741.5974, 'actor_loss':    -2.3237, 'eps_e':     0.1000})
Step:  142000, Reward:   239.337 [  42.392], Avg:    23.260 (0.100) <0-00:54:04> ({'r_t':  1745.0120, 'eps':     0.1000, 'critic_loss':   928.9478, 'actor_loss':    -2.2320, 'eps_e':     0.1000})
Step:  143000, Reward:   252.158 [  13.708], Avg:    24.850 (0.100) <0-00:54:26> ({'r_t':  1858.7334, 'eps':     0.1000, 'critic_loss':   717.1536, 'actor_loss':    -2.2344, 'eps_e':     0.1000})
Step:  144000, Reward:   250.396 [  31.713], Avg:    26.405 (0.100) <0-00:54:49> ({'r_t':  1924.8980, 'eps':     0.1000, 'critic_loss':   702.2839, 'actor_loss':    -2.0917, 'eps_e':     0.1000})
Step:  145000, Reward:   263.492 [   9.349], Avg:    28.029 (0.100) <0-00:55:11> ({'r_t':  1917.9550, 'eps':     0.1000, 'critic_loss':   431.0275, 'actor_loss':    -2.0284, 'eps_e':     0.1000})
Step:  146000, Reward:   268.831 [   6.904], Avg:    29.667 (0.100) <0-00:55:33> ({'r_t':  1913.4903, 'eps':     0.1000, 'critic_loss':   404.7768, 'actor_loss':    -2.0871, 'eps_e':     0.1000})
Step:  147000, Reward:   269.288 [  39.887], Avg:    31.286 (0.100) <0-00:55:56> ({'r_t':  1976.7635, 'eps':     0.1000, 'critic_loss':   112.6498, 'actor_loss':    -2.3179, 'eps_e':     0.1000})
Step:  148000, Reward:   268.069 [  39.525], Avg:    32.875 (0.100) <0-00:56:18> ({'r_t':  1959.3581, 'eps':     0.1000, 'critic_loss':    48.4678, 'actor_loss':    -2.3532, 'eps_e':     0.1000})
Step:  149000, Reward:   264.532 [  53.103], Avg:    34.420 (0.100) <0-00:56:41> ({'r_t':  2111.0442, 'eps':     0.1000, 'critic_loss':    47.2942, 'actor_loss':    -2.1642, 'eps_e':     0.1000})
Step:  150000, Reward:   265.478 [  82.227], Avg:    35.950 (0.100) <0-00:57:03> ({'r_t':  2103.3432, 'eps':     0.1000, 'critic_loss':    51.3098, 'actor_loss':    -2.5776, 'eps_e':     0.1000})
Step:  151000, Reward:   246.213 [  12.215], Avg:    37.333 (0.100) <0-00:57:25> ({'r_t':  1756.5364, 'eps':     0.1000, 'critic_loss':   215.3471, 'actor_loss':    -2.5921, 'eps_e':     0.1000})
Step:  152000, Reward:   284.613 [  19.549], Avg:    38.949 (0.100) <0-00:57:48> ({'r_t':  1941.3280, 'eps':     0.1000, 'critic_loss':   456.8766, 'actor_loss':    -2.0534, 'eps_e':     0.1000})
Step:  153000, Reward:   282.312 [  44.466], Avg:    40.530 (0.100) <0-00:58:10> ({'r_t':  1849.3210, 'eps':     0.1000, 'critic_loss':   677.7098, 'actor_loss':    -1.9162, 'eps_e':     0.1000})
Step:  154000, Reward:   283.926 [  20.696], Avg:    42.100 (0.100) <0-00:58:33> ({'r_t':  2125.1976, 'eps':     0.1000, 'critic_loss':   863.9990, 'actor_loss':    -2.0378, 'eps_e':     0.1000})
Step:  155000, Reward:   291.211 [  22.461], Avg:    43.697 (0.100) <0-00:58:55> ({'r_t':  2024.4210, 'eps':     0.1000, 'critic_loss':   860.8596, 'actor_loss':    -1.8382, 'eps_e':     0.1000})
Step:  156000, Reward:   285.037 [  39.009], Avg:    45.234 (0.100) <0-00:59:18> ({'r_t':  2027.1979, 'eps':     0.1000, 'critic_loss':  1122.6250, 'actor_loss':    -2.0644, 'eps_e':     0.1000})
Step:  157000, Reward:   262.565 [  46.675], Avg:    46.610 (0.100) <0-00:59:40> ({'r_t':  1883.8922, 'eps':     0.1000, 'critic_loss':   988.9972, 'actor_loss':    -2.0723, 'eps_e':     0.1000})
Step:  158000, Reward:   285.851 [  44.803], Avg:    48.114 (0.100) <0-01:00:02> ({'r_t':  2241.9395, 'eps':     0.1000, 'critic_loss':   481.9372, 'actor_loss':    -2.0709, 'eps_e':     0.1000})
Step:  159000, Reward:   307.828 [  15.498], Avg:    49.737 (0.100) <0-01:00:25> ({'r_t':  2219.0481, 'eps':     0.1000, 'critic_loss':   320.2761, 'actor_loss':    -2.1156, 'eps_e':     0.1000})
Step:  160000, Reward:   209.774 [ 107.119], Avg:    50.731 (0.100) <0-01:00:47> ({'r_t':  2108.0939, 'eps':     0.1000, 'critic_loss':   489.7077, 'actor_loss':    -2.4383, 'eps_e':     0.1000})
Step:  161000, Reward:   298.815 [  15.210], Avg:    52.263 (0.100) <0-01:01:09> ({'r_t':  2136.2118, 'eps':     0.1000, 'critic_loss':   411.8375, 'actor_loss':    -2.5074, 'eps_e':     0.1000})
Step:  162000, Reward:   283.028 [  16.515], Avg:    53.679 (0.100) <0-01:01:31> ({'r_t':  2197.8071, 'eps':     0.1000, 'critic_loss':   401.5838, 'actor_loss':    -2.5221, 'eps_e':     0.1000})
Step:  163000, Reward:   289.156 [  62.202], Avg:    55.114 (0.100) <0-01:01:53> ({'r_t':  2150.0404, 'eps':     0.1000, 'critic_loss':   518.4719, 'actor_loss':    -2.3239, 'eps_e':     0.1000})
Step:  164000, Reward:   282.066 [   8.140], Avg:    56.490 (0.100) <0-01:02:15> ({'r_t':  2315.2722, 'eps':     0.1000, 'critic_loss':   204.6667, 'actor_loss':    -2.0684, 'eps_e':     0.1000})
Step:  165000, Reward:   262.653 [  18.907], Avg:    57.732 (0.100) <0-01:02:38> ({'r_t':  2163.2212, 'eps':     0.1000, 'critic_loss':   409.2339, 'actor_loss':    -1.9367, 'eps_e':     0.1000})
Step:  166000, Reward:   299.519 [  17.108], Avg:    59.180 (0.100) <0-01:03:00> ({'r_t':  2305.7361, 'eps':     0.1000, 'critic_loss':   135.0338, 'actor_loss':    -1.9239, 'eps_e':     0.1000})
Step:  167000, Reward:   252.902 [  70.726], Avg:    60.333 (0.100) <0-01:03:22> ({'r_t':  2008.8869, 'eps':     0.1000, 'critic_loss':   301.1006, 'actor_loss':    -1.7848, 'eps_e':     0.1000})
Step:  168000, Reward:   278.961 [   6.462], Avg:    61.626 (0.100) <0-01:03:44> ({'r_t':  1877.0074, 'eps':     0.1000, 'critic_loss':   735.6470, 'actor_loss':    -1.7930, 'eps_e':     0.1000})
Step:  169000, Reward:   257.926 [  24.017], Avg:    62.781 (0.100) <0-01:04:07> ({'r_t':  2046.3392, 'eps':     0.1000, 'critic_loss':   869.0476, 'actor_loss':    -1.8558, 'eps_e':     0.1000})
Step:  170000, Reward:   280.191 [   8.652], Avg:    64.053 (0.100) <0-01:04:29> ({'r_t':  1759.8952, 'eps':     0.1000, 'critic_loss':  1164.6125, 'actor_loss':    -2.0531, 'eps_e':     0.1000})
Step:  171000, Reward:   270.293 [  23.338], Avg:    65.252 (0.100) <0-01:04:52> ({'r_t':  2095.2231, 'eps':     0.1000, 'critic_loss':  1463.4865, 'actor_loss':    -2.5272, 'eps_e':     0.1000})
Step:  172000, Reward:   287.630 [  17.669], Avg:    66.537 (0.100) <0-01:05:14> ({'r_t':  2118.6852, 'eps':     0.1000, 'critic_loss':  1272.9700, 'actor_loss':    -2.9777, 'eps_e':     0.1000})
Step:  173000, Reward:   303.529 [  18.989], Avg:    67.899 (0.100) <0-01:05:36> ({'r_t':  2185.0169, 'eps':     0.1000, 'critic_loss':  1147.1536, 'actor_loss':    -3.1369, 'eps_e':     0.1000})
Step:  174000, Reward:   288.443 [   5.856], Avg:    69.159 (0.100) <0-01:05:58> ({'r_t':  2091.8080, 'eps':     0.1000, 'critic_loss':  1153.2639, 'actor_loss':    -3.0295, 'eps_e':     0.1000})
Step:  175000, Reward:   292.787 [  18.733], Avg:    70.430 (0.100) <0-01:06:20> ({'r_t':  2170.0244, 'eps':     0.1000, 'critic_loss':   988.3914, 'actor_loss':    -2.9205, 'eps_e':     0.1000})
Step:  176000, Reward:   299.954 [  15.481], Avg:    71.727 (0.100) <0-01:06:42> ({'r_t':  2139.5470, 'eps':     0.1000, 'critic_loss':   768.8641, 'actor_loss':    -2.8797, 'eps_e':     0.1000})
Step:  177000, Reward:   306.646 [  17.413], Avg:    73.046 (0.100) <0-01:07:05> ({'r_t':  2233.7885, 'eps':     0.1000, 'critic_loss':    63.8982, 'actor_loss':    -2.5506, 'eps_e':     0.1000})
Step:  178000, Reward:   305.039 [  17.721], Avg:    74.343 (0.100) <0-01:07:27> ({'r_t':  2317.2732, 'eps':     0.1000, 'critic_loss':    50.1097, 'actor_loss':    -2.3396, 'eps_e':     0.1000})
Step:  179000, Reward:   283.064 [  57.694], Avg:    75.502 (0.100) <0-01:07:49> ({'r_t':  2286.8038, 'eps':     0.1000, 'critic_loss':    44.4894, 'actor_loss':    -2.2185, 'eps_e':     0.1000})
Step:  180000, Reward:   298.464 [  18.073], Avg:    76.734 (0.100) <0-01:08:12> ({'r_t':  2284.0007, 'eps':     0.1000, 'critic_loss':    28.4781, 'actor_loss':    -1.9498, 'eps_e':     0.1000})
Step:  181000, Reward:   301.909 [  17.638], Avg:    77.971 (0.100) <0-01:08:34> ({'r_t':  2295.5522, 'eps':     0.1000, 'critic_loss':    23.0252, 'actor_loss':    -1.6997, 'eps_e':     0.1000})
Step:  182000, Reward:   282.663 [  17.299], Avg:    79.090 (0.100) <0-01:08:56> ({'r_t':  2237.3870, 'eps':     0.1000, 'critic_loss':    23.8640, 'actor_loss':    -1.4663, 'eps_e':     0.1000})
Step:  183000, Reward:   292.021 [  15.981], Avg:    80.247 (0.100) <0-01:09:18> ({'r_t':  2150.9521, 'eps':     0.1000, 'critic_loss':    33.6707, 'actor_loss':    -1.2405, 'eps_e':     0.1000})
Step:  184000, Reward:   163.907 [ 364.957], Avg:    80.699 (0.100) <0-01:09:40> ({'r_t':  2103.5225, 'eps':     0.1000, 'critic_loss':   379.3161, 'actor_loss':    -1.1406, 'eps_e':     0.1000})
Step:  185000, Reward:   289.017 [  15.299], Avg:    81.819 (0.100) <0-01:10:02> ({'r_t':  2187.2399, 'eps':     0.1000, 'critic_loss':   105.1603, 'actor_loss':    -1.1946, 'eps_e':     0.1000})
Step:  186000, Reward:   279.374 [   3.627], Avg:    82.876 (0.100) <0-01:10:24> ({'r_t':  2162.1742, 'eps':     0.1000, 'critic_loss':   260.1576, 'actor_loss':    -1.1907, 'eps_e':     0.1000})
Step:  187000, Reward:   302.558 [  13.409], Avg:    84.044 (0.100) <0-01:10:47> ({'r_t':  2236.0059, 'eps':     0.1000, 'critic_loss':   169.4287, 'actor_loss':    -1.2972, 'eps_e':     0.1000})
Step:  188000, Reward:   298.653 [  20.637], Avg:    85.180 (0.100) <0-01:11:09> ({'r_t':  2254.8504, 'eps':     0.1000, 'critic_loss':   187.5614, 'actor_loss':    -1.3921, 'eps_e':     0.1000})
Step:  189000, Reward:   295.867 [  17.071], Avg:    86.288 (0.100) <0-01:11:31> ({'r_t':  2192.5702, 'eps':     0.1000, 'critic_loss':   293.1823, 'actor_loss':    -1.2809, 'eps_e':     0.1000})
Step:  190000, Reward:   303.023 [  12.681], Avg:    87.423 (0.100) <0-01:11:53> ({'r_t':  2309.4759, 'eps':     0.1000, 'critic_loss':    67.2381, 'actor_loss':    -1.3874, 'eps_e':     0.1000})
Step:  191000, Reward:   288.956 [  10.693], Avg:    88.473 (0.100) <0-01:12:15> ({'r_t':  2251.9500, 'eps':     0.1000, 'critic_loss':    52.8704, 'actor_loss':    -1.2239, 'eps_e':     0.1000})
Step:  192000, Reward:   304.296 [  18.426], Avg:    89.591 (0.100) <0-01:12:38> ({'r_t':  2277.1597, 'eps':     0.1000, 'critic_loss':    50.2210, 'actor_loss':    -1.1869, 'eps_e':     0.1000})
Step:  193000, Reward:   300.933 [  14.187], Avg:    90.680 (0.100) <0-01:13:00> ({'r_t':  2204.1460, 'eps':     0.1000, 'critic_loss':    41.8480, 'actor_loss':    -1.1550, 'eps_e':     0.1000})
Step:  194000, Reward:   300.657 [  12.126], Avg:    91.757 (0.100) <0-01:13:22> ({'r_t':  2201.3488, 'eps':     0.1000, 'critic_loss':   129.4245, 'actor_loss':    -1.1723, 'eps_e':     0.1000})
Step:  195000, Reward:   310.849 [  15.276], Avg:    92.875 (0.100) <0-01:13:44> ({'r_t':  2291.6099, 'eps':     0.1000, 'critic_loss':   213.5394, 'actor_loss':    -1.2165, 'eps_e':     0.1000})
Step:  196000, Reward:   309.653 [  19.775], Avg:    93.975 (0.100) <0-01:14:07> ({'r_t':  2246.2428, 'eps':     0.1000, 'critic_loss':   199.0568, 'actor_loss':    -1.2291, 'eps_e':     0.1000})
Step:  197000, Reward:   288.422 [  13.440], Avg:    94.958 (0.100) <0-01:14:29> ({'r_t':  2192.6293, 'eps':     0.1000, 'critic_loss':   193.6314, 'actor_loss':    -1.2617, 'eps_e':     0.1000})
Step:  198000, Reward:   302.454 [  17.323], Avg:    96.000 (0.100) <0-01:14:51> ({'r_t':  2094.7912, 'eps':     0.1000, 'critic_loss':   478.3992, 'actor_loss':    -1.2660, 'eps_e':     0.1000})
Step:  199000, Reward:   297.539 [   9.308], Avg:    97.008 (0.100) <0-01:15:14> ({'r_t':  2235.4221, 'eps':     0.1000, 'critic_loss':   549.8641, 'actor_loss':    -1.2860, 'eps_e':     0.1000})
Step:  200000, Reward:   314.244 [  16.936], Avg:    98.089 (0.100) <0-01:15:36> ({'r_t':  2271.1466, 'eps':     0.1000, 'critic_loss':   279.3779, 'actor_loss':    -1.2621, 'eps_e':     0.1000})
Step:  201000, Reward:   310.409 [  19.963], Avg:    99.140 (0.100) <0-01:15:58> ({'r_t':  2165.8090, 'eps':     0.1000, 'critic_loss':   534.9019, 'actor_loss':    -1.3638, 'eps_e':     0.1000})
Step:  202000, Reward:   304.074 [  12.224], Avg:   100.149 (0.100) <0-01:16:21> ({'r_t':  2175.0416, 'eps':     0.1000, 'critic_loss':   521.7543, 'actor_loss':    -1.4498, 'eps_e':     0.1000})
Step:  203000, Reward:   291.825 [   9.775], Avg:   101.089 (0.100) <0-01:16:43> ({'r_t':  1967.2552, 'eps':     0.1000, 'critic_loss':   952.6078, 'actor_loss':    -1.7633, 'eps_e':     0.1000})
Step:  204000, Reward:   319.725 [  17.514], Avg:   102.155 (0.100) <0-01:17:06> ({'r_t':  2085.4003, 'eps':     0.1000, 'critic_loss':   582.4929, 'actor_loss':    -1.8684, 'eps_e':     0.1000})
Step:  205000, Reward:   291.064 [   9.230], Avg:   103.072 (0.100) <0-01:17:28> ({'r_t':  2199.6727, 'eps':     0.1000, 'critic_loss':   806.1115, 'actor_loss':    -1.9294, 'eps_e':     0.1000})
Step:  206000, Reward:   279.947 [  17.358], Avg:   103.927 (0.100) <0-01:17:50> ({'r_t':  2197.7803, 'eps':     0.1000, 'critic_loss':  1156.4248, 'actor_loss':    -2.0123, 'eps_e':     0.1000})
Step:  207000, Reward:   322.798 [  20.163], Avg:   104.979 (0.100) <0-01:18:13> ({'r_t':  2195.2947, 'eps':     0.1000, 'critic_loss':   801.7366, 'actor_loss':    -2.1760, 'eps_e':     0.1000})
Step:  208000, Reward:   278.981 [  13.529], Avg:   105.812 (0.100) <0-01:18:36> ({'r_t':  2232.4851, 'eps':     0.1000, 'critic_loss':   895.3500, 'actor_loss':    -2.3971, 'eps_e':     0.1000})
Step:  209000, Reward:   329.338 [  26.943], Avg:   106.876 (0.100) <0-01:18:58> ({'r_t':  2250.2128, 'eps':     0.1000, 'critic_loss':   665.9647, 'actor_loss':    -2.1947, 'eps_e':     0.1000})
Step:  210000, Reward:   321.383 [  14.925], Avg:   107.893 (0.100) <0-01:19:21> ({'r_t':  2277.8757, 'eps':     0.1000, 'critic_loss':   362.6289, 'actor_loss':    -1.9992, 'eps_e':     0.1000})
Step:  211000, Reward:   317.322 [  12.856], Avg:   108.881 (0.100) <0-01:19:43> ({'r_t':  2274.3336, 'eps':     0.1000, 'critic_loss':   219.4928, 'actor_loss':    -1.7202, 'eps_e':     0.1000})
Step:  212000, Reward:   315.657 [   7.727], Avg:   109.851 (0.100) <0-01:20:05> ({'r_t':  2380.1082, 'eps':     0.1000, 'critic_loss':    35.8947, 'actor_loss':    -1.6207, 'eps_e':     0.1000})
Step:  213000, Reward:   290.967 [  63.693], Avg:   110.698 (0.100) <0-01:20:28> ({'r_t':  2336.1622, 'eps':     0.1000, 'critic_loss':    28.8057, 'actor_loss':    -1.5218, 'eps_e':     0.1000})
Step:  214000, Reward:   306.768 [   8.185], Avg:   111.610 (0.100) <0-01:20:50> ({'r_t':  2311.8212, 'eps':     0.1000, 'critic_loss':    24.2718, 'actor_loss':    -1.4349, 'eps_e':     0.1000})
Step:  215000, Reward:   304.324 [   7.327], Avg:   112.502 (0.100) <0-01:21:12> ({'r_t':  2223.7072, 'eps':     0.1000, 'critic_loss':    23.8585, 'actor_loss':    -1.2997, 'eps_e':     0.1000})
Step:  216000, Reward:   275.657 [  12.459], Avg:   113.254 (0.100) <0-01:21:35> ({'r_t':  2206.1430, 'eps':     0.1000, 'critic_loss':    95.9300, 'actor_loss':    -1.1781, 'eps_e':     0.1000})
Step:  217000, Reward:   319.139 [  24.345], Avg:   114.198 (0.100) <0-01:21:57> ({'r_t':  2269.4097, 'eps':     0.1000, 'critic_loss':   239.3632, 'actor_loss':    -1.2409, 'eps_e':     0.1000})
Step:  218000, Reward:   299.426 [  63.354], Avg:   115.044 (0.100) <0-01:22:20> ({'r_t':  2316.4524, 'eps':     0.1000, 'critic_loss':   285.0730, 'actor_loss':    -1.4373, 'eps_e':     0.1000})
Step:  219000, Reward:   334.265 [  22.774], Avg:   116.040 (0.100) <0-01:22:42> ({'r_t':  2279.0082, 'eps':     0.1000, 'critic_loss':   227.8405, 'actor_loss':    -1.5677, 'eps_e':     0.1000})
Step:  220000, Reward:   320.341 [  22.704], Avg:   116.965 (0.100) <0-01:23:04> ({'r_t':  2351.1953, 'eps':     0.1000, 'critic_loss':   198.9318, 'actor_loss':    -1.6601, 'eps_e':     0.1000})
Step:  221000, Reward:   332.962 [  18.296], Avg:   117.938 (0.100) <0-01:23:27> ({'r_t':  2337.6134, 'eps':     0.1000, 'critic_loss':   200.4339, 'actor_loss':    -1.6601, 'eps_e':     0.1000})
Step:  222000, Reward:   318.446 [  23.868], Avg:   118.837 (0.100) <0-01:23:49> ({'r_t':  2396.4255, 'eps':     0.1000, 'critic_loss':   201.0715, 'actor_loss':    -1.6375, 'eps_e':     0.1000})
Step:  223000, Reward:   238.913 [ 363.122], Avg:   119.373 (0.100) <0-01:24:11> ({'r_t':  2410.9165, 'eps':     0.1000, 'critic_loss':    51.5109, 'actor_loss':    -1.4314, 'eps_e':     0.1000})
Step:  224000, Reward:   330.085 [  19.501], Avg:   120.310 (0.100) <0-01:24:34> ({'r_t':  2281.3996, 'eps':     0.1000, 'critic_loss':    74.4817, 'actor_loss':    -1.3999, 'eps_e':     0.1000})
Step:  225000, Reward:   319.646 [  29.340], Avg:   121.192 (0.100) <0-01:24:56> ({'r_t':  2203.8239, 'eps':     0.1000, 'critic_loss':   285.2062, 'actor_loss':    -1.3040, 'eps_e':     0.1000})
Step:  226000, Reward:   323.807 [  16.591], Avg:   122.084 (0.100) <0-01:25:18> ({'r_t':  2336.4875, 'eps':     0.1000, 'critic_loss':   316.5572, 'actor_loss':    -1.2631, 'eps_e':     0.1000})
Step:  227000, Reward:   296.073 [   9.634], Avg:   122.847 (0.100) <0-01:25:41> ({'r_t':  2366.2965, 'eps':     0.1000, 'critic_loss':   369.6707, 'actor_loss':    -1.2785, 'eps_e':     0.1000})
Step:  228000, Reward:   333.084 [  28.116], Avg:   123.765 (0.100) <0-01:26:03> ({'r_t':  2239.6441, 'eps':     0.1000, 'critic_loss':   586.0569, 'actor_loss':    -1.3832, 'eps_e':     0.1000})
Step:  229000, Reward:   328.577 [  18.551], Avg:   124.656 (0.100) <0-01:26:25> ({'r_t':  2373.7187, 'eps':     0.1000, 'critic_loss':   311.4616, 'actor_loss':    -1.4162, 'eps_e':     0.1000})
Step:  230000, Reward:   304.415 [  11.477], Avg:   125.434 (0.100) <0-01:26:48> ({'r_t':  2233.6780, 'eps':     0.1000, 'critic_loss':   601.7294, 'actor_loss':    -1.4858, 'eps_e':     0.1000})
Step:  231000, Reward:   346.238 [  20.782], Avg:   126.386 (0.100) <0-01:27:10> ({'r_t':  2356.9246, 'eps':     0.1000, 'critic_loss':    94.6863, 'actor_loss':    -1.5697, 'eps_e':     0.1000})
Step:  232000, Reward:   202.110 [ 359.234], Avg:   126.711 (0.100) <0-01:27:34> ({'r_t':  1395.4677, 'eps':     0.1000, 'critic_loss':   274.6342, 'actor_loss':    -1.8520, 'eps_e':     0.1000})
Step:  233000, Reward:   304.460 [  12.286], Avg:   127.470 (0.100) <0-01:27:57> ({'r_t':  1945.3274, 'eps':     0.1000, 'critic_loss':   336.8624, 'actor_loss':    -2.2261, 'eps_e':     0.1000})
Step:  234000, Reward:   243.136 [  48.915], Avg:   127.963 (0.100) <0-01:28:20> ({'r_t':  1611.8216, 'eps':     0.1000, 'critic_loss':   570.0061, 'actor_loss':    -2.2980, 'eps_e':     0.1000})
Step:  235000, Reward:   345.656 [  17.158], Avg:   128.885 (0.100) <0-01:28:43> ({'r_t':  1872.5815, 'eps':     0.1000, 'critic_loss':  1113.3850, 'actor_loss':    -2.5911, 'eps_e':     0.1000})
Step:  236000, Reward:   361.344 [  38.782], Avg:   129.866 (0.100) <0-01:29:06> ({'r_t':  1868.5169, 'eps':     0.1000, 'critic_loss':  1654.2013, 'actor_loss':    -2.9569, 'eps_e':     0.1000})
Step:  237000, Reward:   280.194 [   3.378], Avg:   130.497 (0.100) <0-01:29:28> ({'r_t':  1928.5098, 'eps':     0.1000, 'critic_loss':  2043.9741, 'actor_loss':    -3.3164, 'eps_e':     0.1000})
Step:  238000, Reward:  -294.121 [ 747.917], Avg:   128.721 (0.100) <0-01:29:51> ({'r_t':  2279.3547, 'eps':     0.1000, 'critic_loss':  1587.1691, 'actor_loss':    -3.4236, 'eps_e':     0.1000})
Step:  239000, Reward:   324.713 [  59.663], Avg:   129.537 (0.100) <0-01:30:14> ({'r_t':  2223.2114, 'eps':     0.1000, 'critic_loss':  1782.2075, 'actor_loss':    -3.3911, 'eps_e':     0.1000})
Step:  240000, Reward:   388.594 [  25.053], Avg:   130.612 (0.100) <0-01:30:37> ({'r_t':  2240.2205, 'eps':     0.1000, 'critic_loss':  1473.7723, 'actor_loss':    -3.1657, 'eps_e':     0.1000})
Step:  241000, Reward:   383.846 [  15.359], Avg:   131.659 (0.100) <0-01:31:00> ({'r_t':  2128.3497, 'eps':     0.1000, 'critic_loss':  1676.1267, 'actor_loss':    -2.8878, 'eps_e':     0.1000})
Step:  242000, Reward:   328.285 [  83.472], Avg:   132.468 (0.100) <0-01:31:23> ({'r_t':  2242.6392, 'eps':     0.1000, 'critic_loss':  1233.4833, 'actor_loss':    -2.7221, 'eps_e':     0.1000})
Step:  243000, Reward:   372.805 [  30.583], Avg:   133.453 (0.100) <0-01:31:46> ({'r_t':  2164.2661, 'eps':     0.1000, 'critic_loss':   657.4100, 'actor_loss':    -2.8889, 'eps_e':     0.1000})
Step:  244000, Reward:   356.920 [  38.300], Avg:   134.365 (0.100) <0-01:32:09> ({'r_t':  2107.3543, 'eps':     0.1000, 'critic_loss':   677.3618, 'actor_loss':    -2.6435, 'eps_e':     0.1000})
Step:  245000, Reward:   337.067 [  41.858], Avg:   135.189 (0.100) <0-01:32:32> ({'r_t':  2223.0659, 'eps':     0.1000, 'critic_loss':   368.3309, 'actor_loss':    -2.5940, 'eps_e':     0.1000})
Step:  246000, Reward:   401.656 [  11.061], Avg:   136.268 (0.100) <0-01:32:55> ({'r_t':  2274.0042, 'eps':     0.1000, 'critic_loss':   466.5693, 'actor_loss':    -2.6261, 'eps_e':     0.1000})
Step:  247000, Reward:   367.118 [  26.309], Avg:   137.199 (0.100) <0-01:33:18> ({'r_t':  2324.4084, 'eps':     0.1000, 'critic_loss':   318.0743, 'actor_loss':    -2.4595, 'eps_e':     0.1000})
Step:  248000, Reward:   363.784 [  22.709], Avg:   138.109 (0.100) <0-01:33:41> ({'r_t':  2248.1270, 'eps':     0.1000, 'critic_loss':   114.9541, 'actor_loss':    -2.3351, 'eps_e':     0.1000})
Step:  249000, Reward:   190.415 [ 492.484], Avg:   138.318 (0.100) <0-01:34:06> ({'r_t':  2034.8341, 'eps':     0.1000, 'critic_loss':   201.2384, 'actor_loss':    -2.3368, 'eps_e':     0.1000})
Step:  250000, Reward:   387.633 [  15.426], Avg:   139.311 (0.100) <0-01:34:29> ({'r_t':  1761.0964, 'eps':     0.1000, 'critic_loss':   554.4326, 'actor_loss':    -2.0590, 'eps_e':     0.1000})
Step:  251000, Reward:   314.921 [ 359.249], Avg:   140.008 (0.100) <0-01:34:52> ({'r_t':  2291.9493, 'eps':     0.1000, 'critic_loss':  1107.9569, 'actor_loss':    -2.1218, 'eps_e':     0.1000})
Step:  252000, Reward:   388.005 [  14.366], Avg:   140.988 (0.100) <0-01:35:15> ({'r_t':  2165.1186, 'eps':     0.1000, 'critic_loss':  1227.6718, 'actor_loss':    -2.1885, 'eps_e':     0.1000})
Step:  253000, Reward:   398.926 [  12.804], Avg:   142.004 (0.100) <0-01:35:38> ({'r_t':  2370.3045, 'eps':     0.1000, 'critic_loss':  1168.8940, 'actor_loss':    -2.1963, 'eps_e':     0.1000})
Step:  254000, Reward:    97.045 [ 570.283], Avg:   141.827 (0.100) <0-01:36:02> ({'r_t':  2263.2426, 'eps':     0.1000, 'critic_loss':  1206.8966, 'actor_loss':    -2.2214, 'eps_e':     0.1000})
Step:  255000, Reward:   401.937 [  11.623], Avg:   142.843 (0.100) <0-01:36:25> ({'r_t':  1874.4229, 'eps':     0.1000, 'critic_loss':  1400.4855, 'actor_loss':    -1.9772, 'eps_e':     0.1000})
Step:  256000, Reward:   371.064 [ 125.953], Avg:   143.732 (0.100) <0-01:36:48> ({'r_t':  2072.5301, 'eps':     0.1000, 'critic_loss':  1806.3281, 'actor_loss':    -2.2954, 'eps_e':     0.1000})
Step:  257000, Reward:   360.067 [  21.905], Avg:   144.570 (0.100) <0-01:37:11> ({'r_t':  1828.2235, 'eps':     0.1000, 'critic_loss':  1020.8574, 'actor_loss':    -2.8765, 'eps_e':     0.1000})
Step:  258000, Reward:   392.256 [  19.281], Avg:   145.526 (0.100) <0-01:37:34> ({'r_t':  2083.4817, 'eps':     0.1000, 'critic_loss':  1379.2028, 'actor_loss':    -3.2130, 'eps_e':     0.1000})
Step:  259000, Reward:   356.804 [  37.193], Avg:   146.339 (0.100) <0-01:37:57> ({'r_t':  2277.4230, 'eps':     0.1000, 'critic_loss':  2095.8279, 'actor_loss':    -3.4846, 'eps_e':     0.1000})
Step:  260000, Reward:   363.896 [  35.551], Avg:   147.172 (0.100) <0-01:38:20> ({'r_t':  2238.2901, 'eps':     0.1000, 'critic_loss':  1943.4219, 'actor_loss':    -3.7670, 'eps_e':     0.1000})
Step:  261000, Reward:   417.804 [  46.475], Avg:   148.205 (0.100) <0-01:38:43> ({'r_t':  2421.4461, 'eps':     0.1000, 'critic_loss':  1988.6078, 'actor_loss':    -3.7411, 'eps_e':     0.1000})
Step:  262000, Reward:   393.794 [  88.329], Avg:   149.139 (0.100) <0-01:39:06> ({'r_t':  2367.6141, 'eps':     0.1000, 'critic_loss':  1034.7699, 'actor_loss':    -3.0405, 'eps_e':     0.1000})
Step:  263000, Reward:   359.542 [  30.714], Avg:   149.936 (0.100) <0-01:39:29> ({'r_t':  2341.3193, 'eps':     0.1000, 'critic_loss':  1914.2960, 'actor_loss':    -3.0998, 'eps_e':     0.1000})
Step:  264000, Reward:   404.099 [  17.280], Avg:   150.895 (0.100) <0-01:39:51> ({'r_t':  2025.2702, 'eps':     0.1000, 'critic_loss':   984.6862, 'actor_loss':    -2.3931, 'eps_e':     0.1000})
Step:  265000, Reward:   424.042 [  20.849], Avg:   151.922 (0.100) <0-01:40:14> ({'r_t':  2476.1856, 'eps':     0.1000, 'critic_loss':   504.0605, 'actor_loss':    -2.2526, 'eps_e':     0.1000})
Step:  266000, Reward:   397.957 [  19.578], Avg:   152.844 (0.100) <0-01:40:37> ({'r_t':  2364.4229, 'eps':     0.1000, 'critic_loss':   638.7626, 'actor_loss':    -2.0628, 'eps_e':     0.1000})
Step:  267000, Reward:   373.599 [  37.848], Avg:   153.667 (0.100) <0-01:41:00> ({'r_t':  2128.6884, 'eps':     0.1000, 'critic_loss':  1137.9884, 'actor_loss':    -2.2871, 'eps_e':     0.1000})
Step:  268000, Reward:   401.940 [  17.284], Avg:   154.590 (0.100) <0-01:41:23> ({'r_t':  2455.4953, 'eps':     0.1000, 'critic_loss':  1022.7468, 'actor_loss':    -2.1830, 'eps_e':     0.1000})
Step:  269000, Reward:   419.483 [  16.918], Avg:   155.571 (0.100) <0-01:41:45> ({'r_t':  2256.6242, 'eps':     0.1000, 'critic_loss':   547.7922, 'actor_loss':    -2.5018, 'eps_e':     0.1000})
Step:  270000, Reward:   407.639 [   9.795], Avg:   156.502 (0.100) <0-01:42:08> ({'r_t':  2463.1928, 'eps':     0.1000, 'critic_loss':   956.3381, 'actor_loss':    -2.3059, 'eps_e':     0.1000})
Step:  271000, Reward:   424.699 [  16.889], Avg:   157.488 (0.100) <0-01:42:31> ({'r_t':  2378.9822, 'eps':     0.1000, 'critic_loss':   950.3255, 'actor_loss':    -2.2476, 'eps_e':     0.1000})
Step:  272000, Reward:   421.324 [  26.590], Avg:   158.454 (0.100) <0-01:42:55> ({'r_t':  2394.2248, 'eps':     0.1000, 'critic_loss':   688.5167, 'actor_loss':    -2.1290, 'eps_e':     0.1000})
Step:  273000, Reward:   404.753 [  18.446], Avg:   159.353 (0.100) <0-01:43:18> ({'r_t':  2322.3574, 'eps':     0.1000, 'critic_loss':   896.9749, 'actor_loss':    -2.5838, 'eps_e':     0.1000})
Step:  274000, Reward:   428.804 [  34.116], Avg:   160.333 (0.100) <0-01:43:40> ({'r_t':  2437.5493, 'eps':     0.1000, 'critic_loss':   777.7227, 'actor_loss':    -2.9676, 'eps_e':     0.1000})
Step:  275000, Reward:   427.740 [  23.004], Avg:   161.302 (0.100) <0-01:44:03> ({'r_t':  2522.3634, 'eps':     0.1000, 'critic_loss':   886.7841, 'actor_loss':    -2.9781, 'eps_e':     0.1000})
Step:  276000, Reward:   394.034 [  33.970], Avg:   162.142 (0.100) <0-01:44:26> ({'r_t':  2506.0326, 'eps':     0.1000, 'critic_loss':   900.6361, 'actor_loss':    -2.9164, 'eps_e':     0.1000})
Step:  277000, Reward:   442.979 [  28.981], Avg:   163.152 (0.100) <0-01:44:49> ({'r_t':  2604.5688, 'eps':     0.1000, 'critic_loss':   704.3421, 'actor_loss':    -2.8323, 'eps_e':     0.1000})
Step:  278000, Reward:   315.573 [  43.587], Avg:   163.698 (0.100) <0-01:45:12> ({'r_t':  2387.2643, 'eps':     0.1000, 'critic_loss':   751.7789, 'actor_loss':    -2.5761, 'eps_e':     0.1000})
Step:  279000, Reward:   378.633 [  71.166], Avg:   164.466 (0.100) <0-01:45:35> ({'r_t':  2482.7529, 'eps':     0.1000, 'critic_loss':   493.7399, 'actor_loss':    -2.6534, 'eps_e':     0.1000})
Step:  280000, Reward:   446.015 [  22.672], Avg:   165.468 (0.100) <0-01:45:58> ({'r_t':  2560.0099, 'eps':     0.1000, 'critic_loss':   476.3176, 'actor_loss':    -2.3736, 'eps_e':     0.1000})
Step:  281000, Reward:   445.360 [  26.780], Avg:   166.460 (0.100) <0-01:46:21> ({'r_t':  2617.2548, 'eps':     0.1000, 'critic_loss':   320.0079, 'actor_loss':    -1.9771, 'eps_e':     0.1000})
Step:  282000, Reward:   466.772 [  18.532], Avg:   167.522 (0.100) <0-01:46:44> ({'r_t':  2634.2472, 'eps':     0.1000, 'critic_loss':   254.5553, 'actor_loss':    -2.0898, 'eps_e':     0.1000})
Step:  283000, Reward:   438.032 [  32.827], Avg:   168.474 (0.100) <0-01:47:07> ({'r_t':  2737.3574, 'eps':     0.1000, 'critic_loss':   147.7430, 'actor_loss':    -2.4385, 'eps_e':     0.1000})
Step:  284000, Reward:   443.361 [  26.450], Avg:   169.439 (0.100) <0-01:47:30> ({'r_t':  2796.3697, 'eps':     0.1000, 'critic_loss':   221.5285, 'actor_loss':    -2.5943, 'eps_e':     0.1000})
Step:  285000, Reward:   436.647 [  55.799], Avg:   170.373 (0.100) <0-01:47:52> ({'r_t':  2602.2955, 'eps':     0.1000, 'critic_loss':    86.3322, 'actor_loss':    -2.3338, 'eps_e':     0.1000})
Step:  286000, Reward:   437.501 [  41.206], Avg:   171.304 (0.100) <0-01:48:15> ({'r_t':  2695.8156, 'eps':     0.1000, 'critic_loss':    84.8717, 'actor_loss':    -2.0046, 'eps_e':     0.1000})
Step:  287000, Reward:   373.451 [  94.171], Avg:   172.006 (0.100) <0-01:48:38> ({'r_t':  2542.4471, 'eps':     0.1000, 'critic_loss':   153.4489, 'actor_loss':    -1.7801, 'eps_e':     0.1000})
Step:  288000, Reward:   448.314 [  52.965], Avg:   172.962 (0.100) <0-01:49:01> ({'r_t':  2554.0749, 'eps':     0.1000, 'critic_loss':   433.7514, 'actor_loss':    -1.7658, 'eps_e':     0.1000})
Step:  289000, Reward:   387.477 [  72.641], Avg:   173.701 (0.100) <0-01:49:24> ({'r_t':  2768.9229, 'eps':     0.1000, 'critic_loss':   639.2349, 'actor_loss':    -1.7989, 'eps_e':     0.1000})
Step:  290000, Reward:   467.213 [  45.110], Avg:   174.710 (0.100) <0-01:49:47> ({'r_t':  2585.5039, 'eps':     0.1000, 'critic_loss':   621.5175, 'actor_loss':    -1.8542, 'eps_e':     0.1000})
Step:  291000, Reward:   455.931 [  32.016], Avg:   175.673 (0.100) <0-01:50:09> ({'r_t':  2701.6582, 'eps':     0.1000, 'critic_loss':   428.3555, 'actor_loss':    -1.8788, 'eps_e':     0.1000})
Step:  292000, Reward:   452.802 [  28.802], Avg:   176.619 (0.100) <0-01:50:32> ({'r_t':  2764.4964, 'eps':     0.1000, 'critic_loss':   625.3120, 'actor_loss':    -2.0717, 'eps_e':     0.1000})
Step:  293000, Reward:   376.107 [  75.221], Avg:   177.297 (0.100) <0-01:50:55> ({'r_t':  2738.0989, 'eps':     0.1000, 'critic_loss':   401.9884, 'actor_loss':    -2.0008, 'eps_e':     0.1000})
Step:  294000, Reward:   450.686 [  10.970], Avg:   178.224 (0.100) <0-01:51:18> ({'r_t':  2807.9608, 'eps':     0.1000, 'critic_loss':   374.7303, 'actor_loss':    -1.8360, 'eps_e':     0.1000})
Step:  295000, Reward:   339.269 [ 184.974], Avg:   178.768 (0.100) <0-01:51:41> ({'r_t':  2736.7050, 'eps':     0.1000, 'critic_loss':   339.5599, 'actor_loss':    -1.6783, 'eps_e':     0.1000})
Step:  296000, Reward:   396.055 [ 103.361], Avg:   179.500 (0.100) <0-01:52:04> ({'r_t':  1465.9049, 'eps':     0.1000, 'critic_loss':   783.1697, 'actor_loss':    -1.6515, 'eps_e':     0.1000})
Step:  297000, Reward:   339.439 [  42.568], Avg:   180.037 (0.100) <0-01:52:27> ({'r_t':  1967.3268, 'eps':     0.1000, 'critic_loss':  2565.5930, 'actor_loss':    -2.5173, 'eps_e':     0.1000})
Step:  298000, Reward:   445.887 [  56.008], Avg:   180.926 (0.100) <0-01:52:51> ({'r_t':  1704.0592, 'eps':     0.1000, 'critic_loss':  3251.0371, 'actor_loss':    -4.2924, 'eps_e':     0.1000})
Step:  299000, Reward:   276.548 [ 341.283], Avg:   181.244 (0.100) <0-01:53:14> ({'r_t':  1973.7993, 'eps':     0.1000, 'critic_loss':  3413.4519, 'actor_loss':    -5.4352, 'eps_e':     0.1000})
Step:  300000, Reward:   421.471 [  45.192], Avg:   182.043 (0.100) <0-01:53:37> ({'r_t':  1874.8171, 'eps':     0.1000, 'critic_loss':  4000.6992, 'actor_loss':    -6.2750, 'eps_e':     0.1000})
Step:  301000, Reward:   456.667 [  46.793], Avg:   182.952 (0.100) <0-01:54:00> ({'r_t':  2232.0858, 'eps':     0.1000, 'critic_loss':  4544.0342, 'actor_loss':    -6.2476, 'eps_e':     0.1000})
Step:  302000, Reward:   405.711 [  21.326], Avg:   183.687 (0.100) <0-01:54:23> ({'r_t':  2405.3649, 'eps':     0.1000, 'critic_loss':  4188.1538, 'actor_loss':    -6.8062, 'eps_e':     0.1000})
Step:  303000, Reward:   438.236 [  45.840], Avg:   184.524 (0.100) <0-01:54:46> ({'r_t':  2421.2553, 'eps':     0.1000, 'critic_loss':  3153.3931, 'actor_loss':    -6.1650, 'eps_e':     0.1000})
Step:  304000, Reward:   402.488 [  50.190], Avg:   185.239 (0.100) <0-01:55:09> ({'r_t':  2507.1876, 'eps':     0.1000, 'critic_loss':  2796.2986, 'actor_loss':    -5.2259, 'eps_e':     0.1000})
Step:  305000, Reward:   459.630 [  26.945], Avg:   186.136 (0.100) <0-01:55:33> ({'r_t':  2364.0793, 'eps':     0.1000, 'critic_loss':  2236.6965, 'actor_loss':    -4.5724, 'eps_e':     0.1000})
Step:  306000, Reward:   465.565 [  44.424], Avg:   187.046 (0.100) <0-01:55:56> ({'r_t':  2474.5846, 'eps':     0.1000, 'critic_loss':  1969.2816, 'actor_loss':    -3.6319, 'eps_e':     0.1000})
Step:  307000, Reward:   472.242 [  10.863], Avg:   187.972 (0.100) <0-01:56:19> ({'r_t':  2637.7353, 'eps':     0.1000, 'critic_loss':  1098.1865, 'actor_loss':    -3.0329, 'eps_e':     0.1000})
Step:  308000, Reward:   460.661 [  28.171], Avg:   188.854 (0.100) <0-01:56:42> ({'r_t':  2599.7396, 'eps':     0.1000, 'critic_loss':   706.1174, 'actor_loss':    -2.5082, 'eps_e':     0.1000})
Step:  309000, Reward:   345.314 [ 361.601], Avg:   189.359 (0.100) <0-01:57:05> ({'r_t':  2682.1397, 'eps':     0.1000, 'critic_loss':   472.1992, 'actor_loss':    -2.4919, 'eps_e':     0.1000})
Step:  310000, Reward:   446.694 [  31.702], Avg:   190.187 (0.100) <0-01:57:28> ({'r_t':  2544.9322, 'eps':     0.1000, 'critic_loss':   367.8474, 'actor_loss':    -2.5403, 'eps_e':     0.1000})
Step:  311000, Reward:   434.736 [  55.850], Avg:   190.970 (0.100) <0-01:57:51> ({'r_t':  2451.7432, 'eps':     0.1000, 'critic_loss':   314.8816, 'actor_loss':    -2.6023, 'eps_e':     0.1000})
Step:  312000, Reward:   427.082 [ 104.024], Avg:   191.725 (0.100) <0-01:58:14> ({'r_t':  2756.6425, 'eps':     0.1000, 'critic_loss':   426.2744, 'actor_loss':    -2.5875, 'eps_e':     0.1000})
Step:  313000, Reward:   441.463 [  31.076], Avg:   192.520 (0.100) <0-01:58:37> ({'r_t':  2716.7501, 'eps':     0.1000, 'critic_loss':   489.3914, 'actor_loss':    -2.3621, 'eps_e':     0.1000})
Step:  314000, Reward:   460.400 [  31.637], Avg:   193.370 (0.100) <0-01:59:00> ({'r_t':  2702.7762, 'eps':     0.1000, 'critic_loss':   406.6350, 'actor_loss':    -2.5028, 'eps_e':     0.1000})
Step:  315000, Reward:   457.996 [  42.828], Avg:   194.208 (0.100) <0-01:59:22> ({'r_t':  2768.6609, 'eps':     0.1000, 'critic_loss':   375.2818, 'actor_loss':    -2.2802, 'eps_e':     0.1000})
Step:  316000, Reward:   480.617 [  10.279], Avg:   195.111 (0.100) <0-01:59:45> ({'r_t':  2689.6001, 'eps':     0.1000, 'critic_loss':   323.1447, 'actor_loss':    -2.2978, 'eps_e':     0.1000})
Step:  317000, Reward:   423.838 [  49.306], Avg:   195.831 (0.100) <0-02:00:09> ({'r_t':  2321.2565, 'eps':     0.1000, 'critic_loss':   505.9910, 'actor_loss':    -2.9872, 'eps_e':     0.1000})
Step:  318000, Reward:   407.054 [  51.472], Avg:   196.493 (0.100) <0-02:00:31> ({'r_t':  2651.9682, 'eps':     0.1000, 'critic_loss':   343.5578, 'actor_loss':    -2.4320, 'eps_e':     0.1000})
Step:  319000, Reward:   447.097 [  11.693], Avg:   197.276 (0.100) <0-02:00:54> ({'r_t':  2663.5473, 'eps':     0.1000, 'critic_loss':   427.4249, 'actor_loss':    -2.4807, 'eps_e':     0.1000})
Step:  320000, Reward:   369.148 [  67.752], Avg:   197.811 (0.100) <0-02:01:17> ({'r_t':  2710.7173, 'eps':     0.1000, 'critic_loss':   429.7762, 'actor_loss':    -2.7395, 'eps_e':     0.1000})
Step:  321000, Reward:   435.637 [  32.103], Avg:   198.550 (0.100) <0-02:01:40> ({'r_t':  2683.2514, 'eps':     0.1000, 'critic_loss':   267.4301, 'actor_loss':    -2.8993, 'eps_e':     0.1000})
Step:  322000, Reward:   459.562 [  42.399], Avg:   199.358 (0.100) <0-02:02:03> ({'r_t':  2780.1008, 'eps':     0.1000, 'critic_loss':   327.8599, 'actor_loss':    -3.0425, 'eps_e':     0.1000})
Step:  323000, Reward:   466.512 [  16.186], Avg:   200.183 (0.100) <0-02:02:26> ({'r_t':  2739.0276, 'eps':     0.1000, 'critic_loss':   307.0751, 'actor_loss':    -3.1388, 'eps_e':     0.1000})
Step:  324000, Reward:   466.565 [  34.890], Avg:   201.002 (0.100) <0-02:02:49> ({'r_t':  2787.9826, 'eps':     0.1000, 'critic_loss':   164.6026, 'actor_loss':    -2.7858, 'eps_e':     0.1000})
Step:  325000, Reward:   480.439 [   7.402], Avg:   201.859 (0.100) <0-02:03:12> ({'r_t':  2777.1220, 'eps':     0.1000, 'critic_loss':   129.8504, 'actor_loss':    -2.7564, 'eps_e':     0.1000})
Step:  326000, Reward:   470.248 [  34.447], Avg:   202.680 (0.100) <0-02:03:35> ({'r_t':  2868.5509, 'eps':     0.1000, 'critic_loss':   146.1170, 'actor_loss':    -2.5183, 'eps_e':     0.1000})
Step:  327000, Reward:   459.236 [  56.455], Avg:   203.462 (0.100) <0-02:03:57> ({'r_t':  2824.1384, 'eps':     0.1000, 'critic_loss':   153.6644, 'actor_loss':    -2.3891, 'eps_e':     0.1000})
Step:  328000, Reward:   449.458 [  47.392], Avg:   204.210 (0.100) <0-02:04:21> ({'r_t':  2762.3659, 'eps':     0.1000, 'critic_loss':   125.4985, 'actor_loss':    -2.1846, 'eps_e':     0.1000})
Step:  329000, Reward:   365.227 [ 362.904], Avg:   204.698 (0.100) <0-02:04:45> ({'r_t':  2798.5050, 'eps':     0.1000, 'critic_loss':    94.5408, 'actor_loss':    -1.9932, 'eps_e':     0.1000})
Step:  330000, Reward:   447.934 [  41.065], Avg:   205.433 (0.100) <0-02:05:08> ({'r_t':  2664.6254, 'eps':     0.1000, 'critic_loss':   130.6743, 'actor_loss':    -1.7083, 'eps_e':     0.1000})
Step:  331000, Reward:   451.917 [ 100.739], Avg:   206.175 (0.100) <0-02:05:31> ({'r_t':  2538.3000, 'eps':     0.1000, 'critic_loss':   316.7944, 'actor_loss':    -1.7044, 'eps_e':     0.1000})
Step:  332000, Reward:   447.877 [  60.342], Avg:   206.901 (0.100) <0-02:05:54> ({'r_t':  2326.0606, 'eps':     0.1000, 'critic_loss':   750.8430, 'actor_loss':    -1.9239, 'eps_e':     0.1000})
Step:  333000, Reward:   136.165 [ 519.106], Avg:   206.689 (0.100) <0-02:06:19> ({'r_t':  2349.9110, 'eps':     0.1000, 'critic_loss':  1103.0341, 'actor_loss':    -2.2174, 'eps_e':     0.1000})
Step:  334000, Reward:   384.534 [ 132.982], Avg:   207.220 (0.100) <0-02:06:43> ({'r_t':  1989.6340, 'eps':     0.1000, 'critic_loss':  1627.1737, 'actor_loss':    -2.5168, 'eps_e':     0.1000})
Step:  335000, Reward:  -202.061 [ 725.650], Avg:   206.002 (0.100) <0-02:07:07> ({'r_t':  1281.2406, 'eps':     0.1000, 'critic_loss':  3474.4126, 'actor_loss':    -3.3444, 'eps_e':     0.1000})
Step:  336000, Reward:   426.266 [  38.904], Avg:   206.656 (0.100) <0-02:07:29> ({'r_t':  2528.3565, 'eps':     0.1000, 'critic_loss':  4303.8794, 'actor_loss':    -4.8546, 'eps_e':     0.1000})
Step:  337000, Reward:   416.570 [ 103.886], Avg:   207.277 (0.100) <0-02:07:52> ({'r_t':  2609.1220, 'eps':     0.1000, 'critic_loss':  5461.7920, 'actor_loss':    -5.2365, 'eps_e':     0.1000})
Step:  338000, Reward:   427.665 [  42.033], Avg:   207.927 (0.100) <0-02:08:15> ({'r_t':  1800.8728, 'eps':     0.1000, 'critic_loss':  5566.7017, 'actor_loss':    -5.7241, 'eps_e':     0.1000})
Step:  339000, Reward:   447.388 [  24.696], Avg:   208.631 (0.100) <0-02:08:38> ({'r_t':  2550.3799, 'eps':     0.1000, 'critic_loss':  4968.4531, 'actor_loss':    -5.4932, 'eps_e':     0.1000})
Step:  340000, Reward:   472.861 [   7.292], Avg:   209.406 (0.100) <0-02:09:01> ({'r_t':  2561.0306, 'eps':     0.1000, 'critic_loss':  4735.3242, 'actor_loss':    -5.5532, 'eps_e':     0.1000})
Step:  341000, Reward:   464.734 [  27.154], Avg:   210.153 (0.100) <0-02:09:25> ({'r_t':  2595.9253, 'eps':     0.1000, 'critic_loss':  3854.9648, 'actor_loss':    -5.3934, 'eps_e':     0.1000})
Step:  342000, Reward:   390.118 [  34.289], Avg:   210.677 (0.100) <0-02:09:48> ({'r_t':  2647.5654, 'eps':     0.1000, 'critic_loss':  2842.7859, 'actor_loss':    -4.8732, 'eps_e':     0.1000})
Step:  343000, Reward:   469.859 [   9.835], Avg:   211.431 (0.100) <0-02:10:12> ({'r_t':  2710.8766, 'eps':     0.1000, 'critic_loss':  1845.9125, 'actor_loss':    -4.3111, 'eps_e':     0.1000})
Step:  344000, Reward:   486.073 [   8.494], Avg:   212.227 (0.100) <0-02:10:37> ({'r_t':  2779.0561, 'eps':     0.1000, 'critic_loss':   939.4473, 'actor_loss':    -3.7644, 'eps_e':     0.1000})
Step:  345000, Reward:   437.003 [  55.325], Avg:   212.876 (0.100) <0-02:11:01> ({'r_t':  2782.0275, 'eps':     0.1000, 'critic_loss':   599.3860, 'actor_loss':    -3.3898, 'eps_e':     0.1000})
Step:  346000, Reward:   489.173 [   9.144], Avg:   213.673 (0.100) <0-02:11:25> ({'r_t':  2715.0326, 'eps':     0.1000, 'critic_loss':   579.3591, 'actor_loss':    -2.9906, 'eps_e':     0.1000})
Step:  347000, Reward:   466.087 [  11.432], Avg:   214.398 (0.100) <0-02:11:49> ({'r_t':  2846.4424, 'eps':     0.1000, 'critic_loss':   342.8091, 'actor_loss':    -3.1155, 'eps_e':     0.1000})
Step:  348000, Reward:   426.507 [  85.167], Avg:   215.006 (0.100) <0-02:12:12> ({'r_t':  2843.4401, 'eps':     0.1000, 'critic_loss':    85.3957, 'actor_loss':    -2.8504, 'eps_e':     0.1000})
Step:  349000, Reward:   476.330 [  40.952], Avg:   215.752 (0.100) <0-02:12:35> ({'r_t':  2831.5312, 'eps':     0.1000, 'critic_loss':    75.7096, 'actor_loss':    -2.4196, 'eps_e':     0.1000})
Step:  350000, Reward:   460.540 [  55.653], Avg:   216.450 (0.100) <0-02:12:58> ({'r_t':  2789.0018, 'eps':     0.1000, 'critic_loss':    72.8800, 'actor_loss':    -2.0858, 'eps_e':     0.1000})
Step:  351000, Reward:   492.043 [   7.685], Avg:   217.233 (0.100) <0-02:13:21> ({'r_t':  2750.7354, 'eps':     0.1000, 'critic_loss':    94.6076, 'actor_loss':    -1.9075, 'eps_e':     0.1000})
Step:  352000, Reward:   489.478 [   7.941], Avg:   218.004 (0.100) <0-02:13:44> ({'r_t':  2915.0228, 'eps':     0.1000, 'critic_loss':    93.3934, 'actor_loss':    -1.9821, 'eps_e':     0.1000})
Step:  353000, Reward:   437.886 [ 123.515], Avg:   218.625 (0.100) <0-02:14:07> ({'r_t':  2852.1575, 'eps':     0.1000, 'critic_loss':   101.0960, 'actor_loss':    -1.9423, 'eps_e':     0.1000})
Step:  354000, Reward:   461.262 [  39.010], Avg:   219.309 (0.100) <0-02:14:30> ({'r_t':  2814.5275, 'eps':     0.1000, 'critic_loss':   104.7823, 'actor_loss':    -1.9678, 'eps_e':     0.1000})
Step:  355000, Reward:   -81.413 [ 602.462], Avg:   218.464 (0.100) <0-02:14:53> ({'r_t':  2108.2998, 'eps':     0.1000, 'critic_loss':   122.0029, 'actor_loss':    -1.7180, 'eps_e':     0.1000})
Step:  356000, Reward:   392.461 [  71.072], Avg:   218.951 (0.100) <0-02:15:17> ({'r_t':  1870.6428, 'eps':     0.1000, 'critic_loss':   635.0251, 'actor_loss':    -1.6512, 'eps_e':     0.1000})
Step:  357000, Reward:   317.017 [  82.819], Avg:   219.225 (0.100) <0-02:15:40> ({'r_t':  1531.7540, 'eps':     0.1000, 'critic_loss':  1897.5295, 'actor_loss':    -2.3761, 'eps_e':     0.1000})
Step:  358000, Reward:   448.414 [  99.202], Avg:   219.864 (0.100) <0-02:16:04> ({'r_t':  1328.9744, 'eps':     0.1000, 'critic_loss':  3707.9775, 'actor_loss':    -3.3422, 'eps_e':     0.1000})
Step:  359000, Reward:   410.018 [  66.011], Avg:   220.392 (0.100) <0-02:16:27> ({'r_t':  2341.8044, 'eps':     0.1000, 'critic_loss':  4072.8081, 'actor_loss':    -3.9589, 'eps_e':     0.1000})
Step:  360000, Reward:   313.893 [ 230.510], Avg:   220.651 (0.100) <0-02:16:50> ({'r_t':  2223.4293, 'eps':     0.1000, 'critic_loss':  4161.7046, 'actor_loss':    -5.5569, 'eps_e':     0.1000})
Step:  361000, Reward:   340.170 [ 101.863], Avg:   220.981 (0.100) <0-02:17:13> ({'r_t':  2331.3582, 'eps':     0.1000, 'critic_loss':  4656.9424, 'actor_loss':    -5.9515, 'eps_e':     0.1000})
Step:  362000, Reward:   435.261 [  80.557], Avg:   221.571 (0.100) <0-02:17:36> ({'r_t':  2473.8949, 'eps':     0.1000, 'critic_loss':  3795.9321, 'actor_loss':    -5.7494, 'eps_e':     0.1000})
Step:  363000, Reward:   215.242 [  98.479], Avg:   221.554 (0.100) <0-02:17:59> ({'r_t':  2510.2153, 'eps':     0.1000, 'critic_loss':  2556.7214, 'actor_loss':    -5.7034, 'eps_e':     0.1000})
Step:  364000, Reward:   479.505 [   5.036], Avg:   222.261 (0.100) <0-02:18:22> ({'r_t':  2555.4457, 'eps':     0.1000, 'critic_loss':  1538.7072, 'actor_loss':    -4.6694, 'eps_e':     0.1000})
Step:  365000, Reward:   469.939 [  47.405], Avg:   222.937 (0.100) <0-02:18:45> ({'r_t':  2769.8572, 'eps':     0.1000, 'critic_loss':   500.6844, 'actor_loss':    -3.8404, 'eps_e':     0.1000})
Step:  366000, Reward:   455.905 [  59.288], Avg:   223.572 (0.100) <0-02:19:08> ({'r_t':  2702.7044, 'eps':     0.1000, 'critic_loss':   393.8840, 'actor_loss':    -3.5736, 'eps_e':     0.1000})
Step:  367000, Reward:   370.523 [ 186.157], Avg:   223.971 (0.100) <0-02:19:31> ({'r_t':  2721.3807, 'eps':     0.1000, 'critic_loss':   365.7039, 'actor_loss':    -2.8667, 'eps_e':     0.1000})
Step:  368000, Reward:   439.541 [ 129.185], Avg:   224.556 (0.100) <0-02:19:54> ({'r_t':  2846.1865, 'eps':     0.1000, 'critic_loss':   243.2879, 'actor_loss':    -2.5596, 'eps_e':     0.1000})
Step:  369000, Reward:   484.504 [  31.133], Avg:   225.258 (0.100) <0-02:20:17> ({'r_t':  2571.7894, 'eps':     0.1000, 'critic_loss':   549.9412, 'actor_loss':    -2.3446, 'eps_e':     0.1000})
Step:  370000, Reward:   468.545 [  51.798], Avg:   225.914 (0.100) <0-02:20:40> ({'r_t':  2814.1272, 'eps':     0.1000, 'critic_loss':   760.9974, 'actor_loss':    -2.1889, 'eps_e':     0.1000})
Step:  371000, Reward:   383.073 [ 109.752], Avg:   226.336 (0.100) <0-02:21:04> ({'r_t':  2599.9976, 'eps':     0.1000, 'critic_loss':   546.9956, 'actor_loss':    -2.0152, 'eps_e':     0.1000})
Step:  372000, Reward:   454.895 [  40.964], Avg:   226.949 (0.100) <0-02:21:27> ({'r_t':  2523.1646, 'eps':     0.1000, 'critic_loss':   709.4257, 'actor_loss':    -1.9466, 'eps_e':     0.1000})
Step:  373000, Reward:   472.074 [  22.178], Avg:   227.605 (0.100) <0-02:21:50> ({'r_t':  2809.2397, 'eps':     0.1000, 'critic_loss':   818.6011, 'actor_loss':    -2.0500, 'eps_e':     0.1000})
Step:  374000, Reward:   438.760 [  16.978], Avg:   228.168 (0.100) <0-02:22:13> ({'r_t':  2718.4609, 'eps':     0.1000, 'critic_loss':  1025.3312, 'actor_loss':    -2.4193, 'eps_e':     0.1000})
Step:  375000, Reward:   465.972 [  61.223], Avg:   228.800 (0.100) <0-02:22:36> ({'r_t':  2757.9967, 'eps':     0.1000, 'critic_loss':   924.1553, 'actor_loss':    -2.5649, 'eps_e':     0.1000})
Step:  376000, Reward:   339.867 [  68.604], Avg:   229.095 (0.100) <0-02:23:01> ({'r_t':  2346.1439, 'eps':     0.1000, 'critic_loss':   608.5928, 'actor_loss':    -3.0927, 'eps_e':     0.1000})
Step:  377000, Reward:   376.357 [ 130.649], Avg:   229.484 (0.100) <0-02:23:24> ({'r_t':  1497.2389, 'eps':     0.1000, 'critic_loss':  1432.0129, 'actor_loss':    -3.5307, 'eps_e':     0.1000})
Step:  378000, Reward:   380.130 [ 148.303], Avg:   229.882 (0.100) <0-02:23:47> ({'r_t':  2625.2175, 'eps':     0.1000, 'critic_loss':  1286.3717, 'actor_loss':    -4.8968, 'eps_e':     0.1000})
Step:  379000, Reward:   426.434 [  62.985], Avg:   230.399 (0.100) <0-02:24:11> ({'r_t':  2437.9960, 'eps':     0.1000, 'critic_loss':  1221.2775, 'actor_loss':    -6.0167, 'eps_e':     0.1000})
Step:  380000, Reward:   488.790 [   7.621], Avg:   231.077 (0.100) <0-02:24:34> ({'r_t':  2612.3285, 'eps':     0.1000, 'critic_loss':  1478.3330, 'actor_loss':    -6.4962, 'eps_e':     0.1000})
Step:  381000, Reward:   487.365 [   8.022], Avg:   231.748 (0.100) <0-02:24:57> ({'r_t':  2688.9562, 'eps':     0.1000, 'critic_loss':  1075.7880, 'actor_loss':    -6.4266, 'eps_e':     0.1000})
Step:  382000, Reward:   451.323 [  69.291], Avg:   232.321 (0.100) <0-02:25:19> ({'r_t':  2705.1569, 'eps':     0.1000, 'critic_loss':  1142.7688, 'actor_loss':    -5.9675, 'eps_e':     0.1000})
Step:  383000, Reward:   476.416 [  11.660], Avg:   232.957 (0.100) <0-02:25:43> ({'r_t':  2781.0427, 'eps':     0.1000, 'critic_loss':  1116.9994, 'actor_loss':    -5.5306, 'eps_e':     0.1000})
Step:  384000, Reward:   487.008 [  44.478], Avg:   233.617 (0.100) <0-02:26:06> ({'r_t':  2867.2310, 'eps':     0.1000, 'critic_loss':   276.3726, 'actor_loss':    -3.6186, 'eps_e':     0.1000})
Step:  385000, Reward:   467.854 [  59.825], Avg:   234.224 (0.100) <0-02:26:29> ({'r_t':  2829.0561, 'eps':     0.1000, 'critic_loss':   346.2137, 'actor_loss':    -3.1926, 'eps_e':     0.1000})
Step:  386000, Reward:   473.537 [  42.664], Avg:   234.842 (0.100) <0-02:26:51> ({'r_t':  2841.4885, 'eps':     0.1000, 'critic_loss':   182.7894, 'actor_loss':    -2.8653, 'eps_e':     0.1000})
Step:  387000, Reward:   485.730 [   9.753], Avg:   235.489 (0.100) <0-02:27:14> ({'r_t':  2778.2946, 'eps':     0.1000, 'critic_loss':   259.6639, 'actor_loss':    -2.7296, 'eps_e':     0.1000})
Step:  388000, Reward:   464.228 [ 102.180], Avg:   236.077 (0.100) <0-02:27:37> ({'r_t':  2793.4030, 'eps':     0.1000, 'critic_loss':   157.6332, 'actor_loss':    -2.4243, 'eps_e':     0.1000})
Step:  389000, Reward:   486.029 [   8.047], Avg:   236.718 (0.100) <0-02:28:00> ({'r_t':  2861.1242, 'eps':     0.1000, 'critic_loss':   381.0789, 'actor_loss':    -2.2116, 'eps_e':     0.1000})
Step:  390000, Reward:   478.890 [   6.803], Avg:   237.337 (0.100) <0-02:28:23> ({'r_t':  2857.3589, 'eps':     0.1000, 'critic_loss':   294.8500, 'actor_loss':    -2.0876, 'eps_e':     0.1000})
Step:  391000, Reward:   496.004 [   7.976], Avg:   237.997 (0.100) <0-02:28:46> ({'r_t':  2693.1711, 'eps':     0.1000, 'critic_loss':   368.2706, 'actor_loss':    -2.0938, 'eps_e':     0.1000})
Step:  392000, Reward:   486.977 [  13.828], Avg:   238.630 (0.100) <0-02:29:09> ({'r_t':  2832.2007, 'eps':     0.1000, 'critic_loss':   536.7111, 'actor_loss':    -2.3396, 'eps_e':     0.1000})
Step:  393000, Reward:   501.956 [   9.425], Avg:   239.299 (0.100) <0-02:29:32> ({'r_t':  2580.2409, 'eps':     0.1000, 'critic_loss':   803.1860, 'actor_loss':    -2.5041, 'eps_e':     0.1000})
Step:  394000, Reward:   474.695 [  31.122], Avg:   239.895 (0.100) <0-02:29:55> ({'r_t':  2671.0384, 'eps':     0.1000, 'critic_loss':   848.3018, 'actor_loss':    -2.3518, 'eps_e':     0.1000})
Step:  395000, Reward:   294.516 [ 348.582], Avg:   240.033 (0.100) <0-02:30:20> ({'r_t':  2139.2726, 'eps':     0.1000, 'critic_loss':  1167.3097, 'actor_loss':    -2.6039, 'eps_e':     0.1000})
Step:  396000, Reward:   373.575 [ 112.807], Avg:   240.369 (0.100) <0-02:30:43> ({'r_t':  2232.3755, 'eps':     0.1000, 'critic_loss':  2347.6975, 'actor_loss':    -3.9458, 'eps_e':     0.1000})
Step:  397000, Reward:   463.740 [ 106.618], Avg:   240.930 (0.100) <0-02:31:06> ({'r_t':  2659.7444, 'eps':     0.1000, 'critic_loss':  1838.6730, 'actor_loss':    -4.4531, 'eps_e':     0.1000})
Step:  398000, Reward:   470.099 [ 106.944], Avg:   241.505 (0.100) <0-02:31:29> ({'r_t':  2745.1661, 'eps':     0.1000, 'critic_loss':  1737.7666, 'actor_loss':    -4.3803, 'eps_e':     0.1000})
Step:  399000, Reward:   453.938 [ 105.261], Avg:   242.036 (0.100) <0-02:31:52> ({'r_t':  2696.3095, 'eps':     0.1000, 'critic_loss':  2236.7600, 'actor_loss':    -4.7030, 'eps_e':     0.1000})
Step:  400000, Reward:   480.999 [  12.713], Avg:   242.632 (0.100) <0-02:32:15> ({'r_t':  2800.4669, 'eps':     0.1000, 'critic_loss':  1326.7078, 'actor_loss':    -4.7051, 'eps_e':     0.1000})
Step:  401000, Reward:   476.777 [  10.895], Avg:   243.214 (0.100) <0-02:32:38> ({'r_t':  2750.4831, 'eps':     0.1000, 'critic_loss':  1295.3364, 'actor_loss':    -4.2665, 'eps_e':     0.1000})
Step:  402000, Reward:   476.646 [  10.650], Avg:   243.793 (0.100) <0-02:33:01> ({'r_t':  2726.8028, 'eps':     0.1000, 'critic_loss':   542.1750, 'actor_loss':    -3.1140, 'eps_e':     0.1000})
Step:  403000, Reward:   472.949 [  63.016], Avg:   244.361 (0.100) <0-02:33:24> ({'r_t':  2782.6422, 'eps':     0.1000, 'critic_loss':   187.4457, 'actor_loss':    -2.4797, 'eps_e':     0.1000})
Step:  404000, Reward:   490.336 [   7.666], Avg:   244.968 (0.100) <0-02:33:47> ({'r_t':  2698.4344, 'eps':     0.1000, 'critic_loss':   108.9040, 'actor_loss':    -2.3915, 'eps_e':     0.1000})
Step:  405000, Reward:   501.151 [   8.510], Avg:   245.599 (0.100) <0-02:34:10> ({'r_t':  2846.0705, 'eps':     0.1000, 'critic_loss':   321.1003, 'actor_loss':    -2.2599, 'eps_e':     0.1000})
Step:  406000, Reward:   213.714 [ 585.295], Avg:   245.521 (0.100) <0-02:34:33> ({'r_t':  2706.2676, 'eps':     0.1000, 'critic_loss':   400.1826, 'actor_loss':    -1.9316, 'eps_e':     0.1000})
Step:  407000, Reward:   487.467 [  13.324], Avg:   246.114 (0.100) <0-02:34:57> ({'r_t':  2737.2635, 'eps':     0.1000, 'critic_loss':   557.7876, 'actor_loss':    -1.7745, 'eps_e':     0.1000})
Step:  408000, Reward:   452.368 [  39.910], Avg:   246.618 (0.100) <0-02:35:20> ({'r_t':  2806.9845, 'eps':     0.1000, 'critic_loss':   636.7274, 'actor_loss':    -1.7982, 'eps_e':     0.1000})
Step:  409000, Reward:   474.090 [  50.817], Avg:   247.173 (0.100) <0-02:35:43> ({'r_t':  2427.6948, 'eps':     0.1000, 'critic_loss':  1112.6606, 'actor_loss':    -1.9898, 'eps_e':     0.1000})
Step:  410000, Reward:   470.714 [  49.332], Avg:   247.717 (0.100) <0-02:36:08> ({'r_t':  2535.4946, 'eps':     0.1000, 'critic_loss':  1086.6008, 'actor_loss':    -2.0899, 'eps_e':     0.1000})
Step:  411000, Reward:   356.497 [  77.305], Avg:   247.981 (0.100) <0-02:36:32> ({'r_t':  2494.1297, 'eps':     0.1000, 'critic_loss':  1424.3419, 'actor_loss':    -2.2414, 'eps_e':     0.1000})
Step:  412000, Reward:   386.397 [ 195.410], Avg:   248.316 (0.100) <0-02:36:55> ({'r_t':  2654.5345, 'eps':     0.1000, 'critic_loss':  1319.9810, 'actor_loss':    -2.2956, 'eps_e':     0.1000})
Step:  413000, Reward:   396.292 [  21.396], Avg:   248.673 (0.100) <0-02:37:18> ({'r_t':  2569.7748, 'eps':     0.1000, 'critic_loss':  1341.1436, 'actor_loss':    -2.3793, 'eps_e':     0.1000})
Step:  414000, Reward:   490.533 [  14.068], Avg:   249.256 (0.100) <0-02:37:40> ({'r_t':  2492.6851, 'eps':     0.1000, 'critic_loss':  1001.1672, 'actor_loss':    -2.8973, 'eps_e':     0.1000})
Step:  415000, Reward:   415.959 [  97.365], Avg:   249.657 (0.100) <0-02:38:05> ({'r_t':  2590.1207, 'eps':     0.1000, 'critic_loss':  1546.2715, 'actor_loss':    -3.0340, 'eps_e':     0.1000})
Step:  416000, Reward:   418.144 [  23.831], Avg:   250.061 (0.100) <0-02:38:28> ({'r_t':  2410.0292, 'eps':     0.1000, 'critic_loss':  1185.7805, 'actor_loss':    -2.9095, 'eps_e':     0.1000})
Step:  417000, Reward:   485.420 [   7.826], Avg:   250.624 (0.100) <0-02:38:51> ({'r_t':  2738.1998, 'eps':     0.1000, 'critic_loss':  1197.8593, 'actor_loss':    -3.0523, 'eps_e':     0.1000})
Step:  418000, Reward:   469.223 [  20.154], Avg:   251.145 (0.100) <0-02:39:14> ({'r_t':  2329.5098, 'eps':     0.1000, 'critic_loss':  1487.1656, 'actor_loss':    -3.1851, 'eps_e':     0.1000})
Step:  419000, Reward:   453.154 [  13.455], Avg:   251.626 (0.100) <0-02:39:37> ({'r_t':  2553.7521, 'eps':     0.1000, 'critic_loss':  1715.1965, 'actor_loss':    -3.3059, 'eps_e':     0.1000})
Step:  420000, Reward:   456.512 [ 106.354], Avg:   252.113 (0.100) <0-02:40:00> ({'r_t':  2683.5703, 'eps':     0.1000, 'critic_loss':  1597.3101, 'actor_loss':    -3.3027, 'eps_e':     0.1000})
Step:  421000, Reward:   445.845 [  51.603], Avg:   252.572 (0.100) <0-02:40:24> ({'r_t':  2506.6690, 'eps':     0.1000, 'critic_loss':  1399.8400, 'actor_loss':    -3.4607, 'eps_e':     0.1000})
Step:  422000, Reward:   452.820 [  13.548], Avg:   253.046 (0.100) <0-02:40:47> ({'r_t':  2512.3411, 'eps':     0.1000, 'critic_loss':  1374.3635, 'actor_loss':    -3.5064, 'eps_e':     0.1000})
Step:  423000, Reward:   351.150 [ 362.072], Avg:   253.277 (0.100) <0-02:41:11> ({'r_t':  2565.2611, 'eps':     0.1000, 'critic_loss':   799.1760, 'actor_loss':    -3.3759, 'eps_e':     0.1000})
Step:  424000, Reward:   506.174 [   7.207], Avg:   253.872 (0.100) <0-02:41:34> ({'r_t':  2499.5789, 'eps':     0.1000, 'critic_loss':   887.6418, 'actor_loss':    -3.2853, 'eps_e':     0.1000})
Step:  425000, Reward:   480.553 [   8.212], Avg:   254.404 (0.100) <0-02:41:57> ({'r_t':  2704.1001, 'eps':     0.1000, 'critic_loss':   840.1057, 'actor_loss':    -3.2598, 'eps_e':     0.1000})
Step:  426000, Reward:   467.246 [  55.640], Avg:   254.903 (0.100) <0-02:42:22> ({'r_t':  2537.3631, 'eps':     0.1000, 'critic_loss':   874.5173, 'actor_loss':    -2.9892, 'eps_e':     0.1000})
Step:  427000, Reward:   433.099 [  45.099], Avg:   255.319 (0.100) <0-02:42:45> ({'r_t':  2703.7285, 'eps':     0.1000, 'critic_loss':   955.3087, 'actor_loss':    -2.8662, 'eps_e':     0.1000})
Step:  428000, Reward:   477.240 [  32.017], Avg:   255.836 (0.100) <0-02:43:09> ({'r_t':  2545.3410, 'eps':     0.1000, 'critic_loss':   946.2135, 'actor_loss':    -2.9759, 'eps_e':     0.1000})
Step:  429000, Reward:   455.575 [  57.375], Avg:   256.301 (0.100) <0-02:43:33> ({'r_t':  2240.0606, 'eps':     0.1000, 'critic_loss':  1244.6827, 'actor_loss':    -2.8184, 'eps_e':     0.1000})
Step:  430000, Reward:   448.872 [ 110.479], Avg:   256.748 (0.100) <0-02:44:03> ({'r_t':  2549.4103, 'eps':     0.1000, 'critic_loss':  1194.4137, 'actor_loss':    -2.8659, 'eps_e':     0.1000})
Step:  431000, Reward:   462.018 [  28.196], Avg:   257.223 (0.100) <0-02:44:26> ({'r_t':  2542.1984, 'eps':     0.1000, 'critic_loss':   846.6455, 'actor_loss':    -3.2265, 'eps_e':     0.1000})
Step:  432000, Reward:   476.223 [  17.507], Avg:   257.729 (0.100) <0-02:44:50> ({'r_t':  2323.4902, 'eps':     0.1000, 'critic_loss':   978.4855, 'actor_loss':    -3.3182, 'eps_e':     0.1000})
Step:  433000, Reward:   460.761 [  45.952], Avg:   258.196 (0.100) <0-02:45:15> ({'r_t':  2285.1220, 'eps':     0.1000, 'critic_loss':   657.6295, 'actor_loss':    -2.9769, 'eps_e':     0.1000})
Step:  434000, Reward:   322.676 [ 175.704], Avg:   258.345 (0.100) <0-02:45:43> ({'r_t':  1542.3436, 'eps':     0.1000, 'critic_loss':   579.4586, 'actor_loss':    -2.9092, 'eps_e':     0.1000})
Step:  435000, Reward:   409.440 [ 160.784], Avg:   258.691 (0.100) <0-02:46:07> ({'r_t':  1659.0131, 'eps':     0.1000, 'critic_loss':   713.5087, 'actor_loss':    -3.3776, 'eps_e':     0.1000})
Step:  436000, Reward:   502.660 [  74.669], Avg:   259.249 (0.100) <0-02:46:31> ({'r_t':  2419.4802, 'eps':     0.1000, 'critic_loss':   371.9090, 'actor_loss':    -3.7337, 'eps_e':     0.1000})
Step:  437000, Reward:   383.158 [ 227.107], Avg:   259.532 (0.100) <0-02:46:54> ({'r_t':  2180.8767, 'eps':     0.1000, 'critic_loss':   737.6270, 'actor_loss':    -4.0375, 'eps_e':     0.1000})
Step:  438000, Reward:   415.262 [ 359.654], Avg:   259.887 (0.100) <0-02:47:18> ({'r_t':  2254.5111, 'eps':     0.1000, 'critic_loss':   832.8977, 'actor_loss':    -4.7334, 'eps_e':     0.1000})
Step:  439000, Reward:   543.585 [  23.889], Avg:   260.532 (0.100) <0-02:47:42> ({'r_t':  1753.1784, 'eps':     0.1000, 'critic_loss':  2182.7695, 'actor_loss':    -4.2480, 'eps_e':     0.1000})
Step:  440000, Reward:   398.977 [ 112.408], Avg:   260.846 (0.100) <0-02:48:08> ({'r_t':  2010.2031, 'eps':     0.1000, 'critic_loss':  2807.6345, 'actor_loss':    -4.4944, 'eps_e':     0.1000})
Step:  441000, Reward:   501.520 [  82.021], Avg:   261.390 (0.100) <0-02:48:37> ({'r_t':  1694.2563, 'eps':     0.1000, 'critic_loss':  2945.9312, 'actor_loss':    -5.1776, 'eps_e':     0.1000})
Step:  442000, Reward:   276.899 [ 114.044], Avg:   261.425 (0.100) <0-02:49:05> ({'r_t':  1731.8619, 'eps':     0.1000, 'critic_loss':  3289.2537, 'actor_loss':    -5.7770, 'eps_e':     0.1000})
Step:  443000, Reward:   151.703 [ 202.251], Avg:   261.178 (0.100) <0-02:49:32> ({'r_t':  1443.4157, 'eps':     0.1000, 'critic_loss':  3254.2520, 'actor_loss':    -5.8463, 'eps_e':     0.1000})
Step:  444000, Reward:   420.215 [ 165.818], Avg:   261.536 (0.100) <0-02:50:00> ({'r_t':    63.0176, 'eps':     0.1000, 'critic_loss':  3394.2676, 'actor_loss':    -5.8458, 'eps_e':     0.1000})
Step:  445000, Reward:   401.490 [ 225.021], Avg:   261.849 (0.100) <0-02:50:26> ({'r_t':  1654.5964, 'eps':     0.1000, 'critic_loss':  4297.0342, 'actor_loss':    -6.6836, 'eps_e':     0.1000})
Step:  446000, Reward:   533.894 [  74.711], Avg:   262.458 (0.100) <0-02:50:53> ({'r_t':  2057.8944, 'eps':     0.1000, 'critic_loss':  2865.8843, 'actor_loss':    -6.2006, 'eps_e':     0.1000})
Step:  447000, Reward:   481.678 [  24.094], Avg:   262.947 (0.100) <0-02:51:19> ({'r_t':  1985.3019, 'eps':     0.1000, 'critic_loss':  2495.4470, 'actor_loss':    -6.1808, 'eps_e':     0.1000})
Step:  448000, Reward:   519.687 [  39.307], Avg:   263.519 (0.100) <0-02:51:45> ({'r_t':  1900.4291, 'eps':     0.1000, 'critic_loss':  2331.6948, 'actor_loss':    -6.0606, 'eps_e':     0.1000})
Step:  449000, Reward:   405.327 [ 174.464], Avg:   263.834 (0.100) <0-02:52:14> ({'r_t':  1960.6607, 'eps':     0.1000, 'critic_loss':  2832.8726, 'actor_loss':    -6.2309, 'eps_e':     0.1000})
Step:  450000, Reward:   481.100 [  55.639], Avg:   264.316 (0.100) <0-02:52:40> ({'r_t':  2275.1025, 'eps':     0.1000, 'critic_loss':  2326.2500, 'actor_loss':    -5.4935, 'eps_e':     0.1000})
Step:  451000, Reward:   527.131 [ 142.084], Avg:   264.897 (0.100) <0-02:53:05> ({'r_t':  2344.7378, 'eps':     0.1000, 'critic_loss':  1158.4830, 'actor_loss':    -4.6747, 'eps_e':     0.1000})
Step:  452000, Reward:   588.405 [  42.960], Avg:   265.612 (0.100) <0-02:53:30> ({'r_t':  2036.3453, 'eps':     0.1000, 'critic_loss':   544.3068, 'actor_loss':    -4.6959, 'eps_e':     0.1000})
Step:  453000, Reward:   598.842 [  28.056], Avg:   266.345 (0.100) <0-02:53:54> ({'r_t':  2585.6398, 'eps':     0.1000, 'critic_loss':   633.9553, 'actor_loss':    -5.1739, 'eps_e':     0.1000})
Step:  454000, Reward:   596.021 [  23.556], Avg:   267.070 (0.100) <0-02:54:18> ({'r_t':  2665.2122, 'eps':     0.1000, 'critic_loss':   313.1056, 'actor_loss':    -4.5707, 'eps_e':     0.1000})
Step:  455000, Reward:   562.262 [  42.467], Avg:   267.717 (0.100) <0-02:54:44> ({'r_t':  2462.5113, 'eps':     0.1000, 'critic_loss':   291.8899, 'actor_loss':    -4.4525, 'eps_e':     0.1000})
Step:  456000, Reward:   595.425 [  30.230], Avg:   268.434 (0.100) <0-02:55:10> ({'r_t':  2212.7735, 'eps':     0.1000, 'critic_loss':   383.6005, 'actor_loss':    -3.7436, 'eps_e':     0.1000})
Step:  457000, Reward:   543.787 [  30.951], Avg:   269.036 (0.100) <0-02:55:37> ({'r_t':  2286.3202, 'eps':     0.1000, 'critic_loss':   494.2770, 'actor_loss':    -3.3214, 'eps_e':     0.1000})
Step:  458000, Reward:   642.156 [  26.772], Avg:   269.849 (0.100) <0-02:56:02> ({'r_t':  1977.9623, 'eps':     0.1000, 'critic_loss':   390.4752, 'actor_loss':    -3.2174, 'eps_e':     0.1000})
Step:  459000, Reward:  -319.519 [ 705.962], Avg:   268.567 (0.100) <0-02:56:27> ({'r_t':  1898.0931, 'eps':     0.1000, 'critic_loss':  1131.8381, 'actor_loss':    -3.8394, 'eps_e':     0.1000})
Step:  460000, Reward:   135.069 [ 692.171], Avg:   268.278 (0.100) <0-02:56:52> ({'r_t':  1867.4517, 'eps':     0.1000, 'critic_loss':  2176.4624, 'actor_loss':    -4.1601, 'eps_e':     0.1000})
Step:  461000, Reward:  -435.058 [ 697.430], Avg:   266.755 (0.100) <0-02:57:20> ({'r_t':  1912.7596, 'eps':     0.1000, 'critic_loss':  3232.2566, 'actor_loss':    -4.1588, 'eps_e':     0.1000})
Step:  462000, Reward:   616.340 [  94.924], Avg:   267.510 (0.100) <0-02:57:49> ({'r_t':  2178.6396, 'eps':     0.1000, 'critic_loss':  3493.4854, 'actor_loss':    -3.8703, 'eps_e':     0.1000})
Step:  463000, Reward:   559.638 [ 152.226], Avg:   268.140 (0.100) <0-02:58:14> ({'r_t':  2262.7374, 'eps':     0.1000, 'critic_loss':  3593.5430, 'actor_loss':    -4.2165, 'eps_e':     0.1000})
Step:  464000, Reward:   695.327 [  15.029], Avg:   269.059 (0.100) <0-02:58:39> ({'r_t':  2295.7456, 'eps':     0.1000, 'critic_loss':  3231.2932, 'actor_loss':    -4.6702, 'eps_e':     0.1000})
Step:  465000, Reward:   626.780 [  83.138], Avg:   269.826 (0.100) <0-02:59:06> ({'r_t':  2228.2088, 'eps':     0.1000, 'critic_loss':  3506.3279, 'actor_loss':    -4.9196, 'eps_e':     0.1000})
Step:  466000, Reward:   677.252 [  42.140], Avg:   270.699 (0.100) <0-02:59:32> ({'r_t':  2153.8772, 'eps':     0.1000, 'critic_loss':  2548.0586, 'actor_loss':    -4.4779, 'eps_e':     0.1000})
Step:  467000, Reward:   532.211 [ 345.272], Avg:   271.258 (0.100) <0-03:00:00> ({'r_t':  1792.4186, 'eps':     0.1000, 'critic_loss':  1602.2982, 'actor_loss':    -3.8397, 'eps_e':     0.1000})
Step:  468000, Reward:   666.298 [  33.197], Avg:   272.100 (0.100) <0-03:00:26> ({'r_t':  1998.7967, 'eps':     0.1000, 'critic_loss':  1286.5067, 'actor_loss':    -3.5215, 'eps_e':     0.1000})
Step:  469000, Reward:   515.151 [ 226.590], Avg:   272.617 (0.100) <0-03:00:55> ({'r_t':  1987.1340, 'eps':     0.1000, 'critic_loss':   946.4434, 'actor_loss':    -3.3300, 'eps_e':     0.1000})
Step:  470000, Reward:   653.731 [ 182.923], Avg:   273.426 (0.100) <0-03:01:21> ({'r_t':  2070.1458, 'eps':     0.1000, 'critic_loss':   829.7175, 'actor_loss':    -3.6441, 'eps_e':     0.1000})
Step:  471000, Reward:   669.317 [  92.947], Avg:   274.265 (0.100) <0-03:01:47> ({'r_t':  2566.2827, 'eps':     0.1000, 'critic_loss':   815.8066, 'actor_loss':    -4.5321, 'eps_e':     0.1000})
Step:  472000, Reward:   743.054 [  28.869], Avg:   275.256 (0.100) <0-03:02:13> ({'r_t':  1969.3772, 'eps':     0.1000, 'critic_loss':  1014.6150, 'actor_loss':    -5.5780, 'eps_e':     0.1000})
Step:  473000, Reward:   407.908 [ 583.700], Avg:   275.536 (0.100) <0-03:02:41> ({'r_t':  2109.3750, 'eps':     0.1000, 'critic_loss':  1447.1427, 'actor_loss':    -5.2550, 'eps_e':     0.1000})
Step:  474000, Reward:   125.573 [ 794.322], Avg:   275.220 (0.100) <0-03:03:06> ({'r_t':  2165.1091, 'eps':     0.1000, 'critic_loss':  1324.8219, 'actor_loss':    -4.8633, 'eps_e':     0.1000})
Step:  475000, Reward:   501.234 [ 250.100], Avg:   275.695 (0.100) <0-03:03:32> ({'r_t':  1900.6764, 'eps':     0.1000, 'critic_loss':  1879.5544, 'actor_loss':    -4.3657, 'eps_e':     0.1000})
Step:  476000, Reward:   597.883 [  80.734], Avg:   276.370 (0.100) <0-03:03:57> ({'r_t':  1982.6291, 'eps':     0.1000, 'critic_loss':  2405.3940, 'actor_loss':    -4.3481, 'eps_e':     0.1000})
Step:  477000, Reward:   396.150 [  59.663], Avg:   276.621 (0.100) <0-03:04:21> ({'r_t':  2129.7477, 'eps':     0.1000, 'critic_loss':  2782.1626, 'actor_loss':    -4.7671, 'eps_e':     0.1000})
Step:  478000, Reward:   616.957 [ 168.601], Avg:   277.332 (0.100) <0-03:04:51> ({'r_t':  1929.8310, 'eps':     0.1000, 'critic_loss':  3002.7676, 'actor_loss':    -5.3754, 'eps_e':     0.1000})
Step:  479000, Reward:   642.076 [ 132.919], Avg:   278.091 (0.100) <0-03:05:19> ({'r_t':  1712.5371, 'eps':     0.1000, 'critic_loss':  2622.6086, 'actor_loss':    -5.7404, 'eps_e':     0.1000})
Step:  480000, Reward:   310.408 [ 396.637], Avg:   278.159 (0.100) <0-03:05:48> ({'r_t':  1827.3986, 'eps':     0.1000, 'critic_loss':  2203.4236, 'actor_loss':    -6.0146, 'eps_e':     0.1000})
Step:  481000, Reward:   636.674 [ 114.078], Avg:   278.902 (0.100) <0-03:06:16> ({'r_t':  2009.0547, 'eps':     0.1000, 'critic_loss':  2173.8696, 'actor_loss':    -5.6969, 'eps_e':     0.1000})
Step:  482000, Reward:   522.868 [ 194.760], Avg:   279.408 (0.100) <0-03:06:45> ({'r_t':  2201.4507, 'eps':     0.1000, 'critic_loss':  1671.8053, 'actor_loss':    -5.0677, 'eps_e':     0.1000})
Step:  483000, Reward:   719.338 [  98.549], Avg:   280.316 (0.100) <0-03:07:11> ({'r_t':  2106.4019, 'eps':     0.1000, 'critic_loss':   784.2014, 'actor_loss':    -4.1168, 'eps_e':     0.1000})
Step:  484000, Reward:   747.879 [  69.844], Avg:   281.281 (0.100) <0-03:07:37> ({'r_t':  2280.7064, 'eps':     0.1000, 'critic_loss':   728.6030, 'actor_loss':    -3.9639, 'eps_e':     0.1000})
Step:  485000, Reward:   578.043 [ 270.329], Avg:   281.891 (0.100) <0-03:08:03> ({'r_t':  2584.4866, 'eps':     0.1000, 'critic_loss':   542.8695, 'actor_loss':    -4.0441, 'eps_e':     0.1000})
Step:  486000, Reward:   717.815 [ 100.438], Avg:   282.786 (0.100) <0-03:08:28> ({'r_t':  2615.4172, 'eps':     0.1000, 'critic_loss':   316.2644, 'actor_loss':    -4.0241, 'eps_e':     0.1000})
Step:  487000, Reward:   629.030 [ 148.180], Avg:   283.496 (0.100) <0-03:08:55> ({'r_t':  2563.9889, 'eps':     0.1000, 'critic_loss':   258.8597, 'actor_loss':    -3.8440, 'eps_e':     0.1000})
Step:  488000, Reward:   759.209 [  19.425], Avg:   284.469 (0.100) <0-03:09:21> ({'r_t':  2564.1608, 'eps':     0.1000, 'critic_loss':   238.7330, 'actor_loss':    -3.5386, 'eps_e':     0.1000})
Step:  489000, Reward:   502.409 [ 417.942], Avg:   284.913 (0.100) <0-03:09:46> ({'r_t':  2572.5657, 'eps':     0.1000, 'critic_loss':   206.0821, 'actor_loss':    -3.3385, 'eps_e':     0.1000})
Step:  490000, Reward:   733.652 [  81.733], Avg:   285.827 (0.100) <0-03:10:13> ({'r_t':  2462.9935, 'eps':     0.1000, 'critic_loss':   370.1187, 'actor_loss':    -3.4106, 'eps_e':     0.1000})
Step:  491000, Reward:   764.688 [  87.764], Avg:   286.801 (0.100) <0-03:10:39> ({'r_t':  2673.2337, 'eps':     0.1000, 'critic_loss':   269.6846, 'actor_loss':    -3.4202, 'eps_e':     0.1000})
Step:  492000, Reward:   761.165 [  37.477], Avg:   287.763 (0.100) <0-03:11:08> ({'r_t':  2678.5168, 'eps':     0.1000, 'critic_loss':   290.3974, 'actor_loss':    -3.7445, 'eps_e':     0.1000})
Step:  493000, Reward:   791.886 [  29.454], Avg:   288.783 (0.100) <0-03:11:34> ({'r_t':  2521.2767, 'eps':     0.1000, 'critic_loss':   348.1270, 'actor_loss':    -3.7240, 'eps_e':     0.1000})
Step:  494000, Reward:   744.218 [  77.622], Avg:   289.703 (0.100) <0-03:11:59> ({'r_t':  2693.3828, 'eps':     0.1000, 'critic_loss':   329.2050, 'actor_loss':    -3.9906, 'eps_e':     0.1000})
Step:  495000, Reward:   635.535 [ 154.443], Avg:   290.401 (0.100) <0-03:12:25> ({'r_t':  2557.6301, 'eps':     0.1000, 'critic_loss':   548.0296, 'actor_loss':    -3.9740, 'eps_e':     0.1000})
Step:  496000, Reward:   708.968 [  71.830], Avg:   291.243 (0.100) <0-03:12:50> ({'r_t':  2592.1110, 'eps':     0.1000, 'critic_loss':  1086.9954, 'actor_loss':    -3.6466, 'eps_e':     0.1000})
Step:  497000, Reward:   244.164 [ 397.071], Avg:   291.148 (0.100) <0-03:13:16> ({'r_t':  2511.4595, 'eps':     0.1000, 'critic_loss':  1020.9673, 'actor_loss':    -3.7099, 'eps_e':     0.1000})
Step:  498000, Reward:   685.448 [  95.221], Avg:   291.938 (0.100) <0-03:13:42> ({'r_t':  2526.8400, 'eps':     0.1000, 'critic_loss':  1621.1517, 'actor_loss':    -3.5610, 'eps_e':     0.1000})
Step:  499000, Reward:   671.921 [ 245.732], Avg:   292.698 (0.100) <0-03:14:07> ({'r_t':  2808.4531, 'eps':     0.1000, 'critic_loss':  1527.7720, 'actor_loss':    -3.4721, 'eps_e':     0.1000})
Step:  500000, Reward:   725.233 [ 139.123], Avg:   293.562 (0.100) <0-03:14:32> ({'r_t':  2570.6000, 'eps':     0.1000, 'critic_loss':  2009.4635, 'actor_loss':    -3.4441, 'eps_e':     0.1000})
Step:  501000, Reward:   564.157 [ 111.288], Avg:   294.101 (0.100) <0-03:14:58> ({'r_t':  2518.1796, 'eps':     0.1000, 'critic_loss':  1789.0410, 'actor_loss':    -3.4424, 'eps_e':     0.1000})
Step:  502000, Reward:   715.087 [ 183.532], Avg:   294.938 (0.100) <0-03:15:23> ({'r_t':  2822.8343, 'eps':     0.1000, 'critic_loss':  1448.0083, 'actor_loss':    -3.4001, 'eps_e':     0.1000})
Step:  503000, Reward:   592.263 [ 227.546], Avg:   295.528 (0.100) <0-03:15:48> ({'r_t':  2818.0822, 'eps':     0.1000, 'critic_loss':  1164.8265, 'actor_loss':    -3.1488, 'eps_e':     0.1000})
Step:  504000, Reward:   723.178 [ 182.536], Avg:   296.374 (0.100) <0-03:16:13> ({'r_t':  2844.5062, 'eps':     0.1000, 'critic_loss':   926.9731, 'actor_loss':    -3.3918, 'eps_e':     0.1000})
Step:  505000, Reward:   620.190 [ 197.167], Avg:   297.014 (0.100) <0-03:16:39> ({'r_t':  2674.9027, 'eps':     0.1000, 'critic_loss':   610.2054, 'actor_loss':    -3.4711, 'eps_e':     0.1000})
Step:  506000, Reward:   590.670 [ 273.190], Avg:   297.594 (0.100) <0-03:17:05> ({'r_t':  1954.7306, 'eps':     0.1000, 'critic_loss':   529.8787, 'actor_loss':    -3.5043, 'eps_e':     0.1000})
Step:  507000, Reward:   647.832 [ 229.725], Avg:   298.283 (0.100) <0-03:17:30> ({'r_t':  2790.3688, 'eps':     0.1000, 'critic_loss':  1001.3449, 'actor_loss':    -3.4240, 'eps_e':     0.1000})
Step:  508000, Reward:   704.804 [ 201.369], Avg:   299.082 (0.100) <0-03:17:55> ({'r_t':  2824.3701, 'eps':     0.1000, 'critic_loss':   901.2656, 'actor_loss':    -3.2849, 'eps_e':     0.1000})
Step:  509000, Reward:   311.587 [ 247.771], Avg:   299.106 (0.100) <0-03:18:25> ({'r_t':  2613.4181, 'eps':     0.1000, 'critic_loss':   987.4380, 'actor_loss':    -3.2142, 'eps_e':     0.1000})
Step:  510000, Reward:   664.820 [ 276.073], Avg:   299.822 (0.100) <0-03:18:51> ({'r_t':  2751.7516, 'eps':     0.1000, 'critic_loss':  1186.8386, 'actor_loss':    -3.1502, 'eps_e':     0.1000})
Step:  511000, Reward:   726.630 [ 213.291], Avg:   300.656 (0.100) <0-03:19:16> ({'r_t':  2880.2105, 'eps':     0.1000, 'critic_loss':  1213.0732, 'actor_loss':    -3.5993, 'eps_e':     0.1000})
Step:  512000, Reward:   689.461 [ 405.699], Avg:   301.413 (0.100) <0-03:19:42> ({'r_t':  2759.0551, 'eps':     0.1000, 'critic_loss':  1502.9259, 'actor_loss':    -3.9774, 'eps_e':     0.1000})
Step:  513000, Reward:   731.379 [ 194.381], Avg:   302.250 (0.100) <0-03:20:08> ({'r_t':  2831.4146, 'eps':     0.1000, 'critic_loss':  1156.4915, 'actor_loss':    -4.0344, 'eps_e':     0.1000})
Step:  514000, Reward:   801.661 [  51.704], Avg:   303.220 (0.100) <0-03:20:34> ({'r_t':  2793.2719, 'eps':     0.1000, 'critic_loss':  1291.4895, 'actor_loss':    -3.9715, 'eps_e':     0.1000})
Step:  515000, Reward:   821.259 [  57.533], Avg:   304.224 (0.100) <0-03:20:59> ({'r_t':  2882.3157, 'eps':     0.1000, 'critic_loss':  1063.0544, 'actor_loss':    -3.8121, 'eps_e':     0.1000})
Step:  516000, Reward:   842.713 [  29.837], Avg:   305.265 (0.100) <0-03:21:24> ({'r_t':  2943.4292, 'eps':     0.1000, 'critic_loss':  1123.4065, 'actor_loss':    -3.6358, 'eps_e':     0.1000})
Step:  517000, Reward:   834.507 [  52.907], Avg:   306.287 (0.100) <0-03:21:50> ({'r_t':  2949.5173, 'eps':     0.1000, 'critic_loss':   625.1326, 'actor_loss':    -3.3939, 'eps_e':     0.1000})
Step:  518000, Reward:   851.371 [  16.248], Avg:   307.337 (0.100) <0-03:22:15> ({'r_t':  3192.1214, 'eps':     0.1000, 'critic_loss':   797.0307, 'actor_loss':    -3.6316, 'eps_e':     0.1000})
Step:  519000, Reward:   864.852 [  18.145], Avg:   308.409 (0.100) <0-03:22:40> ({'r_t':  3069.2627, 'eps':     0.1000, 'critic_loss':   644.5505, 'actor_loss':    -3.3677, 'eps_e':     0.1000})
Step:  520000, Reward:   833.356 [  57.326], Avg:   309.417 (0.100) <0-03:23:05> ({'r_t':  3214.3280, 'eps':     0.1000, 'critic_loss':   487.1409, 'actor_loss':    -3.4226, 'eps_e':     0.1000})
Step:  521000, Reward:   737.460 [  83.455], Avg:   310.237 (0.100) <0-03:23:31> ({'r_t':  3118.5946, 'eps':     0.1000, 'critic_loss':   335.5465, 'actor_loss':    -3.0714, 'eps_e':     0.1000})
Step:  522000, Reward:   533.637 [ 271.573], Avg:   310.664 (0.100) <0-03:23:56> ({'r_t':  2817.7515, 'eps':     0.1000, 'critic_loss':  1007.7938, 'actor_loss':    -2.7856, 'eps_e':     0.1000})
Step:  523000, Reward:   697.818 [ 281.706], Avg:   311.403 (0.100) <0-03:24:21> ({'r_t':  2709.0923, 'eps':     0.1000, 'critic_loss':  1145.7216, 'actor_loss':    -2.5524, 'eps_e':     0.1000})
Step:  524000, Reward:   368.686 [ 623.056], Avg:   311.512 (0.100) <0-03:24:46> ({'r_t':  3034.8949, 'eps':     0.1000, 'critic_loss':  1594.3931, 'actor_loss':    -2.9422, 'eps_e':     0.1000})
Step:  525000, Reward:   688.431 [ 281.623], Avg:   312.229 (0.100) <0-03:25:11> ({'r_t':  3016.4094, 'eps':     0.1000, 'critic_loss':  1261.7385, 'actor_loss':    -2.9169, 'eps_e':     0.1000})
Step:  526000, Reward:   865.278 [  17.734], Avg:   313.278 (0.100) <0-03:25:36> ({'r_t':  2782.5994, 'eps':     0.1000, 'critic_loss':  1315.8752, 'actor_loss':    -3.1188, 'eps_e':     0.1000})
Step:  527000, Reward:   555.907 [ 242.690], Avg:   313.738 (0.100) <0-03:26:02> ({'r_t':  2986.8515, 'eps':     0.1000, 'critic_loss':  1656.3242, 'actor_loss':    -3.6877, 'eps_e':     0.1000})
Step:  528000, Reward:   817.985 [ 141.876], Avg:   314.691 (0.100) <0-03:26:27> ({'r_t':  3069.1976, 'eps':     0.1000, 'critic_loss':  1282.3761, 'actor_loss':    -3.9000, 'eps_e':     0.1000})
Step:  529000, Reward:   880.547 [  12.843], Avg:   315.758 (0.100) <0-03:26:52> ({'r_t':  2948.0907, 'eps':     0.1000, 'critic_loss':  1033.0032, 'actor_loss':    -3.5699, 'eps_e':     0.1000})
Step:  530000, Reward:   826.376 [  99.897], Avg:   316.720 (0.100) <0-03:27:17> ({'r_t':  3214.4616, 'eps':     0.1000, 'critic_loss':   964.6741, 'actor_loss':    -3.2210, 'eps_e':     0.1000})
Step:  531000, Reward:   746.970 [ 271.637], Avg:   317.529 (0.100) <0-03:27:43> ({'r_t':  3344.5004, 'eps':     0.1000, 'critic_loss':  1098.9647, 'actor_loss':    -3.2152, 'eps_e':     0.1000})
Step:  532000, Reward:   877.470 [  14.101], Avg:   318.579 (0.100) <0-03:28:08> ({'r_t':  3293.6261, 'eps':     0.1000, 'critic_loss':  1070.3918, 'actor_loss':    -3.1341, 'eps_e':     0.1000})
Step:  533000, Reward:   807.488 [ 173.605], Avg:   319.495 (0.100) <0-03:28:33> ({'r_t':  3355.1626, 'eps':     0.1000, 'critic_loss':   820.6756, 'actor_loss':    -2.7197, 'eps_e':     0.1000})
Step:  534000, Reward:   316.081 [ 730.709], Avg:   319.488 (0.100) <0-03:28:58> ({'r_t':  3303.1772, 'eps':     0.1000, 'critic_loss':   800.2028, 'actor_loss':    -2.7425, 'eps_e':     0.1000})
Step:  535000, Reward:   525.801 [ 293.369], Avg:   319.873 (0.100) <0-03:29:27> ({'r_t':  2652.6178, 'eps':     0.1000, 'critic_loss':   501.5630, 'actor_loss':    -2.6802, 'eps_e':     0.1000})
Step:  536000, Reward:   344.271 [ 135.643], Avg:   319.919 (0.100) <0-03:29:57> ({'r_t':  1599.7027, 'eps':     0.1000, 'critic_loss':   622.1910, 'actor_loss':    -3.0680, 'eps_e':     0.1000})
Step:  537000, Reward:   728.399 [ 102.736], Avg:   320.678 (0.100) <0-03:30:22> ({'r_t':  1758.8265, 'eps':     0.1000, 'critic_loss':   902.6079, 'actor_loss':    -3.4028, 'eps_e':     0.1000})
Step:  538000, Reward:   132.110 [ 689.725], Avg:   320.328 (0.100) <0-03:30:46> ({'r_t':  2739.1099, 'eps':     0.1000, 'critic_loss':  1156.1677, 'actor_loss':    -3.7590, 'eps_e':     0.1000})
Step:  539000, Reward:   660.176 [ 151.135], Avg:   320.958 (0.100) <0-03:31:11> ({'r_t':  2257.2265, 'eps':     0.1000, 'critic_loss':  2066.5586, 'actor_loss':    -4.4924, 'eps_e':     0.1000})
Step:  540000, Reward:   816.701 [ 127.892], Avg:   321.874 (0.100) <0-03:31:36> ({'r_t':  3127.2456, 'eps':     0.1000, 'critic_loss':  2850.3525, 'actor_loss':    -5.2313, 'eps_e':     0.1000})
Step:  541000, Reward:   813.876 [  47.384], Avg:   322.782 (0.100) <0-03:32:01> ({'r_t':  3196.5055, 'eps':     0.1000, 'critic_loss':  2143.8218, 'actor_loss':    -4.8837, 'eps_e':     0.1000})
Step:  542000, Reward:   800.078 [  46.362], Avg:   323.661 (0.100) <0-03:32:25> ({'r_t':  3267.2197, 'eps':     0.1000, 'critic_loss':  2175.9551, 'actor_loss':    -4.8018, 'eps_e':     0.1000})
Step:  543000, Reward:   833.768 [  42.714], Avg:   324.598 (0.100) <0-03:32:51> ({'r_t':  3278.4037, 'eps':     0.1000, 'critic_loss':  2384.9312, 'actor_loss':    -4.3557, 'eps_e':     0.1000})
Step:  544000, Reward:   842.515 [  21.177], Avg:   325.549 (0.100) <0-03:33:15> ({'r_t':  3089.4024, 'eps':     0.1000, 'critic_loss':  1935.2926, 'actor_loss':    -4.3535, 'eps_e':     0.1000})
Step:  545000, Reward:   834.221 [  12.628], Avg:   326.480 (0.100) <0-03:33:40> ({'r_t':  3139.3111, 'eps':     0.1000, 'critic_loss':  1326.9137, 'actor_loss':    -3.8381, 'eps_e':     0.1000})
Step:  546000, Reward:   780.473 [ 184.273], Avg:   327.310 (0.100) <0-03:34:05> ({'r_t':  3035.0275, 'eps':     0.1000, 'critic_loss':   556.6246, 'actor_loss':    -3.0138, 'eps_e':     0.1000})
Step:  547000, Reward:   794.066 [  58.595], Avg:   328.162 (0.100) <0-03:34:30> ({'r_t':  3207.5933, 'eps':     0.1000, 'critic_loss':   970.4599, 'actor_loss':    -2.9147, 'eps_e':     0.1000})
Step:  548000, Reward:   604.396 [ 453.574], Avg:   328.665 (0.100) <0-03:34:55> ({'r_t':  2984.7252, 'eps':     0.1000, 'critic_loss':  1066.6482, 'actor_loss':    -3.0513, 'eps_e':     0.1000})
Step:  549000, Reward:   813.881 [  71.381], Avg:   329.547 (0.100) <0-03:35:19> ({'r_t':  3260.5663, 'eps':     0.1000, 'critic_loss':  1212.9893, 'actor_loss':    -3.3790, 'eps_e':     0.1000})
Step:  550000, Reward:   752.629 [ 236.430], Avg:   330.315 (0.100) <0-03:35:44> ({'r_t':  2909.0139, 'eps':     0.1000, 'critic_loss':  1154.6346, 'actor_loss':    -3.4137, 'eps_e':     0.1000})
Step:  551000, Reward:   734.973 [ 380.749], Avg:   331.048 (0.100) <0-03:36:09> ({'r_t':  2907.0732, 'eps':     0.1000, 'critic_loss':  1692.6116, 'actor_loss':    -3.8078, 'eps_e':     0.1000})
Step:  552000, Reward:   571.384 [ 577.020], Avg:   331.483 (0.100) <0-03:36:34> ({'r_t':  3206.7907, 'eps':     0.1000, 'critic_loss':  1339.0403, 'actor_loss':    -3.9585, 'eps_e':     0.1000})
Step:  553000, Reward:   772.417 [ 362.620], Avg:   332.279 (0.100) <0-03:36:58> ({'r_t':  3294.1610, 'eps':     0.1000, 'critic_loss':  1463.5519, 'actor_loss':    -3.9867, 'eps_e':     0.1000})
Step:  554000, Reward:   799.642 [ 191.613], Avg:   333.121 (0.100) <0-03:37:23> ({'r_t':  3343.1065, 'eps':     0.1000, 'critic_loss':   864.9886, 'actor_loss':    -3.9443, 'eps_e':     0.1000})
Step:  555000, Reward:   879.818 [  13.849], Avg:   334.104 (0.100) <0-03:37:48> ({'r_t':  3214.6210, 'eps':     0.1000, 'critic_loss':  1119.5081, 'actor_loss':    -3.7274, 'eps_e':     0.1000})
Step:  556000, Reward:   819.978 [ 136.475], Avg:   334.977 (0.100) <0-03:38:13> ({'r_t':  3285.9963, 'eps':     0.1000, 'critic_loss':   774.8486, 'actor_loss':    -3.6470, 'eps_e':     0.1000})
Step:  557000, Reward:   876.560 [  12.480], Avg:   335.947 (0.100) <0-03:38:38> ({'r_t':  3298.3065, 'eps':     0.1000, 'critic_loss':   654.3611, 'actor_loss':    -3.3191, 'eps_e':     0.1000})
Step:  558000, Reward:   834.280 [   9.030], Avg:   336.839 (0.100) <0-03:39:03> ({'r_t':  3303.8861, 'eps':     0.1000, 'critic_loss':   266.2669, 'actor_loss':    -2.6625, 'eps_e':     0.1000})
Step:  559000, Reward:   842.994 [ 109.995], Avg:   337.742 (0.100) <0-03:39:28> ({'r_t':  3327.0332, 'eps':     0.1000, 'critic_loss':   209.0714, 'actor_loss':    -2.5356, 'eps_e':     0.1000})
Step:  560000, Reward:   880.521 [  13.899], Avg:   338.710 (0.100) <0-03:39:53> ({'r_t':  3349.1345, 'eps':     0.1000, 'critic_loss':   200.8712, 'actor_loss':    -2.7436, 'eps_e':     0.1000})
Step:  561000, Reward:   881.599 [  14.412], Avg:   339.676 (0.100) <0-03:40:18> ({'r_t':  3340.1415, 'eps':     0.1000, 'critic_loss':   186.5902, 'actor_loss':    -2.9622, 'eps_e':     0.1000})
Step:  562000, Reward:   703.230 [ 232.511], Avg:   340.322 (0.100) <0-03:40:42> ({'r_t':  3328.8893, 'eps':     0.1000, 'critic_loss':   204.7576, 'actor_loss':    -2.8997, 'eps_e':     0.1000})
Step:  563000, Reward:   797.391 [ 125.865], Avg:   341.132 (0.100) <0-03:41:07> ({'r_t':  3258.4789, 'eps':     0.1000, 'critic_loss':   192.5380, 'actor_loss':    -2.7153, 'eps_e':     0.1000})
Step:  564000, Reward:   819.752 [ 135.879], Avg:   341.979 (0.100) <0-03:41:32> ({'r_t':  3328.6224, 'eps':     0.1000, 'critic_loss':   307.8049, 'actor_loss':    -2.5482, 'eps_e':     0.1000})
Step:  565000, Reward:   843.613 [ 118.195], Avg:   342.865 (0.100) <0-03:41:57> ({'r_t':  3201.7458, 'eps':     0.1000, 'critic_loss':   391.4276, 'actor_loss':    -2.4178, 'eps_e':     0.1000})
Step:  566000, Reward:   845.562 [  22.282], Avg:   343.752 (0.100) <0-03:42:22> ({'r_t':  3253.4335, 'eps':     0.1000, 'critic_loss':   494.9416, 'actor_loss':    -2.2896, 'eps_e':     0.1000})
Step:  567000, Reward:   873.741 [  19.665], Avg:   344.685 (0.100) <0-03:42:47> ({'r_t':  3067.4215, 'eps':     0.1000, 'critic_loss':   881.9369, 'actor_loss':    -2.4043, 'eps_e':     0.1000})
Step:  568000, Reward:   889.809 [  16.902], Avg:   345.643 (0.100) <0-03:43:11> ({'r_t':  3381.1274, 'eps':     0.1000, 'critic_loss':  1048.3129, 'actor_loss':    -2.5791, 'eps_e':     0.1000})
Step:  569000, Reward:   730.395 [ 163.353], Avg:   346.318 (0.100) <0-03:43:41> ({'r_t':  3066.0545, 'eps':     0.1000, 'critic_loss':   925.7318, 'actor_loss':    -2.4342, 'eps_e':     0.1000})
Step:  570000, Reward:   793.862 [ 245.656], Avg:   347.102 (0.100) <0-03:44:05> ({'r_t':  3025.2870, 'eps':     0.1000, 'critic_loss':  1284.8394, 'actor_loss':    -2.5075, 'eps_e':     0.1000})
Step:  571000, Reward:   868.125 [  19.434], Avg:   348.013 (0.100) <0-03:44:30> ({'r_t':  3296.4909, 'eps':     0.1000, 'critic_loss':  1473.0350, 'actor_loss':    -2.7067, 'eps_e':     0.1000})
Step:  572000, Reward:   762.260 [ 228.807], Avg:   348.736 (0.100) <0-03:44:55> ({'r_t':  2841.3041, 'eps':     0.1000, 'critic_loss':  1773.6096, 'actor_loss':    -3.1179, 'eps_e':     0.1000})
Step:  573000, Reward:   822.175 [ 207.030], Avg:   349.561 (0.100) <0-03:45:20> ({'r_t':  2846.7736, 'eps':     0.1000, 'critic_loss':  1839.5245, 'actor_loss':    -3.5287, 'eps_e':     0.1000})
Step:  574000, Reward:   872.338 [  49.807], Avg:   350.470 (0.100) <0-03:45:45> ({'r_t':  3336.6221, 'eps':     0.1000, 'critic_loss':  1466.2156, 'actor_loss':    -3.9100, 'eps_e':     0.1000})
Step:  575000, Reward:   881.340 [  17.058], Avg:   351.391 (0.100) <0-03:46:10> ({'r_t':  3188.5579, 'eps':     0.1000, 'critic_loss':  1573.7378, 'actor_loss':    -4.3860, 'eps_e':     0.1000})
Step:  576000, Reward:   871.402 [  35.811], Avg:   352.293 (0.100) <0-03:46:36> ({'r_t':  3263.0486, 'eps':     0.1000, 'critic_loss':  1383.8218, 'actor_loss':    -4.1226, 'eps_e':     0.1000})
Step:  577000, Reward:   847.018 [ 117.330], Avg:   353.149 (0.100) <0-03:47:01> ({'r_t':  3230.1401, 'eps':     0.1000, 'critic_loss':  1269.6379, 'actor_loss':    -4.3900, 'eps_e':     0.1000})
Step:  578000, Reward:   868.548 [  42.303], Avg:   354.039 (0.100) <0-03:47:26> ({'r_t':  3328.4333, 'eps':     0.1000, 'critic_loss':   826.6205, 'actor_loss':    -4.3151, 'eps_e':     0.1000})
Step:  579000, Reward:   878.591 [  16.011], Avg:   354.943 (0.100) <0-03:47:52> ({'r_t':  3307.0639, 'eps':     0.1000, 'critic_loss':   413.2301, 'actor_loss':    -3.8791, 'eps_e':     0.1000})
Step:  580000, Reward:   847.787 [ 102.287], Avg:   355.791 (0.100) <0-03:48:17> ({'r_t':  3149.2135, 'eps':     0.1000, 'critic_loss':   219.4225, 'actor_loss':    -2.7776, 'eps_e':     0.1000})
Step:  581000, Reward:   895.810 [  15.934], Avg:   356.719 (0.100) <0-03:48:42> ({'r_t':  3194.0934, 'eps':     0.1000, 'critic_loss':   534.3324, 'actor_loss':    -2.6298, 'eps_e':     0.1000})
Step:  582000, Reward:   892.833 [  16.621], Avg:   357.639 (0.100) <0-03:49:07> ({'r_t':  3214.8728, 'eps':     0.1000, 'critic_loss':   505.9161, 'actor_loss':    -2.3395, 'eps_e':     0.1000})
Step:  583000, Reward:   844.217 [ 126.580], Avg:   358.472 (0.100) <0-03:49:32> ({'r_t':  3303.0381, 'eps':     0.1000, 'critic_loss':   444.3087, 'actor_loss':    -2.4585, 'eps_e':     0.1000})
Step:  584000, Reward:   904.156 [  18.829], Avg:   359.405 (0.100) <0-03:49:58> ({'r_t':  3297.6397, 'eps':     0.1000, 'critic_loss':   324.0774, 'actor_loss':    -2.3727, 'eps_e':     0.1000})
Step:  585000, Reward:   879.697 [  58.502], Avg:   360.293 (0.100) <0-03:50:23> ({'r_t':  3196.6783, 'eps':     0.1000, 'critic_loss':   370.4644, 'actor_loss':    -2.5943, 'eps_e':     0.1000})
Step:  586000, Reward:   885.127 [  81.533], Avg:   361.187 (0.100) <0-03:50:49> ({'r_t':  2965.8722, 'eps':     0.1000, 'critic_loss':   559.3149, 'actor_loss':    -2.6615, 'eps_e':     0.1000})
Step:  587000, Reward:   441.615 [ 211.604], Avg:   361.324 (0.100) <0-03:51:18> ({'r_t':  2703.5454, 'eps':     0.1000, 'critic_loss':  1033.9031, 'actor_loss':    -2.7041, 'eps_e':     0.1000})
Step:  588000, Reward:   603.841 [ 373.339], Avg:   361.735 (0.100) <0-03:51:46> ({'r_t':  2660.4893, 'eps':     0.1000, 'critic_loss':  1945.0145, 'actor_loss':    -3.2032, 'eps_e':     0.1000})
Step:  589000, Reward:   877.877 [ 102.764], Avg:   362.610 (0.100) <0-03:52:11> ({'r_t':  3208.3509, 'eps':     0.1000, 'critic_loss':  1744.6266, 'actor_loss':    -3.5526, 'eps_e':     0.1000})
Step:  590000, Reward:   871.895 [  86.249], Avg:   363.472 (0.100) <0-03:52:36> ({'r_t':  3276.5462, 'eps':     0.1000, 'critic_loss':  2494.3989, 'actor_loss':    -3.8058, 'eps_e':     0.1000})
Step:  591000, Reward:   729.864 [ 374.062], Avg:   364.091 (0.100) <0-03:53:01> ({'r_t':  3163.2662, 'eps':     0.1000, 'critic_loss':  1977.6870, 'actor_loss':    -3.8419, 'eps_e':     0.1000})
Step:  592000, Reward:   822.620 [ 113.354], Avg:   364.864 (0.100) <0-03:53:26> ({'r_t':  3224.5775, 'eps':     0.1000, 'critic_loss':  2384.4431, 'actor_loss':    -4.5449, 'eps_e':     0.1000})
Step:  593000, Reward:   863.548 [  97.197], Avg:   365.704 (0.100) <0-03:53:51> ({'r_t':  2911.4578, 'eps':     0.1000, 'critic_loss':  1560.5131, 'actor_loss':    -4.1181, 'eps_e':     0.1000})
Step:  594000, Reward:   911.319 [  19.164], Avg:   366.621 (0.100) <0-03:54:16> ({'r_t':  3042.2472, 'eps':     0.1000, 'critic_loss':  1335.7607, 'actor_loss':    -4.1361, 'eps_e':     0.1000})
Step:  595000, Reward:   148.344 [ 698.635], Avg:   366.254 (0.100) <0-03:54:42> ({'r_t':  3226.0015, 'eps':     0.1000, 'critic_loss':  1185.4058, 'actor_loss':    -3.4650, 'eps_e':     0.1000})
Step:  596000, Reward:   654.696 [ 368.777], Avg:   366.737 (0.100) <0-03:55:08> ({'r_t':  3151.1547, 'eps':     0.1000, 'critic_loss':   917.8766, 'actor_loss':    -2.9471, 'eps_e':     0.1000})
Step:  597000, Reward:   914.332 [  16.927], Avg:   367.653 (0.100) <0-03:55:33> ({'r_t':  3150.8322, 'eps':     0.1000, 'critic_loss':   832.6103, 'actor_loss':    -2.9173, 'eps_e':     0.1000})
Step:  598000, Reward:   883.576 [  48.682], Avg:   368.514 (0.100) <0-03:56:00> ({'r_t':  3216.8832, 'eps':     0.1000, 'critic_loss':   622.8286, 'actor_loss':    -2.9210, 'eps_e':     0.1000})
Step:  599000, Reward:   864.640 [ 154.367], Avg:   369.341 (0.100) <0-03:56:25> ({'r_t':  2952.4061, 'eps':     0.1000, 'critic_loss':   746.3256, 'actor_loss':    -2.8868, 'eps_e':     0.1000})
Step:  600000, Reward:   907.829 [  10.146], Avg:   370.237 (0.100) <0-03:56:51> ({'r_t':  3110.5889, 'eps':     0.1000, 'critic_loss':   512.6562, 'actor_loss':    -2.6292, 'eps_e':     0.1000})
Step:  601000, Reward:   838.790 [  65.299], Avg:   371.016 (0.100) <0-03:57:17> ({'r_t':  3113.5691, 'eps':     0.1000, 'critic_loss':   277.7053, 'actor_loss':    -2.4903, 'eps_e':     0.1000})
Step:  602000, Reward:   575.418 [ 304.984], Avg:   371.355 (0.100) <0-03:57:43> ({'r_t':  3213.4950, 'eps':     0.1000, 'critic_loss':   282.6454, 'actor_loss':    -2.6469, 'eps_e':     0.1000})
Step:  603000, Reward:   811.716 [ 244.941], Avg:   372.084 (0.100) <0-03:58:08> ({'r_t':  3352.6302, 'eps':     0.1000, 'critic_loss':   370.6620, 'actor_loss':    -2.9458, 'eps_e':     0.1000})
Step:  604000, Reward:   928.294 [   9.749], Avg:   373.003 (0.100) <0-03:58:32> ({'r_t':  3374.0906, 'eps':     0.1000, 'critic_loss':   424.9431, 'actor_loss':    -3.0962, 'eps_e':     0.1000})
Step:  605000, Reward:   928.787 [   9.222], Avg:   373.920 (0.100) <0-03:58:57> ({'r_t':  3455.5610, 'eps':     0.1000, 'critic_loss':   408.5188, 'actor_loss':    -3.3503, 'eps_e':     0.1000})
Step:  606000, Reward:   921.339 [  53.470], Avg:   374.822 (0.100) <0-03:59:23> ({'r_t':  2292.8810, 'eps':     0.1000, 'critic_loss':   534.0654, 'actor_loss':    -3.3270, 'eps_e':     0.1000})
Step:  607000, Reward:   696.441 [ 227.847], Avg:   375.351 (0.100) <0-03:59:48> ({'r_t':  3006.9555, 'eps':     0.1000, 'critic_loss':   732.5503, 'actor_loss':    -2.9347, 'eps_e':     0.1000})
Step:  608000, Reward:   862.660 [  11.316], Avg:   376.151 (0.100) <0-04:00:13> ({'r_t':  3117.3794, 'eps':     0.1000, 'critic_loss':   973.2933, 'actor_loss':    -2.9004, 'eps_e':     0.1000})
Step:  609000, Reward:   883.750 [  20.096], Avg:   376.983 (0.100) <0-04:00:38> ({'r_t':  2916.5873, 'eps':     0.1000, 'critic_loss':  1168.8779, 'actor_loss':    -3.3397, 'eps_e':     0.1000})
Step:  610000, Reward:   218.643 [ 598.856], Avg:   376.724 (0.100) <0-04:01:08> ({'r_t':  2768.0359, 'eps':     0.1000, 'critic_loss':  2561.3115, 'actor_loss':    -4.0482, 'eps_e':     0.1000})
Step:  611000, Reward:   909.883 [  15.430], Avg:   377.595 (0.100) <0-04:01:33> ({'r_t':  3348.2887, 'eps':     0.1000, 'critic_loss':  2001.7560, 'actor_loss':    -4.3900, 'eps_e':     0.1000})
Step:  612000, Reward:   692.329 [ 438.923], Avg:   378.109 (0.100) <0-04:01:58> ({'r_t':  2984.4174, 'eps':     0.1000, 'critic_loss':  2349.4822, 'actor_loss':    -4.8215, 'eps_e':     0.1000})
Step:  613000, Reward:   870.782 [ 133.276], Avg:   378.911 (0.100) <0-04:02:23> ({'r_t':  2958.9270, 'eps':     0.1000, 'critic_loss':  2508.1538, 'actor_loss':    -4.7320, 'eps_e':     0.1000})
Step:  614000, Reward:   932.947 [  13.300], Avg:   379.812 (0.100) <0-04:02:49> ({'r_t':  3164.2015, 'eps':     0.1000, 'critic_loss':  2633.2075, 'actor_loss':    -5.3247, 'eps_e':     0.1000})
Step:  615000, Reward:   931.887 [   9.724], Avg:   380.708 (0.100) <0-04:03:14> ({'r_t':  3120.1350, 'eps':     0.1000, 'critic_loss':  1941.9569, 'actor_loss':    -5.2941, 'eps_e':     0.1000})
Step:  616000, Reward:   926.267 [  11.981], Avg:   381.593 (0.100) <0-04:03:39> ({'r_t':  3212.6704, 'eps':     0.1000, 'critic_loss':  1702.7317, 'actor_loss':    -5.0375, 'eps_e':     0.1000})
Step:  617000, Reward:   928.721 [  10.986], Avg:   382.478 (0.100) <0-04:04:04> ({'r_t':  3356.0238, 'eps':     0.1000, 'critic_loss':  1019.7202, 'actor_loss':    -4.1551, 'eps_e':     0.1000})
Step:  618000, Reward:   535.294 [ 223.517], Avg:   382.725 (0.100) <0-04:04:29> ({'r_t':  3236.9589, 'eps':     0.1000, 'critic_loss':   954.3314, 'actor_loss':    -4.0007, 'eps_e':     0.1000})
Step:  619000, Reward:   751.623 [ 400.153], Avg:   383.320 (0.100) <0-04:04:54> ({'r_t':  3336.9827, 'eps':     0.1000, 'critic_loss':  1098.4822, 'actor_loss':    -3.6343, 'eps_e':     0.1000})
Step:  620000, Reward:   937.054 [  17.000], Avg:   384.211 (0.100) <0-04:05:20> ({'r_t':  3244.2369, 'eps':     0.1000, 'critic_loss':   807.0958, 'actor_loss':    -3.3584, 'eps_e':     0.1000})
Step:  621000, Reward:   887.668 [  97.219], Avg:   385.021 (0.100) <0-04:05:45> ({'r_t':  3181.7275, 'eps':     0.1000, 'critic_loss':   803.2128, 'actor_loss':    -3.3183, 'eps_e':     0.1000})
Step:  622000, Reward:   781.833 [ 237.226], Avg:   385.658 (0.100) <0-04:06:10> ({'r_t':  3211.9032, 'eps':     0.1000, 'critic_loss':   786.9243, 'actor_loss':    -3.2879, 'eps_e':     0.1000})
Step:  623000, Reward:   900.832 [  92.150], Avg:   386.483 (0.100) <0-04:06:36> ({'r_t':  3289.2378, 'eps':     0.1000, 'critic_loss':   680.0051, 'actor_loss':    -3.1520, 'eps_e':     0.1000})
Step:  624000, Reward:   616.704 [ 303.333], Avg:   386.852 (0.100) <0-04:07:01> ({'r_t':  3262.4589, 'eps':     0.1000, 'critic_loss':   821.2598, 'actor_loss':    -3.3284, 'eps_e':     0.1000})
Step:  625000, Reward:   849.337 [ 188.744], Avg:   387.590 (0.100) <0-04:07:26> ({'r_t':  3135.4275, 'eps':     0.1000, 'critic_loss':   605.6162, 'actor_loss':    -3.1124, 'eps_e':     0.1000})
Step:  626000, Reward:   789.988 [ 214.307], Avg:   388.232 (0.100) <0-04:07:54> ({'r_t':  3169.0766, 'eps':     0.1000, 'critic_loss':   917.7376, 'actor_loss':    -3.0389, 'eps_e':     0.1000})
Step:  627000, Reward:   888.898 [ 114.038], Avg:   389.030 (0.100) <0-04:08:19> ({'r_t':  3158.0950, 'eps':     0.1000, 'critic_loss':   958.5645, 'actor_loss':    -3.1259, 'eps_e':     0.1000})
Step:  628000, Reward:   859.399 [ 213.557], Avg:   389.777 (0.100) <0-04:08:45> ({'r_t':  3343.5599, 'eps':     0.1000, 'critic_loss':   812.0377, 'actor_loss':    -2.9296, 'eps_e':     0.1000})
Step:  629000, Reward:   533.209 [ 567.495], Avg:   390.005 (0.100) <0-04:09:13> ({'r_t':  3857.1061, 'eps':     0.1000, 'critic_loss':   811.9495, 'actor_loss':    -4.1421, 'eps_e':     0.1000})
Step:  630000, Reward:  1001.194 [ 574.710], Avg:   390.974 (0.100) <0-04:09:43> ({'r_t':  1987.6010, 'eps':     0.1000, 'critic_loss':  1712.8599, 'actor_loss':    -6.9158, 'eps_e':     0.1000})
Step:  631000, Reward:   586.510 [ 334.351], Avg:   391.283 (0.100) <0-04:10:09> ({'r_t':  2473.0218, 'eps':     0.1000, 'critic_loss':  3152.4692, 'actor_loss':    -6.8794, 'eps_e':     0.1000})
Step:  632000, Reward:   622.148 [ 254.907], Avg:   391.648 (0.100) <0-04:10:34> ({'r_t':  2920.2222, 'eps':     0.1000, 'critic_loss':  3679.3337, 'actor_loss':    -8.4401, 'eps_e':     0.1000})
Step:  633000, Reward:   831.172 [  11.391], Avg:   392.341 (0.100) <0-04:11:00> ({'r_t':  2962.1321, 'eps':     0.1000, 'critic_loss':  3025.0042, 'actor_loss':    -8.0729, 'eps_e':     0.1000})
Step:  634000, Reward:   795.135 [ 452.122], Avg:   392.975 (0.100) <0-04:11:27> ({'r_t':  2639.9439, 'eps':     0.1000, 'critic_loss':  2966.1362, 'actor_loss':    -7.5283, 'eps_e':     0.1000})
Step:  635000, Reward:   968.222 [ 790.807], Avg:   393.880 (0.100) <0-04:11:57> ({'r_t':  3487.2589, 'eps':     0.1000, 'critic_loss':  3401.1631, 'actor_loss':    -6.6095, 'eps_e':     0.1000})
Step:  636000, Reward:   427.766 [  35.958], Avg:   393.933 (0.100) <0-04:12:26> ({'r_t':  1750.5704, 'eps':     0.1000, 'critic_loss':  3082.3955, 'actor_loss':    -6.7545, 'eps_e':     0.1000})
Step:  637000, Reward:   806.656 [  72.820], Avg:   394.580 (0.100) <0-04:12:52> ({'r_t':  2567.1537, 'eps':     0.1000, 'critic_loss':  2550.5874, 'actor_loss':    -6.3109, 'eps_e':     0.1000})
Step:  638000, Reward:   235.023 [ 725.734], Avg:   394.330 (0.100) <0-04:13:21> ({'r_t':  1797.6858, 'eps':     0.1000, 'critic_loss':  2060.6807, 'actor_loss':    -5.7299, 'eps_e':     0.1000})
Step:  639000, Reward:  1198.613 [ 890.887], Avg:   395.587 (0.100) <0-04:13:51> ({'r_t':  1468.5099, 'eps':     0.1000, 'critic_loss':  2630.6270, 'actor_loss':    -6.1781, 'eps_e':     0.1000})
Step:  640000, Reward:   538.260 [ 138.414], Avg:   395.809 (0.100) <0-04:14:16> ({'r_t':  3221.6379, 'eps':     0.1000, 'critic_loss':  4611.5781, 'actor_loss':    -6.5745, 'eps_e':     0.1000})
Step:  641000, Reward:   883.054 [ 560.810], Avg:   396.568 (0.100) <0-04:14:45> ({'r_t':  2775.1374, 'eps':     0.1000, 'critic_loss':  5297.8413, 'actor_loss':    -7.0624, 'eps_e':     0.1000})
Step:  642000, Reward:  1307.773 [ 728.573], Avg:   397.985 (0.100) <0-04:15:15> ({'r_t':  3239.3897, 'eps':     0.1000, 'critic_loss':  4122.8965, 'actor_loss':    -7.9348, 'eps_e':     0.1000})
Step:  643000, Reward:  1217.471 [ 643.212], Avg:   399.258 (0.100) <0-04:15:44> ({'r_t':  3123.1543, 'eps':     0.1000, 'critic_loss':  5935.1226, 'actor_loss':    -8.0485, 'eps_e':     0.1000})
Step:  644000, Reward:  1393.788 [ 620.600], Avg:   400.800 (0.100) <0-04:16:14> ({'r_t':  3617.6776, 'eps':     0.1000, 'critic_loss':  5458.6948, 'actor_loss':    -8.8489, 'eps_e':     0.1000})
Step:  645000, Reward:  1918.292 [ 355.913], Avg:   403.149 (0.100) <0-04:16:43> ({'r_t':  3368.5992, 'eps':     0.1000, 'critic_loss':  7105.8721, 'actor_loss':    -8.7187, 'eps_e':     0.1000})
Step:  646000, Reward:  1992.427 [ 543.292], Avg:   405.605 (0.100) <0-04:17:13> ({'r_t':  4035.1665, 'eps':     0.1000, 'critic_loss':  4972.5439, 'actor_loss':    -8.5612, 'eps_e':     0.1000})
Step:  647000, Reward:  1504.172 [ 660.455], Avg:   407.301 (0.100) <0-04:17:42> ({'r_t':  3476.4021, 'eps':     0.1000, 'critic_loss':  3581.3433, 'actor_loss':    -8.6801, 'eps_e':     0.1000})
Step:  648000, Reward:   379.793 [ 781.182], Avg:   407.258 (0.100) <0-04:18:11> ({'r_t':  2831.7420, 'eps':     0.1000, 'critic_loss':  4924.5239, 'actor_loss':    -8.7487, 'eps_e':     0.1000})
Step:  649000, Reward:  1926.077 [ 373.653], Avg:   409.595 (0.100) <0-04:18:41> ({'r_t':  2713.6267, 'eps':     0.1000, 'critic_loss':  6428.2744, 'actor_loss':    -8.2997, 'eps_e':     0.1000})
Step:  650000, Reward:  1944.143 [ 626.828], Avg:   411.952 (0.100) <0-04:19:10> ({'r_t':  3601.6650, 'eps':     0.1000, 'critic_loss':  7880.2617, 'actor_loss':    -7.4826, 'eps_e':     0.1000})
Step:  651000, Reward:  2225.328 [  33.479], Avg:   414.733 (0.100) <0-04:19:40> ({'r_t':  4168.0190, 'eps':     0.1000, 'critic_loss':  8968.3242, 'actor_loss':    -7.2174, 'eps_e':     0.1000})
Step:  652000, Reward:  1850.240 [ 559.342], Avg:   416.932 (0.100) <0-04:20:09> ({'r_t':  4184.2492, 'eps':     0.1000, 'critic_loss':  7181.7754, 'actor_loss':    -7.0791, 'eps_e':     0.1000})
Step:  653000, Reward:  1652.481 [ 769.592], Avg:   418.821 (0.100) <0-04:20:39> ({'r_t':  3383.8557, 'eps':     0.1000, 'critic_loss':  7084.4023, 'actor_loss':    -8.9677, 'eps_e':     0.1000})
Step:  654000, Reward:   697.917 [ 653.380], Avg:   419.247 (0.100) <0-04:21:08> ({'r_t':  3294.1955, 'eps':     0.1000, 'critic_loss':  8140.7251, 'actor_loss':    -8.8867, 'eps_e':     0.1000})
Step:  655000, Reward:  1178.620 [ 903.954], Avg:   420.405 (0.100) <0-04:21:38> ({'r_t':  4029.4734, 'eps':     0.1000, 'critic_loss':  6684.0088, 'actor_loss':    -8.1625, 'eps_e':     0.1000})
Step:  656000, Reward:  2016.496 [ 463.106], Avg:   422.834 (0.100) <0-04:22:08> ({'r_t':  4232.4865, 'eps':     0.1000, 'critic_loss':  4659.0151, 'actor_loss':    -8.0120, 'eps_e':     0.1000})
Step:  657000, Reward:  1640.022 [ 720.868], Avg:   424.684 (0.100) <0-04:22:37> ({'r_t':  4274.5578, 'eps':     0.1000, 'critic_loss':  4195.6504, 'actor_loss':    -9.3232, 'eps_e':     0.1000})
Step:  658000, Reward:   548.802 [ 761.290], Avg:   424.872 (0.100) <0-04:23:07> ({'r_t':  3416.5782, 'eps':     0.1000, 'critic_loss':  4171.7861, 'actor_loss':   -11.9323, 'eps_e':     0.1000})
Step:  659000, Reward:   572.999 [ 224.678], Avg:   425.097 (0.100) <0-04:23:35> ({'r_t':  3435.5392, 'eps':     0.1000, 'critic_loss':  4186.6929, 'actor_loss':   -10.8916, 'eps_e':     0.1000})
Step:  660000, Reward:  2027.327 [ 520.861], Avg:   427.521 (0.100) <0-04:24:04> ({'r_t':  3948.2979, 'eps':     0.1000, 'critic_loss':  4039.1748, 'actor_loss':    -8.9087, 'eps_e':     0.1000})
Step:  661000, Reward:  1827.373 [ 615.073], Avg:   429.635 (0.100) <0-04:24:34> ({'r_t':  4217.5605, 'eps':     0.1000, 'critic_loss':  3071.6528, 'actor_loss':    -7.4191, 'eps_e':     0.1000})
Step:  662000, Reward:  2022.440 [ 610.971], Avg:   432.038 (0.100) <0-04:25:03> ({'r_t':  4336.7528, 'eps':     0.1000, 'critic_loss':  2947.0076, 'actor_loss':    -7.3981, 'eps_e':     0.1000})
Step:  663000, Reward:  1654.731 [ 747.655], Avg:   433.879 (0.100) <0-04:25:33> ({'r_t':  4272.7181, 'eps':     0.1000, 'critic_loss':  3307.4729, 'actor_loss':    -8.4740, 'eps_e':     0.1000})
Step:  664000, Reward:  2195.008 [ 431.148], Avg:   436.527 (0.100) <0-04:26:02> ({'r_t':  4074.8516, 'eps':     0.1000, 'critic_loss':  2948.7917, 'actor_loss':    -8.9516, 'eps_e':     0.1000})
Step:  665000, Reward:  2299.420 [ 187.319], Avg:   439.324 (0.100) <0-04:26:32> ({'r_t':  4458.3282, 'eps':     0.1000, 'critic_loss':  2411.2383, 'actor_loss':    -8.4233, 'eps_e':     0.1000})
Step:  666000, Reward:  2003.417 [ 629.662], Avg:   441.669 (0.100) <0-04:27:01> ({'r_t':  4576.4268, 'eps':     0.1000, 'critic_loss':  2197.9299, 'actor_loss':    -6.8351, 'eps_e':     0.1000})
Step:  667000, Reward:  2381.069 [  33.982], Avg:   444.573 (0.100) <0-04:27:31> ({'r_t':  4507.3084, 'eps':     0.1000, 'critic_loss':  2072.0259, 'actor_loss':    -6.4955, 'eps_e':     0.1000})
Step:  668000, Reward:  2096.239 [ 280.832], Avg:   447.041 (0.100) <0-04:28:00> ({'r_t':  4368.2356, 'eps':     0.1000, 'critic_loss':  2200.8945, 'actor_loss':    -6.2844, 'eps_e':     0.1000})
Step:  669000, Reward:  2255.865 [  38.663], Avg:   449.741 (0.100) <0-04:28:29> ({'r_t':  4073.4554, 'eps':     0.1000, 'critic_loss':  2259.9590, 'actor_loss':    -6.3980, 'eps_e':     0.1000})
Step:  670000, Reward:  2058.675 [ 612.024], Avg:   452.139 (0.100) <0-04:28:59> ({'r_t':  4344.7960, 'eps':     0.1000, 'critic_loss':  2031.4309, 'actor_loss':    -5.9573, 'eps_e':     0.1000})
Step:  671000, Reward:  1586.787 [ 852.352], Avg:   453.828 (0.100) <0-04:29:28> ({'r_t':  4169.6826, 'eps':     0.1000, 'critic_loss':  1878.1075, 'actor_loss':    -4.5964, 'eps_e':     0.1000})
Step:  672000, Reward:  1616.692 [ 613.677], Avg:   455.555 (0.100) <0-04:29:58> ({'r_t':  4303.6762, 'eps':     0.1000, 'critic_loss':  2067.9143, 'actor_loss':    -4.6212, 'eps_e':     0.1000})
Step:  673000, Reward:   735.958 [ 927.750], Avg:   455.971 (0.100) <0-04:30:27> ({'r_t':  4209.0971, 'eps':     0.1000, 'critic_loss':  2261.2161, 'actor_loss':    -4.5409, 'eps_e':     0.1000})
Step:  674000, Reward:  2018.673 [ 483.763], Avg:   458.287 (0.100) <0-04:30:57> ({'r_t':  3699.2948, 'eps':     0.1000, 'critic_loss':  3118.9126, 'actor_loss':    -5.3078, 'eps_e':     0.1000})
Step:  675000, Reward:  2074.153 [ 581.638], Avg:   460.677 (0.100) <0-04:31:26> ({'r_t':  4126.1850, 'eps':     0.1000, 'critic_loss':  3720.2476, 'actor_loss':    -5.3989, 'eps_e':     0.1000})
Step:  676000, Reward:  2297.638 [  76.830], Avg:   463.390 (0.100) <0-04:31:56> ({'r_t':  4339.9537, 'eps':     0.1000, 'critic_loss':  3481.3474, 'actor_loss':    -5.8941, 'eps_e':     0.1000})
Step:  677000, Reward:  2165.829 [ 373.007], Avg:   465.901 (0.100) <0-04:32:25> ({'r_t':  4304.6575, 'eps':     0.1000, 'critic_loss':  3863.6992, 'actor_loss':    -6.5022, 'eps_e':     0.1000})
Step:  678000, Reward:  1999.441 [ 657.327], Avg:   468.160 (0.100) <0-04:32:55> ({'r_t':  4216.2417, 'eps':     0.1000, 'critic_loss':  3958.4482, 'actor_loss':    -6.4750, 'eps_e':     0.1000})
Step:  679000, Reward:  1972.225 [ 551.787], Avg:   470.372 (0.100) <0-04:33:24> ({'r_t':  4232.4779, 'eps':     0.1000, 'critic_loss':  4089.8979, 'actor_loss':    -6.9608, 'eps_e':     0.1000})
Step:  680000, Reward:   899.377 [ 863.154], Avg:   471.002 (0.100) <0-04:33:54> ({'r_t':  3846.6664, 'eps':     0.1000, 'critic_loss':  4338.7339, 'actor_loss':    -6.7014, 'eps_e':     0.1000})
Step:  681000, Reward:  1180.997 [ 998.109], Avg:   472.043 (0.100) <0-04:34:23> ({'r_t':  4249.4232, 'eps':     0.1000, 'critic_loss':  4416.9932, 'actor_loss':    -5.6242, 'eps_e':     0.1000})
Step:  682000, Reward:  1364.503 [1085.355], Avg:   473.349 (0.100) <0-04:34:53> ({'r_t':  3925.5033, 'eps':     0.1000, 'critic_loss':  4522.8579, 'actor_loss':    -5.4405, 'eps_e':     0.1000})
Step:  683000, Reward:  2256.365 [ 246.296], Avg:   475.956 (0.100) <0-04:35:22> ({'r_t':  4427.5218, 'eps':     0.1000, 'critic_loss':  4641.6753, 'actor_loss':    -5.1711, 'eps_e':     0.1000})
Step:  684000, Reward:  1467.499 [ 999.567], Avg:   477.404 (0.100) <0-04:35:52> ({'r_t':  4285.1672, 'eps':     0.1000, 'critic_loss':  5089.7510, 'actor_loss':    -5.7188, 'eps_e':     0.1000})
Step:  685000, Reward:  2133.423 [ 542.983], Avg:   479.818 (0.100) <0-04:36:21> ({'r_t':  4212.6205, 'eps':     0.1000, 'critic_loss':  4403.8350, 'actor_loss':    -5.4745, 'eps_e':     0.1000})
Step:  686000, Reward:  1675.219 [ 792.303], Avg:   481.558 (0.100) <0-04:36:51> ({'r_t':  3783.4072, 'eps':     0.1000, 'critic_loss':  4738.3501, 'actor_loss':    -5.5341, 'eps_e':     0.1000})
Step:  687000, Reward:   743.019 [ 672.169], Avg:   481.938 (0.100) <0-04:37:21> ({'r_t':  3528.8052, 'eps':     0.1000, 'critic_loss':  4609.9131, 'actor_loss':    -6.1861, 'eps_e':     0.1000})
Step:  688000, Reward:  1881.474 [ 852.537], Avg:   483.969 (0.100) <0-04:37:52> ({'r_t':  4195.2908, 'eps':     0.1000, 'critic_loss':  3858.0112, 'actor_loss':    -7.3694, 'eps_e':     0.1000})
Step:  689000, Reward:  2260.417 [ 520.959], Avg:   486.543 (0.100) <0-04:38:23> ({'r_t':  4596.8054, 'eps':     0.1000, 'critic_loss':  3583.4734, 'actor_loss':    -7.4340, 'eps_e':     0.1000})
Step:  690000, Reward:   551.181 [1008.101], Avg:   486.637 (0.100) <0-04:38:53> ({'r_t':  4519.6470, 'eps':     0.1000, 'critic_loss':  3690.2476, 'actor_loss':    -8.2140, 'eps_e':     0.1000})
Step:  691000, Reward:  1904.895 [ 928.076], Avg:   488.687 (0.100) <0-04:39:23> ({'r_t':  4594.6635, 'eps':     0.1000, 'critic_loss':  3602.8774, 'actor_loss':    -8.1778, 'eps_e':     0.1000})
Step:  692000, Reward:  2265.628 [ 618.990], Avg:   491.251 (0.100) <0-04:39:53> ({'r_t':  4366.6662, 'eps':     0.1000, 'critic_loss':  3588.9417, 'actor_loss':    -8.2157, 'eps_e':     0.1000})
Step:  693000, Reward:  2132.775 [ 599.445], Avg:   493.616 (0.100) <0-04:40:23> ({'r_t':  4120.6342, 'eps':     0.1000, 'critic_loss':  4273.4375, 'actor_loss':    -8.6408, 'eps_e':     0.1000})
Step:  694000, Reward:  1358.966 [ 966.597], Avg:   494.861 (0.100) <0-04:40:53> ({'r_t':  4439.6405, 'eps':     0.1000, 'critic_loss':  4178.7964, 'actor_loss':    -8.4455, 'eps_e':     0.1000})
Step:  695000, Reward:  1928.982 [ 847.957], Avg:   496.922 (0.100) <0-04:41:22> ({'r_t':  4419.9336, 'eps':     0.1000, 'critic_loss':  4390.2271, 'actor_loss':    -8.3205, 'eps_e':     0.1000})
Step:  696000, Reward:  1911.331 [ 852.829], Avg:   498.951 (0.100) <0-04:41:51> ({'r_t':  4716.1202, 'eps':     0.1000, 'critic_loss':  4433.0259, 'actor_loss':    -8.2284, 'eps_e':     0.1000})
Step:  697000, Reward:  2266.065 [ 430.413], Avg:   501.483 (0.100) <0-04:42:21> ({'r_t':  4710.7259, 'eps':     0.1000, 'critic_loss':  4355.7520, 'actor_loss':    -7.4512, 'eps_e':     0.1000})
Step:  698000, Reward:  2388.082 [ 378.395], Avg:   504.182 (0.100) <0-04:42:50> ({'r_t':  4832.7395, 'eps':     0.1000, 'critic_loss':  4303.0112, 'actor_loss':    -6.9528, 'eps_e':     0.1000})
Step:  699000, Reward:  2142.471 [ 702.121], Avg:   506.522 (0.100) <0-04:43:20> ({'r_t':  4499.1656, 'eps':     0.1000, 'critic_loss':  3808.2073, 'actor_loss':    -6.2046, 'eps_e':     0.1000})
Step:  700000, Reward:  2157.037 [ 724.161], Avg:   508.876 (0.100) <0-04:43:49> ({'r_t':  4456.1588, 'eps':     0.1000, 'critic_loss':  3551.4329, 'actor_loss':    -5.6465, 'eps_e':     0.1000})
Step:  701000, Reward:  2441.180 [  33.018], Avg:   511.629 (0.100) <0-04:44:19> ({'r_t':  4685.5766, 'eps':     0.1000, 'critic_loss':  3202.6509, 'actor_loss':    -5.6883, 'eps_e':     0.1000})
Step:  702000, Reward:  2414.258 [ 270.499], Avg:   514.335 (0.100) <0-04:44:48> ({'r_t':  4657.1860, 'eps':     0.1000, 'critic_loss':  3077.1758, 'actor_loss':    -5.7560, 'eps_e':     0.1000})
Step:  703000, Reward:  2371.931 [ 605.098], Avg:   516.974 (0.100) <0-04:45:18> ({'r_t':  4788.1417, 'eps':     0.1000, 'critic_loss':  3644.0295, 'actor_loss':    -6.0595, 'eps_e':     0.1000})
Step:  704000, Reward:  2000.655 [ 765.413], Avg:   519.079 (0.100) <0-04:45:47> ({'r_t':  4570.6281, 'eps':     0.1000, 'critic_loss':  3424.3760, 'actor_loss':    -5.9697, 'eps_e':     0.1000})
Step:  705000, Reward:  1316.054 [ 823.562], Avg:   520.207 (0.100) <0-04:46:17> ({'r_t':  4383.9928, 'eps':     0.1000, 'critic_loss':  3930.2290, 'actor_loss':    -5.8019, 'eps_e':     0.1000})
Step:  706000, Reward:  2330.025 [ 302.819], Avg:   522.767 (0.100) <0-04:46:46> ({'r_t':  4742.7365, 'eps':     0.1000, 'critic_loss':  3972.6550, 'actor_loss':    -5.5705, 'eps_e':     0.1000})
Step:  707000, Reward:  2226.422 [ 524.778], Avg:   525.174 (0.100) <0-04:47:15> ({'r_t':  4448.9650, 'eps':     0.1000, 'critic_loss':  4566.4707, 'actor_loss':    -5.5940, 'eps_e':     0.1000})
Step:  708000, Reward:  2338.188 [ 448.665], Avg:   527.731 (0.100) <0-04:47:45> ({'r_t':  4509.3452, 'eps':     0.1000, 'critic_loss':  4448.8628, 'actor_loss':    -5.4073, 'eps_e':     0.1000})
Step:  709000, Reward:  2328.837 [ 460.903], Avg:   530.268 (0.100) <0-04:48:14> ({'r_t':  4608.6087, 'eps':     0.1000, 'critic_loss':  4318.7524, 'actor_loss':    -5.2537, 'eps_e':     0.1000})
Step:  710000, Reward:  2400.807 [ 304.437], Avg:   532.898 (0.100) <0-04:48:44> ({'r_t':  4548.7787, 'eps':     0.1000, 'critic_loss':  4772.0259, 'actor_loss':    -5.6952, 'eps_e':     0.1000})
Step:  711000, Reward:  2014.732 [ 683.069], Avg:   534.980 (0.100) <0-04:49:13> ({'r_t':  4654.3003, 'eps':     0.1000, 'critic_loss':  3820.9846, 'actor_loss':    -5.2151, 'eps_e':     0.1000})
Step:  712000, Reward:  1930.632 [ 785.658], Avg:   536.937 (0.100) <0-04:49:43> ({'r_t':  4604.6965, 'eps':     0.1000, 'critic_loss':  3708.1641, 'actor_loss':    -5.6796, 'eps_e':     0.1000})
Step:  713000, Reward:  1934.964 [ 781.366], Avg:   538.895 (0.100) <0-04:50:12> ({'r_t':  4589.6922, 'eps':     0.1000, 'critic_loss':  3666.0605, 'actor_loss':    -6.2111, 'eps_e':     0.1000})
Step:  714000, Reward:  2002.829 [ 702.349], Avg:   540.943 (0.100) <0-04:50:42> ({'r_t':  4265.0145, 'eps':     0.1000, 'critic_loss':  3194.1533, 'actor_loss':    -6.1218, 'eps_e':     0.1000})
Step:  715000, Reward:  1550.565 [ 697.013], Avg:   542.353 (0.100) <0-04:51:11> ({'r_t':  4125.9877, 'eps':     0.1000, 'critic_loss':  3776.5520, 'actor_loss':    -5.4665, 'eps_e':     0.1000})
Step:  716000, Reward:  1710.716 [ 930.076], Avg:   543.982 (0.100) <0-04:51:41> ({'r_t':  4434.6928, 'eps':     0.1000, 'critic_loss':  4447.5542, 'actor_loss':    -6.3734, 'eps_e':     0.1000})
Step:  717000, Reward:  2196.416 [ 586.408], Avg:   546.284 (0.100) <0-04:52:10> ({'r_t':  4325.6956, 'eps':     0.1000, 'critic_loss':  4678.0947, 'actor_loss':    -6.5564, 'eps_e':     0.1000})
Step:  718000, Reward:  1762.733 [ 987.466], Avg:   547.975 (0.100) <0-04:52:40> ({'r_t':  4741.9021, 'eps':     0.1000, 'critic_loss':  4471.6294, 'actor_loss':    -7.1532, 'eps_e':     0.1000})
Step:  719000, Reward:  2530.515 [  16.430], Avg:   550.729 (0.100) <0-04:53:09> ({'r_t':  4754.2411, 'eps':     0.1000, 'critic_loss':  4568.5239, 'actor_loss':    -7.1187, 'eps_e':     0.1000})
Step:  720000, Reward:  2263.503 [ 635.480], Avg:   553.105 (0.100) <0-04:53:39> ({'r_t':  4767.1255, 'eps':     0.1000, 'critic_loss':  4130.7798, 'actor_loss':    -7.4408, 'eps_e':     0.1000})
Step:  721000, Reward:  2401.100 [ 374.696], Avg:   555.664 (0.100) <0-04:54:08> ({'r_t':  4658.2481, 'eps':     0.1000, 'critic_loss':  3855.9617, 'actor_loss':    -6.9457, 'eps_e':     0.1000})
Step:  722000, Reward:  2481.054 [ 124.523], Avg:   558.327 (0.100) <0-04:54:38> ({'r_t':  4968.0474, 'eps':     0.1000, 'critic_loss':  2939.7014, 'actor_loss':    -5.9025, 'eps_e':     0.1000})
Step:  723000, Reward:  2207.103 [ 686.055], Avg:   560.604 (0.100) <0-04:55:07> ({'r_t':  5019.8548, 'eps':     0.1000, 'critic_loss':  2659.6116, 'actor_loss':    -5.4410, 'eps_e':     0.1000})
Step:  724000, Reward:  2213.105 [ 665.425], Avg:   562.884 (0.100) <0-04:55:36> ({'r_t':  4565.2636, 'eps':     0.1000, 'critic_loss':  2553.3193, 'actor_loss':    -4.9872, 'eps_e':     0.1000})
Step:  725000, Reward:  2459.128 [ 359.804], Avg:   565.496 (0.100) <0-04:56:06> ({'r_t':  4618.0924, 'eps':     0.1000, 'critic_loss':  3419.4397, 'actor_loss':    -4.8559, 'eps_e':     0.1000})
Step:  726000, Reward:  2016.025 [ 853.775], Avg:   567.491 (0.100) <0-04:56:35> ({'r_t':  4365.8142, 'eps':     0.1000, 'critic_loss':  3832.0337, 'actor_loss':    -4.5552, 'eps_e':     0.1000})
Step:  727000, Reward:  1506.120 [1023.716], Avg:   568.780 (0.100) <0-04:57:05> ({'r_t':  4326.2716, 'eps':     0.1000, 'critic_loss':  5212.0806, 'actor_loss':    -4.7895, 'eps_e':     0.1000})
Step:  728000, Reward:  2157.413 [ 622.092], Avg:   570.959 (0.100) <0-04:57:34> ({'r_t':  4842.2581, 'eps':     0.1000, 'critic_loss':  5605.8325, 'actor_loss':    -5.1074, 'eps_e':     0.1000})
Step:  729000, Reward:  2237.204 [ 511.560], Avg:   573.242 (0.100) <0-04:58:04> ({'r_t':  4474.0852, 'eps':     0.1000, 'critic_loss':  5808.2690, 'actor_loss':    -6.0724, 'eps_e':     0.1000})
Step:  730000, Reward:  2150.642 [ 654.362], Avg:   575.400 (0.100) <0-04:58:33> ({'r_t':  4182.2824, 'eps':     0.1000, 'critic_loss':  5573.2139, 'actor_loss':    -6.3276, 'eps_e':     0.1000})
Step:  731000, Reward:  2411.367 [ 423.159], Avg:   577.908 (0.100) <0-04:59:03> ({'r_t':  4797.2162, 'eps':     0.1000, 'critic_loss':  6540.0327, 'actor_loss':    -6.4532, 'eps_e':     0.1000})
Step:  732000, Reward:  2409.368 [ 480.460], Avg:   580.407 (0.100) <0-04:59:32> ({'r_t':  4992.3300, 'eps':     0.1000, 'critic_loss':  6357.7471, 'actor_loss':    -6.0768, 'eps_e':     0.1000})
Step:  733000, Reward:  2512.757 [ 238.885], Avg:   583.039 (0.100) <0-05:00:02> ({'r_t':  4799.0242, 'eps':     0.1000, 'critic_loss':  4801.4336, 'actor_loss':    -6.1392, 'eps_e':     0.1000})
Step:  734000, Reward:   954.394 [1175.534], Avg:   583.544 (0.100) <0-05:00:31> ({'r_t':  5065.9830, 'eps':     0.1000, 'critic_loss':  4500.7339, 'actor_loss':    -5.5437, 'eps_e':     0.1000})
Step:  735000, Reward:  2058.798 [ 779.963], Avg:   585.549 (0.100) <0-05:01:01> ({'r_t':  4740.2284, 'eps':     0.1000, 'critic_loss':  4773.5850, 'actor_loss':    -5.8468, 'eps_e':     0.1000})
Step:  736000, Reward:  2482.277 [ 298.719], Avg:   588.122 (0.100) <0-05:01:30> ({'r_t':  5049.6117, 'eps':     0.1000, 'critic_loss':  4091.4504, 'actor_loss':    -5.6491, 'eps_e':     0.1000})
Step:  737000, Reward:  2555.745 [ 152.272], Avg:   590.789 (0.100) <0-05:02:00> ({'r_t':  5176.0242, 'eps':     0.1000, 'critic_loss':  3212.0835, 'actor_loss':    -5.2841, 'eps_e':     0.1000})
Step:  738000, Reward:  2392.925 [ 427.814], Avg:   593.227 (0.100) <0-05:02:29> ({'r_t':  4952.9802, 'eps':     0.1000, 'critic_loss':  2882.0085, 'actor_loss':    -4.8479, 'eps_e':     0.1000})
Step:  739000, Reward:  1988.566 [ 796.688], Avg:   595.113 (0.100) <0-05:02:59> ({'r_t':  4869.4111, 'eps':     0.1000, 'critic_loss':  2856.5085, 'actor_loss':    -5.2251, 'eps_e':     0.1000})
Step:  740000, Reward:  2292.026 [ 699.895], Avg:   597.403 (0.100) <0-05:03:28> ({'r_t':  4683.7184, 'eps':     0.1000, 'critic_loss':  2978.1270, 'actor_loss':    -4.7092, 'eps_e':     0.1000})
Step:  741000, Reward:  2424.013 [ 549.000], Avg:   599.865 (0.100) <0-05:03:57> ({'r_t':  5081.1526, 'eps':     0.1000, 'critic_loss':  3120.7849, 'actor_loss':    -4.6259, 'eps_e':     0.1000})
Step:  742000, Reward:  2364.101 [ 445.658], Avg:   602.239 (0.100) <0-05:04:27> ({'r_t':  5060.4616, 'eps':     0.1000, 'critic_loss':  2865.9895, 'actor_loss':    -4.5747, 'eps_e':     0.1000})
Step:  743000, Reward:  2577.654 [  31.214], Avg:   604.894 (0.100) <0-05:04:56> ({'r_t':  4411.3471, 'eps':     0.1000, 'critic_loss':  2689.0464, 'actor_loss':    -4.9549, 'eps_e':     0.1000})
Step:  744000, Reward:  1994.264 [ 685.293], Avg:   606.759 (0.100) <0-05:05:26> ({'r_t':  4304.1527, 'eps':     0.1000, 'critic_loss':  3607.2681, 'actor_loss':    -4.5141, 'eps_e':     0.1000})
Step:  745000, Reward:  2382.940 [ 385.921], Avg:   609.140 (0.100) <0-05:05:55> ({'r_t':  4664.7560, 'eps':     0.1000, 'critic_loss':  4428.4639, 'actor_loss':    -5.2596, 'eps_e':     0.1000})
Step:  746000, Reward:  1365.494 [ 673.734], Avg:   610.153 (0.100) <0-05:06:25> ({'r_t':  4702.8385, 'eps':     0.1000, 'critic_loss':  4469.9990, 'actor_loss':    -5.3801, 'eps_e':     0.1000})
Step:  747000, Reward:  2106.391 [ 839.861], Avg:   612.153 (0.100) <0-05:06:54> ({'r_t':  4447.6797, 'eps':     0.1000, 'critic_loss':  5233.8418, 'actor_loss':    -5.4899, 'eps_e':     0.1000})
Step:  748000, Reward:  2539.922 [ 309.402], Avg:   614.727 (0.100) <0-05:07:24> ({'r_t':  4748.2347, 'eps':     0.1000, 'critic_loss':  5088.6074, 'actor_loss':    -5.7907, 'eps_e':     0.1000})
Step:  749000, Reward:  2147.898 [ 719.910], Avg:   616.771 (0.100) <0-05:07:53> ({'r_t':  5104.3942, 'eps':     0.1000, 'critic_loss':  5257.2744, 'actor_loss':    -5.9706, 'eps_e':     0.1000})
Step:  750000, Reward:  2176.639 [ 649.914], Avg:   618.848 (0.100) <0-05:08:23> ({'r_t':  4926.6003, 'eps':     0.1000, 'critic_loss':  4514.2231, 'actor_loss':    -6.0057, 'eps_e':     0.1000})
Step:  751000, Reward:  2323.932 [ 705.665], Avg:   621.115 (0.100) <0-05:08:52> ({'r_t':  4741.6654, 'eps':     0.1000, 'critic_loss':  4331.9902, 'actor_loss':    -5.5890, 'eps_e':     0.1000})
Step:  752000, Reward:  1318.858 [ 860.051], Avg:   622.042 (0.100) <0-05:09:22> ({'r_t':  4955.0517, 'eps':     0.1000, 'critic_loss':  4161.7925, 'actor_loss':    -5.3635, 'eps_e':     0.1000})
Step:  753000, Reward:  2628.965 [  23.647], Avg:   624.704 (0.100) <0-05:09:51> ({'r_t':  4713.9037, 'eps':     0.1000, 'critic_loss':  3816.5081, 'actor_loss':    -5.4356, 'eps_e':     0.1000})
Step:  754000, Reward:   680.587 [ 513.650], Avg:   624.778 (0.100) <0-05:10:20> ({'r_t':  4995.2002, 'eps':     0.1000, 'critic_loss':  3330.5725, 'actor_loss':    -5.2808, 'eps_e':     0.1000})
Step:  755000, Reward:  2242.816 [ 527.927], Avg:   626.918 (0.100) <0-05:10:50> ({'r_t':  5006.6567, 'eps':     0.1000, 'critic_loss':  3495.5215, 'actor_loss':    -5.3239, 'eps_e':     0.1000})
Step:  756000, Reward:  2301.542 [ 509.638], Avg:   629.130 (0.100) <0-05:11:19> ({'r_t':  4411.6147, 'eps':     0.1000, 'critic_loss':  3609.0000, 'actor_loss':    -6.0126, 'eps_e':     0.1000})
Step:  757000, Reward:  1981.024 [ 844.428], Avg:   630.914 (0.100) <0-05:11:49> ({'r_t':  3765.9969, 'eps':     0.1000, 'critic_loss':  5720.6763, 'actor_loss':    -7.1825, 'eps_e':     0.1000})
Step:  758000, Reward:  2343.457 [ 703.878], Avg:   633.170 (0.100) <0-05:12:18> ({'r_t':  4735.3731, 'eps':     0.1000, 'critic_loss':  6499.8740, 'actor_loss':    -7.0175, 'eps_e':     0.1000})
Step:  759000, Reward:  2532.681 [ 209.650], Avg:   635.669 (0.100) <0-05:12:48> ({'r_t':  4936.6869, 'eps':     0.1000, 'critic_loss':  5434.2222, 'actor_loss':    -6.5421, 'eps_e':     0.1000})
Step:  760000, Reward:  1186.488 [1093.019], Avg:   636.393 (0.100) <0-05:13:17> ({'r_t':  4994.2934, 'eps':     0.1000, 'critic_loss':  5758.4751, 'actor_loss':    -6.1631, 'eps_e':     0.1000})
Step:  761000, Reward:  2523.621 [  72.007], Avg:   638.870 (0.100) <0-05:13:47> ({'r_t':  4782.1923, 'eps':     0.1000, 'critic_loss':  5679.2124, 'actor_loss':    -7.1701, 'eps_e':     0.1000})
Step:  762000, Reward:  2557.688 [  29.812], Avg:   641.385 (0.100) <0-05:14:16> ({'r_t':  4752.2414, 'eps':     0.1000, 'critic_loss':  5022.9429, 'actor_loss':    -6.4712, 'eps_e':     0.1000})
Step:  763000, Reward:  2456.758 [ 296.770], Avg:   643.761 (0.100) <0-05:14:46> ({'r_t':  4747.6743, 'eps':     0.1000, 'critic_loss':  4376.0781, 'actor_loss':    -5.4045, 'eps_e':     0.1000})
Step:  764000, Reward:  2391.030 [ 520.644], Avg:   646.045 (0.100) <0-05:15:15> ({'r_t':  5138.5106, 'eps':     0.1000, 'critic_loss':  2824.9158, 'actor_loss':    -4.5584, 'eps_e':     0.1000})
Step:  765000, Reward:  2377.864 [ 518.026], Avg:   648.306 (0.100) <0-05:15:45> ({'r_t':  4872.3733, 'eps':     0.1000, 'critic_loss':  2741.4878, 'actor_loss':    -4.6960, 'eps_e':     0.1000})
Step:  766000, Reward:  2405.017 [ 328.860], Avg:   650.596 (0.100) <0-05:16:14> ({'r_t':  4529.5371, 'eps':     0.1000, 'critic_loss':  3069.9543, 'actor_loss':    -4.1759, 'eps_e':     0.1000})
Step:  767000, Reward:  1386.561 [1077.696], Avg:   651.554 (0.100) <0-05:16:43> ({'r_t':  4846.9218, 'eps':     0.1000, 'critic_loss':  2999.8875, 'actor_loss':    -4.2773, 'eps_e':     0.1000})
Step:  768000, Reward:  2505.656 [ 116.677], Avg:   653.965 (0.100) <0-05:17:13> ({'r_t':  4772.6939, 'eps':     0.1000, 'critic_loss':  3009.4041, 'actor_loss':    -4.4387, 'eps_e':     0.1000})
Step:  769000, Reward:  2547.652 [  31.758], Avg:   656.425 (0.100) <0-05:17:42> ({'r_t':  4979.1067, 'eps':     0.1000, 'critic_loss':  2581.7065, 'actor_loss':    -4.4499, 'eps_e':     0.1000})
Step:  770000, Reward:  2350.814 [ 404.082], Avg:   658.622 (0.100) <0-05:18:12> ({'r_t':  4872.0245, 'eps':     0.1000, 'critic_loss':  2651.0464, 'actor_loss':    -4.6478, 'eps_e':     0.1000})
Step:  771000, Reward:  2459.129 [ 294.418], Avg:   660.955 (0.100) <0-05:18:41> ({'r_t':  4734.5320, 'eps':     0.1000, 'critic_loss':  2883.9636, 'actor_loss':    -4.4705, 'eps_e':     0.1000})
Step:  772000, Reward:  2423.788 [ 298.302], Avg:   663.235 (0.100) <0-05:19:11> ({'r_t':  4747.7521, 'eps':     0.1000, 'critic_loss':  3004.5857, 'actor_loss':    -4.5577, 'eps_e':     0.1000})
Step:  773000, Reward:  2459.123 [ 298.181], Avg:   665.555 (0.100) <0-05:19:40> ({'r_t':  5019.3038, 'eps':     0.1000, 'critic_loss':  2838.3052, 'actor_loss':    -4.3804, 'eps_e':     0.1000})
Step:  774000, Reward:  2555.013 [  26.976], Avg:   667.993 (0.100) <0-05:20:10> ({'r_t':  4971.9606, 'eps':     0.1000, 'critic_loss':  3119.5122, 'actor_loss':    -4.1846, 'eps_e':     0.1000})
Step:  775000, Reward:  2532.680 [  31.561], Avg:   670.396 (0.100) <0-05:20:39> ({'r_t':  4844.4010, 'eps':     0.1000, 'critic_loss':  2640.5576, 'actor_loss':    -3.6876, 'eps_e':     0.1000})
Step:  776000, Reward:  2489.348 [ 135.774], Avg:   672.737 (0.100) <0-05:21:09> ({'r_t':  4633.9867, 'eps':     0.1000, 'critic_loss':  2931.4358, 'actor_loss':    -3.6871, 'eps_e':     0.1000})
Step:  777000, Reward:  2318.542 [ 675.823], Avg:   674.853 (0.100) <0-05:21:38> ({'r_t':  4676.2136, 'eps':     0.1000, 'critic_loss':  2979.0940, 'actor_loss':    -3.8997, 'eps_e':     0.1000})
Step:  778000, Reward:  2282.070 [ 650.419], Avg:   676.916 (0.100) <0-05:22:08> ({'r_t':  4947.7678, 'eps':     0.1000, 'critic_loss':  2956.5688, 'actor_loss':    -3.9139, 'eps_e':     0.1000})
Step:  779000, Reward:  2369.921 [ 422.979], Avg:   679.086 (0.100) <0-05:22:37> ({'r_t':  4778.5618, 'eps':     0.1000, 'critic_loss':  2851.2104, 'actor_loss':    -3.8996, 'eps_e':     0.1000})
Step:  780000, Reward:  2260.426 [ 718.336], Avg:   681.111 (0.100) <0-05:23:06> ({'r_t':  4941.0135, 'eps':     0.1000, 'critic_loss':  2795.0544, 'actor_loss':    -4.3698, 'eps_e':     0.1000})
Step:  781000, Reward:  2435.116 [ 288.091], Avg:   683.354 (0.100) <0-05:23:36> ({'r_t':  4857.5225, 'eps':     0.1000, 'critic_loss':  2948.6006, 'actor_loss':    -4.4348, 'eps_e':     0.1000})
Step:  782000, Reward:  1891.104 [ 643.579], Avg:   684.897 (0.100) <0-05:24:05> ({'r_t':  4695.3684, 'eps':     0.1000, 'critic_loss':  3095.8228, 'actor_loss':    -4.3057, 'eps_e':     0.1000})
Step:  783000, Reward:  2136.195 [ 800.077], Avg:   686.748 (0.100) <0-05:24:35> ({'r_t':  4513.4579, 'eps':     0.1000, 'critic_loss':  3111.8723, 'actor_loss':    -4.3567, 'eps_e':     0.1000})
Step:  784000, Reward:  2133.150 [ 644.386], Avg:   688.590 (0.100) <0-05:25:04> ({'r_t':  4066.0621, 'eps':     0.1000, 'critic_loss':  3606.0286, 'actor_loss':    -4.3047, 'eps_e':     0.1000})
Step:  785000, Reward:  1965.340 [ 692.821], Avg:   690.215 (0.100) <0-05:25:34> ({'r_t':  4949.3868, 'eps':     0.1000, 'critic_loss':  4226.0503, 'actor_loss':    -4.7557, 'eps_e':     0.1000})
Step:  786000, Reward:  1344.563 [ 933.663], Avg:   691.046 (0.100) <0-05:26:03> ({'r_t':  4488.1799, 'eps':     0.1000, 'critic_loss':  4835.7891, 'actor_loss':    -5.3193, 'eps_e':     0.1000})
Step:  787000, Reward:  2556.153 [  25.826], Avg:   693.413 (0.100) <0-05:26:33> ({'r_t':  4682.6585, 'eps':     0.1000, 'critic_loss':  6021.8848, 'actor_loss':    -5.8902, 'eps_e':     0.1000})
Step:  788000, Reward:   592.411 [  28.537], Avg:   693.285 (0.100) <0-05:26:58> ({'r_t':  4857.7280, 'eps':     0.1000, 'critic_loss':  5506.2900, 'actor_loss':    -6.3788, 'eps_e':     0.1000})
Step:  789000, Reward:  2562.114 [ 146.367], Avg:   695.651 (0.100) <0-05:27:27> ({'r_t':  3922.0641, 'eps':     0.1000, 'critic_loss':  5467.6812, 'actor_loss':    -6.9391, 'eps_e':     0.1000})
Step:  790000, Reward:  2318.954 [ 581.339], Avg:   697.703 (0.100) <0-05:27:57> ({'r_t':  4863.8421, 'eps':     0.1000, 'critic_loss':  5838.9160, 'actor_loss':    -8.4571, 'eps_e':     0.1000})
Step:  791000, Reward:  2477.723 [ 293.685], Avg:   699.950 (0.100) <0-05:28:26> ({'r_t':  4465.8310, 'eps':     0.1000, 'critic_loss':  4465.8008, 'actor_loss':    -7.2872, 'eps_e':     0.1000})
Step:  792000, Reward:  2148.307 [ 637.760], Avg:   701.777 (0.100) <0-05:28:55> ({'r_t':  4701.4983, 'eps':     0.1000, 'critic_loss':  4396.9785, 'actor_loss':    -7.9226, 'eps_e':     0.1000})
Step:  793000, Reward:  2448.362 [ 345.965], Avg:   703.976 (0.100) <0-05:29:25> ({'r_t':  5152.8668, 'eps':     0.1000, 'critic_loss':  3729.3406, 'actor_loss':    -7.0974, 'eps_e':     0.1000})
Step:  794000, Reward:  2624.852 [  18.775], Avg:   706.393 (0.100) <0-05:29:54> ({'r_t':  5053.8067, 'eps':     0.1000, 'critic_loss':  3391.0925, 'actor_loss':    -7.3641, 'eps_e':     0.1000})
Step:  795000, Reward:  2238.620 [ 728.711], Avg:   708.318 (0.100) <0-05:30:24> ({'r_t':  5069.6645, 'eps':     0.1000, 'critic_loss':  3273.8713, 'actor_loss':    -6.4495, 'eps_e':     0.1000})
Step:  796000, Reward:  2292.476 [ 379.587], Avg:   710.305 (0.100) <0-05:30:53> ({'r_t':  4873.4160, 'eps':     0.1000, 'critic_loss':  3248.6147, 'actor_loss':    -5.7218, 'eps_e':     0.1000})
Step:  797000, Reward:  2597.510 [  26.488], Avg:   712.670 (0.100) <0-05:31:23> ({'r_t':  4584.8189, 'eps':     0.1000, 'critic_loss':  3153.5002, 'actor_loss':    -5.5682, 'eps_e':     0.1000})
Step:  798000, Reward:  1960.328 [1041.583], Avg:   714.232 (0.100) <0-05:31:52> ({'r_t':  4964.4543, 'eps':     0.1000, 'critic_loss':  3087.7837, 'actor_loss':    -5.1722, 'eps_e':     0.1000})
Step:  799000, Reward:  2582.867 [  59.607], Avg:   716.567 (0.100) <0-05:32:22> ({'r_t':  5037.3966, 'eps':     0.1000, 'critic_loss':  2999.6145, 'actor_loss':    -4.6410, 'eps_e':     0.1000})
Step:  800000, Reward:  2531.846 [ 367.357], Avg:   718.834 (0.100) <0-05:32:51> ({'r_t':  4818.3642, 'eps':     0.1000, 'critic_loss':  2906.7991, 'actor_loss':    -4.8220, 'eps_e':     0.1000})
Step:  801000, Reward:  2241.859 [ 653.667], Avg:   720.733 (0.100) <0-05:33:21> ({'r_t':  4200.4081, 'eps':     0.1000, 'critic_loss':  3993.7385, 'actor_loss':    -5.8668, 'eps_e':     0.1000})
Step:  802000, Reward:  1909.745 [ 824.763], Avg:   722.214 (0.100) <0-05:33:50> ({'r_t':  4990.8952, 'eps':     0.1000, 'critic_loss':  4245.9424, 'actor_loss':    -6.2684, 'eps_e':     0.1000})
Step:  803000, Reward:  2657.306 [  22.173], Avg:   724.620 (0.100) <0-05:34:20> ({'r_t':  5084.1103, 'eps':     0.1000, 'critic_loss':  4272.2017, 'actor_loss':    -6.4080, 'eps_e':     0.1000})
Step:  804000, Reward:  2563.424 [  54.190], Avg:   726.905 (0.100) <0-05:34:49> ({'r_t':  5087.0374, 'eps':     0.1000, 'critic_loss':  4868.8521, 'actor_loss':    -7.0724, 'eps_e':     0.1000})
Step:  805000, Reward:  2259.247 [ 558.831], Avg:   728.806 (0.100) <0-05:35:18> ({'r_t':  4919.5472, 'eps':     0.1000, 'critic_loss':  4817.2710, 'actor_loss':    -6.3838, 'eps_e':     0.1000})
Step:  806000, Reward:   828.637 [ 682.553], Avg:   728.929 (0.100) <0-05:35:48> ({'r_t':  5131.5719, 'eps':     0.1000, 'critic_loss':  5009.5698, 'actor_loss':    -6.4562, 'eps_e':     0.1000})
Step:  807000, Reward:  2164.295 [ 828.826], Avg:   730.706 (0.100) <0-05:36:17> ({'r_t':  5096.6331, 'eps':     0.1000, 'critic_loss':  4826.1118, 'actor_loss':    -5.9330, 'eps_e':     0.1000})
Step:  808000, Reward:  2370.060 [ 561.686], Avg:   732.732 (0.100) <0-05:36:47> ({'r_t':  4956.2040, 'eps':     0.1000, 'critic_loss':  3540.0288, 'actor_loss':    -4.6130, 'eps_e':     0.1000})
Step:  809000, Reward:  2573.733 [  37.989], Avg:   735.005 (0.100) <0-05:37:16> ({'r_t':  4882.1895, 'eps':     0.1000, 'critic_loss':  3650.8411, 'actor_loss':    -4.2245, 'eps_e':     0.1000})
Step:  810000, Reward:  2641.458 [  19.088], Avg:   737.356 (0.100) <0-05:37:46> ({'r_t':  4788.2944, 'eps':     0.1000, 'critic_loss':  4076.4221, 'actor_loss':    -4.3388, 'eps_e':     0.1000})
Step:  811000, Reward:  2521.470 [ 326.916], Avg:   739.553 (0.100) <0-05:38:15> ({'r_t':  4817.8969, 'eps':     0.1000, 'critic_loss':  3766.9807, 'actor_loss':    -4.1079, 'eps_e':     0.1000})
Step:  812000, Reward:  2153.332 [ 709.359], Avg:   741.292 (0.100) <0-05:38:45> ({'r_t':  4134.4937, 'eps':     0.1000, 'critic_loss':  4390.8003, 'actor_loss':    -3.9772, 'eps_e':     0.1000})
Step:  813000, Reward:  2081.837 [ 774.981], Avg:   742.939 (0.100) <0-05:39:14> ({'r_t':  3865.9245, 'eps':     0.1000, 'critic_loss':  5910.7319, 'actor_loss':    -4.2801, 'eps_e':     0.1000})
Step:  814000, Reward:  2280.886 [ 729.676], Avg:   744.826 (0.100) <0-05:39:44> ({'r_t':  4714.1771, 'eps':     0.1000, 'critic_loss':  8231.2734, 'actor_loss':    -5.7245, 'eps_e':     0.1000})
Step:  815000, Reward:  2605.442 [  29.370], Avg:   747.106 (0.100) <0-05:40:13> ({'r_t':  4982.6192, 'eps':     0.1000, 'critic_loss':  7924.5391, 'actor_loss':    -6.6970, 'eps_e':     0.1000})
Step:  816000, Reward:  2410.068 [ 402.457], Avg:   749.142 (0.100) <0-05:40:43> ({'r_t':  4807.6083, 'eps':     0.1000, 'critic_loss':  7493.0830, 'actor_loss':    -7.4974, 'eps_e':     0.1000})
Step:  817000, Reward:  2456.850 [ 557.196], Avg:   751.229 (0.100) <0-05:41:12> ({'r_t':  4663.0762, 'eps':     0.1000, 'critic_loss':  7614.7368, 'actor_loss':    -6.9059, 'eps_e':     0.1000})
Step:  818000, Reward:  2548.378 [ 119.198], Avg:   753.424 (0.100) <0-05:41:42> ({'r_t':  4864.5953, 'eps':     0.1000, 'critic_loss':  6776.2217, 'actor_loss':    -6.9436, 'eps_e':     0.1000})
Step:  819000, Reward:  2435.594 [ 236.356], Avg:   755.475 (0.100) <0-05:42:11> ({'r_t':  4800.9373, 'eps':     0.1000, 'critic_loss':  5349.4512, 'actor_loss':    -6.8477, 'eps_e':     0.1000})
Step:  820000, Reward:  2220.193 [ 596.765], Avg:   757.259 (0.100) <0-05:42:41> ({'r_t':  4716.9492, 'eps':     0.1000, 'critic_loss':  3996.3235, 'actor_loss':    -5.7108, 'eps_e':     0.1000})
Step:  821000, Reward:  2115.436 [ 623.829], Avg:   758.911 (0.100) <0-05:43:10> ({'r_t':  4949.6452, 'eps':     0.1000, 'critic_loss':  4276.1816, 'actor_loss':    -5.3534, 'eps_e':     0.1000})
Step:  822000, Reward:  1920.297 [ 647.279], Avg:   760.322 (0.100) <0-05:43:40> ({'r_t':  4824.1021, 'eps':     0.1000, 'critic_loss':  4117.7495, 'actor_loss':    -6.1218, 'eps_e':     0.1000})
Step:  823000, Reward:  1979.656 [ 682.441], Avg:   761.802 (0.100) <0-05:44:09> ({'r_t':  4669.8115, 'eps':     0.1000, 'critic_loss':  3765.3279, 'actor_loss':    -5.7288, 'eps_e':     0.1000})
Step:  824000, Reward:  2039.818 [ 748.930], Avg:   763.351 (0.100) <0-05:44:39> ({'r_t':  4649.1586, 'eps':     0.1000, 'critic_loss':  3198.1479, 'actor_loss':    -5.0803, 'eps_e':     0.1000})
Step:  825000, Reward:  1073.041 [ 926.659], Avg:   763.726 (0.100) <0-05:45:08> ({'r_t':  4792.8059, 'eps':     0.1000, 'critic_loss':  3352.3396, 'actor_loss':    -4.9912, 'eps_e':     0.1000})
Step:  826000, Reward:  2156.013 [ 607.561], Avg:   765.410 (0.100) <0-05:45:38> ({'r_t':  4690.5354, 'eps':     0.1000, 'critic_loss':  3212.5635, 'actor_loss':    -4.5674, 'eps_e':     0.1000})
Step:  827000, Reward:  2385.184 [ 585.083], Avg:   767.366 (0.100) <0-05:46:07> ({'r_t':  4757.2761, 'eps':     0.1000, 'critic_loss':  3627.9475, 'actor_loss':    -4.7346, 'eps_e':     0.1000})
Step:  828000, Reward:  1618.381 [ 916.590], Avg:   768.393 (0.100) <0-05:46:37> ({'r_t':  4841.7493, 'eps':     0.1000, 'critic_loss':  3593.3567, 'actor_loss':    -5.1310, 'eps_e':     0.1000})
Step:  829000, Reward:  2171.980 [ 618.904], Avg:   770.084 (0.100) <0-05:47:06> ({'r_t':  4807.8487, 'eps':     0.1000, 'critic_loss':  3073.6880, 'actor_loss':    -4.8302, 'eps_e':     0.1000})
Step:  830000, Reward:  2349.554 [ 655.110], Avg:   771.984 (0.100) <0-05:47:36> ({'r_t':  4913.1564, 'eps':     0.1000, 'critic_loss':  3301.5281, 'actor_loss':    -4.9722, 'eps_e':     0.1000})
Step:  831000, Reward:   982.094 [ 794.083], Avg:   772.237 (0.100) <0-05:48:05> ({'r_t':  4712.2659, 'eps':     0.1000, 'critic_loss':  3170.0166, 'actor_loss':    -5.1410, 'eps_e':     0.1000})
Step:  832000, Reward:  1768.476 [ 833.059], Avg:   773.433 (0.100) <0-05:48:34> ({'r_t':  4972.4675, 'eps':     0.1000, 'critic_loss':  2837.4370, 'actor_loss':    -4.3846, 'eps_e':     0.1000})
Step:  833000, Reward:  2515.956 [  61.799], Avg:   775.522 (0.100) <0-05:49:04> ({'r_t':  4903.2794, 'eps':     0.1000, 'critic_loss':  3477.2839, 'actor_loss':    -4.5377, 'eps_e':     0.1000})
Step:  834000, Reward:  1714.106 [ 658.940], Avg:   776.646 (0.100) <0-05:49:33> ({'r_t':  4418.5581, 'eps':     0.1000, 'critic_loss':  3099.3271, 'actor_loss':    -4.3488, 'eps_e':     0.1000})
Step:  835000, Reward:  2099.865 [ 738.856], Avg:   778.229 (0.100) <0-05:50:03> ({'r_t':  3990.2286, 'eps':     0.1000, 'critic_loss':  4024.1724, 'actor_loss':    -4.3273, 'eps_e':     0.1000})
Step:  836000, Reward:  2140.737 [ 832.059], Avg:   779.857 (0.100) <0-05:50:32> ({'r_t':  4609.9604, 'eps':     0.1000, 'critic_loss':  5208.6631, 'actor_loss':    -4.8147, 'eps_e':     0.1000})
Step:  837000, Reward:  2457.114 [ 348.106], Avg:   781.858 (0.100) <0-05:51:02> ({'r_t':  4515.8350, 'eps':     0.1000, 'critic_loss':  5567.7075, 'actor_loss':    -5.8839, 'eps_e':     0.1000})
Step:  838000, Reward:  2370.488 [ 375.336], Avg:   783.752 (0.100) <0-05:51:31> ({'r_t':  5009.5087, 'eps':     0.1000, 'critic_loss':  5066.2827, 'actor_loss':    -6.3060, 'eps_e':     0.1000})
Step:  839000, Reward:  2552.109 [  28.255], Avg:   785.857 (0.100) <0-05:52:01> ({'r_t':  3966.5527, 'eps':     0.1000, 'critic_loss':  5580.9541, 'actor_loss':    -6.7733, 'eps_e':     0.1000})
Step:  840000, Reward:  2230.510 [ 559.890], Avg:   787.575 (0.100) <0-05:52:30> ({'r_t':  4910.7253, 'eps':     0.1000, 'critic_loss':  6220.9043, 'actor_loss':    -7.4027, 'eps_e':     0.1000})
Step:  841000, Reward:  2067.674 [ 769.478], Avg:   789.095 (0.100) <0-05:53:00> ({'r_t':  4701.5710, 'eps':     0.1000, 'critic_loss':  6095.0317, 'actor_loss':    -8.0934, 'eps_e':     0.1000})
Step:  842000, Reward:  2366.677 [ 483.320], Avg:   790.967 (0.100) <0-05:53:29> ({'r_t':  4724.1404, 'eps':     0.1000, 'critic_loss':  5497.1367, 'actor_loss':    -7.2032, 'eps_e':     0.1000})
Step:  843000, Reward:  2405.339 [ 590.709], Avg:   792.879 (0.100) <0-05:53:59> ({'r_t':  4615.6384, 'eps':     0.1000, 'critic_loss':  5144.0117, 'actor_loss':    -6.1451, 'eps_e':     0.1000})
Step:  844000, Reward:  2272.500 [ 675.481], Avg:   794.630 (0.100) <0-05:54:28> ({'r_t':  4886.5255, 'eps':     0.1000, 'critic_loss':  5445.3711, 'actor_loss':    -5.2945, 'eps_e':     0.1000})
Step:  845000, Reward:  1875.140 [ 873.216], Avg:   795.908 (0.100) <0-05:54:58> ({'r_t':  4842.3576, 'eps':     0.1000, 'critic_loss':  5177.6475, 'actor_loss':    -5.9593, 'eps_e':     0.1000})
Step:  846000, Reward:  2533.856 [ 247.907], Avg:   797.959 (0.100) <0-05:55:27> ({'r_t':  4673.8670, 'eps':     0.1000, 'critic_loss':  4364.2432, 'actor_loss':    -5.4401, 'eps_e':     0.1000})
Step:  847000, Reward:  1900.326 [ 817.578], Avg:   799.259 (0.100) <0-05:55:57> ({'r_t':  4924.5628, 'eps':     0.1000, 'critic_loss':  4502.7612, 'actor_loss':    -5.0850, 'eps_e':     0.1000})
Step:  848000, Reward:  2359.404 [ 499.450], Avg:   801.097 (0.100) <0-05:56:26> ({'r_t':  4725.1666, 'eps':     0.1000, 'critic_loss':  3951.4617, 'actor_loss':    -5.2230, 'eps_e':     0.1000})
Step:  849000, Reward:  1567.609 [ 921.555], Avg:   801.999 (0.100) <0-05:56:55> ({'r_t':  4798.8044, 'eps':     0.1000, 'critic_loss':  4005.2595, 'actor_loss':    -5.0657, 'eps_e':     0.1000})
Step:  850000, Reward:  2218.013 [ 341.571], Avg:   803.663 (0.100) <0-05:57:25> ({'r_t':  4142.8270, 'eps':     0.1000, 'critic_loss':  4016.9265, 'actor_loss':    -5.1169, 'eps_e':     0.1000})
Step:  851000, Reward:  2600.464 [  21.148], Avg:   805.772 (0.100) <0-05:57:54> ({'r_t':  4742.3984, 'eps':     0.1000, 'critic_loss':  5197.6113, 'actor_loss':    -5.5883, 'eps_e':     0.1000})
Step:  852000, Reward:  2166.389 [ 863.282], Avg:   807.367 (0.100) <0-05:58:24> ({'r_t':  4444.9668, 'eps':     0.1000, 'critic_loss':  5243.6592, 'actor_loss':    -6.0115, 'eps_e':     0.1000})
Step:  853000, Reward:  2320.947 [ 744.566], Avg:   809.139 (0.100) <0-05:58:53> ({'r_t':  4768.4128, 'eps':     0.1000, 'critic_loss':  5515.5278, 'actor_loss':    -6.5963, 'eps_e':     0.1000})
Step:  854000, Reward:  2573.973 [  66.615], Avg:   811.203 (0.100) <0-05:59:23> ({'r_t':  4796.6024, 'eps':     0.1000, 'critic_loss':  5547.7158, 'actor_loss':    -6.8486, 'eps_e':     0.1000})
Step:  855000, Reward:  2496.364 [ 362.853], Avg:   813.172 (0.100) <0-05:59:52> ({'r_t':  4878.7299, 'eps':     0.1000, 'critic_loss':  5744.3701, 'actor_loss':    -7.1755, 'eps_e':     0.1000})
Step:  856000, Reward:  2595.168 [  50.334], Avg:   815.251 (0.100) <0-06:00:22> ({'r_t':  4681.9791, 'eps':     0.1000, 'critic_loss':  6540.7031, 'actor_loss':    -7.9259, 'eps_e':     0.1000})
Step:  857000, Reward:  1903.377 [1045.849], Avg:   816.519 (0.100) <0-06:00:51> ({'r_t':  4604.3331, 'eps':     0.1000, 'critic_loss':  5982.0747, 'actor_loss':    -7.5607, 'eps_e':     0.1000})
Step:  858000, Reward:  2393.650 [ 551.027], Avg:   818.355 (0.100) <0-06:01:21> ({'r_t':  4804.4594, 'eps':     0.1000, 'critic_loss':  6257.0366, 'actor_loss':    -7.3533, 'eps_e':     0.1000})
Step:  859000, Reward:  2376.171 [ 684.475], Avg:   820.167 (0.100) <0-06:01:50> ({'r_t':  4999.9057, 'eps':     0.1000, 'critic_loss':  5266.0122, 'actor_loss':    -7.3793, 'eps_e':     0.1000})
Step:  860000, Reward:  2561.783 [  36.386], Avg:   822.190 (0.100) <0-06:02:20> ({'r_t':  4859.6356, 'eps':     0.1000, 'critic_loss':  4923.6060, 'actor_loss':    -6.6556, 'eps_e':     0.1000})
Step:  861000, Reward:  2273.917 [ 549.007], Avg:   823.874 (0.100) <0-06:02:49> ({'r_t':  4892.8341, 'eps':     0.1000, 'critic_loss':  4526.2739, 'actor_loss':    -6.9751, 'eps_e':     0.1000})
Step:  862000, Reward:  1729.941 [ 948.271], Avg:   824.924 (0.100) <0-06:03:19> ({'r_t':  4979.4838, 'eps':     0.1000, 'critic_loss':  4575.1201, 'actor_loss':    -6.7577, 'eps_e':     0.1000})
Step:  863000, Reward:  2608.341 [  19.729], Avg:   826.988 (0.100) <0-06:03:48> ({'r_t':  5014.8210, 'eps':     0.1000, 'critic_loss':  4121.4561, 'actor_loss':    -6.1165, 'eps_e':     0.1000})
Step:  864000, Reward:  2556.510 [ 273.434], Avg:   828.987 (0.100) <0-06:04:17> ({'r_t':  5208.3471, 'eps':     0.1000, 'critic_loss':  3405.7727, 'actor_loss':    -5.3413, 'eps_e':     0.1000})
Step:  865000, Reward:  2624.420 [  18.815], Avg:   831.061 (0.100) <0-06:04:47> ({'r_t':  5104.6745, 'eps':     0.1000, 'critic_loss':  3314.9575, 'actor_loss':    -4.5250, 'eps_e':     0.1000})
Step:  866000, Reward:   805.723 [ 803.862], Avg:   831.031 (0.100) <0-06:05:16> ({'r_t':  5176.2511, 'eps':     0.1000, 'critic_loss':  3051.7178, 'actor_loss':    -4.4674, 'eps_e':     0.1000})
Step:  867000, Reward:  1591.367 [1064.544], Avg:   831.907 (0.100) <0-06:05:46> ({'r_t':  5051.8531, 'eps':     0.1000, 'critic_loss':  3186.3091, 'actor_loss':    -4.1897, 'eps_e':     0.1000})
Step:  868000, Reward:  2637.412 [  21.764], Avg:   833.985 (0.100) <0-06:06:15> ({'r_t':  4791.4364, 'eps':     0.1000, 'critic_loss':  2970.1462, 'actor_loss':    -4.0620, 'eps_e':     0.1000})
Step:  869000, Reward:  2322.876 [ 673.474], Avg:   835.696 (0.100) <0-06:06:45> ({'r_t':  4650.5738, 'eps':     0.1000, 'critic_loss':  4054.0913, 'actor_loss':    -3.9086, 'eps_e':     0.1000})
Step:  870000, Reward:  2516.827 [ 362.470], Avg:   837.626 (0.100) <0-06:07:14> ({'r_t':  4678.0133, 'eps':     0.1000, 'critic_loss':  4803.8921, 'actor_loss':    -4.6012, 'eps_e':     0.1000})
Step:  871000, Reward:  2531.727 [ 383.276], Avg:   839.569 (0.100) <0-06:07:44> ({'r_t':  4617.1934, 'eps':     0.1000, 'critic_loss':  4433.6455, 'actor_loss':    -4.7558, 'eps_e':     0.1000})
Step:  872000, Reward:  2608.611 [  20.442], Avg:   841.596 (0.100) <0-06:08:13> ({'r_t':  5134.6800, 'eps':     0.1000, 'critic_loss':  4232.5806, 'actor_loss':    -4.5348, 'eps_e':     0.1000})
Step:  873000, Reward:  2587.492 [  25.823], Avg:   843.593 (0.100) <0-06:08:43> ({'r_t':  5092.9076, 'eps':     0.1000, 'critic_loss':  4237.5576, 'actor_loss':    -4.5803, 'eps_e':     0.1000})
Step:  874000, Reward:  2623.116 [  34.028], Avg:   845.627 (0.100) <0-06:09:12> ({'r_t':  4887.4081, 'eps':     0.1000, 'critic_loss':  4404.1011, 'actor_loss':    -4.5281, 'eps_e':     0.1000})
Step:  875000, Reward:  2499.755 [ 515.006], Avg:   847.515 (0.100) <0-06:09:42> ({'r_t':  5002.9092, 'eps':     0.1000, 'critic_loss':  3875.8086, 'actor_loss':    -4.6733, 'eps_e':     0.1000})
Step:  876000, Reward:  2605.320 [ 118.540], Avg:   849.520 (0.100) <0-06:10:11> ({'r_t':  5011.0798, 'eps':     0.1000, 'critic_loss':  3952.7532, 'actor_loss':    -4.4729, 'eps_e':     0.1000})
Step:  877000, Reward:   410.913 [ 522.140], Avg:   849.020 (0.100) <0-06:10:41> ({'r_t':  4259.2489, 'eps':     0.1000, 'critic_loss':  4312.8481, 'actor_loss':    -4.6197, 'eps_e':     0.1000})
Step:  878000, Reward:  2448.319 [ 444.875], Avg:   850.839 (0.100) <0-06:11:10> ({'r_t':  4985.3370, 'eps':     0.1000, 'critic_loss':  4517.1060, 'actor_loss':    -4.4192, 'eps_e':     0.1000})
Step:  879000, Reward:  2489.581 [ 359.931], Avg:   852.702 (0.100) <0-06:11:39> ({'r_t':  5170.0968, 'eps':     0.1000, 'critic_loss':  4377.3286, 'actor_loss':    -4.6498, 'eps_e':     0.1000})
Step:  880000, Reward:  2386.398 [ 441.224], Avg:   854.443 (0.100) <0-06:12:09> ({'r_t':  4629.4714, 'eps':     0.1000, 'critic_loss':  4465.3750, 'actor_loss':    -4.4642, 'eps_e':     0.1000})
Step:  881000, Reward:  2614.398 [  17.847], Avg:   856.438 (0.100) <0-06:12:38> ({'r_t':  4890.8373, 'eps':     0.1000, 'critic_loss':  5313.3545, 'actor_loss':    -4.7079, 'eps_e':     0.1000})
Step:  882000, Reward:  2593.944 [  24.544], Avg:   858.406 (0.100) <0-06:13:08> ({'r_t':  5130.8274, 'eps':     0.1000, 'critic_loss':  4812.4795, 'actor_loss':    -4.5751, 'eps_e':     0.1000})
Step:  883000, Reward:  2518.817 [ 297.574], Avg:   860.284 (0.100) <0-06:13:37> ({'r_t':  4967.2275, 'eps':     0.1000, 'critic_loss':  4632.5513, 'actor_loss':    -4.6580, 'eps_e':     0.1000})
Step:  884000, Reward:  1717.041 [1015.641], Avg:   861.252 (0.100) <0-06:14:07> ({'r_t':  4559.9160, 'eps':     0.1000, 'critic_loss':  4263.3242, 'actor_loss':    -4.8117, 'eps_e':     0.1000})
Step:  885000, Reward:  2488.913 [ 488.928], Avg:   863.089 (0.100) <0-06:14:36> ({'r_t':  4970.6526, 'eps':     0.1000, 'critic_loss':  4035.3484, 'actor_loss':    -4.9486, 'eps_e':     0.1000})
Step:  886000, Reward:   140.829 [ 458.367], Avg:   862.275 (0.100) <0-06:15:06> ({'r_t':  4823.9179, 'eps':     0.1000, 'critic_loss':  3985.1353, 'actor_loss':    -5.3218, 'eps_e':     0.1000})
Step:  887000, Reward:  1355.679 [1094.594], Avg:   862.831 (0.100) <0-06:15:35> ({'r_t':  4109.8683, 'eps':     0.1000, 'critic_loss':  4173.8184, 'actor_loss':    -5.8436, 'eps_e':     0.1000})
Step:  888000, Reward:  2076.240 [ 869.540], Avg:   864.195 (0.100) <0-06:16:05> ({'r_t':  4692.0072, 'eps':     0.1000, 'critic_loss':  4990.4458, 'actor_loss':    -6.1875, 'eps_e':     0.1000})
Step:  889000, Reward:  2384.436 [ 551.441], Avg:   865.904 (0.100) <0-06:16:34> ({'r_t':  4653.7138, 'eps':     0.1000, 'critic_loss':  5661.6958, 'actor_loss':    -6.7034, 'eps_e':     0.1000})
Step:  890000, Reward:  2574.589 [ 120.465], Avg:   867.821 (0.100) <0-06:17:04> ({'r_t':  5053.3425, 'eps':     0.1000, 'critic_loss':  5473.2104, 'actor_loss':    -6.4778, 'eps_e':     0.1000})
Step:  891000, Reward:  1888.150 [ 112.588], Avg:   868.965 (0.100) <0-06:17:33> ({'r_t':  4146.9196, 'eps':     0.1000, 'critic_loss':  5051.3359, 'actor_loss':    -6.4190, 'eps_e':     0.1000})
Step:  892000, Reward:  2319.826 [ 514.793], Avg:   870.590 (0.100) <0-06:18:02> ({'r_t':  4758.1007, 'eps':     0.1000, 'critic_loss':  5531.1475, 'actor_loss':    -8.4343, 'eps_e':     0.1000})
Step:  893000, Reward:  2221.058 [ 722.447], Avg:   872.100 (0.100) <0-06:18:32> ({'r_t':  5001.2204, 'eps':     0.1000, 'critic_loss':  4863.0396, 'actor_loss':    -8.1332, 'eps_e':     0.1000})
Step:  894000, Reward:  2300.075 [ 798.884], Avg:   873.696 (0.100) <0-06:19:01> ({'r_t':  4823.7127, 'eps':     0.1000, 'critic_loss':  4359.9600, 'actor_loss':    -7.3295, 'eps_e':     0.1000})
Step:  895000, Reward:  2616.944 [  29.561], Avg:   875.642 (0.100) <0-06:19:31> ({'r_t':  5047.9553, 'eps':     0.1000, 'critic_loss':  3867.7454, 'actor_loss':    -7.4073, 'eps_e':     0.1000})
Step:  896000, Reward:  2431.693 [ 492.169], Avg:   877.376 (0.100) <0-06:20:00> ({'r_t':  4880.6063, 'eps':     0.1000, 'critic_loss':  3825.8816, 'actor_loss':    -6.9702, 'eps_e':     0.1000})
Step:  897000, Reward:  2211.224 [ 829.966], Avg:   878.862 (0.100) <0-06:20:30> ({'r_t':  4623.0837, 'eps':     0.1000, 'critic_loss':  3966.0979, 'actor_loss':    -7.2159, 'eps_e':     0.1000})
Step:  898000, Reward:  1866.902 [ 800.286], Avg:   879.961 (0.100) <0-06:20:59> ({'r_t':  4809.7473, 'eps':     0.1000, 'critic_loss':  3720.6958, 'actor_loss':    -5.0303, 'eps_e':     0.1000})
Step:  899000, Reward:  1169.963 [ 901.191], Avg:   880.283 (0.100) <0-06:21:29> ({'r_t':  4660.6773, 'eps':     0.1000, 'critic_loss':  4168.5488, 'actor_loss':    -4.7207, 'eps_e':     0.1000})
Step:  900000, Reward:  2038.036 [1206.302], Avg:   881.568 (0.100) <0-06:21:58> ({'r_t':  3976.1500, 'eps':     0.1000, 'critic_loss':  4870.8262, 'actor_loss':    -5.0344, 'eps_e':     0.1000})
Step:  901000, Reward:  2537.635 [ 308.307], Avg:   883.404 (0.100) <0-06:22:28> ({'r_t':  4955.0360, 'eps':     0.1000, 'critic_loss':  5716.7144, 'actor_loss':    -5.8226, 'eps_e':     0.1000})
Step:  902000, Reward:  2510.534 [ 361.710], Avg:   885.206 (0.100) <0-06:22:58> ({'r_t':  4556.7629, 'eps':     0.1000, 'critic_loss':  5178.3711, 'actor_loss':    -6.0796, 'eps_e':     0.1000})
Step:  903000, Reward:  1761.781 [ 930.497], Avg:   886.175 (0.100) <0-06:23:27> ({'r_t':  4127.6875, 'eps':     0.1000, 'critic_loss':  6556.5679, 'actor_loss':    -5.9630, 'eps_e':     0.1000})
Step:  904000, Reward:  2192.957 [ 630.783], Avg:   887.619 (0.100) <0-06:23:57> ({'r_t':  4643.3113, 'eps':     0.1000, 'critic_loss':  7227.1226, 'actor_loss':    -6.7910, 'eps_e':     0.1000})
Step:  905000, Reward:  2345.190 [ 705.480], Avg:   889.228 (0.100) <0-06:24:26> ({'r_t':  4886.9978, 'eps':     0.1000, 'critic_loss':  6336.7583, 'actor_loss':    -7.0397, 'eps_e':     0.1000})
Step:  906000, Reward:  2501.678 [ 364.544], Avg:   891.006 (0.100) <0-06:24:56> ({'r_t':  4586.9651, 'eps':     0.1000, 'critic_loss':  7448.6914, 'actor_loss':    -7.0500, 'eps_e':     0.1000})
Step:  907000, Reward:  2452.754 [ 357.598], Avg:   892.726 (0.100) <0-06:25:25> ({'r_t':  5106.5142, 'eps':     0.1000, 'critic_loss':  6755.9912, 'actor_loss':    -7.1677, 'eps_e':     0.1000})
Step:  908000, Reward:  1415.604 [ 768.246], Avg:   893.301 (0.100) <0-06:25:55> ({'r_t':  5123.0684, 'eps':     0.1000, 'critic_loss':  6899.8594, 'actor_loss':    -6.9266, 'eps_e':     0.1000})
Step:  909000, Reward:  2517.657 [ 379.485], Avg:   895.086 (0.100) <0-06:26:24> ({'r_t':  4994.9326, 'eps':     0.1000, 'critic_loss':  5421.2300, 'actor_loss':    -7.1087, 'eps_e':     0.1000})
Step:  910000, Reward:  2194.049 [ 765.550], Avg:   896.512 (0.100) <0-06:26:54> ({'r_t':  4736.5099, 'eps':     0.1000, 'critic_loss':  5306.0879, 'actor_loss':    -6.5215, 'eps_e':     0.1000})
Step:  911000, Reward:  2195.684 [ 704.864], Avg:   897.937 (0.100) <0-06:27:23> ({'r_t':  4916.2742, 'eps':     0.1000, 'critic_loss':  5051.0576, 'actor_loss':    -6.1930, 'eps_e':     0.1000})
Step:  912000, Reward:  2593.858 [ 226.420], Avg:   899.794 (0.100) <0-06:27:53> ({'r_t':  4810.5230, 'eps':     0.1000, 'critic_loss':  4030.4617, 'actor_loss':    -5.7238, 'eps_e':     0.1000})
Step:  913000, Reward:  2274.251 [ 709.493], Avg:   901.298 (0.100) <0-06:28:22> ({'r_t':  4812.6604, 'eps':     0.1000, 'critic_loss':  4092.3071, 'actor_loss':    -5.1219, 'eps_e':     0.1000})
Step:  914000, Reward:  2205.003 [ 727.202], Avg:   902.723 (0.100) <0-06:28:52> ({'r_t':  4752.0768, 'eps':     0.1000, 'critic_loss':  4470.6509, 'actor_loss':    -5.1046, 'eps_e':     0.1000})
Step:  915000, Reward:  2255.417 [ 595.156], Avg:   904.199 (0.100) <0-06:29:21> ({'r_t':  5005.4027, 'eps':     0.1000, 'critic_loss':  4368.4858, 'actor_loss':    -4.8680, 'eps_e':     0.1000})
Step:  916000, Reward:  2399.948 [ 452.730], Avg:   905.831 (0.100) <0-06:29:51> ({'r_t':  4611.0797, 'eps':     0.1000, 'critic_loss':  4859.9731, 'actor_loss':    -5.0546, 'eps_e':     0.1000})
Step:  917000, Reward:  2428.176 [ 419.516], Avg:   907.489 (0.100) <0-06:30:20> ({'r_t':  4852.0347, 'eps':     0.1000, 'critic_loss':  4858.0288, 'actor_loss':    -5.0600, 'eps_e':     0.1000})
Step:  918000, Reward:  1879.677 [ 834.777], Avg:   908.547 (0.100) <0-06:30:50> ({'r_t':  4390.3036, 'eps':     0.1000, 'critic_loss':  5588.3550, 'actor_loss':    -4.9779, 'eps_e':     0.1000})
Step:  919000, Reward:  2414.806 [ 484.182], Avg:   910.184 (0.100) <0-06:31:19> ({'r_t':  3977.2375, 'eps':     0.1000, 'critic_loss':  5907.6650, 'actor_loss':    -5.3996, 'eps_e':     0.1000})
Step:  920000, Reward:  2568.874 [ 367.075], Avg:   911.985 (0.100) <0-06:31:48> ({'r_t':  4700.9557, 'eps':     0.1000, 'critic_loss':  6214.8159, 'actor_loss':    -6.5928, 'eps_e':     0.1000})
Step:  921000, Reward:  2166.150 [ 666.645], Avg:   913.345 (0.100) <0-06:32:18> ({'r_t':  4590.6112, 'eps':     0.1000, 'critic_loss':  6038.2412, 'actor_loss':    -7.1442, 'eps_e':     0.1000})
Step:  922000, Reward:  2108.990 [ 746.917], Avg:   914.641 (0.100) <0-06:32:47> ({'r_t':  4886.8676, 'eps':     0.1000, 'critic_loss':  6350.2920, 'actor_loss':    -7.6982, 'eps_e':     0.1000})
Step:  923000, Reward:  2229.140 [ 749.974], Avg:   916.063 (0.100) <0-06:33:17> ({'r_t':  4659.1346, 'eps':     0.1000, 'critic_loss':  5727.7822, 'actor_loss':    -7.0963, 'eps_e':     0.1000})
Step:  924000, Reward:  2305.552 [ 747.866], Avg:   917.565 (0.100) <0-06:33:46> ({'r_t':  4652.3611, 'eps':     0.1000, 'critic_loss':  5640.8369, 'actor_loss':    -7.2269, 'eps_e':     0.1000})
Step:  925000, Reward:  2526.961 [ 368.870], Avg:   919.303 (0.100) <0-06:34:16> ({'r_t':  4910.3256, 'eps':     0.1000, 'critic_loss':  5656.7451, 'actor_loss':    -7.3807, 'eps_e':     0.1000})
Step:  926000, Reward:  2487.992 [ 512.467], Avg:   920.996 (0.100) <0-06:34:45> ({'r_t':  4972.5613, 'eps':     0.1000, 'critic_loss':  5178.0830, 'actor_loss':    -6.5215, 'eps_e':     0.1000})
Step:  927000, Reward:  2398.326 [ 549.628], Avg:   922.588 (0.100) <0-06:35:15> ({'r_t':  5206.3673, 'eps':     0.1000, 'critic_loss':  5168.6440, 'actor_loss':    -6.8555, 'eps_e':     0.1000})
Step:  928000, Reward:  2371.808 [ 496.919], Avg:   924.148 (0.100) <0-06:35:44> ({'r_t':  4673.9245, 'eps':     0.1000, 'critic_loss':  4704.1362, 'actor_loss':    -6.6344, 'eps_e':     0.1000})
Step:  929000, Reward:  2622.982 [   9.527], Avg:   925.974 (0.100) <0-06:36:14> ({'r_t':  5030.9330, 'eps':     0.1000, 'critic_loss':  4678.4380, 'actor_loss':    -6.0620, 'eps_e':     0.1000})
Step:  930000, Reward:  2607.069 [  19.121], Avg:   927.780 (0.100) <0-06:36:43> ({'r_t':  4463.7279, 'eps':     0.1000, 'critic_loss':  4510.1079, 'actor_loss':    -5.3140, 'eps_e':     0.1000})
Step:  931000, Reward:  2524.183 [ 241.639], Avg:   929.493 (0.100) <0-06:37:13> ({'r_t':  5024.4985, 'eps':     0.1000, 'critic_loss':  4426.9780, 'actor_loss':    -5.3625, 'eps_e':     0.1000})
Step:  932000, Reward:  2588.016 [  37.580], Avg:   931.270 (0.100) <0-06:37:42> ({'r_t':  4971.9951, 'eps':     0.1000, 'critic_loss':  4390.2383, 'actor_loss':    -5.4531, 'eps_e':     0.1000})
Step:  933000, Reward:  2223.584 [ 606.586], Avg:   932.654 (0.100) <0-06:38:12> ({'r_t':  5032.2734, 'eps':     0.1000, 'critic_loss':  4169.0200, 'actor_loss':    -5.1810, 'eps_e':     0.1000})
Step:  934000, Reward:  2500.011 [ 384.363], Avg:   934.330 (0.100) <0-06:38:41> ({'r_t':  4832.3540, 'eps':     0.1000, 'critic_loss':  3887.4431, 'actor_loss':    -5.1279, 'eps_e':     0.1000})
Step:  935000, Reward:  2592.916 [  33.326], Avg:   936.102 (0.100) <0-06:39:11> ({'r_t':  4946.7415, 'eps':     0.1000, 'critic_loss':  3834.4741, 'actor_loss':    -4.8734, 'eps_e':     0.1000})
Step:  936000, Reward:  2270.222 [ 773.983], Avg:   937.526 (0.100) <0-06:39:40> ({'r_t':  4581.5290, 'eps':     0.1000, 'critic_loss':  3924.7549, 'actor_loss':    -5.4569, 'eps_e':     0.1000})
Step:  937000, Reward:  2305.815 [ 543.891], Avg:   938.985 (0.100) <0-06:40:10> ({'r_t':  4805.8584, 'eps':     0.1000, 'critic_loss':  3611.4275, 'actor_loss':    -4.9155, 'eps_e':     0.1000})
Step:  938000, Reward:  2223.267 [ 602.287], Avg:   940.353 (0.100) <0-06:40:39> ({'r_t':  4944.4229, 'eps':     0.1000, 'critic_loss':  3889.5444, 'actor_loss':    -4.6452, 'eps_e':     0.1000})
Step:  939000, Reward:  2222.618 [ 833.872], Avg:   941.717 (0.100) <0-06:41:09> ({'r_t':  4342.4100, 'eps':     0.1000, 'critic_loss':  3713.3953, 'actor_loss':    -4.5720, 'eps_e':     0.1000})
Step:  940000, Reward:  2461.725 [ 365.550], Avg:   943.332 (0.100) <0-06:41:38> ({'r_t':  4765.1145, 'eps':     0.1000, 'critic_loss':  3919.0029, 'actor_loss':    -4.4918, 'eps_e':     0.1000})
Step:  941000, Reward:  2133.465 [ 656.027], Avg:   944.595 (0.100) <0-06:42:08> ({'r_t':  5119.2834, 'eps':     0.1000, 'critic_loss':  4146.0474, 'actor_loss':    -4.5695, 'eps_e':     0.1000})
Step:  942000, Reward:  2328.414 [ 529.298], Avg:   946.063 (0.100) <0-06:42:37> ({'r_t':  4980.3687, 'eps':     0.1000, 'critic_loss':  3593.2810, 'actor_loss':    -4.8490, 'eps_e':     0.1000})
Step:  943000, Reward:  1865.937 [ 951.653], Avg:   947.037 (0.100) <0-06:43:07> ({'r_t':  3962.1883, 'eps':     0.1000, 'critic_loss':  4442.5630, 'actor_loss':    -4.6885, 'eps_e':     0.1000})
Step:  944000, Reward:  2029.754 [ 765.372], Avg:   948.183 (0.100) <0-06:43:36> ({'r_t':  4875.5275, 'eps':     0.1000, 'critic_loss':  4954.4917, 'actor_loss':    -5.0127, 'eps_e':     0.1000})
Step:  945000, Reward:  2207.406 [ 613.605], Avg:   949.514 (0.100) <0-06:44:06> ({'r_t':  4671.2352, 'eps':     0.1000, 'critic_loss':  5369.7959, 'actor_loss':    -6.4557, 'eps_e':     0.1000})
Step:  946000, Reward:  2203.704 [ 820.476], Avg:   950.839 (0.100) <0-06:44:35> ({'r_t':  5116.0912, 'eps':     0.1000, 'critic_loss':  5203.0298, 'actor_loss':    -5.6815, 'eps_e':     0.1000})
Step:  947000, Reward:  1581.684 [ 845.141], Avg:   951.504 (0.100) <0-06:45:05> ({'r_t':  4798.7961, 'eps':     0.1000, 'critic_loss':  4918.6543, 'actor_loss':    -5.6484, 'eps_e':     0.1000})
Step:  948000, Reward:  2596.335 [  36.270], Avg:   953.237 (0.100) <0-06:45:34> ({'r_t':  4630.6410, 'eps':     0.1000, 'critic_loss':  4717.0068, 'actor_loss':    -5.6341, 'eps_e':     0.1000})
Step:  949000, Reward:  2512.523 [ 300.803], Avg:   954.879 (0.100) <0-06:46:03> ({'r_t':  4841.4980, 'eps':     0.1000, 'critic_loss':  4767.5332, 'actor_loss':    -5.6628, 'eps_e':     0.1000})
Step:  950000, Reward:  2171.446 [ 648.972], Avg:   956.158 (0.100) <0-06:46:33> ({'r_t':  5199.0000, 'eps':     0.1000, 'critic_loss':  3820.2524, 'actor_loss':    -5.0198, 'eps_e':     0.1000})
Step:  951000, Reward:  2646.732 [  25.276], Avg:   957.934 (0.100) <0-06:47:02> ({'r_t':  5009.3072, 'eps':     0.1000, 'critic_loss':  3457.1345, 'actor_loss':    -4.5461, 'eps_e':     0.1000})
Step:  952000, Reward:  2609.817 [  74.108], Avg:   959.667 (0.100) <0-06:47:32> ({'r_t':  5086.0403, 'eps':     0.1000, 'critic_loss':  3404.9324, 'actor_loss':    -4.2927, 'eps_e':     0.1000})
Step:  953000, Reward:  1782.279 [ 854.830], Avg:   960.529 (0.100) <0-06:48:01> ({'r_t':  4835.9600, 'eps':     0.1000, 'critic_loss':  3511.7820, 'actor_loss':    -4.4494, 'eps_e':     0.1000})
Step:  954000, Reward:  2357.270 [ 594.396], Avg:   961.992 (0.100) <0-06:48:31> ({'r_t':  4938.0021, 'eps':     0.1000, 'critic_loss':  3580.0996, 'actor_loss':    -4.3259, 'eps_e':     0.1000})
Step:  955000, Reward:  2408.389 [ 439.865], Avg:   963.505 (0.100) <0-06:49:00> ({'r_t':  4922.6033, 'eps':     0.1000, 'critic_loss':  3875.7556, 'actor_loss':    -4.6277, 'eps_e':     0.1000})
Step:  956000, Reward:  2447.089 [ 422.631], Avg:   965.055 (0.100) <0-06:49:30> ({'r_t':  4781.7556, 'eps':     0.1000, 'critic_loss':  3633.8796, 'actor_loss':    -4.5864, 'eps_e':     0.1000})
Step:  957000, Reward:  2478.588 [ 481.853], Avg:   966.635 (0.100) <0-06:49:59> ({'r_t':  4893.0526, 'eps':     0.1000, 'critic_loss':  3863.7573, 'actor_loss':    -4.9772, 'eps_e':     0.1000})
Step:  958000, Reward:  2183.307 [ 689.925], Avg:   967.904 (0.100) <0-06:50:29> ({'r_t':  4858.9953, 'eps':     0.1000, 'critic_loss':  3831.2454, 'actor_loss':    -4.7104, 'eps_e':     0.1000})
Step:  959000, Reward:  2303.277 [ 670.850], Avg:   969.295 (0.100) <0-06:50:58> ({'r_t':  4370.7002, 'eps':     0.1000, 'critic_loss':  4797.7759, 'actor_loss':    -4.4717, 'eps_e':     0.1000})
Step:  960000, Reward:  1373.080 [ 930.672], Avg:   969.715 (0.100) <0-06:51:28> ({'r_t':  4767.6080, 'eps':     0.1000, 'critic_loss':  4859.7324, 'actor_loss':    -5.0649, 'eps_e':     0.1000})
Step:  961000, Reward:  2293.003 [ 695.483], Avg:   971.090 (0.100) <0-06:51:57> ({'r_t':  5080.7337, 'eps':     0.1000, 'critic_loss':  5040.1060, 'actor_loss':    -6.0953, 'eps_e':     0.1000})
Step:  962000, Reward:  2540.501 [ 361.828], Avg:   972.720 (0.100) <0-06:52:26> ({'r_t':  4835.3161, 'eps':     0.1000, 'critic_loss':  4410.7251, 'actor_loss':    -5.7055, 'eps_e':     0.1000})
Step:  963000, Reward:  2603.902 [  21.329], Avg:   974.412 (0.100) <0-06:52:58> ({'r_t':  4656.7273, 'eps':     0.1000, 'critic_loss':  4983.1104, 'actor_loss':    -5.9039, 'eps_e':     0.1000})
Step:  964000, Reward:  2587.521 [  62.441], Avg:   976.084 (0.100) <0-06:53:27> ({'r_t':  5016.9422, 'eps':     0.1000, 'critic_loss':  4992.2441, 'actor_loss':    -6.5087, 'eps_e':     0.1000})
Step:  965000, Reward:  2552.436 [  61.125], Avg:   977.716 (0.100) <0-06:53:57> ({'r_t':  4968.1716, 'eps':     0.1000, 'critic_loss':  5115.4556, 'actor_loss':    -5.9513, 'eps_e':     0.1000})
Step:  966000, Reward:  2528.223 [ 363.770], Avg:   979.319 (0.100) <0-06:54:26> ({'r_t':  5072.6615, 'eps':     0.1000, 'critic_loss':  4268.2466, 'actor_loss':    -5.4030, 'eps_e':     0.1000})
Step:  967000, Reward:  2155.406 [ 834.591], Avg:   980.534 (0.100) <0-06:54:56> ({'r_t':  4840.6569, 'eps':     0.1000, 'critic_loss':  4202.5518, 'actor_loss':    -4.2876, 'eps_e':     0.1000})
Step:  968000, Reward:  2622.568 [  16.401], Avg:   982.229 (0.100) <0-06:55:25> ({'r_t':  4850.8679, 'eps':     0.1000, 'critic_loss':  4074.8301, 'actor_loss':    -4.5226, 'eps_e':     0.1000})
Step:  969000, Reward:  2154.412 [ 793.000], Avg:   983.437 (0.100) <0-06:55:55> ({'r_t':  4893.5124, 'eps':     0.1000, 'critic_loss':  4104.6484, 'actor_loss':    -4.3860, 'eps_e':     0.1000})
Step:  970000, Reward:  2238.867 [ 616.422], Avg:   984.730 (0.100) <0-06:56:24> ({'r_t':  4962.9589, 'eps':     0.1000, 'critic_loss':  3659.9788, 'actor_loss':    -4.3147, 'eps_e':     0.1000})
Step:  971000, Reward:  1045.734 [ 876.731], Avg:   984.793 (0.100) <0-06:56:54> ({'r_t':  4698.1984, 'eps':     0.1000, 'critic_loss':  3539.3132, 'actor_loss':    -4.1948, 'eps_e':     0.1000})
Step:  972000, Reward:  2559.303 [ 166.990], Avg:   986.411 (0.100) <0-06:57:23> ({'r_t':  5011.5697, 'eps':     0.1000, 'critic_loss':  3556.2439, 'actor_loss':    -4.0381, 'eps_e':     0.1000})
Step:  973000, Reward:  2586.781 [ 122.233], Avg:   988.054 (0.100) <0-06:57:53> ({'r_t':  5161.4555, 'eps':     0.1000, 'critic_loss':  3905.3323, 'actor_loss':    -4.3347, 'eps_e':     0.1000})
Step:  974000, Reward:  2303.504 [ 791.153], Avg:   989.403 (0.100) <0-06:58:22> ({'r_t':  4773.7237, 'eps':     0.1000, 'critic_loss':  3890.9846, 'actor_loss':    -4.3461, 'eps_e':     0.1000})
Step:  975000, Reward:  2460.247 [ 477.273], Avg:   990.910 (0.100) <0-06:58:52> ({'r_t':  4746.8930, 'eps':     0.1000, 'critic_loss':  4246.7739, 'actor_loss':    -4.2239, 'eps_e':     0.1000})
Step:  976000, Reward:   638.092 [ 768.595], Avg:   990.549 (0.100) <0-06:59:21> ({'r_t':  4627.5763, 'eps':     0.1000, 'critic_loss':  4485.4810, 'actor_loss':    -5.2764, 'eps_e':     0.1000})
Step:  977000, Reward:  1342.189 [ 979.497], Avg:   990.909 (0.100) <0-06:59:51> ({'r_t':  4237.4830, 'eps':     0.1000, 'critic_loss':  4926.7842, 'actor_loss':    -5.2990, 'eps_e':     0.1000})
Step:  978000, Reward:  1893.455 [ 793.904], Avg:   991.831 (0.100) <0-07:00:20> ({'r_t':  4877.6016, 'eps':     0.1000, 'critic_loss':  4628.2891, 'actor_loss':    -4.8694, 'eps_e':     0.1000})
Step:  979000, Reward:  1816.776 [ 862.705], Avg:   992.672 (0.100) <0-07:00:49> ({'r_t':  4482.9026, 'eps':     0.1000, 'critic_loss':  4497.4810, 'actor_loss':    -5.4618, 'eps_e':     0.1000})
Step:  980000, Reward:  1749.952 [ 956.613], Avg:   993.444 (0.100) <0-07:01:19> ({'r_t':  4584.2293, 'eps':     0.1000, 'critic_loss':  4368.3740, 'actor_loss':    -5.7059, 'eps_e':     0.1000})
Step:  981000, Reward:  2399.857 [ 616.751], Avg:   994.876 (0.100) <0-07:01:48> ({'r_t':  4360.8375, 'eps':     0.1000, 'critic_loss':  4712.7700, 'actor_loss':    -5.3639, 'eps_e':     0.1000})
Step:  982000, Reward:  1794.726 [1036.485], Avg:   995.690 (0.100) <0-07:02:18> ({'r_t':  4834.6367, 'eps':     0.1000, 'critic_loss':  4345.7349, 'actor_loss':    -5.2592, 'eps_e':     0.1000})
Step:  983000, Reward:  2462.471 [ 626.558], Avg:   997.181 (0.100) <0-07:02:47> ({'r_t':  4989.0245, 'eps':     0.1000, 'critic_loss':  3937.9663, 'actor_loss':    -5.1577, 'eps_e':     0.1000})
Step:  984000, Reward:  2406.183 [ 567.186], Avg:   998.611 (0.100) <0-07:03:17> ({'r_t':  4725.0789, 'eps':     0.1000, 'critic_loss':  3962.1997, 'actor_loss':    -4.6836, 'eps_e':     0.1000})
Step:  985000, Reward:  1872.615 [ 977.584], Avg:   999.498 (0.100) <0-07:03:46> ({'r_t':  4814.3939, 'eps':     0.1000, 'critic_loss':  4060.9688, 'actor_loss':    -5.0609, 'eps_e':     0.1000})
Step:  986000, Reward:  1674.160 [ 755.079], Avg:  1000.181 (0.100) <0-07:04:16> ({'r_t':  4721.5812, 'eps':     0.1000, 'critic_loss':  5361.0083, 'actor_loss':    -5.9898, 'eps_e':     0.1000})
Step:  987000, Reward:  2628.488 [  40.152], Avg:  1001.829 (0.100) <0-07:04:45> ({'r_t':  4902.5480, 'eps':     0.1000, 'critic_loss':  4814.2241, 'actor_loss':    -5.3294, 'eps_e':     0.1000})
Step:  988000, Reward:  2576.495 [ 277.408], Avg:  1003.421 (0.100) <0-07:05:15> ({'r_t':  4759.5224, 'eps':     0.1000, 'critic_loss':  4494.2183, 'actor_loss':    -5.4506, 'eps_e':     0.1000})
Step:  989000, Reward:  2552.200 [ 364.386], Avg:  1004.986 (0.100) <0-07:05:44> ({'r_t':  5216.0794, 'eps':     0.1000, 'critic_loss':  4023.3162, 'actor_loss':    -5.0312, 'eps_e':     0.1000})
Step:  990000, Reward:  2629.739 [  14.320], Avg:  1006.625 (0.100) <0-07:06:14> ({'r_t':  4988.0447, 'eps':     0.1000, 'critic_loss':  3856.3184, 'actor_loss':    -5.4958, 'eps_e':     0.1000})
Step:  991000, Reward:  2495.846 [ 509.779], Avg:  1008.127 (0.100) <0-07:06:43> ({'r_t':  4850.9035, 'eps':     0.1000, 'critic_loss':  3816.9468, 'actor_loss':    -4.8532, 'eps_e':     0.1000})
Step:  992000, Reward:  2231.965 [ 762.941], Avg:  1009.359 (0.100) <0-07:07:13> ({'r_t':  4868.4182, 'eps':     0.1000, 'critic_loss':  4049.5034, 'actor_loss':    -5.3092, 'eps_e':     0.1000})
Step:  993000, Reward:  1742.176 [ 918.296], Avg:  1010.096 (0.100) <0-07:07:42> ({'r_t':  4179.7920, 'eps':     0.1000, 'critic_loss':  4412.8101, 'actor_loss':    -5.3284, 'eps_e':     0.1000})
Step:  994000, Reward:  2424.267 [ 619.216], Avg:  1011.518 (0.100) <0-07:08:11> ({'r_t':  4150.5334, 'eps':     0.1000, 'critic_loss':  6032.2690, 'actor_loss':    -6.0268, 'eps_e':     0.1000})
Step:  995000, Reward:  2613.026 [  45.623], Avg:  1013.126 (0.100) <0-07:08:41> ({'r_t':  4870.1292, 'eps':     0.1000, 'critic_loss':  5940.7930, 'actor_loss':    -6.6609, 'eps_e':     0.1000})
Step:  996000, Reward:  2553.281 [ 314.469], Avg:  1014.670 (0.100) <0-07:09:11> ({'r_t':  5100.6843, 'eps':     0.1000, 'critic_loss':  6257.0737, 'actor_loss':    -6.9984, 'eps_e':     0.1000})
Step:  997000, Reward:  2323.376 [ 595.177], Avg:  1015.982 (0.100) <0-07:09:40> ({'r_t':  5230.0700, 'eps':     0.1000, 'critic_loss':  6300.8115, 'actor_loss':    -7.9905, 'eps_e':     0.1000})
Step:  998000, Reward:  2594.924 [  16.089], Avg:  1017.562 (0.100) <0-07:10:10> ({'r_t':  5036.0586, 'eps':     0.1000, 'critic_loss':  5738.5396, 'actor_loss':    -7.9722, 'eps_e':     0.1000})
Step:  999000, Reward:  2586.938 [  59.309], Avg:  1019.132 (0.100) <0-07:10:39> ({'r_t':  5175.0879, 'eps':     0.1000, 'critic_loss':  4931.4902, 'actor_loss':    -7.4866, 'eps_e':     0.1000})
Step: 1000000, Reward:  2630.555 [  25.511], Avg:  1020.741 (0.100) <0-07:11:08> ({'r_t':  4809.3268, 'eps':     0.1000, 'critic_loss':  3966.5630, 'actor_loss':    -5.6625, 'eps_e':     0.1000})
